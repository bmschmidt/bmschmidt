<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-c09d08cd.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-c09d08cd.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/generating-an-ordered-data-set-from-an-OCR-text-file"),
					params: {lang:"en",lessons:"lessons",slug:"generating-an-ordered-data-set-from-an-OCR-text-file"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Generating an Ordered Data Set from an OCR Text File</h1>

<!-- HTML_TAG_START --><h1 id="generating-an-ordered-data-set-from-a-text-file">Generating an Ordered Data Set from a Text File</h1>
<h2 id="lesson-goals">Lesson goals</h2>
<p>This tutorial illustrates strategies for taking raw OCR output from a scanned text, parsing it to isolate and correct essential elements of metadata, and generating an ordered data set (a python dictionary) from it. These illustrations are specific to a particular text, but the overall strategy, and some of the individual procedures, can be adapted to organize any scanned text, even if it doesn&#39;t look like this one.</p>
<p>{% include toc.html %}</p>
<h2 id="introduction">Introduction</h2>
<p>It is often the case that historians involved in digital projects wish to work with digitized texts, so they think &quot;OK, I&#39;ll just scan this fabulously rich and useful collection of original source material and do wonderful things with the digital text that results&quot;. (Those of us who have done this, now smile ruefully). Such historians quickly discover that even the best OCR results in unacceptably high error rates. So the historian now thinks &quot;OK I&#39;ll get some grant money, and I&#39;ll enlist the help of an army of RAs/Grad students/Undergrads/Barely literate street urchins, to correct errors in my OCR output. (We smile again, even more sadly now).</p>
<ol>
<li><p>There is little funding for this kind of thing. Increasingly, projects in the humanities have focused upon NLP/Data Mining/Machine Learning/Graph Analysis, and the like, frequently overlooking the fundamental problem of generating useable digital texts. The presumption has often been, well, Google scanned all that stuff didn&#39;t they? What&#39;s the matter with their scans?</p>
</li>
<li><p>Even if you had such an army of helpers, proof-reading the OCR output of, say, a collection of twelfth century Italian charters transcribed and published in 1935, will quickly drive them all mad, make their eyes bleed, and the result will still be a great wad of text containing a great many errors, and you will <strong>still</strong> have to do <strong>something</strong> to it before it becomes useful in any context.</p>
</li>
</ol>
<p>Going through a text file line by line and correcting OCR errors one at a time is hugely error-prone, as any proof reader will tell you. There are ways to automate some of this tedious work. A scripting language like Perl or Python can allow you to search your OCR output text for common errors and correct them using &quot;Regular Expressions&quot;, a language for describing patterns in text. (So called because they express a <a href="http://en.wikipedia.org/wiki/Regular_language">&quot;regular language&quot;</a>. See L.T. O&#39;Hara&#39;s <a href="/lessons/cleaning-ocrd-text-with-regular-expressions.html">tutorial on Regular Expressions</a> here at the PM.) Regular Expressions, however, are only useful if the expressions you are searching for are ... well ... regular. Unfortunately, much of what you have in OCR output is highly <em>irregular</em>. If you could impose some order on it: create an ordered data set out of it, your Regular Expression tools would become much more powerful.</p>
<p>Consider, for example, what happens if your OCR interpreted a lot of strings like this &quot;21 July, 1921&quot; as &quot;2l July, 192l&quot;, turning the integer &#39;1&#39; into an &#39;l&#39;. You would love to be able to write a search and replace script that would turn all instances of 2l into 21, but then what would happen if you had lots of occurrences of strings like this in your text: &quot;2lb. hammer&quot;. You&#39;d get a bunch of 21b. hammers; not what you want. If only you could tell your script: only change 2l into 21 in sections where there are dates, not weights. If you had an ordered data set, you could do things like that.</p>
<p>Very often the texts that historians wish to digitize are, in fact, ordered data sets: ordered collections of primary source documents, or a legal code say, or a cartulary. But the editorial structure imposed upon such resources is usually designed for a particular kind of data retrieval technology i.e., a codex, a book. For a digitized text you need a different kind of structure. If you can get rid of the book related infrastructure and reorganize the text according to the sections and divisions that you&#39;re interested in, you will wind up with data that is much easier to do search and replace operations on, and as a bonus, your text will become immediately useful in a variety of other contexts as well.</p>
<p>This is where a scripting language like Python comes very much in handy. For our project we wanted to prepare some of the documents from a 12th century collection of <em>imbreviatura</em> from the Italian scribe known as <a href="http://www.worldcat.org/oclc/17591390">Giovanni Scriba</a> so that they could be marked up by historians for subsequent NLP analysis or potentially for other purposes as well. The pages of the 1935 published edition look like this.</p>
<p>{% include figure.html filename=&quot;gs_pg110.png&quot; caption=&quot;GS page 110&quot; %}</p>
<p>The OCR output from such scans look like this even after some substantial clean-up (I&#39;ve wrapped the longest lines so that they fit here):</p>
<pre><code>110    MARIO CHIAUDANO MATTIA MORESCO
    professi sunt Alvernacium habere de i;psa societate lb. .c., in reditu
    tracto predicto capitali .ccc. lb. proficuum. debent dividere per medium. Ultra
    vero .cc. lb. capitalis Ingo de Volta lb. .xiv. habet quas cum ipso capitali de
    scicietate extrahere debet. Dedit preterea prefatus Ingo de Volta licenciam (1)
    ipsi Ingoni Nocentio portandi lb. .xxxvII. 2 Oberti Spinule et Ib. .xxvII.
    Wuilielmi Aradelli. Actum ante domum W. Buronis .MCLVII., .iiii. kalendas
    iulias, indicione quarta (2).
L f o. 26 v.] .    CCVIII.
Ingone Della Volta si obbliga verso Ingone Nocenzio di indennizzarlo di ogni
danno che gli fosse derivato dalle societa che egli aveva con i suoi figli (28
giugno 1157).
Testes Ingonis Nocentii] .
    Die loco (3) ,predicto et testibus Wuilielmo Burone, Bono Iohanne
    Malfiiastro, Anselmo de Cafara, W. de Racedo, Wuilielmo Callige Pallii. Ego Ingo
    de Volta promitto tibi Ingoni Nocentio quod si aliquod dampnum acciderit tibi
    pro societate vel societatibus quam olim habueris cum filiis meis ego illud
    totum tibi restaurato et hoc tibi promitto sub pena dupli de quanto inde dampno
    habueris. Do tibi preterea licentiam accipiendi bisancios quos ultra mare
    acciipere debeo et inde facias tona fide quicquid tibi videbitur et inde ab omni
    danpno te absolvo quicquid inde contingerit.
CCIX.
    Guglielmo di Razedo dichiara d&#39;aver ricevuto in societatem da Guglielmo
Barone una somma di denaro che portera laboratum ultramare (28 giugno 1157).
Wuilielmi Buronis] .
        Testes Anselmus de Cafara, Albertus de Volta, W. Capdorgol, Corsus
Serre, Angelotus, Ingo Noncencius. Ego W. de Raeedo profiteor me accepisse a te
Wuilielmo Burone lb. duocentum sexaginta tre et s. .XIII. 1/2 in societatem ad
quartam proficui, eas debeo portare laboratum ultra mare et inde quo voluero, in
reditu,
(11 Licentiam in sopralinea in potestatem cancellato.
(2) A margine le postille: Pro Ingone Nocentio scripta e due pro Alvernacio.
(3) Cancellato: et testibus supradictis.
</code></pre>
<p>In the scan of the original, the reader&#39;s eye readily parses the page: the layout has meaning. But as you can see, reduced to plain text like this, none of the metadata implied by the page layout and typography can be differentiated by automated processes.</p>
<p>You can see from the scan that each charter has the following metadata associated with it.</p>
<ul>
<li>Charter number</li>
<li>Page number</li>
<li>Folio number</li>
<li>An Italian summary, ending in a date of some kind</li>
<li>A line, usually ending with a &#39;]&#39; that marks a marginal notation in the original</li>
<li>Frequently a collection of in-text numbered footnote markers, whose text appears at the bottom of each page, sequentially numbered, and restarting from 1 on each new page.</li>
<li>The Latin text of the charter itself</li>
</ul>
<p>This is typical of such resources, though editorial conventions will vary widely. The point is: this is an <strong>ordered</strong> data set, not just a great big string of characters. With some fairly straightforward Python scripts, we can turn our OCR output into an ordered data set, in this case, a <a href="https://docs.python.org/3.7/tutorial/datastructures.html#dictionaries">python dictionary</a>, <strong>before</strong> we start trying to proofread the Latin charter texts. With such an ordered data set in hand, we can do proofreading, and potentially many other kinds of tasks, much more effectively.</p>
<p>So, the aim of this tutorial is to take a plain text file, like the OCR output above and turn it into a python dictionary with fields for the Latin text of the charter and for each of the metadata elements mentioned above:</p>
<pre><code class="language-python">{
.
.
.
 52: {&#39;chid&#39;: &#39;GScriba_LII&#39;,
      &#39;chno&#39;: 52,
      &#39;date&#39;: datetime.date(1156, 3, 27),
      &#39;folio&#39;: &#39;[fo. 6 r.]&#39;,
      &#39;footnotes&#39;: [(1, &#39;Cancellato: m.&#39;)],
      &#39;marginal&#39;: &#39;no marginal]&#39;,
      &#39;pgno&#39;: 29,
      &#39;summary&#39;: &#39;I consoli di Genova riconoscono con sentenza il diritto di Romano di Casella di pagarsi sui beni di Gerardo Confector per un credito che aveva verso il medesimo (27 marzo 1156).&#39;,
      &#39;text&#39;: [&#39;    In pontili capituli consules E. Aurie, W. Buronus, Ogerius Ventus laudaverunt quod Romanus de Casella haberet in bonis Gerardi Confectoris s. .xxvi. denariorum et possit eos accipere sine contradicione eius et omnium pro eo. Hoc ideo quia, cum; Romanus ante ipsos inde conquereretur, ipso Gerardo debitum non negante, sed quod de usura esset obiiciendo, iuravit nominatus Romanus quod capitalis erat (1) et non de usura, unde ut supra laudaverunt , .MCLVI., sexto kalendas aprilis, indicione tercia.\n&#39;]},
 53: {&#39;chid&#39;: &#39;GScriba_LIII&#39;,
      &#39;chno&#39;: 53,
      &#39;date&#39;: datetime.date(1156, 3, 27),
      &#39;folio&#39;: &#39;[fo. 6 r.]&#39;,
      &#39;footnotes&#39;: [],
      &#39;marginal&#39;: &#39;Belmusti]&#39;,
      &#39;pgno&#39;: 29,
      &#39;summary&#39;: &quot;Maestro Arnaldo e Giordan nipote del fu Giovanni di Piacenza si obbligano di pagare una somma nell&#39;ottava della prossima Pasqua, per merce ricevuta (27 marzo 1156).&quot;,
      &#39;text&#39;: [&#39;  Testes Conradus Porcellus, Albericus, Vassallus Gambalixa, Petrus Artodi. Nos Arnaldus magister et Iordan nepos quondam Iohannis Placentie accepimus a te Belmusto tantum bracile unde promittimus dare tibi vel tuo certo misso lb. .xLIII. denariorum usque octavam proximi pasce, quod si non fecerimus penam dupli tibi stipulanti promittimus, bona pignori, possis unumquemque convenire de toto. Actum prope campanile Sancti Laurentii, millesimo centesimo .Lv., sexto kalendas aprilis, indictione tercia.\n&#39;]},
.
.
. etc.
}
</code></pre>
<p>Remember, this is just a text representation of a data structure that lives in computer memory. Python calls this sort of structure a &#39;dictionary&#39;, other programming languages may call it a &#39;hash&#39;, or an &#39;associative array&#39;. The point is that it is infinitely easier to do any sort of programmatic analysis or manipulation of a digital text if it is in such a form, rather than in the form of a plain text file. The advantage is that such a data structure can be queried, or calculations can be performed on the data, without first having to parse the text.</p>
<h2 id="a-couple-of-useful-functions-before-we-start">A couple of useful functions before we start:</h2>
<p>We&#39;re going to borrow a couple of functions written by others. They both represent some pretty sophisticated programming. Understanding what&#39;s going on in these functions is instructive, but not necessary. Reading and using other people&#39;s code is how you learn programming, and is the soul of the Open-Source movement. Even if you don&#39;t fully understand how somebody does it, you can nevertheless test functions like this to see that they reliably do what they say they can, and then just apply it to your immediate problem if they are relevant.</p>
<h3 id="levenshtein-distance">Levenshtein distance</h3>
<p>You will note that some of the metadata listed above is page-bound and some of it is charter-bound. Getting these untangled from each other is our aim. There is a class of page-bound data that is useless for our purposes, and only meaningful in the context of a physical book: page headers and footers. In our text, these look like this on <em>recto</em> leaves (in a codex, a book, <em>recto</em> is the right-side page, and <em>verso</em> its reverse, the left-side page)</p>
<p>{% include figure.html filename=&quot;gs_recto_header.png&quot; caption=&quot;recto header&quot; %}</p>
<p>and this on <em>verso</em> leaves:</p>
<p>{% include figure.html filename=&quot;gs_verso_header.png&quot; caption=&quot;verso header&quot; %}</p>
<p>We&#39;d like to preserve the page number information for each charter on the page, but the header text isn&#39;t useful to us and will just make any search and replace operation more difficult. So we&#39;d like to find header text and replace it with a string that&#39;s easy to find with a Regular Expression, and store the page number.</p>
<p>Unfortunately, regular expressions won&#39;t help you much here. This text can appear on any line of our OCR output text, and the ways in which OCR software can foul it up are effectively limitless. Here are some examples of page headers, both <em>recto</em> and <em>verso</em> in our raw OCR output.</p>
<pre><code>260    11141110 CH[AUDANO MATTIA MORESCO
IL CIRTOL4RE DI CIOVINN1 St&#39;Itlltl    269
IL CJIRTOL.%RE DI G:OVeNNl FIM P%    297
IL CIP.TQLIRE DI G&#39;OVeNNI SCI Dt    r.23
332    T1uu:0 CHIAUDANO M:11TIA MGRESCO
IL CIRTOL.&#39;RE DI G:OV.I\N( sca:FR    339
342    NI .\ßlO CHIAUDANO 9LtTTIA MORESCO
</code></pre>
<p>These strings are not regular enough to reliably find with regular expressions; however, if you know what the strings are <em>supposed</em> to look like, you can compose some kind of string similarity algorithm to test each string against an exemplar and measure the likelihood that it is a page header. Fortunately, I didn&#39;t have to compose such an algorithm, Vladimir Levenshtein did it for us in 1965 (see: <a href="http://en.wikipedia.org/wiki/Levenshtein_distance">http://en.wikipedia.org/wiki/Levenshtein_distance</a>). A computer language can encode this algorithm in any number of ways; here&#39;s an effective Python function that will work for us:</p>
<pre><code class="language-python">&#39;&#39;&#39;
Code ripped from https://www.datacamp.com/community/tutorials/fuzzy-string-python
&#39;&#39;&#39;
def lev(seq1, seq2):
    &quot;&quot;&quot; levenshtein_ratio_and_distance:
        For all i and j, distance[i,j] will contain the Levenshtein
        distance between the first i characters of seq1 and the
        first j characters of seq2
    &quot;&quot;&quot;
    # Initialize matrix of zeros
    rows = len(seq1)+1
    cols = len(seq2)+1
    distance = np.zeros((rows,cols),dtype = int)

    # Populate matrix of zeros with the indeces of each character of both strings
    for i in range(1, rows):
        for k in range(1,cols):
            distance[i][0] = i
            distance[0][k] = k

    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    
    for col in range(1, cols):
        for row in range(1, rows):
            if seq1[row-1] == seq2[col-1]:
                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0
            else:
                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio
                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.
                cost = 1
            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions
                                 distance[row][col-1] + 1,          # Cost of insertions
                                 distance[row-1][col-1] + cost)     # Cost of substitutions

    return distance[row][col]
</code></pre>
<p>Again, this is some pretty sophisticated programming, but for our purposes all we need to know is that the <code>lev()</code> function takes two strings as parameters and returns a number that indicates the &#39;string distance&#39; between them, or, how many changes had to be made to turn the first string into the second. So: <code>lev(&quot;fizz&quot;, &quot;buzz&quot;)</code> returns &#39;2&#39;</p>
<h3 id="roman-to-arabic-numerals">Roman to Arabic numerals</h3>
<p>You&#39;ll also note that in the published edition, the charters are numbered with roman numerals. Converting roman numerals into arabic is an instructive puzzle to work out in Python. Here&#39;s the cleanest and most elegant solution I know:</p>
<pre><code class="language-python">def rom2ar(rom):
    &quot;&quot;&quot; From the Python tutor mailing list:
    János Juhász janos.juhasz at VELUX.com
    returns arabic equivalent of a Roman numeral &quot;&quot;&quot;
    roman_codec = {&#39;M&#39;:1000, &#39;D&#39;:500, &#39;C&#39;:100, &#39;L&#39;:50, &#39;X&#39;:10, &#39;V&#39;:5, &#39;I&#39;:1}
    roman = rom.upper()
    roman = list(roman)
    roman.reverse()
    decimal = [roman_codec[ch] for ch in roman]
    result = 0

    while len(decimal):
        act = decimal.pop()
        if len(decimal) and act &lt; max(decimal):
            act = -act
        result += act

    return result
</code></pre>
<p>(run &lt;<a href="/assets/Roman_to_Arabic.txt">this little script</a>&gt; to see in detail how <code>rome2ar</code> works. Elegant programming like this can offer insight; like poetry.)</p>
<h2 id="some-other-things-well-need">Some other things we&#39;ll need:</h2>
<p>At the top of your Python module, you&#39;re going to want to import some python modules that are a part of the standard library. (see Fred Gibbs&#39;s tutorial <a href="/lessons/installing-python-modules-pip"><em>Installing Python Modules with pip</em></a>).</p>
<ol>
<li><p>First among these is the &quot;re&quot; (regular expression) module <code>import re</code>. Regular expressions are your friends. However, bear in mind Jamie Zawinski&#39;s quip:</p>
<blockquote>
<p>Some people, when confronted with a problem, think &quot;I know, I&#39;ll use regular expressions.&quot; Now they have two problems.</p>
</blockquote>
<p> (Again, have a look at L.T. O&#39;Hara&#39;s introduction here at the Programming Historian <a href="/lessons/cleaning-ocrd-text-with-regular-expressions.html">Cleaning OCR’d text with Regular Expressions</a>)</p>
</li>
<li><p>Also: <code>from pprint import pprint</code>. <code>pprint</code> is just a pretty-printer for python objects like lists and dictionaries. You&#39;ll want it because python dictionaries are much easier to read if they are formatted.</p>
</li>
<li><p>And: <code>from collections import Counter</code>. We&#39;ll want this for the <a href="#footnotes">Find and normalize footnote markers and texts</a> section below. This is not really necessary, but we&#39;ll do some counting that would require a lot of lines of fiddly code and this will save us the trouble. The collections module has lots of deep magic in it and is well worth getting familiar with. (Again, see Doug Hellmann&#39;s PyMOTW for the <a href="https://pymotw.com/3/collections/index.html">collections</a> module. I should also point out that his book <a href="https://doughellmann.com/books/the-python-3-standard-library-by-example/"><em>The Python Standard Library By Example</em></a> is one well worth having.)</p>
</li>
</ol>
<h2 id="a-very-brief-review-of-regular-expressions-as-they-are-implemented-in-python">A very brief review of regular expressions as they are implemented in python</h2>
<p>L.T. O&#39;Hara&#39;s <a href="/lessons/cleaning-ocrd-text-with-regular-expressions.html">introduction</a> to using python flavored regular expressions is invaluable. In this context we should review a couple of basic facts about Python&#39;s implementation of regular expressions, the <code>re</code> module, which is part of Python&#39;s standard library.</p>
<ol>
<li><code>re.compile()</code> creates a regular expression object that has a number of methods. You should be familiar with <code>.match()</code>, and <code>.search()</code>, but also <code>.findall()</code> and <code>.finditer()</code></li>
<li>Bear in mind the difference between <code>.match()</code> and <code>.search()</code>: <code>.match()</code> will only match at the <strong>beginning</strong> of a line, whereas <code>.search()</code> will match anywhere in the line <strong>but then it stops</strong>, it&#39;ll <strong>only</strong> return the first match it finds.</li>
<li><code>.match()</code> and <code>.search()</code> return match objects. To retrieve the matched string you need <code>mymatch.group(0)</code>. If your compiled regular expression has grouping parentheses in it (like our &#39;slug&#39; regex below), you can retrieve those substrings of the matched string using <code>mymatch.group(1)</code> etc.</li>
<li><code>.findall()</code> and <code>.finditer()</code> will return <strong>all</strong> occurrences of the matched string; <code>.findall()</code> returns them as a list of strings, but .finditer() returns an <strong>iterator of match objects</strong>. (read the docs on the method <a href="hhttps://docs.python.org/3.7/library/re.html#re.finditer">.finditer()</a>.)</li>
</ol>
<h1 id="iterative-processing-of-text-files">Iterative processing of text files</h1>
<p>We&#39;ll start with a single file of OCR output. We will iteratively generate new, corrected versions of this file by using it as input for our python scripts. Sometimes our script will make corrections automatically, more often, our scripts will simply alert us to where problems lie in the input file, and we will make corrections manually. So, for the first several operations we&#39;re going to want to produce new and revised text files to use as input for our subsequent operations. Every time you produce a text file, you should version it and duplicate it so that you can always return to it. The next time you run your code (as you&#39;re developing it) you might alter the file in an unhelpful way and it&#39;s easiest just to restore the old version.</p>
<p>The code in this tutorial is highly edited; it is <strong>not</strong> comprehensive. As you continue to refine your input files, you will write lots of little <em>ad hoc</em> scripts to check on the efficacy of what you&#39;ve done so far. Versioning will ensure that such experimentation will not destroy any progress that you&#39;ve made.</p>
<h2 id="a-note-on-how-to-deploy-the-code-in-this-tutorial">A note on how to deploy the code in this tutorial:</h2>
<p>The code in this tutorial is for Python 3.</p>
<p>When you write code in a text file and then execute it, either at the command line, or from within your text editor or IDE, the Python interpreter executes the code line by line, from top to bottom. So, often the code on the bottom of the page will depend on code above it.</p>
<p>One way to use the code snippets in section 2 might be to have all of them in a single file and comment out the bits that you don&#39;t want to run. Each time you execute the file, you will want to be sure that there is a logical control flow from the <code>#!</code> line at the top, through your various <code>import</code>s and assignment of global variables, and each loop, or block.</p>
<p>Or, each of the subsections in section 2 can also be treated as a separate script, each would then have to do its own <code>import</code>ing and assignment of global variables.</p>
<p>In section 3, &quot;Creating the Dictionary&quot;, you will be operating on a data set in computer memory (the <code>charters</code> dictionary) that will be generated from the latest, most correct, input text you have. So you will want to maintain a single python module in which you define the dictionary at the top, along with your <code>import</code> statements and the assignment of global variables, followed by each of the four loops that will populate and then modify that dictionary.</p>
<pre><code class="language-python">#!/usr/bin/python

import re
from pprint import pprint
from collections import Counter

# followed by any global variables you will need, like:

n = 0
this_folio  = &#39;[fo. 1 r.]&#39;
this_page = 1

# compiled regular expressions like:
slug = re.compile(&quot;(\[~~~~\sGScriba_)(.*)\s::::\s(\d+)\s~~~~\]&quot;)
fol = re.compile(&quot;\[fo\.\s?\d+\s?[rv]\.\s?\]&quot;)
pgbrk = re.compile(&quot;~~~~~ PAGE (\d+) ~~~~~&quot;)

# the canonical file you will be reading from
fin = open(&quot;/path/to/your/current/canonical.txt&quot;, &#39;r&#39;)
GScriba = fin.readlines()


# then the empty dictionary:
charters = dict()

# followed by the 4 &#39;for&#39; loops in section 2 that will populate and then modify this dictionary
</code></pre>
<h2 id="chunk-up-the-text-by-pages">Chunk up the text by pages</h2>
<p>First of all, we want to find all the page headers, both <em>recto</em> and <em>verso</em> and replace them with consistent strings that we can easily find with a regular expression. The following code looks for lines that are similar to what we know are our page headers to within a certain threshold. It will take some experimentation to find what this threshold is for your text. Since my <em>recto</em> and <em>verso</em> headers are roughly the same length, both have the same similarity score of 26.</p>
<blockquote>
<p>NOTA BENE: The <code>lev()</code> function described above returns a measure of the &#39;distance&#39; between two strings, so, the shorter the page header string, the more likely it is that this trick will not work. If your page header is just &quot;Header&quot;, then any line comprised of a six letter word might give you a string distance of 6, eg: <code>lev(&quot;Header&quot;, &quot;Foobar&quot;)</code> returns &#39;6&#39;, leaving you none the wiser. In our text, however, the header strings are long and complex enough to give you meaningful scores, eg:</p>
</blockquote>
<p><code>lev(&quot;RANDOM STRING OF SIMILAR LENGTH:    38&quot;, &#39;IL CARTOLARE DI GIOVANNI SCRIBA&#39;)</code></p>
<p>returns 33, but one of our header strings, even badly mangled by the OCR, returns 20:</p>
<p><code>lev(&quot;IL CIRTOL4RE DI CIOVINN1 St&#39;Itlltl     269&quot;, &#39;IL CARTOLARE DI GIOVANNI SCRIBA&#39;)</code></p>
<p>So we can use <code>lev()</code> to find and modify our header strings thus:</p>
<pre><code class="language-python"># At the top, do the importing you need and define the lev() function as described above, and then:

fin = open(&quot;our_base_OCR_result.txt&quot;, &#39;r&#39;) # read our OCR output text
fout = open(&quot;out1.txt&quot;, &#39;w&#39;) # create a new textfile to write to when we&#39;re ready
GScriba = fin.readlines() # turn our input file into a list of lines

for line in GScriba:
    # get a Levenshtein distance score for each line in the text
    recto_lev_score = lev(line, &#39;IL CARTOLARE DI GIOVANNI SCRIBA&#39;)
    verso_lev_score = lev(line, &#39;MARIO CHIAUDANO - MATTIA MORESCO&#39;)

    # you want to use a score that&#39;s as high as possible,
    # but still finds only potential page header texts.
    if recto_lev_score &lt; 26 :

        # If we increment a variable &#39;n&#39; to count the number of headers we&#39;ve found,
        # then the value of that variable should be our page number.
        n += 1
        print(f&quot;recto: {recto_lev_score} {line}&quot;)

        # Once we&#39;ve figured out our optimal &#39;lev&#39; score, we can &#39;uncomment&#39;
        # all these `fout.write()` lines to write out our new text file,
        # replacing each header with an easy-to-find string that contains
        # the page number: our variable &#39;n&#39;.

        #fout.write(&quot;~~~~~ PAGE %d ~~~~~\n\n&quot; % n)
    elif verso_lev_score &lt; 26 :
        n += 1
        print(f&quot;verso: {verso_lev_score} {line}&quot;)
        #fout.write(&quot;~~~~~ PAGE %d ~~~~~\n\n&quot; % n)
    else:
        #fout.write(line)
        pass

print(n)
</code></pre>
<p>There&#39;s a lot of calculation going on in the <code>lev()</code> function. It isn&#39;t very efficient to call it on every line in our text, so this might take some time, depending on how long our text is. We&#39;ve only got 803 charters in vol. 1. That&#39;s a pretty small number. If it takes 30 seconds, or even a minute, to run our script, so be it.</p>
<p>If we run this script on our OCR output text, we get output that looks like this:</p>
<pre><code class="language-python">.
.
.
verso: 8 426    MARIO CHIAUDANO MAITIA MORESCO
recto: 5 IL CARTOLARE DI GIOVANNI SCRIBA    427
verso: 11 , ,    428    MARIO CHIAUDANO MATTIA MORESCO
recto: 5 IL CARTOLARE DI GIOVANNI SCRIBA    499
verso: 7 430    MARIO CHIAUDANO MATTIA MORESCO
recto: 5 IL CARTOLARE DI GIOVANNI SCRIBA    431
verso: 8 432    MARIO CHIAUDASO MATTIA MORESCO
430
</code></pre>
<p>For each line, the output tells us that it&#39;s page <em>verso</em> or <em>recto</em>, the Levenshtein &quot;score&quot;, and then the text of the line (complete with all the errors in it. Note that the OCR misread the pg. number for pg. 429). The lower the Levenshtein &quot;score&quot;, the closer the line is to the model you&#39;ve given it.</p>
<p>This tells you that the script found 430 lines that are probably page headers. You know how many pages there should be, so if the script didn&#39;t find all the headers, you can go through the output looking at the page numbers, find the pages it missed, and fix the headers manually, then repeat until the script finds all the page headers.</p>
<p>Once you&#39;ve found and fixed the headers that the script didn&#39;t find, you can then write out the corrected text to a new file that will serve as the basis for the other operations below. So, instead of</p>
<pre><code class="language-python">quicquid volueris sine omni mea et
(1) Spazio bianco nel ms.

12    MARIO CSIAUDANO MATTIA MORESCO
heredum meorum contradicione. Actum in capitulo .MCLV., mensis iulii, indicione secunda.
</code></pre>
<p>we&#39;ll have a textfile like this:</p>
<pre><code class="language-python">quicquid volueris sine omni mea et
(1) Spazio bianco nel ms.

~~~~~ PAGE 12 ~~~~~

heredum meorum contradicione. Actum in capitulo .MCLV., mensis iulii, indicione secunda.
</code></pre>
<p>Note that for many of the following operations, we will use <code>GScriba = fin.readlines()</code> so <code>GScriba</code> will be a <strong>python list</strong> of the lines in our input text. Keep this firmly in mind, as the <code>for</code> loops that we will use will depend on the fact that we will iterate through the lines of our text <strong>In Document Order</strong>.</p>
<h2 id="chunk-up-the-text-by-charter-or-sections-or-letters-or-what-have-you">Chunk up the text by charter (or sections, or letters, or what-have-you)</h2>
<p>The most important functional divisions in our text are signaled by upper case roman numerals on a separate line for each of the charters. So we need a regex to find roman numerals like that. Here&#39;s one: <code>romstr = re.compile(&quot;\s*[IVXLCDM]{2,}&quot;)</code>. We&#39;ll put it at the top of our module file as a &#39;global&#39; variable so it will be available to any of the bits of code that come later.</p>
<p>The script below will look for capital roman numerals that appear on a line by themselves. Many of our charter numbers will fail that test and the script will report <code>there&#39;s a charter roman numeral missing?</code>, often because there&#39;s something before or after it on the line; or, <code>KeyError</code>, often because the OCR has garbled the characters (e.g. CCG for 300, XOII for 492). Run this script repeatedly, correcting <code>out1.txt</code> as you do until all the charters are accounted for.</p>
<pre><code class="language-python"># At the top, do the importing you need, then define rom2ar() as described above, and then:
n = 0
romstr = re.compile(&quot;\s*[IVXLCDM]{2,}&quot;)
fin = open(&quot;out1.txt&quot;, &#39;r&#39;)
fout = open(&quot;out2.txt&quot;, &#39;w&#39;)
GScriba = fin.readlines()

for line in GScriba:
    if romstr.match(line):
        rnum = line.strip().strip(&#39;.&#39;)
        # each time we find a roman numeral by itself on a line we increment n:
        # that&#39;s our charter number.
        n += 1
        try:
            # translate the roman to the arabic and it should be equal to n.
            if n != rom2ar(rnum):
                # if it&#39;s not, then alert us
                print(f&quot;{n}, there&#39;s a charter roman numeral missing?, because line number {GScriba.index(line)} reads: {line}&quot;)
                # then set &#39;n&#39; to the right number
                n = rom2ar(rnum)
        except KeyError:
            print(f&quot;{n}, KeyError, line number {GScriba.index(line)} reads: {line}&quot;)
</code></pre>
<p>Since we know how many charters there should be. At the end of our loop, the value of n should be the same as the number of charters. And, in any iteration of the loop, if the value of n does not correspond to the next successive charter number, then we know we&#39;ve got a problem somewhere, and the print statements should help us find it.</p>
<p>Here&#39;s a sample of the output our script will give us:</p>
<pre><code class="language-python">23 there&#39;s a charter roman numeral missing?, because line number  156  reads:  XXIV.
25 there&#39;s a charter roman numeral missing?, because line number  186  reads:  XXVIII.
36 KeyError, line number  235  reads:  XXXV1.
37 KeyError, line number  239  reads:  XXXV II.
38 there&#39;s a charter roman numeral missing?, because line number  252  reads:  XL.
41 there&#39;s a charter roman numeral missing?, because line number  262  reads:  XLII.
43 KeyError, line number  265  reads:  XL:III.
</code></pre>
<blockquote>
<p>NOTA BENE: Our regex will report an error for the single digit Roman numerals (&#39;I&#39;,&#39;V&#39;,&#39;X&#39; etc.). You could test for these in the code, but sometimes leaving a known and regular error is a help to check on the efficacy of what you&#39;re doing. Our aim is to satisfy ourselves that any inconsistencies on the charter number line are understood and accounted for.</p>
</blockquote>
<p>Once we&#39;ve found, and fixed, all the roman numeral charter headings, then we can write out a new file with an easy-to-find-by-regex string, a &#39;slug,&#39; for each charter in place of the bare roman numeral. Comment out the <code>for</code> loop above, and replace it with this one:</p>
<pre><code class="language-python">for line in GScriba:
    if romstr.match(line):
        rnum = line.strip().strip(&#39;.&#39;)
        num = rom2ar(rnum)
        fout.write(f&quot;[~~~~ GScriba_{rnum} :::: {num} ~~~~]\n&quot;)
    else:
        fout.write(line)
</code></pre>
<p>While it&#39;s important in itself for us to have our OCR output reliably divided up by page and by charter, the most important thing about these initial operations is that you know how many pages there are, and how many charters there are, and you can use that knowledge to check on subsequent operations. If you want to do something to every charter, you can reliably test whether or not it worked because you can count the number of charters that it worked on.</p>
<h2 id="find-and-normalize-folio-markers">Find and normalize folio markers</h2>
<p>Our OCR&#39;d text is from the 1935 published edition of <em>Giovanni Scriba</em>. This is a transcription of a manuscript cartulary which was in the form of a bound book. The published edition preserves the pagination of that original by noting where the original pages change: [fo. 16 r.] the face side of the 16th leaf in the book, followed by its reverse [fo. 16 v.]. This is metadata that we want to preserve for each of the charters so that they can be referenced with respect to the original, as well as with respect to the published edition by page number.</p>
<p>Many of the folio markers (e.g. &quot;[fo. 16 v.]&quot;) appear on the same line as the roman numeral for the charter heading. To normalize those charter headings for the operation above, we had to put a line break between the folio marker and the charter number, so many of the folio markers are on their own line already. However, sometimes the folio changes in the middle of the charter text somewhere. We want these markers to stay where they are; we will have to treat those two cases differently. For either case, we need to make sure all the folio markers are free of errors so that we can reliably find them by means of a regular expression. Again, since we know how many folios there are, we can know if we&#39;ve found them all. Note that because we used <code>.readlines()</code>, <code>GScriba</code> is a list, so the script below will print the line number from the source file as well as the line itself. This will report all the correctly formated folio markers, so that you can find and fix the ones that are broken.</p>
<pre><code class="language-python"># note the optional quantifiers &#39;\s?&#39;. We want to find as many as we can, and
# the OCR is erratic about whitespace, so our regex is permissive. But as
# you find and correct these strings, you will want to make them consistent.
fol = re.compile(&quot;\[fo\.\s?\d+\s?[rv]\.\s?\]&quot;)

for line in GScriba:
    if fol.match(line):
        # since GScriba is a list, we can get the index of any of its members to find the line number in our input file.
        print GScriba.index(line), line
</code></pre>
<p>We would also like to ensure that no line has more than one folio marker. We can test that like this:</p>
<pre><code class="language-python">for line in GScriba:
    all = fol.findall(line)
    if len(all) &gt; 1:
        print GScriba.index(line), line
</code></pre>
<p>Again, as before, once you&#39;ve found and corrected all the folio markers in your input file, save it with a new name and use it as the input to the next section.</p>
<h2 id="find-and-normalize-the-italian-summary-lines">Find and normalize the Italian summary lines.</h2>
<p>This important line is invariably the first one after the charter heading.</p>
<p>{% include figure.html filename=&quot;gs_italian_summary.png&quot; caption=&quot;italian summary line&quot; %}</p>
<p>Since those roman numeral headings are now reliably findable with our &#39;slug&#39; regex, we can now isolate the line that appears immediately after it. We also know that the summaries always end with some kind of parenthesized date expression. So, we can compose a regular expression to find the slug and the line following:</p>
<pre><code class="language-python">slug_and_firstline = re.compile(&quot;(\[~~~~\sGScriba_)(.*)\s::::\s(\d+)\s~~~~\]\n(.*)(\(\d?.*\d+\))&quot;)
</code></pre>
<p>Let&#39;s break down that regex using the verbose mode (again, see O&#39;Hara&#39;s <a href="/lessons/cleaning-ocrd-text-with-regular-expressions.html">tutorial</a>). Our &#39;slug&#39; for each charter takes the form &quot;[<del>\</del>~~ GScriba_CCVII :::: 207 ~~~~]&quot; for example. The compiled pattern above is exactly equivalent to the folowing (note the re.VERBOSE switch at the end):</p>
<pre><code class="language-python">slug_and_firstline = re.compile(r&quot;&quot;&quot;
    (\[~~~~\sGScriba_)  # matches the &quot;[~~~~ GScriba_&quot; bit
    (.*)                # matches the charter&#39;s roman numeral
    \s::::\s            # matches the &quot; :::: &quot; bit
    (\d+)               # matches the arabic charter number
    \s~~~~\]\n          # matches the last &quot; ~~~~ &quot; bit and the line ending
    (.*)                # matches all of the next line up to:
    (\(\d?.*\d+\))      # the paranthetical expression at the end
    &quot;&quot;&quot;, re.VERBOSE)
</code></pre>
<p>the parentheses mark match groups, so each time our regex finds a match, we can refer in our code to specific bits of the match it found:</p>
<ul>
<li><code>match.group(0)</code> is the whole match, both our slug and the line that follows it.</li>
<li><code>match.group(1)</code> = &quot;[~~~~ GScriba_&quot;</li>
<li><code>match.group(2)</code> = the charter&#39;s roman numeral</li>
<li><code>match.group(3)</code> = the arabic charter number</li>
<li><code>match.group(4)</code> = the whole of the Italian summary line up to the parenthesized date expression</li>
<li><code>match.group(5)</code> = the parenthesized date expression. Note the escaped parentheses.</li>
</ul>
<p>Because our OCR has a lot of mysterious whitespace (OCR software is not good at parsing whitespace and you&#39;re likely to get newlines, tabs, spaces, all mixed up without rhyme or reason), we want to hunt for this regex as substrings of a great big string, so this time we&#39;re going to use <code>.read()</code> instead of <code>.readlines()</code>. And we&#39;ll also need a counter to keep track of the lines we find. This script will report the charter numbers where the first line does not conform to our regex model. This will usually happen if there&#39;s no line break after our charter header, or if the Italian summary line has been broken up into multiple lines.</p>
<pre><code class="language-python">num_firstlines = 0
fin = open(&quot;your_current_source_file.txt&quot;, &#39;r&#39;)
# NB: GScriba is not a list of lines this time, but a single big string.
GScriba = fin.read()

# finditer() creates an iterator &#39;i&#39; that we can do a &#39;for&#39; loop over.
i = slug_and_firstline.finditer(GScriba)

# each element &#39;x&#39; in that iterator is a regex match object.
for x in i:
    # count the summary lines we find. Remember, we know how many
    # there should be, because we know how many charters there are.
    num_firstlines += 1

    chno = int(x.group(3)) # our charter number is a string, we need an integer

    # chno should equal n + 1, if it doesn&#39;t, report to us
    if chno != n + 1:
        print(f&quot;problem in charter: {(n + 1)}&quot;) #NB: this will miss consecutive problems.
    # then set n to the right charter number
    n = chno

# print out the number of summary lines we found
print(f&quot;number of italian summaries: {num_firstlines}&quot;)
</code></pre>
<p>Again, run the script repeatedly until all the Italian Summary lines are present and correct, then save your input file with a new name and use it the input file for the next bit:</p>
<h2 id="find-and-normalize-footnote-markers-and-texts">Find and normalize footnote markers and texts</h2>
<p>One of the trickiest bits to untangle, is the infuriating editorial convention of restarting the footnote numbering with each new page. This makes it hard to associate a footnote text (page-bound data), with a footnote marker (charter-bound data). Before we can do that we have to ensure that each footnote text that appears at the bottom of the page, appears in our source file on its own separate line with no leading white-space. And that <strong>none</strong> of the footnote markers within the text appears at the beginning of a line. And we must ensure that every footnote string, &quot;(1)&quot; for example, appears <strong>exactly</strong> twice on a page: once as an in-text marker, and once at the bottom for the footnote text. The following script reports the page number of any page that fails that test, along with a list of the footnote strings it found on that page.</p>
<pre><code class="language-python"># Don&#39;t forget to import the Counter module:
from collections import Counter
fin = open(&quot;your_current_source_file.txt&quot;, &#39;r&#39;)
GScriba = fin.readlines() # GScriba is a list again
r = re.compile(&quot;\(\d{1,2}\)&quot;) # there&#39;s lots of ways for OCR to screw this up, so be alert.
pg = re.compile(&quot;~~~~~ PAGE \d+ ~~~~~&quot;)
pgno = 0

pgfnlist = []
# remember, we&#39;re processing lines in document order. So for each page
# we&#39;ll populate a temporary container, &#39;pgfnlist&#39;, with values. Then
# when we come to a new page, we&#39;ll report what those values are and
# then reset our container to the empty list.

for line in GScriba:
    if pg.match(line):
        # if this test is True, then we&#39;re starting a new page, so increment pgno
        pgno += 1

        # if we&#39;ve started a new page, then test our list of footnote markers
        if pgfnlist:
            c = Counter(pgfnlist)

            # if there are fn markers that do not appear exactly twice,
            # then report the page number to us
            if 1 in c.values(): print(pgno, pgfnlist)

            # then reset our list to empty
            pgfnlist = []

    # for each line, look for ALL occurences of our footnote marker regex
    i = r.finditer(line)
    for mark in [eval(x.group(0)) for x in i]:
        # and add them to our list for this page
        pgfnlist.append(mark)
</code></pre>
<blockquote>
<p>Note: the elements in the iterator &#39;i&#39; are string matches. We want the strings that were matched, <code>group(0)</code>. e.g. &quot;(1)&quot;. And if we do eval(&quot;(1)&quot;) we get an integer that we can add to our list.</p>
</blockquote>
<p>Our <code>Counter</code> is a very handy special data structure. We know that we want each value in our <code>pgfnlist</code> to appear twice. Our <code>Counter</code> will give us a hash where the keys are the elements that appear, and the values are how many times each element appears. Like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; l = [1,2,3,1,3]
&gt;&gt;&gt; c = Counter(l)
&gt;&gt;&gt; print(c)
Counter({1: 2, 3: 2, 2: 1})
</code></pre>
<p>So if for a given page we get a list of footnote markers like this <code>[1,2,3,1,3]</code>, then the test <code>if 1 in c.values()</code> will indicate a problem because we know each element must appear <strong>exactly twice</strong>:</p>
<pre><code class="language-python">&gt;&gt;&gt; l = [1,2,3,1,3]
&gt;&gt;&gt; c = Counter(l)
&gt;&gt;&gt; print(c.values())
[2, 1, 2]
</code></pre>
<p>whereas, if our footnote marker list for the page is complete <code>[1,2,3,1,2,3]</code>, then:</p>
<pre><code class="language-python">&gt;&gt;&gt; l = [1,2,3,1,2,3]
&gt;&gt;&gt; c = Counter(l)
&gt;&gt;&gt; print c.values()
[2, 2, 2] # i.e. 1 is not in c.values()
</code></pre>
<p>As before, run this script repeatedly, correcting your input file manually as you discover errors, until you are satisfied that all footnotes are present and correct for each page. Then save your corrected input file with a new name.</p>
<p>Our text file still has lots of OCR errors in it, but we have now gone through it and found and corrected all the specific metadata bits that we want in our ordered data set. Now we can use our corrected text file to build a Python dictionary.</p>
<h1 id="creating-the-dictionary">Creating the Dictionary</h1>
<p>Now that we&#39;ve cleaned up enough of the OCR that we can successfully differentiate the component parts of the page from each other, we can now sort the various bits of the meta-data, and the charter text itself, into their own separate fields of a Python dictionary.</p>
<p>We have a number of things to do: correctly number each charter as to charter number, folio, and page; separate out the Italian summary and the marginal notation lines; and associate the footnote texts with their appropriate charter. To do all this, sometimes it is convenient to make more than one pass.</p>
<h2 id="create-a-skeleton-dictionary">Create a skeleton dictionary.</h2>
<p>We&#39;ll start by generating a python dictionary whose keys are the charter numbers, and whose values are a nested dictionary that has fields for some of the metadata we want to store for each charter. So it will have the form:</p>
<pre><code class="language-python">charters = {
    .
    .
    .
    300: {
            &#39;chid&#39;: &quot;our charter ID&quot;,
            &#39;chno&#39;: 300,
            &#39;footnotes&#39;: [], # an empty list for now
            &#39;folio&#39;: &quot;the folio marker for this charter&quot;,
            &#39;pgno&#39;: &quot;the page number in the printed edition for this charter,
            &#39;text&#39;: [] # an empty list for now
          },
    301: {
            &#39;chid&#39;: &quot;our charter ID&quot;,
            &#39;chno&#39;: 301,
            &#39;footnotes&#39;: [], # an empty list for now
            &#39;folio&#39;: &quot;the folio marker for this charter&quot;,
            &#39;pgno&#39;: &quot;the page number in the printed edition for this charter,
            &#39;text&#39;: [] # an empty list for now
          },
    .
    .
    . etc.
}
</code></pre>
<p>For this first pass, we&#39;ll just create this basic structure and then in subsequent loops we will add to and modify this dictionary until we get a dictionary for each charter, and fields for all the metadata for each charter. Once this loop disposes of the easily searched lines (folio, page, and charter headers) and creates an empty container for footnotes, the fall-through default will be to append the remaining lines to the text field, which is a python list.</p>
<pre><code class="language-python">slug = re.compile(&quot;(\[~~~~\sGScriba_)(.*)\s::::\s(\d+)\s~~~~\]&quot;)
fol = re.compile(&quot;\[fo\.\s?\d+\s?[rv]\.\s?\]&quot;)
pgbrk = re.compile(&quot;~~~~~ PAGE (\d+) ~~~~~&quot;)

fin = open(&quot;your_current_source_file.txt&quot;, &#39;r&#39;)
GScriba = fin.readlines()

# we&#39;ll also need these global variables with starting values as we mentioned at the top
n = 0
this_folio  = &#39;[fo. 1 r.]&#39;
this_page = 1

# &#39;charters&#39; is also defined as a global variable. The &#39;for&#39; loop below
#  and in the following sections, will build on and modify this dictionary
charters = dict()

for line in GScriba:
    if fol.match(line):
        # use this global variable to keep track of the folio number.
        # we&#39;ll create the &#39;folio&#39; field using the value of this variable
        this_folio = fol.match(line).group(0)
        continue # update the variable but otherwise do nothing with this line.
    if slug.match(line):
        # if our &#39;slug&#39; regex matches, we know we have a new charter
        # so get the data from the match groups
        m = slug.match(line)
        chid = &quot;GScriba_&quot; + m.group(2)
        chno = int(m.group(3))

        # then create an empty nested dictionary
        charters[chno] = {}

        # and an empty container for all the lines we won&#39;t use on this pass
        templist = [] # this works because we&#39;re proceeding in document order: templist continues to exist as we iterate through each line in the charter, then is reset to the empty list when we start a new charter(slug.match(line))
        continue # we generate the entry, but do nothing with the text of this line.
    if chno:
        # if a charter dictionary has been created,
        # then we can now populate it with data from our slug.match above.
        d = charters[chno] # &#39;d&#39; is just more convenient than &#39;charters[chno]&#39;
        d[&#39;footnotes&#39;] = [] # We&#39;ll populate this empty list in a separate operation
        d[&#39;chid&#39;] = chid
        d[&#39;chno&#39;] = chno
        d[&#39;folio&#39;] = this_folio
        d[&#39;pgno&#39;] = this_page

        if re.match(&#39;^\(\d+\)&#39;, line):
            # this line is footnote text, because it has a footnote marker
            # like &quot;(1)&quot; at the beginning. So we&#39;ll deal with it later
            continue
        if pgbrk.match(line):
            # if line is a pagebreak, update the variable
            this_page = int(pgbrk.match(line).group(1))
        elif fol.search(line):
            # if folio changes within the charter text, update the variable
            this_folio = fol.search(line).group(0)
            templist.append(line)
        else:
            # any line not otherwise accounted for, add to our temporary container
            templist.append(line)
        # add the temporary container to the dictionary after using
        # a list comprehension to strip out any empty lines.
        d[&#39;text&#39;] = [x for x in templist if not x == &#39;\n&#39;] # strip empty lines
</code></pre>
<h2 id="add-the-marginal-notation-and-italian-summary-lines-to-the-dictionary">Add the &#39;marginal notation&#39; and Italian summary lines to the dictionary</h2>
<p>When we generated the dictionary of dictionaries above, we assigned fields for footnotes (just an empty list for now), charterID, charter number, the folio, and the page number. All remaining lines were appended to a list and assigned to the field &#39;text&#39;. In all cases, the first line of each charter&#39;s text field should be the Italian summary as we have insured above. The second line in MOST cases, represents a kind of marginal notation usually ended by the &#39;]&#39; character (which OCR misreads a lot). We have to find the cases that do not meet this criterion, supply or correct the missing &#39;]&#39;, and in the cases where there is no marginal notation I&#39;ve supplied &quot;no marginal]&quot; in my working text. The following diagnostic script will print the charter number and first two lines of the text field for those charters that do not meet these criteria. Run this script separately against the <code>charters</code> dictionary, and correct and update your canonical text accordingly.</p>
<pre><code class="language-python">n = 0
for ch in charters:
    txt = charters[ch][&#39;text&#39;] # remember: the text field is a python list of strings
    try:
        line1 = txt[0]
        line2 = txt[1]
        if line2 and &#39;]&#39; not in line2:
            n += 1
            print(f&quot;charter: {ch}\ntext, line 1: {line1}\ntext, line 2: {line2}&quot;)
    except:
        print(ch, &quot;oops&quot;) # to pass the charters from the missing page 214 # to pass the charters from the missing page 214
</code></pre>
<blockquote>
<p>Note: The <code>try: except:</code> blocks are made necessary by the fact that in my OCR output, the data for pg 214 somehow got missed out. This often happens. Scanning or photographing each page of a 600 page book is tedious in the extreme. It&#39;s very easy to skip a page. You will inevitably have anomalies like this in your text that you will have to isolate and work around. The Python <code>try: except:</code> pattern makes this easy. Python is also very helpful here in that you can do a lot more in the <code>except:</code> clause beyond just printing &quot;oops&quot;. You could call a function that performs a whole separate operation on those anomalous bits.</p>
</blockquote>
<p>Once we&#39;re satisfied that line 1 and line 2 in the &#39;text&#39; field for each charter in the <code>charters</code> dictionary are the Italian summary and the marginal notation respectively, we can make another iteration of the <code>charters</code> dictionary, removing those lines from the text field and creating new fields in the charter entry for them.</p>
<blockquote>
<p>NOTA BENE: we are now modifying a data structure in memory rather than editing successive text files. So this script should be <strong>added</strong> to the one above that created your skeleton dictionary. That script creates the <code>charters</code> dictionary in memory, and this one modifies it</p>
</blockquote>
<pre><code class="language-python">for ch in charters:
    d = charters[ch]
    try:
        d[&#39;summary&#39;] = d[&#39;text&#39;].pop(0).strip()
        d[&#39;marginal&#39;] = d[&#39;text&#39;].pop(0).strip()
    except IndexError: # this will report that the charters on p 214 are missing
        print(f&quot;missing charter {ch}&quot;)
</code></pre>
<h2 id="assign-footnotes-to-their-respective-charters-and-add-to-dictionary">Assign footnotes to their respective charters and add to dictionary</h2>
<p>The trickiest part is to get the footnote texts appearing at the bottom of the page associated with their appropriate charters. Since we are, perforce, analyzing our text line by line, we&#39;re faced with the problem of associating a given footnote reference with its appropriate footnote text when there are perhaps many lines intervening.</p>
<p>For this we go back to the same list of lines that we built the dictionary from. We&#39;re depending on all the footnote markers appearing within the charter text, i.e. none of them are at the beginning of a line. And, each of the footnote texts is on a separate line beginning with &#39;(1)&#39; etc. We design regexes that can distinguish between the two and construct a container to hold them as we iterate over the lines. As we iterate over the lines of the text file, we find and assign markers and texts to our temporary container, and then, each time we reach a page break, we assign them to their appropriate fields in our existing Python dictionary <code>charters</code> and reset our temporary container to the empty <code>dict</code>.</p>
<p>Note how we construct that temporary container. <code>fndict</code> starts out as an empty dictionary. As we iterate through the lines of our input text, if we find footnote markers within the line, we create an entry in <code>fndict</code> whose key is the footnote number, and whose value is another dictionary. In that dictionary we record the id of the charter that the footnote belongs to, and we create an empty field for the footnote text. When we find the footnote texts (<code>ntexts</code>) at the bottom of the page, we look up the footnote number in our container <code>fndict</code> and write the text of the line to the empty field we made. So when we come to the end of the page, we have a dictionary of footnotes that looks like this:</p>
<pre><code class="language-python">{1: {&#39;chid&#39;: 158, &#39;fntext&#39;: &#39;Nel ms. de due volte e ripa cancellato.&#39;},
 2: {&#39;chid&#39;: 158, &#39;fntext&#39;: &#39;Sic nel ms.&#39;},
 3: {&#39;chid&#39;: 159, &#39;fntext&#39;: &#39;genero cancellato nel ms.&#39;}}
</code></pre>
<p>Now we have all the necessary information to assign the footnotes to the empty &#39;footnotes&#39; list in the <code>charters</code> dictionary: the number of the footnote (the key), the charter it belongs to (chid), and the text of the footnote (fntext).</p>
<p>This is a common pattern in programming, and very useful: in an iterative process of some kind, you use an accumulator (our <code>fndict</code>) to gather bits of data, then when your sentinel encounters a specified condition (the pagebreak) it does something with the data.</p>
<pre><code class="language-python">fin = open(&quot;your_current_source_file.txt&quot;, &#39;r&#39;)
GScriba = fin.readlines()

# in notemark, note the &#39;lookbehind&#39; expression &#39;?&lt;!&#39; to insure that
# the marker &#39;(1)&#39; does not begin the string
notemark = re.compile(r&quot;\(\d+\)(?&lt;!^\(\d+\))&quot;)
notetext = re.compile(r&quot;^\(\d+\)&quot;)
this_charter = 1
pg = re.compile(&quot;~~~~~ PAGE \d+ ~~~~~&quot;)
pgno = 1
fndict = {}

for line in GScriba:
    nmarkers = notemark.findall(line)
    ntexts = notetext.findall(line)
    if pg.match(line):
        # This is our &#39;sentinel&#39;. We&#39;ve come to the end of a page,
        # so we record our accumulated footnote data in the &#39;charters&#39; dict.
        for fn in fndict:
            chid = fndict[fn][&#39;chid&#39;]
            fntext = fndict[fn][&#39;fntext&#39;]
            charters[int(chid)][&#39;footnotes&#39;].append((fn, fntext))
        pgno += 1
        fndict = {}  # and then re-initialize our temporary container
    if slug.match(line): # here&#39;s the beginning of a charter, so update the variable.
        this_charter = int(slug.match(line).group(3))
    if nmarkers:
        for marker in [eval(x) for x in nmarkers]:
            # create an entry with the charter&#39;s id and an empty text field
            fndict[marker] = {&#39;chid&#39;:this_charter, &#39;fntext&#39;: &#39;&#39;}
    if ntexts:
        for text in [eval(x) for x in ntexts]:
            try:
                # fill in the appropriate empty field.
                fndict[text][&#39;fntext&#39;] = re.sub(&#39;\(\d+\)&#39;, &#39;&#39;, line).strip()
            except KeyError:
                print(&quot;printer&#39;s error? &quot;, &quot;pgno:&quot;, pgno, line)
</code></pre>
<p>Note that the <code>try: except:</code> blocks come to the rescue again here. The loop above kept breaking because in 3 instances it emerged that there existed footnotes at the bottom of a page for which there were no markers within the text. This was an editorial oversight in the published edition, not an OCR error. The result was that when I tried to address the non-existent entry in <code>fndict</code>, I got a <code>KeyError</code>. My <code>except:</code> clause allowed me to find and look at the error, and determine that the error was in the original and nothing I could do anything about, so when generating the final version of <code>charters</code> I replaced the <code>print</code> statement with <code>pass</code>. Texts made by humans are messy; no getting around it. <code>try: except:</code> exists to deal with that reality.</p>
<blockquote>
<p>NOTA BENE: Again, bear in mind that we are modifying a data structure in memory rather than editing successive text files. So this loop should be <strong>added</strong> to your script <strong>below</strong> the summary and marginal loop, which is <strong>below</strong> the loop that created your skeleton dictionary.</p>
</blockquote>
<h2 id="parse-dates-and-add-to-the-dictionary">Parse Dates and add to the dictionary</h2>
<p>Dates are hard. Students of British history cling to <a href="http://www.worldcat.org/oclc/41238508">Cheyney</a> as to a spar on a troubled ocean. And, given the way the Gregorian calendar was adopted so gradually, and innumerable other local variations, correct date reckoning for medieval sources will always require care and local knowledge. Nevertheless, here too Python can be of some help.</p>
<p>Our Italian summary line invariably contains a date drawn from the text, and it&#39;s conveniently set off from the rest of the line by parentheses. So we can parse them and create Python <code>date</code> objects. Then, if we want, we can do some simple calendar arithmetic.</p>
<p>First we have to find and correct all the dates in the same way as we have done for the other metadata elements. Devise a diagnostic script that will iterate over your <code>charters</code> dictionary, report the location of errors in your canonical text, and then fix them in your canonical text manually. Something like this:</p>
<pre><code class="language-python">summary_date = re.compile(&#39;\((\d{1,2})?(.*?)(\d{1,4})?\)&#39;) # we want to catch them all, and some have no day or month, hence the optional quantifiers: `?`.

# And we want to make Python speak Italian:
ital2int = {&#39;gennaio&#39;: 1, &#39;febbraio&#39;: 2, &#39;marzo&#39;: 3, &#39;aprile&#39;: 4, &#39;maggio&#39;: 5, &#39;giugno&#39;: 6, &#39;luglio&#39;: 7, &#39;agosto&#39;: 8, &#39;settembre&#39;: 9, &#39;ottobre&#39;: 10, &#39;novembre&#39;: 11, &#39;dicembre&#39;: 12}

import sys
for ch in charters:
    try:
        d = charters[ch]
        i = summary_date.finditer(d[&#39;summary&#39;])
        dt = list(i)[-1] # Always the last parenthetical expression, in case there is more than one.
        if dt.group(2).strip() not in ital2int.keys():
            print(f&quot;chno. {d[&#39;chno&#39;]} fix the month {dt.group(2)}&quot;)
    except:
        print(d[&#39;chno&#39;], &quot;The usual suspects &quot;, sys.exc_info()[:2])
</code></pre>
<blockquote>
<p>Note: When using <code>try/except</code> blocks, you should usually trap <strong>specific</strong> errors in the except clause, like <code>ValueError</code> and the like; however, in <em>ad hoc</em> scripts like this, using <code>sys.exc_info</code> is a quick and dirty way to get information about any exception that may be raised. (The <a href="https://pymotw.com/3/sys/index.html">sys</a> module is full of such stuff, useful for debugging)</p>
</blockquote>
<p>Once you&#39;re satisfied that all the parenthetical date expressions are present and correct, and conform to your regular expression, you can parse them and add them to your data structure as dates rather than just strings. For this you can use the <code>datetime</code> module.</p>
<p>This module is part of the standard library, is a deep subject, and ought to be the subject of its own tutorial, given the importance of dates for historians. As with a lot of other python modules, a good introduction is Doug Hellmann&#39;s <a href="https://pymotw.com/3/datetime/index.html">PyMOTW</a>(module of the week). An even more able extension library is <a href="http://www.egenix.com/products/python/mxBase/mxDateTime/">mxDateTime</a>. Suffice it here to say that the <code>datetime.date</code> module expects parameters like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; from datetime import date
&gt;&gt;&gt; dt = date(1160, 12, 25)
&gt;&gt;&gt; dt.isoformat()
&#39;1160-12-25&#39;
</code></pre>
<p>So here&#39;s our loop to parse the dates at the end of the Italian summary lines and store them in our <code>charters</code> dictionary (remembering again that we want to modify our in-memory data structure <code>charters</code> created above):</p>
<pre><code class="language-python">summary_date = re.compile(&#39;\((\d{1,2})?(.*?)(\d{1,4})?\)&#39;)
from datetime import date
for ch in charters:
    c = charters[ch]
    i = summary_date.finditer(c[&#39;summary&#39;])
    for m in i:
        # remember &#39;i&#39; is an iterator so even if there is more than one
        # parenthetical expression in c[&#39;summary&#39;], the try clause will
        # succeed on the last one, or fail on all of them.
        try:
            yr = int(m.group(3))
            mo = ital2int[m.group(2).strip()]
            day = int(m.group(1))
            c[&#39;date&#39;] = date(yr, mo, day)
        except:
            c[&#39;date&#39;] = &quot;date won&#39;t parse, see summary line&quot;
</code></pre>
<p>Out of 803 charters, 29 wouldn&#39;t parse, mostly because the date included only month and year. You can store these as strings, but then you have two data types claiming to be dates. Or you could supply a 01 as the default day and thus store a Python date object, but Jan. 1, 1160 isn&#39;t the same thing as Jan. 1160 and thus distorts your metadata. Or you could just do as I have done and refer to the relevant source text: the Italian summary line in the printed edition.</p>
<p>Once you&#39;ve got date objects, you can do date arithmetic. Supposing we wanted to find all the charters dated to within 3 weeks of Christmas, 1160.</p>
<pre><code class="language-python"># Let&#39;s import the whole thing and use dot notation: datetime.date() etc.
import datetime

# a timedelta is a span of time
week = datetime.timedelta(weeks=1)

for ch in charters:
    try:
        dt = charters[ch][&#39;date&#39;]
        christmas = datetime.date(1160,12,25)
        if abs(dt - christmas) &lt; week * 3:
            print(f&quot;chno: {charters[ch][&#39;chno&#39;]}, date: {dt}&quot;)
    except:
        pass # avoid this idiom in production code
</code></pre>
<p>Which will give us this result:</p>
<pre><code class="language-terminal">chno: 790, date: 1160-12-14
chno: 791, date: 1160-12-15
chno: 792, date: 1161-01-01
chno: 793, date: 1161-01-04
chno: 794, date: 1161-01-05
chno: 795, date: 1161-01-05
chno: 796, date: 1161-01-10
chno: 797, date: 1161-01-10
chno: 798, date: 1161-01-06
</code></pre>
<p>Cool, huh?</p>
<h1 id="our-completed-data-structure">Our completed data structure</h1>
<p>Now we&#39;ve corrected our canonical text as much as we need to to differentiate between the various bits of meta-data that we want to capture, and we&#39;ve created a data structure in memory, our <code>charters</code> dictionary, by making 4 passes, each one extending and modifying the dictionary in memory.</p>
<ol>
<li>create the skeleton</li>
<li>separate out the <code>summary</code> and <code>marginal</code> lines and create dictionary fields for them.</li>
<li>collect and assign footnotes to their respective charters</li>
<li>parse the dates in the <code>summary</code> field, and add them to their respective charters</li>
</ol>
<p>Print out our resulting dictionary using <code>pprint(charters)</code> and you&#39;ll see something like this:</p>
<pre><code class="language-python">{
.
.
.
 52: {&#39;chid&#39;: &#39;GScriba_LII&#39;,
      &#39;chno&#39;: 52,
      &#39;date&#39;: datetime.date(1156, 3, 27),
      &#39;folio&#39;: &#39;[fo. 6 r.]&#39;,
      &#39;footnotes&#39;: [(1, &#39;Cancellato: m.&#39;)],
      &#39;marginal&#39;: &#39;no marginal]&#39;,
      &#39;pgno&#39;: 29,
      &#39;summary&#39;: &#39;I consoli di Genova riconoscono con sentenza il diritto di Romano di Casella di pagarsi sui beni di Gerardo Confector per un credito che aveva verso il medesimo (27 marzo 1156).&#39;,
      &#39;text&#39;: [&#39;    In pontili capituli consules E. Aurie, W. Buronus, Ogerius Ventus laudaverunt quod Romanus de Casella haberet in bonis Gerardi Confectoris s. .xxvi. denariorum et possit eos accipere sine contradicione eius et omnium pro eo. Hoc ideo quia, cum; Romanus ante ipsos inde conquereretur, ipso Gerardo debitum non negante, sed quod de usura esset obiiciendo, iuravit nominatus Romanus quod capitalis erat (1) et non de usura, unde ut supra laudaverunt , .MCLVI., sexto kalendas aprilis, indicione tercia.\n&#39;]},
 53: {&#39;chid&#39;: &#39;GScriba_LIII&#39;,
      &#39;chno&#39;: 53,
      &#39;date&#39;: datetime.date(1156, 3, 27),
      &#39;folio&#39;: &#39;[fo. 6 r.]&#39;,
      &#39;footnotes&#39;: [],
      &#39;marginal&#39;: &#39;Belmusti]&#39;,
      &#39;pgno&#39;: 29,
      &#39;summary&#39;: &quot;Maestro Arnaldo e Giordan nipote del fu Giovanni di Piacenza si obbligano di pagare una somma nell&#39;ottava della prossima Pasqua, per merce ricevuta (27 marzo 1156).&quot;,
      &#39;text&#39;: [&#39;  Testes Conradus Porcellus, Albericus, Vassallus Gambalixa, Petrus Artodi. Nos Arnaldus magister et Iordan nepos quondam Iohannis Placentie accepimus a te Belmusto tantum bracile unde promittimus dare tibi vel tuo certo misso lb. .xLIII. denariorum usque octavam proximi pasce, quod si non fecerimus penam dupli tibi stipulanti promittimus, bona pignori, possis unumquemque convenire de toto. Actum prope campanile Sancti Laurentii, millesimo centesimo .Lv., sexto kalendas aprilis, indictione tercia.\n&#39;]},
.
.
. etc.
}
</code></pre>
<p>Printing out your Python dictionary as a literal string is not a bad thing to do. For a text this size, the resulting file is perfectly manageable, can be mailed around usefully and read into a python repl session very simply using <code>eval()</code>, or pasted directly into a Python module file. On the other hand, if you want an even more reliable way to serialize it in an exclusively Python context, look into <a href="https://docs.python.org/3.7/library/pickle.html"><code>Pickle</code></a>. If you need to move it to some other context, JavaScript for example, or some <code>RDF</code> triple stores, Python&#39;s <a href="https://docs.python.org/3.7/library/json.html#module-json"><code>json</code></a> module will translate effectively. If you have to get some kind of XML output, I will be very sorry for you, but the <a href="http://lxml.de/"><code>lxml</code></a> python module may ease the pain a little.</p>
<h2 id="order-from-disorder-huzzah">Order from disorder, huzzah.</h2>
<p>Now that we have an ordered data structure, we can do many things with it. As a very simple example, let&#39;s append some code that just prints <code>charters</code> out as html for display on a web-site:</p>
<pre><code class="language-python">fout = open(&quot;your_page.html&quot;, &#39;w&#39;) # create a text file to write the html to

# write to the file your html header with some CSS formatting declarations
fout.write(&quot;&quot;&quot;
&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot;&gt;

&lt;html&gt;
&lt;head&gt;
  &lt;title&gt;Giovanni Scriba Vol. I&lt;/title&gt;
  &lt;style&gt;
    h1 {text-align: center; color: #800; font-size: 16pt; margin-bottom: 0px; margin-top: 16px;}
    ul {list-style-type: none;}
    .sep {color: #800; text-align: center}
    .charter {width: 650px; margin-left: auto; margin-right: auto; margin-top: 60px; border-top: double #800;}
    .folio {color: #777;}
    .summary {color: #777; margin: 12px 0px 12px 12px;}
    .marginal {color: red}
    .charter-text {margin-left: 16px}
    .footnotes
    .page-number {font-size: 60%}
  &lt;/style&gt;&lt;/head&gt;

&lt;body&gt;
&quot;&quot;&quot;)

# a loop that will write out a blob of html code for each of the charters in our dictionary:
for x in charters:

    # use a shallow copy so that charters[x] is not modified for this specialized purpose
    d = charters[x].copy()

    try:
        if d[&#39;footnotes&#39;]:
            # remember, this is a list of tuples. So you can feed them directly
            # to the string interpolation operator in the list comprehension.
            fnlist = [&quot;&lt;li&gt;(%s) %s&lt;/li&gt;&quot; % t for t in d[&#39;footnotes&#39;]]
            d[&#39;footnotes&#39;] = &quot;&lt;ul&gt;&quot; + &#39;&#39;.join(fnlist) + &quot;&lt;/ul&gt;&quot;
        else:
            d[&#39;footnotes&#39;] = &quot;&quot;

        d[&#39;text&#39;] = &#39; &#39;.join(d[&#39;text&#39;]) # d[&#39;text&#39;] is a list of strings

        blob = &quot;&quot;&quot;
            &lt;div&gt;
                &lt;div class=&quot;charter&quot;&gt;
                    &lt;h1&gt;%(chid)s&lt;/h1&gt;
                    &lt;div class=&quot;folio&quot;&gt;%(folio)s (pg. %(pgno)d)&lt;/div&gt;
                    &lt;div class=&quot;summary&quot;&gt;%(summary)s&lt;/div&gt;
                    &lt;div class=&quot;marginal&quot;&gt;%(marginal)s&lt;/div&gt;
                    &lt;div class=&quot;text&quot;&gt;%(text)s&lt;/div&gt;
                    &lt;div class=&quot;footnotes&quot;&gt;%(footnotes)s&lt;/div&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &quot;&quot;&quot;

        fout.write(blob % d)

        # `string % dictionary` is a neat trick for html templating
        # that makes use of python&#39;s string interpolation syntax
        # see: https://docs.python.org/3/tutorial/inputoutput.html#the-string-format-method

        fout.write(&quot;\n\n&quot;)
    except:
        # insert entries noting the absence of charters on the missing pg. 214
        erratum = &quot;&quot;&quot;
            &lt;div&gt;
                &lt;div class=&quot;charter&quot;&gt;
                    &lt;h1&gt;Charter no. %d is missing because the scan for Pg. 214 was ommited&lt;/h1&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            &quot;&quot;&quot;  % d[&#39;chno&#39;]

        fout.write(erratum)

fout.write(&quot;&quot;&quot;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;)
</code></pre>
<p>Drop the resulting file on a web browser, and you&#39;ve got a nicely formated electronic edition.</p>
<p>{% include figure.html filename=&quot;gs_gscriba207.png&quot; caption=&quot;html formatted charter example&quot; %}</p>
<p>Being able to do this with your, still mostly uncorrected, OCR output is not a trivial advantage. If you&#39;re serious about creating a clean, error free, electronic edition of anything, you&#39;ve got to do some serious proofreading. Having a source text formatted for reading is crucial; moreover, if your proofreader can change the font, spacing, color, layout, and so forth at will, you can increase their accuracy and productivity substantially. With this example in a modern web browser, tweaking those parameters with some simple CSS declarations is easy. Also, with some ordered HTML to work with, you might crowd-source the OCR error correction, instead of hiring that army of illiterate street urchins.</p>
<p>And, our original problem, OCR cleanup, is now much more tractable because we can target regular expressions for the specific sorts of metadata we have: errors in the Italian summary or in the Latin text? Or we could design search-and-replace routines just for specific charters, or groups of charters.</p>
<p>Beyond this though, there&#39;s lots you can do with an ordered data set, including feeding it back through a markup tool like the <a href="http://brat.nlplab.org">brat</a> as we did for the ChartEx project. Domain experts can then start adding layers of semantic tagging even if you don&#39;t do any further OCR error correction. Moreover, with an ordered dataset we can get all sorts of output, some other flavor of XML (if you must) for example: TEI (Text Encoding Initiative), or EAD (Encoded Archival Description). Or you could read your dataset directly into a relational database, or some kind of key/value store. All of these things are essentially impossible if you&#39;re working simply with a plain text file.</p>
<p>The bits of code above are in no way a turn-key solution for cleaning arbitrary OCR output. There is no such magic wand. The Google approach to scanning the contents of research libraries threatens to drown us in an ocean of bad data. Worse, it elides a fundamental fact of digital scholarship: digital sources are hard to get. Reliable, flexible, and useful digital texts require careful redaction and persistent curation. Google, Amazon, Facebook, <em>et alia</em> do not have to concern themselves with the quality of their data, just its quantity. Historians, on the other hand, must care first for the integrity of their sources.</p>
<p>The vast 18th and 19th century publishing projects, the <em>Rolls Series</em>, the <em>Monumenta Germaniae Historica</em>, and many others, bequeathed a treasure trove of source material to us by dint of a huge amount of very painstaking and detailed work by armies of dedicated and knowledgeable scholars. Their task was the same as ours: to faithfully transmit history&#39;s legacy from its earlier forms into a more modern form, thereby making it more widely accessible. We can do no less. We have powerful tools at our disposal, but while that may change the scale of the task, it does not change its nature.</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="generating-an-ordered-data-set-from-an-OCR-text-file/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Generating an Ordered Data Set from an OCR Text File\",\"layout\":\"lesson\",\"date\":\"2014-11-25T00:00:00.000Z\",\"authors\":[\"Jon Crump\"],\"reviewers\":[\"Brandon Hawk\"],\"editors\":[\"Fred Gibbs\"],\"difficulty\":3,\"exclude_from_check\":[\"review-ticket\"],\"activity\":\"transforming\",\"topics\":[\"data-manipulation\"],\"abstract\":\"This tutorial illustrates strategies for taking raw OCR output from a scanned text, parsing it to isolate and correct essential elements of metadata, and generating an ordered data set (a python dictionary) from it.\",\"redirect_from\":\"\u002Flessons\u002Fgenerating-an-ordered-data-set-from-an-OCR-text-file\",\"avatar_alt\":\"A small case with a set of books\",\"doi\":\"10.46430\u002Fphen0036\"},\"html_body\":\"\u003Ch1 id=\\\"generating-an-ordered-data-set-from-a-text-file\\\"\u003EGenerating an Ordered Data Set from a Text File\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"lesson-goals\\\"\u003ELesson goals\u003C\u002Fh2\u003E\\n\u003Cp\u003EThis tutorial illustrates strategies for taking raw OCR output from a scanned text, parsing it to isolate and correct essential elements of metadata, and generating an ordered data set (a python dictionary) from it. These illustrations are specific to a particular text, but the overall strategy, and some of the individual procedures, can be adapted to organize any scanned text, even if it doesn&#39;t look like this one.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"introduction\\\"\u003EIntroduction\u003C\u002Fh2\u003E\\n\u003Cp\u003EIt is often the case that historians involved in digital projects wish to work with digitized texts, so they think &quot;OK, I&#39;ll just scan this fabulously rich and useful collection of original source material and do wonderful things with the digital text that results&quot;. (Those of us who have done this, now smile ruefully). Such historians quickly discover that even the best OCR results in unacceptably high error rates. So the historian now thinks &quot;OK I&#39;ll get some grant money, and I&#39;ll enlist the help of an army of RAs\u002FGrad students\u002FUndergrads\u002FBarely literate street urchins, to correct errors in my OCR output. (We smile again, even more sadly now).\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Cp\u003EThere is little funding for this kind of thing. Increasingly, projects in the humanities have focused upon NLP\u002FData Mining\u002FMachine Learning\u002FGraph Analysis, and the like, frequently overlooking the fundamental problem of generating useable digital texts. The presumption has often been, well, Google scanned all that stuff didn&#39;t they? What&#39;s the matter with their scans?\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EEven if you had such an army of helpers, proof-reading the OCR output of, say, a collection of twelfth century Italian charters transcribed and published in 1935, will quickly drive them all mad, make their eyes bleed, and the result will still be a great wad of text containing a great many errors, and you will \u003Cstrong\u003Estill\u003C\u002Fstrong\u003E have to do \u003Cstrong\u003Esomething\u003C\u002Fstrong\u003E to it before it becomes useful in any context.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003EGoing through a text file line by line and correcting OCR errors one at a time is hugely error-prone, as any proof reader will tell you. There are ways to automate some of this tedious work. A scripting language like Perl or Python can allow you to search your OCR output text for common errors and correct them using &quot;Regular Expressions&quot;, a language for describing patterns in text. (So called because they express a \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FRegular_language\\\"\u003E&quot;regular language&quot;\u003C\u002Fa\u003E. See L.T. O&#39;Hara&#39;s \u003Ca href=\\\"\u002Flessons\u002Fcleaning-ocrd-text-with-regular-expressions.html\\\"\u003Etutorial on Regular Expressions\u003C\u002Fa\u003E here at the PM.) Regular Expressions, however, are only useful if the expressions you are searching for are ... well ... regular. Unfortunately, much of what you have in OCR output is highly \u003Cem\u003Eirregular\u003C\u002Fem\u003E. If you could impose some order on it: create an ordered data set out of it, your Regular Expression tools would become much more powerful.\u003C\u002Fp\u003E\\n\u003Cp\u003EConsider, for example, what happens if your OCR interpreted a lot of strings like this &quot;21 July, 1921&quot; as &quot;2l July, 192l&quot;, turning the integer &#39;1&#39; into an &#39;l&#39;. You would love to be able to write a search and replace script that would turn all instances of 2l into 21, but then what would happen if you had lots of occurrences of strings like this in your text: &quot;2lb. hammer&quot;. You&#39;d get a bunch of 21b. hammers; not what you want. If only you could tell your script: only change 2l into 21 in sections where there are dates, not weights. If you had an ordered data set, you could do things like that.\u003C\u002Fp\u003E\\n\u003Cp\u003EVery often the texts that historians wish to digitize are, in fact, ordered data sets: ordered collections of primary source documents, or a legal code say, or a cartulary. But the editorial structure imposed upon such resources is usually designed for a particular kind of data retrieval technology i.e., a codex, a book. For a digitized text you need a different kind of structure. If you can get rid of the book related infrastructure and reorganize the text according to the sections and divisions that you&#39;re interested in, you will wind up with data that is much easier to do search and replace operations on, and as a bonus, your text will become immediately useful in a variety of other contexts as well.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis is where a scripting language like Python comes very much in handy. For our project we wanted to prepare some of the documents from a 12th century collection of \u003Cem\u003Eimbreviatura\u003C\u002Fem\u003E from the Italian scribe known as \u003Ca href=\\\"http:\u002F\u002Fwww.worldcat.org\u002Foclc\u002F17591390\\\"\u003EGiovanni Scriba\u003C\u002Fa\u003E so that they could be marked up by historians for subsequent NLP analysis or potentially for other purposes as well. The pages of the 1935 published edition look like this.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;gs_pg110.png&quot; caption=&quot;GS page 110&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe OCR output from such scans look like this even after some substantial clean-up (I&#39;ve wrapped the longest lines so that they fit here):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E110    MARIO CHIAUDANO MATTIA MORESCO\\n    professi sunt Alvernacium habere de i;psa societate lb. .c., in reditu\\n    tracto predicto capitali .ccc. lb. proficuum. debent dividere per medium. Ultra\\n    vero .cc. lb. capitalis Ingo de Volta lb. .xiv. habet quas cum ipso capitali de\\n    scicietate extrahere debet. Dedit preterea prefatus Ingo de Volta licenciam (1)\\n    ipsi Ingoni Nocentio portandi lb. .xxxvII. 2 Oberti Spinule et Ib. .xxvII.\\n    Wuilielmi Aradelli. Actum ante domum W. Buronis .MCLVII., .iiii. kalendas\\n    iulias, indicione quarta (2).\\nL f o. 26 v.] .    CCVIII.\\nIngone Della Volta si obbliga verso Ingone Nocenzio di indennizzarlo di ogni\\ndanno che gli fosse derivato dalle societa che egli aveva con i suoi figli (28\\ngiugno 1157).\\nTestes Ingonis Nocentii] .\\n    Die loco (3) ,predicto et testibus Wuilielmo Burone, Bono Iohanne\\n    Malfiiastro, Anselmo de Cafara, W. de Racedo, Wuilielmo Callige Pallii. Ego Ingo\\n    de Volta promitto tibi Ingoni Nocentio quod si aliquod dampnum acciderit tibi\\n    pro societate vel societatibus quam olim habueris cum filiis meis ego illud\\n    totum tibi restaurato et hoc tibi promitto sub pena dupli de quanto inde dampno\\n    habueris. Do tibi preterea licentiam accipiendi bisancios quos ultra mare\\n    acciipere debeo et inde facias tona fide quicquid tibi videbitur et inde ab omni\\n    danpno te absolvo quicquid inde contingerit.\\nCCIX.\\n    Guglielmo di Razedo dichiara d&#39;aver ricevuto in societatem da Guglielmo\\nBarone una somma di denaro che portera laboratum ultramare (28 giugno 1157).\\nWuilielmi Buronis] .\\n        Testes Anselmus de Cafara, Albertus de Volta, W. Capdorgol, Corsus\\nSerre, Angelotus, Ingo Noncencius. Ego W. de Raeedo profiteor me accepisse a te\\nWuilielmo Burone lb. duocentum sexaginta tre et s. .XIII. 1\u002F2 in societatem ad\\nquartam proficui, eas debeo portare laboratum ultra mare et inde quo voluero, in\\nreditu,\\n(11 Licentiam in sopralinea in potestatem cancellato.\\n(2) A margine le postille: Pro Ingone Nocentio scripta e due pro Alvernacio.\\n(3) Cancellato: et testibus supradictis.\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIn the scan of the original, the reader&#39;s eye readily parses the page: the layout has meaning. But as you can see, reduced to plain text like this, none of the metadata implied by the page layout and typography can be differentiated by automated processes.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou can see from the scan that each charter has the following metadata associated with it.\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ECharter number\u003C\u002Fli\u003E\\n\u003Cli\u003EPage number\u003C\u002Fli\u003E\\n\u003Cli\u003EFolio number\u003C\u002Fli\u003E\\n\u003Cli\u003EAn Italian summary, ending in a date of some kind\u003C\u002Fli\u003E\\n\u003Cli\u003EA line, usually ending with a &#39;]&#39; that marks a marginal notation in the original\u003C\u002Fli\u003E\\n\u003Cli\u003EFrequently a collection of in-text numbered footnote markers, whose text appears at the bottom of each page, sequentially numbered, and restarting from 1 on each new page.\u003C\u002Fli\u003E\\n\u003Cli\u003EThe Latin text of the charter itself\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EThis is typical of such resources, though editorial conventions will vary widely. The point is: this is an \u003Cstrong\u003Eordered\u003C\u002Fstrong\u003E data set, not just a great big string of characters. With some fairly straightforward Python scripts, we can turn our OCR output into an ordered data set, in this case, a \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F3.7\u002Ftutorial\u002Fdatastructures.html#dictionaries\\\"\u003Epython dictionary\u003C\u002Fa\u003E, \u003Cstrong\u003Ebefore\u003C\u002Fstrong\u003E we start trying to proofread the Latin charter texts. With such an ordered data set in hand, we can do proofreading, and potentially many other kinds of tasks, much more effectively.\u003C\u002Fp\u003E\\n\u003Cp\u003ESo, the aim of this tutorial is to take a plain text file, like the OCR output above and turn it into a python dictionary with fields for the Latin text of the charter and for each of the metadata elements mentioned above:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E{\\n.\\n.\\n.\\n 52: {&#39;chid&#39;: &#39;GScriba_LII&#39;,\\n      &#39;chno&#39;: 52,\\n      &#39;date&#39;: datetime.date(1156, 3, 27),\\n      &#39;folio&#39;: &#39;[fo. 6 r.]&#39;,\\n      &#39;footnotes&#39;: [(1, &#39;Cancellato: m.&#39;)],\\n      &#39;marginal&#39;: &#39;no marginal]&#39;,\\n      &#39;pgno&#39;: 29,\\n      &#39;summary&#39;: &#39;I consoli di Genova riconoscono con sentenza il diritto di Romano di Casella di pagarsi sui beni di Gerardo Confector per un credito che aveva verso il medesimo (27 marzo 1156).&#39;,\\n      &#39;text&#39;: [&#39;    In pontili capituli consules E. Aurie, W. Buronus, Ogerius Ventus laudaverunt quod Romanus de Casella haberet in bonis Gerardi Confectoris s. .xxvi. denariorum et possit eos accipere sine contradicione eius et omnium pro eo. Hoc ideo quia, cum; Romanus ante ipsos inde conquereretur, ipso Gerardo debitum non negante, sed quod de usura esset obiiciendo, iuravit nominatus Romanus quod capitalis erat (1) et non de usura, unde ut supra laudaverunt , .MCLVI., sexto kalendas aprilis, indicione tercia.\\\\n&#39;]},\\n 53: {&#39;chid&#39;: &#39;GScriba_LIII&#39;,\\n      &#39;chno&#39;: 53,\\n      &#39;date&#39;: datetime.date(1156, 3, 27),\\n      &#39;folio&#39;: &#39;[fo. 6 r.]&#39;,\\n      &#39;footnotes&#39;: [],\\n      &#39;marginal&#39;: &#39;Belmusti]&#39;,\\n      &#39;pgno&#39;: 29,\\n      &#39;summary&#39;: &quot;Maestro Arnaldo e Giordan nipote del fu Giovanni di Piacenza si obbligano di pagare una somma nell&#39;ottava della prossima Pasqua, per merce ricevuta (27 marzo 1156).&quot;,\\n      &#39;text&#39;: [&#39;  Testes Conradus Porcellus, Albericus, Vassallus Gambalixa, Petrus Artodi. Nos Arnaldus magister et Iordan nepos quondam Iohannis Placentie accepimus a te Belmusto tantum bracile unde promittimus dare tibi vel tuo certo misso lb. .xLIII. denariorum usque octavam proximi pasce, quod si non fecerimus penam dupli tibi stipulanti promittimus, bona pignori, possis unumquemque convenire de toto. Actum prope campanile Sancti Laurentii, millesimo centesimo .Lv., sexto kalendas aprilis, indictione tercia.\\\\n&#39;]},\\n.\\n.\\n. etc.\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ERemember, this is just a text representation of a data structure that lives in computer memory. Python calls this sort of structure a &#39;dictionary&#39;, other programming languages may call it a &#39;hash&#39;, or an &#39;associative array&#39;. The point is that it is infinitely easier to do any sort of programmatic analysis or manipulation of a digital text if it is in such a form, rather than in the form of a plain text file. The advantage is that such a data structure can be queried, or calculations can be performed on the data, without first having to parse the text.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"a-couple-of-useful-functions-before-we-start\\\"\u003EA couple of useful functions before we start:\u003C\u002Fh2\u003E\\n\u003Cp\u003EWe&#39;re going to borrow a couple of functions written by others. They both represent some pretty sophisticated programming. Understanding what&#39;s going on in these functions is instructive, but not necessary. Reading and using other people&#39;s code is how you learn programming, and is the soul of the Open-Source movement. Even if you don&#39;t fully understand how somebody does it, you can nevertheless test functions like this to see that they reliably do what they say they can, and then just apply it to your immediate problem if they are relevant.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"levenshtein-distance\\\"\u003ELevenshtein distance\u003C\u002Fh3\u003E\\n\u003Cp\u003EYou will note that some of the metadata listed above is page-bound and some of it is charter-bound. Getting these untangled from each other is our aim. There is a class of page-bound data that is useless for our purposes, and only meaningful in the context of a physical book: page headers and footers. In our text, these look like this on \u003Cem\u003Erecto\u003C\u002Fem\u003E leaves (in a codex, a book, \u003Cem\u003Erecto\u003C\u002Fem\u003E is the right-side page, and \u003Cem\u003Everso\u003C\u002Fem\u003E its reverse, the left-side page)\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;gs_recto_header.png&quot; caption=&quot;recto header&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003Eand this on \u003Cem\u003Everso\u003C\u002Fem\u003E leaves:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;gs_verso_header.png&quot; caption=&quot;verso header&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EWe&#39;d like to preserve the page number information for each charter on the page, but the header text isn&#39;t useful to us and will just make any search and replace operation more difficult. So we&#39;d like to find header text and replace it with a string that&#39;s easy to find with a Regular Expression, and store the page number.\u003C\u002Fp\u003E\\n\u003Cp\u003EUnfortunately, regular expressions won&#39;t help you much here. This text can appear on any line of our OCR output text, and the ways in which OCR software can foul it up are effectively limitless. Here are some examples of page headers, both \u003Cem\u003Erecto\u003C\u002Fem\u003E and \u003Cem\u003Everso\u003C\u002Fem\u003E in our raw OCR output.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E260    11141110 CH[AUDANO MATTIA MORESCO\\nIL CIRTOL4RE DI CIOVINN1 St&#39;Itlltl    269\\nIL CJIRTOL.%RE DI G:OVeNNl FIM P%    297\\nIL CIP.TQLIRE DI G&#39;OVeNNI SCI Dt    r.23\\n332    T1uu:0 CHIAUDANO M:11TIA MGRESCO\\nIL CIRTOL.&#39;RE DI G:OV.I\\\\N( sca:FR    339\\n342    NI .\\\\ßlO CHIAUDANO 9LtTTIA MORESCO\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThese strings are not regular enough to reliably find with regular expressions; however, if you know what the strings are \u003Cem\u003Esupposed\u003C\u002Fem\u003E to look like, you can compose some kind of string similarity algorithm to test each string against an exemplar and measure the likelihood that it is a page header. Fortunately, I didn&#39;t have to compose such an algorithm, Vladimir Levenshtein did it for us in 1965 (see: \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLevenshtein_distance\\\"\u003Ehttp:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLevenshtein_distance\u003C\u002Fa\u003E). A computer language can encode this algorithm in any number of ways; here&#39;s an effective Python function that will work for us:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E&#39;&#39;&#39;\\nCode ripped from https:\u002F\u002Fwww.datacamp.com\u002Fcommunity\u002Ftutorials\u002Ffuzzy-string-python\\n&#39;&#39;&#39;\\ndef lev(seq1, seq2):\\n    &quot;&quot;&quot; levenshtein_ratio_and_distance:\\n        For all i and j, distance[i,j] will contain the Levenshtein\\n        distance between the first i characters of seq1 and the\\n        first j characters of seq2\\n    &quot;&quot;&quot;\\n    # Initialize matrix of zeros\\n    rows = len(seq1)+1\\n    cols = len(seq2)+1\\n    distance = np.zeros((rows,cols),dtype = int)\\n\\n    # Populate matrix of zeros with the indeces of each character of both strings\\n    for i in range(1, rows):\\n        for k in range(1,cols):\\n            distance[i][0] = i\\n            distance[0][k] = k\\n\\n    # Iterate over the matrix to compute the cost of deletions,insertions and\u002For substitutions    \\n    for col in range(1, cols):\\n        for row in range(1, rows):\\n            if seq1[row-1] == seq2[col-1]:\\n                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\\n            else:\\n                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\\n                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\\n                cost = 1\\n            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\\n                                 distance[row][col-1] + 1,          # Cost of insertions\\n                                 distance[row-1][col-1] + cost)     # Cost of substitutions\\n\\n    return distance[row][col]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAgain, this is some pretty sophisticated programming, but for our purposes all we need to know is that the \u003Ccode\u003Elev()\u003C\u002Fcode\u003E function takes two strings as parameters and returns a number that indicates the &#39;string distance&#39; between them, or, how many changes had to be made to turn the first string into the second. So: \u003Ccode\u003Elev(&quot;fizz&quot;, &quot;buzz&quot;)\u003C\u002Fcode\u003E returns &#39;2&#39;\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"roman-to-arabic-numerals\\\"\u003ERoman to Arabic numerals\u003C\u002Fh3\u003E\\n\u003Cp\u003EYou&#39;ll also note that in the published edition, the charters are numbered with roman numerals. Converting roman numerals into arabic is an instructive puzzle to work out in Python. Here&#39;s the cleanest and most elegant solution I know:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edef rom2ar(rom):\\n    &quot;&quot;&quot; From the Python tutor mailing list:\\n    János Juhász janos.juhasz at VELUX.com\\n    returns arabic equivalent of a Roman numeral &quot;&quot;&quot;\\n    roman_codec = {&#39;M&#39;:1000, &#39;D&#39;:500, &#39;C&#39;:100, &#39;L&#39;:50, &#39;X&#39;:10, &#39;V&#39;:5, &#39;I&#39;:1}\\n    roman = rom.upper()\\n    roman = list(roman)\\n    roman.reverse()\\n    decimal = [roman_codec[ch] for ch in roman]\\n    result = 0\\n\\n    while len(decimal):\\n        act = decimal.pop()\\n        if len(decimal) and act &lt; max(decimal):\\n            act = -act\\n        result += act\\n\\n    return result\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E(run &lt;\u003Ca href=\\\"\u002Fassets\u002FRoman_to_Arabic.txt\\\"\u003Ethis little script\u003C\u002Fa\u003E&gt; to see in detail how \u003Ccode\u003Erome2ar\u003C\u002Fcode\u003E works. Elegant programming like this can offer insight; like poetry.)\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"some-other-things-well-need\\\"\u003ESome other things we&#39;ll need:\u003C\u002Fh2\u003E\\n\u003Cp\u003EAt the top of your Python module, you&#39;re going to want to import some python modules that are a part of the standard library. (see Fred Gibbs&#39;s tutorial \u003Ca href=\\\"\u002Flessons\u002Finstalling-python-modules-pip\\\"\u003E\u003Cem\u003EInstalling Python Modules with pip\u003C\u002Fem\u003E\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Cp\u003EFirst among these is the &quot;re&quot; (regular expression) module \u003Ccode\u003Eimport re\u003C\u002Fcode\u003E. Regular expressions are your friends. However, bear in mind Jamie Zawinski&#39;s quip:\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ESome people, when confronted with a problem, think &quot;I know, I&#39;ll use regular expressions.&quot; Now they have two problems.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003E (Again, have a look at L.T. O&#39;Hara&#39;s introduction here at the Programming Historian \u003Ca href=\\\"\u002Flessons\u002Fcleaning-ocrd-text-with-regular-expressions.html\\\"\u003ECleaning OCR’d text with Regular Expressions\u003C\u002Fa\u003E)\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EAlso: \u003Ccode\u003Efrom pprint import pprint\u003C\u002Fcode\u003E. \u003Ccode\u003Epprint\u003C\u002Fcode\u003E is just a pretty-printer for python objects like lists and dictionaries. You&#39;ll want it because python dictionaries are much easier to read if they are formatted.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EAnd: \u003Ccode\u003Efrom collections import Counter\u003C\u002Fcode\u003E. We&#39;ll want this for the \u003Ca href=\\\"#footnotes\\\"\u003EFind and normalize footnote markers and texts\u003C\u002Fa\u003E section below. This is not really necessary, but we&#39;ll do some counting that would require a lot of lines of fiddly code and this will save us the trouble. The collections module has lots of deep magic in it and is well worth getting familiar with. (Again, see Doug Hellmann&#39;s PyMOTW for the \u003Ca href=\\\"https:\u002F\u002Fpymotw.com\u002F3\u002Fcollections\u002Findex.html\\\"\u003Ecollections\u003C\u002Fa\u003E module. I should also point out that his book \u003Ca href=\\\"https:\u002F\u002Fdoughellmann.com\u002Fbooks\u002Fthe-python-3-standard-library-by-example\u002F\\\"\u003E\u003Cem\u003EThe Python Standard Library By Example\u003C\u002Fem\u003E\u003C\u002Fa\u003E is one well worth having.)\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch2 id=\\\"a-very-brief-review-of-regular-expressions-as-they-are-implemented-in-python\\\"\u003EA very brief review of regular expressions as they are implemented in python\u003C\u002Fh2\u003E\\n\u003Cp\u003EL.T. O&#39;Hara&#39;s \u003Ca href=\\\"\u002Flessons\u002Fcleaning-ocrd-text-with-regular-expressions.html\\\"\u003Eintroduction\u003C\u002Fa\u003E to using python flavored regular expressions is invaluable. In this context we should review a couple of basic facts about Python&#39;s implementation of regular expressions, the \u003Ccode\u003Ere\u003C\u002Fcode\u003E module, which is part of Python&#39;s standard library.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Ccode\u003Ere.compile()\u003C\u002Fcode\u003E creates a regular expression object that has a number of methods. You should be familiar with \u003Ccode\u003E.match()\u003C\u002Fcode\u003E, and \u003Ccode\u003E.search()\u003C\u002Fcode\u003E, but also \u003Ccode\u003E.findall()\u003C\u002Fcode\u003E and \u003Ccode\u003E.finditer()\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EBear in mind the difference between \u003Ccode\u003E.match()\u003C\u002Fcode\u003E and \u003Ccode\u003E.search()\u003C\u002Fcode\u003E: \u003Ccode\u003E.match()\u003C\u002Fcode\u003E will only match at the \u003Cstrong\u003Ebeginning\u003C\u002Fstrong\u003E of a line, whereas \u003Ccode\u003E.search()\u003C\u002Fcode\u003E will match anywhere in the line \u003Cstrong\u003Ebut then it stops\u003C\u002Fstrong\u003E, it&#39;ll \u003Cstrong\u003Eonly\u003C\u002Fstrong\u003E return the first match it finds.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003E.match()\u003C\u002Fcode\u003E and \u003Ccode\u003E.search()\u003C\u002Fcode\u003E return match objects. To retrieve the matched string you need \u003Ccode\u003Emymatch.group(0)\u003C\u002Fcode\u003E. If your compiled regular expression has grouping parentheses in it (like our &#39;slug&#39; regex below), you can retrieve those substrings of the matched string using \u003Ccode\u003Emymatch.group(1)\u003C\u002Fcode\u003E etc.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003E.findall()\u003C\u002Fcode\u003E and \u003Ccode\u003E.finditer()\u003C\u002Fcode\u003E will return \u003Cstrong\u003Eall\u003C\u002Fstrong\u003E occurrences of the matched string; \u003Ccode\u003E.findall()\u003C\u002Fcode\u003E returns them as a list of strings, but .finditer() returns an \u003Cstrong\u003Eiterator of match objects\u003C\u002Fstrong\u003E. (read the docs on the method \u003Ca href=\\\"hhttps:\u002F\u002Fdocs.python.org\u002F3.7\u002Flibrary\u002Fre.html#re.finditer\\\"\u003E.finditer()\u003C\u002Fa\u003E.)\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch1 id=\\\"iterative-processing-of-text-files\\\"\u003EIterative processing of text files\u003C\u002Fh1\u003E\\n\u003Cp\u003EWe&#39;ll start with a single file of OCR output. We will iteratively generate new, corrected versions of this file by using it as input for our python scripts. Sometimes our script will make corrections automatically, more often, our scripts will simply alert us to where problems lie in the input file, and we will make corrections manually. So, for the first several operations we&#39;re going to want to produce new and revised text files to use as input for our subsequent operations. Every time you produce a text file, you should version it and duplicate it so that you can always return to it. The next time you run your code (as you&#39;re developing it) you might alter the file in an unhelpful way and it&#39;s easiest just to restore the old version.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe code in this tutorial is highly edited; it is \u003Cstrong\u003Enot\u003C\u002Fstrong\u003E comprehensive. As you continue to refine your input files, you will write lots of little \u003Cem\u003Ead hoc\u003C\u002Fem\u003E scripts to check on the efficacy of what you&#39;ve done so far. Versioning will ensure that such experimentation will not destroy any progress that you&#39;ve made.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"a-note-on-how-to-deploy-the-code-in-this-tutorial\\\"\u003EA note on how to deploy the code in this tutorial:\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe code in this tutorial is for Python 3.\u003C\u002Fp\u003E\\n\u003Cp\u003EWhen you write code in a text file and then execute it, either at the command line, or from within your text editor or IDE, the Python interpreter executes the code line by line, from top to bottom. So, often the code on the bottom of the page will depend on code above it.\u003C\u002Fp\u003E\\n\u003Cp\u003EOne way to use the code snippets in section 2 might be to have all of them in a single file and comment out the bits that you don&#39;t want to run. Each time you execute the file, you will want to be sure that there is a logical control flow from the \u003Ccode\u003E#!\u003C\u002Fcode\u003E line at the top, through your various \u003Ccode\u003Eimport\u003C\u002Fcode\u003Es and assignment of global variables, and each loop, or block.\u003C\u002Fp\u003E\\n\u003Cp\u003EOr, each of the subsections in section 2 can also be treated as a separate script, each would then have to do its own \u003Ccode\u003Eimport\u003C\u002Fcode\u003Eing and assignment of global variables.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn section 3, &quot;Creating the Dictionary&quot;, you will be operating on a data set in computer memory (the \u003Ccode\u003Echarters\u003C\u002Fcode\u003E dictionary) that will be generated from the latest, most correct, input text you have. So you will want to maintain a single python module in which you define the dictionary at the top, along with your \u003Ccode\u003Eimport\u003C\u002Fcode\u003E statements and the assignment of global variables, followed by each of the four loops that will populate and then modify that dictionary.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E#!\u002Fusr\u002Fbin\u002Fpython\\n\\nimport re\\nfrom pprint import pprint\\nfrom collections import Counter\\n\\n# followed by any global variables you will need, like:\\n\\nn = 0\\nthis_folio  = &#39;[fo. 1 r.]&#39;\\nthis_page = 1\\n\\n# compiled regular expressions like:\\nslug = re.compile(&quot;(\\\\[~~~~\\\\sGScriba_)(.*)\\\\s::::\\\\s(\\\\d+)\\\\s~~~~\\\\]&quot;)\\nfol = re.compile(&quot;\\\\[fo\\\\.\\\\s?\\\\d+\\\\s?[rv]\\\\.\\\\s?\\\\]&quot;)\\npgbrk = re.compile(&quot;~~~~~ PAGE (\\\\d+) ~~~~~&quot;)\\n\\n# the canonical file you will be reading from\\nfin = open(&quot;\u002Fpath\u002Fto\u002Fyour\u002Fcurrent\u002Fcanonical.txt&quot;, &#39;r&#39;)\\nGScriba = fin.readlines()\\n\\n\\n# then the empty dictionary:\\ncharters = dict()\\n\\n# followed by the 4 &#39;for&#39; loops in section 2 that will populate and then modify this dictionary\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"chunk-up-the-text-by-pages\\\"\u003EChunk up the text by pages\u003C\u002Fh2\u003E\\n\u003Cp\u003EFirst of all, we want to find all the page headers, both \u003Cem\u003Erecto\u003C\u002Fem\u003E and \u003Cem\u003Everso\u003C\u002Fem\u003E and replace them with consistent strings that we can easily find with a regular expression. The following code looks for lines that are similar to what we know are our page headers to within a certain threshold. It will take some experimentation to find what this threshold is for your text. Since my \u003Cem\u003Erecto\u003C\u002Fem\u003E and \u003Cem\u003Everso\u003C\u002Fem\u003E headers are roughly the same length, both have the same similarity score of 26.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ENOTA BENE: The \u003Ccode\u003Elev()\u003C\u002Fcode\u003E function described above returns a measure of the &#39;distance&#39; between two strings, so, the shorter the page header string, the more likely it is that this trick will not work. If your page header is just &quot;Header&quot;, then any line comprised of a six letter word might give you a string distance of 6, eg: \u003Ccode\u003Elev(&quot;Header&quot;, &quot;Foobar&quot;)\u003C\u002Fcode\u003E returns &#39;6&#39;, leaving you none the wiser. In our text, however, the header strings are long and complex enough to give you meaningful scores, eg:\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003E\u003Ccode\u003Elev(&quot;RANDOM STRING OF SIMILAR LENGTH:    38&quot;, &#39;IL CARTOLARE DI GIOVANNI SCRIBA&#39;)\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003Ereturns 33, but one of our header strings, even badly mangled by the OCR, returns 20:\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ccode\u003Elev(&quot;IL CIRTOL4RE DI CIOVINN1 St&#39;Itlltl     269&quot;, &#39;IL CARTOLARE DI GIOVANNI SCRIBA&#39;)\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003ESo we can use \u003Ccode\u003Elev()\u003C\u002Fcode\u003E to find and modify our header strings thus:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# At the top, do the importing you need and define the lev() function as described above, and then:\\n\\nfin = open(&quot;our_base_OCR_result.txt&quot;, &#39;r&#39;) # read our OCR output text\\nfout = open(&quot;out1.txt&quot;, &#39;w&#39;) # create a new textfile to write to when we&#39;re ready\\nGScriba = fin.readlines() # turn our input file into a list of lines\\n\\nfor line in GScriba:\\n    # get a Levenshtein distance score for each line in the text\\n    recto_lev_score = lev(line, &#39;IL CARTOLARE DI GIOVANNI SCRIBA&#39;)\\n    verso_lev_score = lev(line, &#39;MARIO CHIAUDANO - MATTIA MORESCO&#39;)\\n\\n    # you want to use a score that&#39;s as high as possible,\\n    # but still finds only potential page header texts.\\n    if recto_lev_score &lt; 26 :\\n\\n        # If we increment a variable &#39;n&#39; to count the number of headers we&#39;ve found,\\n        # then the value of that variable should be our page number.\\n        n += 1\\n        print(f&quot;recto: {recto_lev_score} {line}&quot;)\\n\\n        # Once we&#39;ve figured out our optimal &#39;lev&#39; score, we can &#39;uncomment&#39;\\n        # all these `fout.write()` lines to write out our new text file,\\n        # replacing each header with an easy-to-find string that contains\\n        # the page number: our variable &#39;n&#39;.\\n\\n        #fout.write(&quot;~~~~~ PAGE %d ~~~~~\\\\n\\\\n&quot; % n)\\n    elif verso_lev_score &lt; 26 :\\n        n += 1\\n        print(f&quot;verso: {verso_lev_score} {line}&quot;)\\n        #fout.write(&quot;~~~~~ PAGE %d ~~~~~\\\\n\\\\n&quot; % n)\\n    else:\\n        #fout.write(line)\\n        pass\\n\\nprint(n)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThere&#39;s a lot of calculation going on in the \u003Ccode\u003Elev()\u003C\u002Fcode\u003E function. It isn&#39;t very efficient to call it on every line in our text, so this might take some time, depending on how long our text is. We&#39;ve only got 803 charters in vol. 1. That&#39;s a pretty small number. If it takes 30 seconds, or even a minute, to run our script, so be it.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf we run this script on our OCR output text, we get output that looks like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E.\\n.\\n.\\nverso: 8 426    MARIO CHIAUDANO MAITIA MORESCO\\nrecto: 5 IL CARTOLARE DI GIOVANNI SCRIBA    427\\nverso: 11 , ,    428    MARIO CHIAUDANO MATTIA MORESCO\\nrecto: 5 IL CARTOLARE DI GIOVANNI SCRIBA    499\\nverso: 7 430    MARIO CHIAUDANO MATTIA MORESCO\\nrecto: 5 IL CARTOLARE DI GIOVANNI SCRIBA    431\\nverso: 8 432    MARIO CHIAUDASO MATTIA MORESCO\\n430\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EFor each line, the output tells us that it&#39;s page \u003Cem\u003Everso\u003C\u002Fem\u003E or \u003Cem\u003Erecto\u003C\u002Fem\u003E, the Levenshtein &quot;score&quot;, and then the text of the line (complete with all the errors in it. Note that the OCR misread the pg. number for pg. 429). The lower the Levenshtein &quot;score&quot;, the closer the line is to the model you&#39;ve given it.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis tells you that the script found 430 lines that are probably page headers. You know how many pages there should be, so if the script didn&#39;t find all the headers, you can go through the output looking at the page numbers, find the pages it missed, and fix the headers manually, then repeat until the script finds all the page headers.\u003C\u002Fp\u003E\\n\u003Cp\u003EOnce you&#39;ve found and fixed the headers that the script didn&#39;t find, you can then write out the corrected text to a new file that will serve as the basis for the other operations below. So, instead of\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Equicquid volueris sine omni mea et\\n(1) Spazio bianco nel ms.\\n\\n12    MARIO CSIAUDANO MATTIA MORESCO\\nheredum meorum contradicione. Actum in capitulo .MCLV., mensis iulii, indicione secunda.\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003Ewe&#39;ll have a textfile like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Equicquid volueris sine omni mea et\\n(1) Spazio bianco nel ms.\\n\\n~~~~~ PAGE 12 ~~~~~\\n\\nheredum meorum contradicione. Actum in capitulo .MCLV., mensis iulii, indicione secunda.\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENote that for many of the following operations, we will use \u003Ccode\u003EGScriba = fin.readlines()\u003C\u002Fcode\u003E so \u003Ccode\u003EGScriba\u003C\u002Fcode\u003E will be a \u003Cstrong\u003Epython list\u003C\u002Fstrong\u003E of the lines in our input text. Keep this firmly in mind, as the \u003Ccode\u003Efor\u003C\u002Fcode\u003E loops that we will use will depend on the fact that we will iterate through the lines of our text \u003Cstrong\u003EIn Document Order\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"chunk-up-the-text-by-charter-or-sections-or-letters-or-what-have-you\\\"\u003EChunk up the text by charter (or sections, or letters, or what-have-you)\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe most important functional divisions in our text are signaled by upper case roman numerals on a separate line for each of the charters. So we need a regex to find roman numerals like that. Here&#39;s one: \u003Ccode\u003Eromstr = re.compile(&quot;\\\\s*[IVXLCDM]{2,}&quot;)\u003C\u002Fcode\u003E. We&#39;ll put it at the top of our module file as a &#39;global&#39; variable so it will be available to any of the bits of code that come later.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe script below will look for capital roman numerals that appear on a line by themselves. Many of our charter numbers will fail that test and the script will report \u003Ccode\u003Ethere&#39;s a charter roman numeral missing?\u003C\u002Fcode\u003E, often because there&#39;s something before or after it on the line; or, \u003Ccode\u003EKeyError\u003C\u002Fcode\u003E, often because the OCR has garbled the characters (e.g. CCG for 300, XOII for 492). Run this script repeatedly, correcting \u003Ccode\u003Eout1.txt\u003C\u002Fcode\u003E as you do until all the charters are accounted for.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# At the top, do the importing you need, then define rom2ar() as described above, and then:\\nn = 0\\nromstr = re.compile(&quot;\\\\s*[IVXLCDM]{2,}&quot;)\\nfin = open(&quot;out1.txt&quot;, &#39;r&#39;)\\nfout = open(&quot;out2.txt&quot;, &#39;w&#39;)\\nGScriba = fin.readlines()\\n\\nfor line in GScriba:\\n    if romstr.match(line):\\n        rnum = line.strip().strip(&#39;.&#39;)\\n        # each time we find a roman numeral by itself on a line we increment n:\\n        # that&#39;s our charter number.\\n        n += 1\\n        try:\\n            # translate the roman to the arabic and it should be equal to n.\\n            if n != rom2ar(rnum):\\n                # if it&#39;s not, then alert us\\n                print(f&quot;{n}, there&#39;s a charter roman numeral missing?, because line number {GScriba.index(line)} reads: {line}&quot;)\\n                # then set &#39;n&#39; to the right number\\n                n = rom2ar(rnum)\\n        except KeyError:\\n            print(f&quot;{n}, KeyError, line number {GScriba.index(line)} reads: {line}&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESince we know how many charters there should be. At the end of our loop, the value of n should be the same as the number of charters. And, in any iteration of the loop, if the value of n does not correspond to the next successive charter number, then we know we&#39;ve got a problem somewhere, and the print statements should help us find it.\u003C\u002Fp\u003E\\n\u003Cp\u003EHere&#39;s a sample of the output our script will give us:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E23 there&#39;s a charter roman numeral missing?, because line number  156  reads:  XXIV.\\n25 there&#39;s a charter roman numeral missing?, because line number  186  reads:  XXVIII.\\n36 KeyError, line number  235  reads:  XXXV1.\\n37 KeyError, line number  239  reads:  XXXV II.\\n38 there&#39;s a charter roman numeral missing?, because line number  252  reads:  XL.\\n41 there&#39;s a charter roman numeral missing?, because line number  262  reads:  XLII.\\n43 KeyError, line number  265  reads:  XL:III.\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ENOTA BENE: Our regex will report an error for the single digit Roman numerals (&#39;I&#39;,&#39;V&#39;,&#39;X&#39; etc.). You could test for these in the code, but sometimes leaving a known and regular error is a help to check on the efficacy of what you&#39;re doing. Our aim is to satisfy ourselves that any inconsistencies on the charter number line are understood and accounted for.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EOnce we&#39;ve found, and fixed, all the roman numeral charter headings, then we can write out a new file with an easy-to-find-by-regex string, a &#39;slug,&#39; for each charter in place of the bare roman numeral. Comment out the \u003Ccode\u003Efor\u003C\u002Fcode\u003E loop above, and replace it with this one:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efor line in GScriba:\\n    if romstr.match(line):\\n        rnum = line.strip().strip(&#39;.&#39;)\\n        num = rom2ar(rnum)\\n        fout.write(f&quot;[~~~~ GScriba_{rnum} :::: {num} ~~~~]\\\\n&quot;)\\n    else:\\n        fout.write(line)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWhile it&#39;s important in itself for us to have our OCR output reliably divided up by page and by charter, the most important thing about these initial operations is that you know how many pages there are, and how many charters there are, and you can use that knowledge to check on subsequent operations. If you want to do something to every charter, you can reliably test whether or not it worked because you can count the number of charters that it worked on.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"find-and-normalize-folio-markers\\\"\u003EFind and normalize folio markers\u003C\u002Fh2\u003E\\n\u003Cp\u003EOur OCR&#39;d text is from the 1935 published edition of \u003Cem\u003EGiovanni Scriba\u003C\u002Fem\u003E. This is a transcription of a manuscript cartulary which was in the form of a bound book. The published edition preserves the pagination of that original by noting where the original pages change: [fo. 16 r.] the face side of the 16th leaf in the book, followed by its reverse [fo. 16 v.]. This is metadata that we want to preserve for each of the charters so that they can be referenced with respect to the original, as well as with respect to the published edition by page number.\u003C\u002Fp\u003E\\n\u003Cp\u003EMany of the folio markers (e.g. &quot;[fo. 16 v.]&quot;) appear on the same line as the roman numeral for the charter heading. To normalize those charter headings for the operation above, we had to put a line break between the folio marker and the charter number, so many of the folio markers are on their own line already. However, sometimes the folio changes in the middle of the charter text somewhere. We want these markers to stay where they are; we will have to treat those two cases differently. For either case, we need to make sure all the folio markers are free of errors so that we can reliably find them by means of a regular expression. Again, since we know how many folios there are, we can know if we&#39;ve found them all. Note that because we used \u003Ccode\u003E.readlines()\u003C\u002Fcode\u003E, \u003Ccode\u003EGScriba\u003C\u002Fcode\u003E is a list, so the script below will print the line number from the source file as well as the line itself. This will report all the correctly formated folio markers, so that you can find and fix the ones that are broken.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# note the optional quantifiers &#39;\\\\s?&#39;. We want to find as many as we can, and\\n# the OCR is erratic about whitespace, so our regex is permissive. But as\\n# you find and correct these strings, you will want to make them consistent.\\nfol = re.compile(&quot;\\\\[fo\\\\.\\\\s?\\\\d+\\\\s?[rv]\\\\.\\\\s?\\\\]&quot;)\\n\\nfor line in GScriba:\\n    if fol.match(line):\\n        # since GScriba is a list, we can get the index of any of its members to find the line number in our input file.\\n        print GScriba.index(line), line\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWe would also like to ensure that no line has more than one folio marker. We can test that like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efor line in GScriba:\\n    all = fol.findall(line)\\n    if len(all) &gt; 1:\\n        print GScriba.index(line), line\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAgain, as before, once you&#39;ve found and corrected all the folio markers in your input file, save it with a new name and use it as the input to the next section.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"find-and-normalize-the-italian-summary-lines\\\"\u003EFind and normalize the Italian summary lines.\u003C\u002Fh2\u003E\\n\u003Cp\u003EThis important line is invariably the first one after the charter heading.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;gs_italian_summary.png&quot; caption=&quot;italian summary line&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ESince those roman numeral headings are now reliably findable with our &#39;slug&#39; regex, we can now isolate the line that appears immediately after it. We also know that the summaries always end with some kind of parenthesized date expression. So, we can compose a regular expression to find the slug and the line following:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eslug_and_firstline = re.compile(&quot;(\\\\[~~~~\\\\sGScriba_)(.*)\\\\s::::\\\\s(\\\\d+)\\\\s~~~~\\\\]\\\\n(.*)(\\\\(\\\\d?.*\\\\d+\\\\))&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ELet&#39;s break down that regex using the verbose mode (again, see O&#39;Hara&#39;s \u003Ca href=\\\"\u002Flessons\u002Fcleaning-ocrd-text-with-regular-expressions.html\\\"\u003Etutorial\u003C\u002Fa\u003E). Our &#39;slug&#39; for each charter takes the form &quot;[\u003Cdel\u003E\\\\\u003C\u002Fdel\u003E~~ GScriba_CCVII :::: 207 ~~~~]&quot; for example. The compiled pattern above is exactly equivalent to the folowing (note the re.VERBOSE switch at the end):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eslug_and_firstline = re.compile(r&quot;&quot;&quot;\\n    (\\\\[~~~~\\\\sGScriba_)  # matches the &quot;[~~~~ GScriba_&quot; bit\\n    (.*)                # matches the charter&#39;s roman numeral\\n    \\\\s::::\\\\s            # matches the &quot; :::: &quot; bit\\n    (\\\\d+)               # matches the arabic charter number\\n    \\\\s~~~~\\\\]\\\\n          # matches the last &quot; ~~~~ &quot; bit and the line ending\\n    (.*)                # matches all of the next line up to:\\n    (\\\\(\\\\d?.*\\\\d+\\\\))      # the paranthetical expression at the end\\n    &quot;&quot;&quot;, re.VERBOSE)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003Ethe parentheses mark match groups, so each time our regex finds a match, we can refer in our code to specific bits of the match it found:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Ematch.group(0)\u003C\u002Fcode\u003E is the whole match, both our slug and the line that follows it.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Ematch.group(1)\u003C\u002Fcode\u003E = &quot;[~~~~ GScriba_&quot;\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Ematch.group(2)\u003C\u002Fcode\u003E = the charter&#39;s roman numeral\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Ematch.group(3)\u003C\u002Fcode\u003E = the arabic charter number\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Ematch.group(4)\u003C\u002Fcode\u003E = the whole of the Italian summary line up to the parenthesized date expression\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Ematch.group(5)\u003C\u002Fcode\u003E = the parenthesized date expression. Note the escaped parentheses.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EBecause our OCR has a lot of mysterious whitespace (OCR software is not good at parsing whitespace and you&#39;re likely to get newlines, tabs, spaces, all mixed up without rhyme or reason), we want to hunt for this regex as substrings of a great big string, so this time we&#39;re going to use \u003Ccode\u003E.read()\u003C\u002Fcode\u003E instead of \u003Ccode\u003E.readlines()\u003C\u002Fcode\u003E. And we&#39;ll also need a counter to keep track of the lines we find. This script will report the charter numbers where the first line does not conform to our regex model. This will usually happen if there&#39;s no line break after our charter header, or if the Italian summary line has been broken up into multiple lines.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Enum_firstlines = 0\\nfin = open(&quot;your_current_source_file.txt&quot;, &#39;r&#39;)\\n# NB: GScriba is not a list of lines this time, but a single big string.\\nGScriba = fin.read()\\n\\n# finditer() creates an iterator &#39;i&#39; that we can do a &#39;for&#39; loop over.\\ni = slug_and_firstline.finditer(GScriba)\\n\\n# each element &#39;x&#39; in that iterator is a regex match object.\\nfor x in i:\\n    # count the summary lines we find. Remember, we know how many\\n    # there should be, because we know how many charters there are.\\n    num_firstlines += 1\\n\\n    chno = int(x.group(3)) # our charter number is a string, we need an integer\\n\\n    # chno should equal n + 1, if it doesn&#39;t, report to us\\n    if chno != n + 1:\\n        print(f&quot;problem in charter: {(n + 1)}&quot;) #NB: this will miss consecutive problems.\\n    # then set n to the right charter number\\n    n = chno\\n\\n# print out the number of summary lines we found\\nprint(f&quot;number of italian summaries: {num_firstlines}&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAgain, run the script repeatedly until all the Italian Summary lines are present and correct, then save your input file with a new name and use it the input file for the next bit:\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"find-and-normalize-footnote-markers-and-texts\\\"\u003EFind and normalize footnote markers and texts\u003C\u002Fh2\u003E\\n\u003Cp\u003EOne of the trickiest bits to untangle, is the infuriating editorial convention of restarting the footnote numbering with each new page. This makes it hard to associate a footnote text (page-bound data), with a footnote marker (charter-bound data). Before we can do that we have to ensure that each footnote text that appears at the bottom of the page, appears in our source file on its own separate line with no leading white-space. And that \u003Cstrong\u003Enone\u003C\u002Fstrong\u003E of the footnote markers within the text appears at the beginning of a line. And we must ensure that every footnote string, &quot;(1)&quot; for example, appears \u003Cstrong\u003Eexactly\u003C\u002Fstrong\u003E twice on a page: once as an in-text marker, and once at the bottom for the footnote text. The following script reports the page number of any page that fails that test, along with a list of the footnote strings it found on that page.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# Don&#39;t forget to import the Counter module:\\nfrom collections import Counter\\nfin = open(&quot;your_current_source_file.txt&quot;, &#39;r&#39;)\\nGScriba = fin.readlines() # GScriba is a list again\\nr = re.compile(&quot;\\\\(\\\\d{1,2}\\\\)&quot;) # there&#39;s lots of ways for OCR to screw this up, so be alert.\\npg = re.compile(&quot;~~~~~ PAGE \\\\d+ ~~~~~&quot;)\\npgno = 0\\n\\npgfnlist = []\\n# remember, we&#39;re processing lines in document order. So for each page\\n# we&#39;ll populate a temporary container, &#39;pgfnlist&#39;, with values. Then\\n# when we come to a new page, we&#39;ll report what those values are and\\n# then reset our container to the empty list.\\n\\nfor line in GScriba:\\n    if pg.match(line):\\n        # if this test is True, then we&#39;re starting a new page, so increment pgno\\n        pgno += 1\\n\\n        # if we&#39;ve started a new page, then test our list of footnote markers\\n        if pgfnlist:\\n            c = Counter(pgfnlist)\\n\\n            # if there are fn markers that do not appear exactly twice,\\n            # then report the page number to us\\n            if 1 in c.values(): print(pgno, pgfnlist)\\n\\n            # then reset our list to empty\\n            pgfnlist = []\\n\\n    # for each line, look for ALL occurences of our footnote marker regex\\n    i = r.finditer(line)\\n    for mark in [eval(x.group(0)) for x in i]:\\n        # and add them to our list for this page\\n        pgfnlist.append(mark)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ENote: the elements in the iterator &#39;i&#39; are string matches. We want the strings that were matched, \u003Ccode\u003Egroup(0)\u003C\u002Fcode\u003E. e.g. &quot;(1)&quot;. And if we do eval(&quot;(1)&quot;) we get an integer that we can add to our list.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EOur \u003Ccode\u003ECounter\u003C\u002Fcode\u003E is a very handy special data structure. We know that we want each value in our \u003Ccode\u003Epgfnlist\u003C\u002Fcode\u003E to appear twice. Our \u003Ccode\u003ECounter\u003C\u002Fcode\u003E will give us a hash where the keys are the elements that appear, and the values are how many times each element appears. Like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E&gt;&gt;&gt; l = [1,2,3,1,3]\\n&gt;&gt;&gt; c = Counter(l)\\n&gt;&gt;&gt; print(c)\\nCounter({1: 2, 3: 2, 2: 1})\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESo if for a given page we get a list of footnote markers like this \u003Ccode\u003E[1,2,3,1,3]\u003C\u002Fcode\u003E, then the test \u003Ccode\u003Eif 1 in c.values()\u003C\u002Fcode\u003E will indicate a problem because we know each element must appear \u003Cstrong\u003Eexactly twice\u003C\u002Fstrong\u003E:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E&gt;&gt;&gt; l = [1,2,3,1,3]\\n&gt;&gt;&gt; c = Counter(l)\\n&gt;&gt;&gt; print(c.values())\\n[2, 1, 2]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003Ewhereas, if our footnote marker list for the page is complete \u003Ccode\u003E[1,2,3,1,2,3]\u003C\u002Fcode\u003E, then:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E&gt;&gt;&gt; l = [1,2,3,1,2,3]\\n&gt;&gt;&gt; c = Counter(l)\\n&gt;&gt;&gt; print c.values()\\n[2, 2, 2] # i.e. 1 is not in c.values()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAs before, run this script repeatedly, correcting your input file manually as you discover errors, until you are satisfied that all footnotes are present and correct for each page. Then save your corrected input file with a new name.\u003C\u002Fp\u003E\\n\u003Cp\u003EOur text file still has lots of OCR errors in it, but we have now gone through it and found and corrected all the specific metadata bits that we want in our ordered data set. Now we can use our corrected text file to build a Python dictionary.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"creating-the-dictionary\\\"\u003ECreating the Dictionary\u003C\u002Fh1\u003E\\n\u003Cp\u003ENow that we&#39;ve cleaned up enough of the OCR that we can successfully differentiate the component parts of the page from each other, we can now sort the various bits of the meta-data, and the charter text itself, into their own separate fields of a Python dictionary.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe have a number of things to do: correctly number each charter as to charter number, folio, and page; separate out the Italian summary and the marginal notation lines; and associate the footnote texts with their appropriate charter. To do all this, sometimes it is convenient to make more than one pass.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"create-a-skeleton-dictionary\\\"\u003ECreate a skeleton dictionary.\u003C\u002Fh2\u003E\\n\u003Cp\u003EWe&#39;ll start by generating a python dictionary whose keys are the charter numbers, and whose values are a nested dictionary that has fields for some of the metadata we want to store for each charter. So it will have the form:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Echarters = {\\n    .\\n    .\\n    .\\n    300: {\\n            &#39;chid&#39;: &quot;our charter ID&quot;,\\n            &#39;chno&#39;: 300,\\n            &#39;footnotes&#39;: [], # an empty list for now\\n            &#39;folio&#39;: &quot;the folio marker for this charter&quot;,\\n            &#39;pgno&#39;: &quot;the page number in the printed edition for this charter,\\n            &#39;text&#39;: [] # an empty list for now\\n          },\\n    301: {\\n            &#39;chid&#39;: &quot;our charter ID&quot;,\\n            &#39;chno&#39;: 301,\\n            &#39;footnotes&#39;: [], # an empty list for now\\n            &#39;folio&#39;: &quot;the folio marker for this charter&quot;,\\n            &#39;pgno&#39;: &quot;the page number in the printed edition for this charter,\\n            &#39;text&#39;: [] # an empty list for now\\n          },\\n    .\\n    .\\n    . etc.\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EFor this first pass, we&#39;ll just create this basic structure and then in subsequent loops we will add to and modify this dictionary until we get a dictionary for each charter, and fields for all the metadata for each charter. Once this loop disposes of the easily searched lines (folio, page, and charter headers) and creates an empty container for footnotes, the fall-through default will be to append the remaining lines to the text field, which is a python list.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eslug = re.compile(&quot;(\\\\[~~~~\\\\sGScriba_)(.*)\\\\s::::\\\\s(\\\\d+)\\\\s~~~~\\\\]&quot;)\\nfol = re.compile(&quot;\\\\[fo\\\\.\\\\s?\\\\d+\\\\s?[rv]\\\\.\\\\s?\\\\]&quot;)\\npgbrk = re.compile(&quot;~~~~~ PAGE (\\\\d+) ~~~~~&quot;)\\n\\nfin = open(&quot;your_current_source_file.txt&quot;, &#39;r&#39;)\\nGScriba = fin.readlines()\\n\\n# we&#39;ll also need these global variables with starting values as we mentioned at the top\\nn = 0\\nthis_folio  = &#39;[fo. 1 r.]&#39;\\nthis_page = 1\\n\\n# &#39;charters&#39; is also defined as a global variable. The &#39;for&#39; loop below\\n#  and in the following sections, will build on and modify this dictionary\\ncharters = dict()\\n\\nfor line in GScriba:\\n    if fol.match(line):\\n        # use this global variable to keep track of the folio number.\\n        # we&#39;ll create the &#39;folio&#39; field using the value of this variable\\n        this_folio = fol.match(line).group(0)\\n        continue # update the variable but otherwise do nothing with this line.\\n    if slug.match(line):\\n        # if our &#39;slug&#39; regex matches, we know we have a new charter\\n        # so get the data from the match groups\\n        m = slug.match(line)\\n        chid = &quot;GScriba_&quot; + m.group(2)\\n        chno = int(m.group(3))\\n\\n        # then create an empty nested dictionary\\n        charters[chno] = {}\\n\\n        # and an empty container for all the lines we won&#39;t use on this pass\\n        templist = [] # this works because we&#39;re proceeding in document order: templist continues to exist as we iterate through each line in the charter, then is reset to the empty list when we start a new charter(slug.match(line))\\n        continue # we generate the entry, but do nothing with the text of this line.\\n    if chno:\\n        # if a charter dictionary has been created,\\n        # then we can now populate it with data from our slug.match above.\\n        d = charters[chno] # &#39;d&#39; is just more convenient than &#39;charters[chno]&#39;\\n        d[&#39;footnotes&#39;] = [] # We&#39;ll populate this empty list in a separate operation\\n        d[&#39;chid&#39;] = chid\\n        d[&#39;chno&#39;] = chno\\n        d[&#39;folio&#39;] = this_folio\\n        d[&#39;pgno&#39;] = this_page\\n\\n        if re.match(&#39;^\\\\(\\\\d+\\\\)&#39;, line):\\n            # this line is footnote text, because it has a footnote marker\\n            # like &quot;(1)&quot; at the beginning. So we&#39;ll deal with it later\\n            continue\\n        if pgbrk.match(line):\\n            # if line is a pagebreak, update the variable\\n            this_page = int(pgbrk.match(line).group(1))\\n        elif fol.search(line):\\n            # if folio changes within the charter text, update the variable\\n            this_folio = fol.search(line).group(0)\\n            templist.append(line)\\n        else:\\n            # any line not otherwise accounted for, add to our temporary container\\n            templist.append(line)\\n        # add the temporary container to the dictionary after using\\n        # a list comprehension to strip out any empty lines.\\n        d[&#39;text&#39;] = [x for x in templist if not x == &#39;\\\\n&#39;] # strip empty lines\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"add-the-marginal-notation-and-italian-summary-lines-to-the-dictionary\\\"\u003EAdd the &#39;marginal notation&#39; and Italian summary lines to the dictionary\u003C\u002Fh2\u003E\\n\u003Cp\u003EWhen we generated the dictionary of dictionaries above, we assigned fields for footnotes (just an empty list for now), charterID, charter number, the folio, and the page number. All remaining lines were appended to a list and assigned to the field &#39;text&#39;. In all cases, the first line of each charter&#39;s text field should be the Italian summary as we have insured above. The second line in MOST cases, represents a kind of marginal notation usually ended by the &#39;]&#39; character (which OCR misreads a lot). We have to find the cases that do not meet this criterion, supply or correct the missing &#39;]&#39;, and in the cases where there is no marginal notation I&#39;ve supplied &quot;no marginal]&quot; in my working text. The following diagnostic script will print the charter number and first two lines of the text field for those charters that do not meet these criteria. Run this script separately against the \u003Ccode\u003Echarters\u003C\u002Fcode\u003E dictionary, and correct and update your canonical text accordingly.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003En = 0\\nfor ch in charters:\\n    txt = charters[ch][&#39;text&#39;] # remember: the text field is a python list of strings\\n    try:\\n        line1 = txt[0]\\n        line2 = txt[1]\\n        if line2 and &#39;]&#39; not in line2:\\n            n += 1\\n            print(f&quot;charter: {ch}\\\\ntext, line 1: {line1}\\\\ntext, line 2: {line2}&quot;)\\n    except:\\n        print(ch, &quot;oops&quot;) # to pass the charters from the missing page 214 # to pass the charters from the missing page 214\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ENote: The \u003Ccode\u003Etry: except:\u003C\u002Fcode\u003E blocks are made necessary by the fact that in my OCR output, the data for pg 214 somehow got missed out. This often happens. Scanning or photographing each page of a 600 page book is tedious in the extreme. It&#39;s very easy to skip a page. You will inevitably have anomalies like this in your text that you will have to isolate and work around. The Python \u003Ccode\u003Etry: except:\u003C\u002Fcode\u003E pattern makes this easy. Python is also very helpful here in that you can do a lot more in the \u003Ccode\u003Eexcept:\u003C\u002Fcode\u003E clause beyond just printing &quot;oops&quot;. You could call a function that performs a whole separate operation on those anomalous bits.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EOnce we&#39;re satisfied that line 1 and line 2 in the &#39;text&#39; field for each charter in the \u003Ccode\u003Echarters\u003C\u002Fcode\u003E dictionary are the Italian summary and the marginal notation respectively, we can make another iteration of the \u003Ccode\u003Echarters\u003C\u002Fcode\u003E dictionary, removing those lines from the text field and creating new fields in the charter entry for them.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ENOTA BENE: we are now modifying a data structure in memory rather than editing successive text files. So this script should be \u003Cstrong\u003Eadded\u003C\u002Fstrong\u003E to the one above that created your skeleton dictionary. That script creates the \u003Ccode\u003Echarters\u003C\u002Fcode\u003E dictionary in memory, and this one modifies it\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efor ch in charters:\\n    d = charters[ch]\\n    try:\\n        d[&#39;summary&#39;] = d[&#39;text&#39;].pop(0).strip()\\n        d[&#39;marginal&#39;] = d[&#39;text&#39;].pop(0).strip()\\n    except IndexError: # this will report that the charters on p 214 are missing\\n        print(f&quot;missing charter {ch}&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"assign-footnotes-to-their-respective-charters-and-add-to-dictionary\\\"\u003EAssign footnotes to their respective charters and add to dictionary\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe trickiest part is to get the footnote texts appearing at the bottom of the page associated with their appropriate charters. Since we are, perforce, analyzing our text line by line, we&#39;re faced with the problem of associating a given footnote reference with its appropriate footnote text when there are perhaps many lines intervening.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor this we go back to the same list of lines that we built the dictionary from. We&#39;re depending on all the footnote markers appearing within the charter text, i.e. none of them are at the beginning of a line. And, each of the footnote texts is on a separate line beginning with &#39;(1)&#39; etc. We design regexes that can distinguish between the two and construct a container to hold them as we iterate over the lines. As we iterate over the lines of the text file, we find and assign markers and texts to our temporary container, and then, each time we reach a page break, we assign them to their appropriate fields in our existing Python dictionary \u003Ccode\u003Echarters\u003C\u002Fcode\u003E and reset our temporary container to the empty \u003Ccode\u003Edict\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ENote how we construct that temporary container. \u003Ccode\u003Efndict\u003C\u002Fcode\u003E starts out as an empty dictionary. As we iterate through the lines of our input text, if we find footnote markers within the line, we create an entry in \u003Ccode\u003Efndict\u003C\u002Fcode\u003E whose key is the footnote number, and whose value is another dictionary. In that dictionary we record the id of the charter that the footnote belongs to, and we create an empty field for the footnote text. When we find the footnote texts (\u003Ccode\u003Entexts\u003C\u002Fcode\u003E) at the bottom of the page, we look up the footnote number in our container \u003Ccode\u003Efndict\u003C\u002Fcode\u003E and write the text of the line to the empty field we made. So when we come to the end of the page, we have a dictionary of footnotes that looks like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E{1: {&#39;chid&#39;: 158, &#39;fntext&#39;: &#39;Nel ms. de due volte e ripa cancellato.&#39;},\\n 2: {&#39;chid&#39;: 158, &#39;fntext&#39;: &#39;Sic nel ms.&#39;},\\n 3: {&#39;chid&#39;: 159, &#39;fntext&#39;: &#39;genero cancellato nel ms.&#39;}}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENow we have all the necessary information to assign the footnotes to the empty &#39;footnotes&#39; list in the \u003Ccode\u003Echarters\u003C\u002Fcode\u003E dictionary: the number of the footnote (the key), the charter it belongs to (chid), and the text of the footnote (fntext).\u003C\u002Fp\u003E\\n\u003Cp\u003EThis is a common pattern in programming, and very useful: in an iterative process of some kind, you use an accumulator (our \u003Ccode\u003Efndict\u003C\u002Fcode\u003E) to gather bits of data, then when your sentinel encounters a specified condition (the pagebreak) it does something with the data.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efin = open(&quot;your_current_source_file.txt&quot;, &#39;r&#39;)\\nGScriba = fin.readlines()\\n\\n# in notemark, note the &#39;lookbehind&#39; expression &#39;?&lt;!&#39; to insure that\\n# the marker &#39;(1)&#39; does not begin the string\\nnotemark = re.compile(r&quot;\\\\(\\\\d+\\\\)(?&lt;!^\\\\(\\\\d+\\\\))&quot;)\\nnotetext = re.compile(r&quot;^\\\\(\\\\d+\\\\)&quot;)\\nthis_charter = 1\\npg = re.compile(&quot;~~~~~ PAGE \\\\d+ ~~~~~&quot;)\\npgno = 1\\nfndict = {}\\n\\nfor line in GScriba:\\n    nmarkers = notemark.findall(line)\\n    ntexts = notetext.findall(line)\\n    if pg.match(line):\\n        # This is our &#39;sentinel&#39;. We&#39;ve come to the end of a page,\\n        # so we record our accumulated footnote data in the &#39;charters&#39; dict.\\n        for fn in fndict:\\n            chid = fndict[fn][&#39;chid&#39;]\\n            fntext = fndict[fn][&#39;fntext&#39;]\\n            charters[int(chid)][&#39;footnotes&#39;].append((fn, fntext))\\n        pgno += 1\\n        fndict = {}  # and then re-initialize our temporary container\\n    if slug.match(line): # here&#39;s the beginning of a charter, so update the variable.\\n        this_charter = int(slug.match(line).group(3))\\n    if nmarkers:\\n        for marker in [eval(x) for x in nmarkers]:\\n            # create an entry with the charter&#39;s id and an empty text field\\n            fndict[marker] = {&#39;chid&#39;:this_charter, &#39;fntext&#39;: &#39;&#39;}\\n    if ntexts:\\n        for text in [eval(x) for x in ntexts]:\\n            try:\\n                # fill in the appropriate empty field.\\n                fndict[text][&#39;fntext&#39;] = re.sub(&#39;\\\\(\\\\d+\\\\)&#39;, &#39;&#39;, line).strip()\\n            except KeyError:\\n                print(&quot;printer&#39;s error? &quot;, &quot;pgno:&quot;, pgno, line)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENote that the \u003Ccode\u003Etry: except:\u003C\u002Fcode\u003E blocks come to the rescue again here. The loop above kept breaking because in 3 instances it emerged that there existed footnotes at the bottom of a page for which there were no markers within the text. This was an editorial oversight in the published edition, not an OCR error. The result was that when I tried to address the non-existent entry in \u003Ccode\u003Efndict\u003C\u002Fcode\u003E, I got a \u003Ccode\u003EKeyError\u003C\u002Fcode\u003E. My \u003Ccode\u003Eexcept:\u003C\u002Fcode\u003E clause allowed me to find and look at the error, and determine that the error was in the original and nothing I could do anything about, so when generating the final version of \u003Ccode\u003Echarters\u003C\u002Fcode\u003E I replaced the \u003Ccode\u003Eprint\u003C\u002Fcode\u003E statement with \u003Ccode\u003Epass\u003C\u002Fcode\u003E. Texts made by humans are messy; no getting around it. \u003Ccode\u003Etry: except:\u003C\u002Fcode\u003E exists to deal with that reality.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ENOTA BENE: Again, bear in mind that we are modifying a data structure in memory rather than editing successive text files. So this loop should be \u003Cstrong\u003Eadded\u003C\u002Fstrong\u003E to your script \u003Cstrong\u003Ebelow\u003C\u002Fstrong\u003E the summary and marginal loop, which is \u003Cstrong\u003Ebelow\u003C\u002Fstrong\u003E the loop that created your skeleton dictionary.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Ch2 id=\\\"parse-dates-and-add-to-the-dictionary\\\"\u003EParse Dates and add to the dictionary\u003C\u002Fh2\u003E\\n\u003Cp\u003EDates are hard. Students of British history cling to \u003Ca href=\\\"http:\u002F\u002Fwww.worldcat.org\u002Foclc\u002F41238508\\\"\u003ECheyney\u003C\u002Fa\u003E as to a spar on a troubled ocean. And, given the way the Gregorian calendar was adopted so gradually, and innumerable other local variations, correct date reckoning for medieval sources will always require care and local knowledge. Nevertheless, here too Python can be of some help.\u003C\u002Fp\u003E\\n\u003Cp\u003EOur Italian summary line invariably contains a date drawn from the text, and it&#39;s conveniently set off from the rest of the line by parentheses. So we can parse them and create Python \u003Ccode\u003Edate\u003C\u002Fcode\u003E objects. Then, if we want, we can do some simple calendar arithmetic.\u003C\u002Fp\u003E\\n\u003Cp\u003EFirst we have to find and correct all the dates in the same way as we have done for the other metadata elements. Devise a diagnostic script that will iterate over your \u003Ccode\u003Echarters\u003C\u002Fcode\u003E dictionary, report the location of errors in your canonical text, and then fix them in your canonical text manually. Something like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Esummary_date = re.compile(&#39;\\\\((\\\\d{1,2})?(.*?)(\\\\d{1,4})?\\\\)&#39;) # we want to catch them all, and some have no day or month, hence the optional quantifiers: `?`.\\n\\n# And we want to make Python speak Italian:\\nital2int = {&#39;gennaio&#39;: 1, &#39;febbraio&#39;: 2, &#39;marzo&#39;: 3, &#39;aprile&#39;: 4, &#39;maggio&#39;: 5, &#39;giugno&#39;: 6, &#39;luglio&#39;: 7, &#39;agosto&#39;: 8, &#39;settembre&#39;: 9, &#39;ottobre&#39;: 10, &#39;novembre&#39;: 11, &#39;dicembre&#39;: 12}\\n\\nimport sys\\nfor ch in charters:\\n    try:\\n        d = charters[ch]\\n        i = summary_date.finditer(d[&#39;summary&#39;])\\n        dt = list(i)[-1] # Always the last parenthetical expression, in case there is more than one.\\n        if dt.group(2).strip() not in ital2int.keys():\\n            print(f&quot;chno. {d[&#39;chno&#39;]} fix the month {dt.group(2)}&quot;)\\n    except:\\n        print(d[&#39;chno&#39;], &quot;The usual suspects &quot;, sys.exc_info()[:2])\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ENote: When using \u003Ccode\u003Etry\u002Fexcept\u003C\u002Fcode\u003E blocks, you should usually trap \u003Cstrong\u003Especific\u003C\u002Fstrong\u003E errors in the except clause, like \u003Ccode\u003EValueError\u003C\u002Fcode\u003E and the like; however, in \u003Cem\u003Ead hoc\u003C\u002Fem\u003E scripts like this, using \u003Ccode\u003Esys.exc_info\u003C\u002Fcode\u003E is a quick and dirty way to get information about any exception that may be raised. (The \u003Ca href=\\\"https:\u002F\u002Fpymotw.com\u002F3\u002Fsys\u002Findex.html\\\"\u003Esys\u003C\u002Fa\u003E module is full of such stuff, useful for debugging)\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EOnce you&#39;re satisfied that all the parenthetical date expressions are present and correct, and conform to your regular expression, you can parse them and add them to your data structure as dates rather than just strings. For this you can use the \u003Ccode\u003Edatetime\u003C\u002Fcode\u003E module.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis module is part of the standard library, is a deep subject, and ought to be the subject of its own tutorial, given the importance of dates for historians. As with a lot of other python modules, a good introduction is Doug Hellmann&#39;s \u003Ca href=\\\"https:\u002F\u002Fpymotw.com\u002F3\u002Fdatetime\u002Findex.html\\\"\u003EPyMOTW\u003C\u002Fa\u003E(module of the week). An even more able extension library is \u003Ca href=\\\"http:\u002F\u002Fwww.egenix.com\u002Fproducts\u002Fpython\u002FmxBase\u002FmxDateTime\u002F\\\"\u003EmxDateTime\u003C\u002Fa\u003E. Suffice it here to say that the \u003Ccode\u003Edatetime.date\u003C\u002Fcode\u003E module expects parameters like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E&gt;&gt;&gt; from datetime import date\\n&gt;&gt;&gt; dt = date(1160, 12, 25)\\n&gt;&gt;&gt; dt.isoformat()\\n&#39;1160-12-25&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESo here&#39;s our loop to parse the dates at the end of the Italian summary lines and store them in our \u003Ccode\u003Echarters\u003C\u002Fcode\u003E dictionary (remembering again that we want to modify our in-memory data structure \u003Ccode\u003Echarters\u003C\u002Fcode\u003E created above):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Esummary_date = re.compile(&#39;\\\\((\\\\d{1,2})?(.*?)(\\\\d{1,4})?\\\\)&#39;)\\nfrom datetime import date\\nfor ch in charters:\\n    c = charters[ch]\\n    i = summary_date.finditer(c[&#39;summary&#39;])\\n    for m in i:\\n        # remember &#39;i&#39; is an iterator so even if there is more than one\\n        # parenthetical expression in c[&#39;summary&#39;], the try clause will\\n        # succeed on the last one, or fail on all of them.\\n        try:\\n            yr = int(m.group(3))\\n            mo = ital2int[m.group(2).strip()]\\n            day = int(m.group(1))\\n            c[&#39;date&#39;] = date(yr, mo, day)\\n        except:\\n            c[&#39;date&#39;] = &quot;date won&#39;t parse, see summary line&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EOut of 803 charters, 29 wouldn&#39;t parse, mostly because the date included only month and year. You can store these as strings, but then you have two data types claiming to be dates. Or you could supply a 01 as the default day and thus store a Python date object, but Jan. 1, 1160 isn&#39;t the same thing as Jan. 1160 and thus distorts your metadata. Or you could just do as I have done and refer to the relevant source text: the Italian summary line in the printed edition.\u003C\u002Fp\u003E\\n\u003Cp\u003EOnce you&#39;ve got date objects, you can do date arithmetic. Supposing we wanted to find all the charters dated to within 3 weeks of Christmas, 1160.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# Let&#39;s import the whole thing and use dot notation: datetime.date() etc.\\nimport datetime\\n\\n# a timedelta is a span of time\\nweek = datetime.timedelta(weeks=1)\\n\\nfor ch in charters:\\n    try:\\n        dt = charters[ch][&#39;date&#39;]\\n        christmas = datetime.date(1160,12,25)\\n        if abs(dt - christmas) &lt; week * 3:\\n            print(f&quot;chno: {charters[ch][&#39;chno&#39;]}, date: {dt}&quot;)\\n    except:\\n        pass # avoid this idiom in production code\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWhich will give us this result:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-terminal\\\"\u003Echno: 790, date: 1160-12-14\\nchno: 791, date: 1160-12-15\\nchno: 792, date: 1161-01-01\\nchno: 793, date: 1161-01-04\\nchno: 794, date: 1161-01-05\\nchno: 795, date: 1161-01-05\\nchno: 796, date: 1161-01-10\\nchno: 797, date: 1161-01-10\\nchno: 798, date: 1161-01-06\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ECool, huh?\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"our-completed-data-structure\\\"\u003EOur completed data structure\u003C\u002Fh1\u003E\\n\u003Cp\u003ENow we&#39;ve corrected our canonical text as much as we need to to differentiate between the various bits of meta-data that we want to capture, and we&#39;ve created a data structure in memory, our \u003Ccode\u003Echarters\u003C\u002Fcode\u003E dictionary, by making 4 passes, each one extending and modifying the dictionary in memory.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003Ecreate the skeleton\u003C\u002Fli\u003E\\n\u003Cli\u003Eseparate out the \u003Ccode\u003Esummary\u003C\u002Fcode\u003E and \u003Ccode\u003Emarginal\u003C\u002Fcode\u003E lines and create dictionary fields for them.\u003C\u002Fli\u003E\\n\u003Cli\u003Ecollect and assign footnotes to their respective charters\u003C\u002Fli\u003E\\n\u003Cli\u003Eparse the dates in the \u003Ccode\u003Esummary\u003C\u002Fcode\u003E field, and add them to their respective charters\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003EPrint out our resulting dictionary using \u003Ccode\u003Epprint(charters)\u003C\u002Fcode\u003E and you&#39;ll see something like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E{\\n.\\n.\\n.\\n 52: {&#39;chid&#39;: &#39;GScriba_LII&#39;,\\n      &#39;chno&#39;: 52,\\n      &#39;date&#39;: datetime.date(1156, 3, 27),\\n      &#39;folio&#39;: &#39;[fo. 6 r.]&#39;,\\n      &#39;footnotes&#39;: [(1, &#39;Cancellato: m.&#39;)],\\n      &#39;marginal&#39;: &#39;no marginal]&#39;,\\n      &#39;pgno&#39;: 29,\\n      &#39;summary&#39;: &#39;I consoli di Genova riconoscono con sentenza il diritto di Romano di Casella di pagarsi sui beni di Gerardo Confector per un credito che aveva verso il medesimo (27 marzo 1156).&#39;,\\n      &#39;text&#39;: [&#39;    In pontili capituli consules E. Aurie, W. Buronus, Ogerius Ventus laudaverunt quod Romanus de Casella haberet in bonis Gerardi Confectoris s. .xxvi. denariorum et possit eos accipere sine contradicione eius et omnium pro eo. Hoc ideo quia, cum; Romanus ante ipsos inde conquereretur, ipso Gerardo debitum non negante, sed quod de usura esset obiiciendo, iuravit nominatus Romanus quod capitalis erat (1) et non de usura, unde ut supra laudaverunt , .MCLVI., sexto kalendas aprilis, indicione tercia.\\\\n&#39;]},\\n 53: {&#39;chid&#39;: &#39;GScriba_LIII&#39;,\\n      &#39;chno&#39;: 53,\\n      &#39;date&#39;: datetime.date(1156, 3, 27),\\n      &#39;folio&#39;: &#39;[fo. 6 r.]&#39;,\\n      &#39;footnotes&#39;: [],\\n      &#39;marginal&#39;: &#39;Belmusti]&#39;,\\n      &#39;pgno&#39;: 29,\\n      &#39;summary&#39;: &quot;Maestro Arnaldo e Giordan nipote del fu Giovanni di Piacenza si obbligano di pagare una somma nell&#39;ottava della prossima Pasqua, per merce ricevuta (27 marzo 1156).&quot;,\\n      &#39;text&#39;: [&#39;  Testes Conradus Porcellus, Albericus, Vassallus Gambalixa, Petrus Artodi. Nos Arnaldus magister et Iordan nepos quondam Iohannis Placentie accepimus a te Belmusto tantum bracile unde promittimus dare tibi vel tuo certo misso lb. .xLIII. denariorum usque octavam proximi pasce, quod si non fecerimus penam dupli tibi stipulanti promittimus, bona pignori, possis unumquemque convenire de toto. Actum prope campanile Sancti Laurentii, millesimo centesimo .Lv., sexto kalendas aprilis, indictione tercia.\\\\n&#39;]},\\n.\\n.\\n. etc.\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EPrinting out your Python dictionary as a literal string is not a bad thing to do. For a text this size, the resulting file is perfectly manageable, can be mailed around usefully and read into a python repl session very simply using \u003Ccode\u003Eeval()\u003C\u002Fcode\u003E, or pasted directly into a Python module file. On the other hand, if you want an even more reliable way to serialize it in an exclusively Python context, look into \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F3.7\u002Flibrary\u002Fpickle.html\\\"\u003E\u003Ccode\u003EPickle\u003C\u002Fcode\u003E\u003C\u002Fa\u003E. If you need to move it to some other context, JavaScript for example, or some \u003Ccode\u003ERDF\u003C\u002Fcode\u003E triple stores, Python&#39;s \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F3.7\u002Flibrary\u002Fjson.html#module-json\\\"\u003E\u003Ccode\u003Ejson\u003C\u002Fcode\u003E\u003C\u002Fa\u003E module will translate effectively. If you have to get some kind of XML output, I will be very sorry for you, but the \u003Ca href=\\\"http:\u002F\u002Flxml.de\u002F\\\"\u003E\u003Ccode\u003Elxml\u003C\u002Fcode\u003E\u003C\u002Fa\u003E python module may ease the pain a little.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"order-from-disorder-huzzah\\\"\u003EOrder from disorder, huzzah.\u003C\u002Fh2\u003E\\n\u003Cp\u003ENow that we have an ordered data structure, we can do many things with it. As a very simple example, let&#39;s append some code that just prints \u003Ccode\u003Echarters\u003C\u002Fcode\u003E out as html for display on a web-site:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efout = open(&quot;your_page.html&quot;, &#39;w&#39;) # create a text file to write the html to\\n\\n# write to the file your html header with some CSS formatting declarations\\nfout.write(&quot;&quot;&quot;\\n&lt;!DOCTYPE html PUBLIC &quot;-\u002F\u002FW3C\u002F\u002FDTD HTML 4.01\u002F\u002FEN&quot;&gt;\\n\\n&lt;html&gt;\\n&lt;head&gt;\\n  &lt;title&gt;Giovanni Scriba Vol. I&lt;\u002Ftitle&gt;\\n  &lt;style&gt;\\n    h1 {text-align: center; color: #800; font-size: 16pt; margin-bottom: 0px; margin-top: 16px;}\\n    ul {list-style-type: none;}\\n    .sep {color: #800; text-align: center}\\n    .charter {width: 650px; margin-left: auto; margin-right: auto; margin-top: 60px; border-top: double #800;}\\n    .folio {color: #777;}\\n    .summary {color: #777; margin: 12px 0px 12px 12px;}\\n    .marginal {color: red}\\n    .charter-text {margin-left: 16px}\\n    .footnotes\\n    .page-number {font-size: 60%}\\n  &lt;\u002Fstyle&gt;&lt;\u002Fhead&gt;\\n\\n&lt;body&gt;\\n&quot;&quot;&quot;)\\n\\n# a loop that will write out a blob of html code for each of the charters in our dictionary:\\nfor x in charters:\\n\\n    # use a shallow copy so that charters[x] is not modified for this specialized purpose\\n    d = charters[x].copy()\\n\\n    try:\\n        if d[&#39;footnotes&#39;]:\\n            # remember, this is a list of tuples. So you can feed them directly\\n            # to the string interpolation operator in the list comprehension.\\n            fnlist = [&quot;&lt;li&gt;(%s) %s&lt;\u002Fli&gt;&quot; % t for t in d[&#39;footnotes&#39;]]\\n            d[&#39;footnotes&#39;] = &quot;&lt;ul&gt;&quot; + &#39;&#39;.join(fnlist) + &quot;&lt;\u002Ful&gt;&quot;\\n        else:\\n            d[&#39;footnotes&#39;] = &quot;&quot;\\n\\n        d[&#39;text&#39;] = &#39; &#39;.join(d[&#39;text&#39;]) # d[&#39;text&#39;] is a list of strings\\n\\n        blob = &quot;&quot;&quot;\\n            &lt;div&gt;\\n                &lt;div class=&quot;charter&quot;&gt;\\n                    &lt;h1&gt;%(chid)s&lt;\u002Fh1&gt;\\n                    &lt;div class=&quot;folio&quot;&gt;%(folio)s (pg. %(pgno)d)&lt;\u002Fdiv&gt;\\n                    &lt;div class=&quot;summary&quot;&gt;%(summary)s&lt;\u002Fdiv&gt;\\n                    &lt;div class=&quot;marginal&quot;&gt;%(marginal)s&lt;\u002Fdiv&gt;\\n                    &lt;div class=&quot;text&quot;&gt;%(text)s&lt;\u002Fdiv&gt;\\n                    &lt;div class=&quot;footnotes&quot;&gt;%(footnotes)s&lt;\u002Fdiv&gt;\\n                &lt;\u002Fdiv&gt;\\n            &lt;\u002Fdiv&gt;\\n            &quot;&quot;&quot;\\n\\n        fout.write(blob % d)\\n\\n        # `string % dictionary` is a neat trick for html templating\\n        # that makes use of python&#39;s string interpolation syntax\\n        # see: https:\u002F\u002Fdocs.python.org\u002F3\u002Ftutorial\u002Finputoutput.html#the-string-format-method\\n\\n        fout.write(&quot;\\\\n\\\\n&quot;)\\n    except:\\n        # insert entries noting the absence of charters on the missing pg. 214\\n        erratum = &quot;&quot;&quot;\\n            &lt;div&gt;\\n                &lt;div class=&quot;charter&quot;&gt;\\n                    &lt;h1&gt;Charter no. %d is missing because the scan for Pg. 214 was ommited&lt;\u002Fh1&gt;\\n                &lt;\u002Fdiv&gt;\\n            &lt;\u002Fdiv&gt;\\n            &quot;&quot;&quot;  % d[&#39;chno&#39;]\\n\\n        fout.write(erratum)\\n\\nfout.write(&quot;&quot;&quot;&lt;\u002Fbody&gt;&lt;\u002Fhtml&gt;&quot;&quot;&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EDrop the resulting file on a web browser, and you&#39;ve got a nicely formated electronic edition.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;gs_gscriba207.png&quot; caption=&quot;html formatted charter example&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EBeing able to do this with your, still mostly uncorrected, OCR output is not a trivial advantage. If you&#39;re serious about creating a clean, error free, electronic edition of anything, you&#39;ve got to do some serious proofreading. Having a source text formatted for reading is crucial; moreover, if your proofreader can change the font, spacing, color, layout, and so forth at will, you can increase their accuracy and productivity substantially. With this example in a modern web browser, tweaking those parameters with some simple CSS declarations is easy. Also, with some ordered HTML to work with, you might crowd-source the OCR error correction, instead of hiring that army of illiterate street urchins.\u003C\u002Fp\u003E\\n\u003Cp\u003EAnd, our original problem, OCR cleanup, is now much more tractable because we can target regular expressions for the specific sorts of metadata we have: errors in the Italian summary or in the Latin text? Or we could design search-and-replace routines just for specific charters, or groups of charters.\u003C\u002Fp\u003E\\n\u003Cp\u003EBeyond this though, there&#39;s lots you can do with an ordered data set, including feeding it back through a markup tool like the \u003Ca href=\\\"http:\u002F\u002Fbrat.nlplab.org\\\"\u003Ebrat\u003C\u002Fa\u003E as we did for the ChartEx project. Domain experts can then start adding layers of semantic tagging even if you don&#39;t do any further OCR error correction. Moreover, with an ordered dataset we can get all sorts of output, some other flavor of XML (if you must) for example: TEI (Text Encoding Initiative), or EAD (Encoded Archival Description). Or you could read your dataset directly into a relational database, or some kind of key\u002Fvalue store. All of these things are essentially impossible if you&#39;re working simply with a plain text file.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe bits of code above are in no way a turn-key solution for cleaning arbitrary OCR output. There is no such magic wand. The Google approach to scanning the contents of research libraries threatens to drown us in an ocean of bad data. Worse, it elides a fundamental fact of digital scholarship: digital sources are hard to get. Reliable, flexible, and useful digital texts require careful redaction and persistent curation. Google, Amazon, Facebook, \u003Cem\u003Eet alia\u003C\u002Fem\u003E do not have to concern themselves with the quality of their data, just its quantity. Historians, on the other hand, must care first for the integrity of their sources.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe vast 18th and 19th century publishing projects, the \u003Cem\u003ERolls Series\u003C\u002Fem\u003E, the \u003Cem\u003EMonumenta Germaniae Historica\u003C\u002Fem\u003E, and many others, bequeathed a treasure trove of source material to us by dint of a huge amount of very painstaking and detailed work by armies of dedicated and knowledgeable scholars. Their task was the same as ours: to faithfully transmit history&#39;s legacy from its earlier forms into a more modern form, thereby making it more widely accessible. We can do no less. We have powerful tools at our disposal, but while that may change the scale of the task, it does not change its nature.\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
