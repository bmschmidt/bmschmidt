<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-c09d08cd.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-c09d08cd.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/sonification"),
					params: {lang:"en",lessons:"lessons",slug:"sonification"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>The Sound of Data (a gentle introduction to sonification for historians)</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h1 id="introduction">Introduction</h1>
<p>ποίησις - fabrication, creation, production</p>
<p>I am too tired of seeing the past. There are any number of guides that will help you <em>visualize</em> that past which cannot be seen, but often we forget what a creative act visualization is. We are perhaps too tied to our screens, too much invested in ‘seeing’. Let me hear something of the past instead.</p>
<p>While there is a deep history and literature on archaeoacoustics and soundscapes that try to capture the sound of a place <em>as it was</em> (<a href="https://www.digitalstudies.org/articles/10.16995/dscn.58">see for instance the Virtual St. Paul&#39;s</a> or the work of <a href="https://jeffdveitch.wordpress.com/">Jeff Veitch on ancient Ostia</a>), I am interested instead to ’sonify&#39; what I have <em>right now</em>, the data themselves. I want to figure out a grammar for representing data in sound that is appropriate for history. <a href="#Drucker">Drucker</a> <a href="http://web.archive.org/web/20190203083307/http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html">famously reminds us</a> that ‘data’ are not really things given, but rather things captured, things transformed: that is to say, ‘capta’. In sonifying data, I literally perform the past in the present, and so the assumptions, the transformations, I make are foregrounded. The resulting aural experience is a literal ‘deformance’ (portmanteau of ‘deform’ and ‘perform’) that makes us hear modern layers of the past in a new way.</p>
<p>I want to hear the meaning of the past, but I know that I can’t. Nevertheless, when I hear an instrument, I can imagine the physicality of the player playing it; in its echoes and resonances I can discern the physical space. I can feel the bass; I can move to the rhythm. The music engages my whole body, my whole imagination. Its associations with sounds, music, and tones I’ve heard before create a deep temporal experience, a system of embodied relationships between myself and the past. Visual? We have had visual representations of the past for so long, we have almost forgotten the artistic and performative aspect of those grammars of expression.</p>
<p>In this tutorial, you will learn to make some noise from your data about the past. The <em>meaning</em> of that noise, well... that&#39;s up to you. Part of the point of this tutorial is to make your data unfamiliar again. By translating it, transcoding it, <a href="http://blog.taracopplestone.co.uk/making-things-photobashing-as-archaeological-remediation/"><em>remediating</em></a> it, we begin to see elements of the data that our familiarity with visual modes of expression have blinded us to. This deformation, this deformance, is in keeping with arguments made by for instance Mark Sample on <a href="http://www.samplereality.com/2012/05/02/notes-towards-a-deformed-humanities/">breaking things</a>, or Bethany Nowviskie on the &#39;<a href="http://nowviskie.org/2013/resistance-in-the-materials/">resistance in the materials</a>&#39;. Sonification moves us along the continuum from data to capta, social science to art, <a href="http://nooart.org/post/73353953758/temkin-glitchhumancomputerinteraction">glitch to aesthetic</a>. So let&#39;s see what this all sounds like.</p>
<h2 id="objectives">Objectives</h2>
<p>In this tutorial, I will introduce you to three different ways of generating sound or music from your data.</p>
<p>In the first, we will use a freely available and free-to-use system developed by Jonathan Middleton called <em>Musicalgorithms</em>, to introduce some of the issues and key terms involved. In the second, we will use a small python library to &#39;parameter map&#39; our data against the 88 key keyboard, and introduce some artistry into our work. Finally, we will learn how to load our data into the open source live-coding environment for sound and music, <em>Sonic Pi</em>, at which time I will leave you to explore that project&#39;s copious tutorials and resources.</p>
<p>You will see that &#39;sonification&#39; moves us along the spectrum from mere &#39;visualization/auralization&#39; to actual performance.</p>
<h3 id="tools">Tools</h3>
<ul>
<li>Musicalgorithms <a href="http://musicalgorithms.org/">http://musicalgorithms.org/</a></li>
<li>MIDITime <a href="https://github.com/cirlabs/miditime">https://github.com/cirlabs/miditime</a> (I have forked a copy <a href="https://github.com/shawngraham/miditime">here</a>)</li>
<li>Sonic Pi <a href="http://sonic-pi.net/">http://sonic-pi.net/</a></li>
</ul>
<h3 id="example-data">Example Data</h3>
<ul>
<li><a href="/assets/sonification-roman-data.csv">Roman artefact data</a></li>
<li><a href="/assets/sonification-diary.csv">Excerpt from the Topic model of John Adams&#39; Diary</a></li>
<li><a href="/assets/sonification-jesuittopics.csv">Excerpt from the Topic model of the Jesuit Relations</a></li>
</ul>
<h1 id="some-background-on-sonification">Some Background on Sonification</h1>
<p>Sonification is the practice of mapping aspects of the data to produce sound signals. In general, a technique can be called ‘sonification’ if it meets certain conditions. These include reproducibility (the same data can be transformed the same ways by other researchers and produce the same results) and what might be called intelligibility - that the ‘objective’ elements of the original data are reflected systematically in the resulting sound (see <a href="#Hermann">Hermann</a> <a href="http://www.icad.org/Proceedings/2008/Hermann2008.pdf">2008</a> for a taxonomy of sonification). <a href="#Last">Last and Usyskin</a> <a href="https://www.researchgate.net/publication/282504359_Listen_to_the_Sound_of_Data">(2015)</a> designed a series of experiments to determine what kinds of data-analytic tasks could be performed when the data were sonified. Their experimental results (Last and Usyskin 2015) have shown that even untrained listeners (listeners with no formal training in music) can make useful distinctions in the data. They found listeners could discriminate in the sonified data common data exploration tasks such as classification and clustering. (Their sonified outputs mapped the underlying data to the Western musical scale.)</p>
<p>Last and Usyskin focused on time-series data.  They argue that time-series data are particularly well suited to sonification because there are natural parallels with musical sound. Music is sequential, it has duration, and it evolves over time; so too with time-series data <a href="https://www.researchgate.net/publication/282504359_Listen_to_the_Sound_of_Data">(Last and Usyskin 2015: 424)</a>. It becomes a problem of matching the data to the appropriate sonic outputs. In many applications of sonification, a technique called ‘parameter mapping’ is used to marry aspects of the data along various auditory dimensions such as <a href="#pitch">pitch</a>, variation, brilliance, and onset. The problem with this approach is that where there is no temporal relationship (or rather, no non-linear relationship) between the original data points, the resulting sound can be ‘confusing’ (2015: 422).</p>
<h2 id="hearing-the-gaps">Hearing the Gaps</h2>
<p>There is also the way that we fill in gaps in the sound with our expectations. Consider this video where the <a href="#mp3">mp3</a> has been converted to <a href="#midi">MIDI</a> back to mp3; the music has been &#39;flattened&#39; so that all sonic information is being played by one instrument. (Generating this effect is rather like saving a webpage as .txt, opening it in Word, and then resaving it as .html). All sounds (including vocals) have been translated to their corresponding note values, and then turned back into an mp3.</p>
<p>It is noisy; yet we perceive meaning. Consider the video below:</p>
<iframe src="https://player.vimeo.com/video/149070596" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

<p>What&#39;s going on here? If that song was already known to you, you probably heard the actual &#39;words&#39;. Yet, no words are present in the song! If the song was not already familiar to you, it sounded like garbled nonsense (see more examples on <a href="#Baio">Andy Baio&#39;s</a> <a href="http://waxy.org/2015/12/if_drake_was_born_a_piano/">website</a>). This effect is sometimes called an &#39;auditory hallucination&#39;(cf. <a href="#Koebler">Koebler, 2015</a>). This example shows how in any representation of data we can hear/see what is not, strictly speaking, there. We fill the holes with our own expectations.</p>
<p>Consider the implications for history. If we sonify our data, and begin to hear patterns in the sound, or odd outliers, our cultural expectations about how music works (our memories of similar snippets of music, heard in particular contexts) are going to colour our interpretation. This I would argue is true about all representations of the past, but sonifying is just odd enough to our regular methods that this self-awareness will help us identify or communicate the critical patterns in the (data of the) past.</p>
<p>We will progress through three different tools for sonifying data, noting how choices in one tool affect the output, and can be mitigated by reimagining the data via another tool. Ultimately, there is nothing any more objective in &#39;sonification&#39; than there is in &#39;visualization&#39;, and so the investigator has to be prepared to justify her choices, and to make these choices transparent and reproducible for others. (And lest we think that sonification and algorithmically generated music is somehow a &#39;new&#39; thing, I direct the interested reader to <a href="#hedges">Hedges, (1978)</a>.)</p>
<p>In each section, I will give a conceptual introduction, followed by a walkthrough using sample archaeological or historical data.</p>
<h1 id="musicalgorithms">Musicalgorithms</h1>
<p>There are a wide variety of tools out there to sonify data. Some for instance are packages for the widely-used <a href="https://cran.r-project.org/">R statistical environment</a>, such as ‘<a href="https://cran.r-project.org/web/packages/playitbyr/index.html">playitbyR</a>’ and ‘<a href="https://cran.r-project.org/web/packages/audiolyzR/index.html">AudiolyzR</a>’. The first of these however has not been maintained or updated to work with the current version of R (its last update was a number of years ago), and the second requires considerable configuration of extra software to make it work properly.</p>
<p>By contrast, the <a href="http://musicalgorithms.org/">Musicalgorithms</a> site is quite easy to use. The Musicalgorithms site has been online for over a decade. Though it is not open source, it represents a long-term research project in computational music by its creator, Jonathan Middleton. It is currently in its third major iteration (earlier iterations remain usable online). We will begin with Musicalalgorithms because it allows us to quickly enter and tweak our data to produce a MIDI file representation. Make sure to select &#39;<a href="http://musicalgorithms.org/3.0/index.html">Version 3</a>.&#39;</p>
<p>{% include figure.html filename=&quot;sonification-musicalgorithms-main-site-1.png&quot; caption=&quot;The Musicalgorithms Website as it appeared on February 2nd, 2016&quot; %}</p>
<p>Musicalgorithms effects a number of transformations on the data. In the sample data below (the default from the site itself), there is but one row of data, even if it looks like several rows. The sample data is comprised of comma separated fields that are themselves space delimited.</p>
<pre><code># Of Voices, Text Area Name, Text Area Data
1,morphBox,
,areaPitch1,2 7 1 8 2 8 1 8 2 8 4 5 9 0 4 5 2 3 5 3 6 0 2 8
,dAreaMap1,2 7 1 8 2 8 1 8 2 8 4 5 9 0 4 5 2 3 5 3 6 0 2 8
,mapArea1,20 69 11 78 20 78 11 78 20 78 40 49 88 1 40 49 20 30 49 30 59 1 20 78
,dMapArea1,1 5 1 5 1 5 1 5 1 5 3 3 6 0 3 3 1 2 3 2 4 0 1 5
,so_text_area1,20 69 11 78 20 78 11 78 20 78 40 49 88 1 40 49 20 30 49 30 59 1 20 78
</code></pre>
<p>This data represents the source data and its transformations; sharing this data would allow another investigator to replicate or extend the sonification using other tools. When one begins however, only the basic data below is needed (a list of data points):</p>
<pre><code># Of Voices, Text Area Name, Text Area Data
1,morphBox,
,areaPitch1,24 72 12 84 21 81 14 81 24 81 44 51 94 01 44 51 24 31 5 43 61 04 21 81
</code></pre>
<p>The key field for us is ‘areaPitch1,’ which contains the space-delimited input data.  The other fields will become filled as we move through Musicalgorithms&#39; various settings. In the data above (eg 24 72 12 84 etc), the values are raw counts of inscriptions from a series of sites along a Roman road in Britain. (We will practice with other data in a moment below).</p>
<p>{% include figure.html filename=&quot;sonification-musicalgorithms-pitch-mapping-2.png&quot; caption=&quot;After you load your data, you can select the different operations across the top menu bar of the site. In the screenshot, the information mouseover is explaining what happens to the scaling of your data if you select the division operation to scale your data to the range of notes selected.&quot; %}</p>
<p>Now, as you page across the various tabs in the interface (‘<a href="#duration">duration</a> input’, ‘[pitch mapping](#pitch mapping)’, ‘duration mapping’, ‘scale options’) you can effect various transformations. In ‘pitch mapping’, there are a number of mathematical options for mapping the data against the full 88 keys/pitches of a piano keyboard (in a linear mapping, the <em>mean</em> of one’s data would be mapped to middle C, or 40). One can also choose the kind of scale, whether it is a minor or major and so on. At this point, once you&#39;ve selected your various transformations, you should save the text file. On the file tab, ‘play’, one can download a midi file. Your default audio program can play midi files (often defaulting to a piano tone). More complicated instrumentation can be assigned by opening the midi file in music mixing programs such as GarageBand (Mac) or <a href="https://lmms.io/">LMMS</a> (Windows, Mac, Linux). (Using Garageband or LMMS are outside the scope of this tutorial. A video tutorial on LMMS is available <a href="https://youtu.be/4dYxV3tqTUc">here</a>, while Garageband tutorials proliferate online. Lynda.com has <a href="http://www.lynda.com/GarageBand-tutorials/Importing-audio-tracks/156620/164050-4.html">an excellent one</a>)</p>
<p>If you had several columns of data for the same points - say, in our example from Roman Britain, we also wanted to sonify counts of a pottery type for those same towns - you can reload your next data series, effect the transformations and mappings, and generate another MIDI file. Since Garageband and LMMS allow for overlaying of voices, you can begin to build up complicated sequences of music.</p>
<p>{% include figure.html filename=&quot;sonification-garageband-john-adams-3.png&quot; caption=&quot;Screenshot of Garageband, where the midi files are sonified topics from John Adams&#39; Diary. In the Garageband interface (LMMS is similar), each midi file is dragged-and-dropped into place. The instrumentation for each midi file (ie track) can be selected from Garageband&#39;s menus. The labels for each track have here been changed to reflect the key words in each topic. The green area to the right represents a visualization of the notes in each track. You can watch this interface in action and listen to the music <a href="https://youtu.be/ikqRXtI3JeA">here</a>.&quot; %}</p>
<p>Which transformations should you use? If you had two columns of data, you have two voices. It might make sense, in our hypothetical data, to play the first voice loud, in a major key: inscriptions &#39;speak&#39; to us, in a manner of speaking, after all. (Roman inscriptions do address the reader, the passerby, literally: &#39;O you who passes by...&#39;). Then, perhaps since the pottery you are interested in are humble wares, perhaps they would be mapped against the lower end of the scale, or given longer duration notes to represent their ubiquity across classes in this region.</p>
<p><em>There is no &#39;right&#39; way to represent your data as sound</em>, at least not yet: but even with this simple example, we begin to see how shades of meaning and interpretation can be inflected into our data and into our experience of that data.</p>
<p>But what about time? Historical data often has a punctuation point, a distinct &#39;time when&#39; something occured. Thus, the amount of time between two data points has to be taken into account. This is where our next tool becomes quite useful, for when our data points have a relationship to one another in temporal space. We begin to move from sonfication (data points) to music (relationships between points).</p>
<h3 id="practice">Practice</h3>
<p>The <a href="/assets/sonification-roman-data.csv">sample dataset</a> provided contains counts of Roman coins in its first column and counts of other Roman materials from the same locations, as contained in the Portable Antiquities Scheme database from the British Museum. A sonification of this data might reveal or highlight aspects of the economic situation along Watling street, a major route through Roman Britain. The data points are organized geographically from North West to South East; thus as the sound plays out, we are hearing movement over space. Each note represents another stop along the way.</p>
<ol>
<li><p>Open the<a href="/assets/sonification-roman-data.csv">sonification-roman-data.csv</a> in a spreadsheet. Copy the first column into a text editor. Delete the line endings so that the data is all in a single row.</p>
</li>
<li><p>Add the following column information like so:</p>
<pre><code># Of Voices, Text Area Name, Text Area Data
1,morphBox,
,areaPitch1,
</code></pre>
<p>...so that your data follows immediately after that last comma (as like <a href="/assets/sonification-romancoin-data-music.csv">this</a>). Save the file with a useful name like <code>coinsounds1.csv</code>.</p>
</li>
<li><p>Go to the <a href="http://musicalgorithms.org/3.0/index.html">Musicalgorithms</a> site (version 3), and hit the load button. In the pop-up, click the blue &#39;load&#39; button and select the file saved in step 2. The site will load your materials and display a green check mark if it loaded successfully. If it did not, make sure that your values are separated by spaces, and that they follow immediately the last comma in the code block in step 2. You may also try loading up the <a href="/assets/sonification-romancoin-data-music.csv">demo file for this tutorial</a> instead.{% include figure.html filename=&quot;sonification-musicalgorithms-upload-4.png&quot; caption=&quot;Click &#39;load&#39; on the main screen to get this dialogue box. Then &#39;load csv&#39;. Select your file; it will appear in the box. Then click the bottom load button.&quot; %}</p>
</li>
<li><p>Click on &#39;Pitch Input&#39;. You&#39;ll see the values of your data. For now, <strong>do not select</strong> any further options on this page (thus using the site&#39;s default values).</p>
</li>
<li><p>Click on &#39;Duration Input&#39;. <strong>Do not select any options here for now</strong>. The options here will map various transformations against your data that will alter the duration for each note. Do not worry about these options for now; move on.</p>
</li>
<li><p>Click on &#39;Pitch Mapping&#39;. This is the most crucial choice, as it will transform (that is, scale) your raw data to a mapping against the keys of the keyboard. Leave the <code>mapping</code> set to &#39;division&#39;.  (The other options are modulo or logarithmic). The option <code>Range</code> 1 to 88 uses the full 88 keys of the keyboard; thus your lowest value would accord to the deepest note on the piano and your highest value with the highest note. You might wish instead to constrain your music around middle C, so enter 25 to 60 as your range. The output should change to: <code>31,34,34,34,25,28,30,60,28,25,26,26,25,25,60,25,25,38,33,26,25,25,25</code> These are no longer your counts; they are notes on the keyboard.{% include figure.html filename=&quot;sonification-musicalgorithms-settings-for-pitch-mapping-5.png&quot; caption=&quot;Click into the &#39;range&#39; box and set it to 25. The values underneath will change automatically. Click into the &#39;to&#39; box and set it to 60. Click back into the other box; the values will update.&quot; %}</p>
</li>
<li><p>Click on &#39;Duration Mapping&#39;. Like Pitch Mapping, this takes a range of times that you specify and uses the various mathematical options to map that range of possibilities against your notes. If you mouse over the <code>i</code> you will see how the numbers correspond with whole notes, quarter notes, eigth notes, and so on. Leave the default values for now.</p>
</li>
<li><p>Click on &#39;Scale Options&#39;. Here we can begin to select something of what might be called the &#39;emotional&#39; aspect to sound. We commonly think of major scales being &#39;happy&#39; while minor scales are &#39;sad&#39;; for an accessible discussion see <a href="http://www.ethanhein.com/wp/2010/scales-and-emotions/">this blog post</a>. For now, select &#39;scale by: major&#39;. Leave the &#39;scale&#39; as <code>C</code>.</p>
</li>
</ol>
<p>You have now sonified one column of data! Click on the &#39;save&#39; button, then &#39;save csv&#39;. {% include figure.html filename=&quot;sonification-musicalgorithms-save-6.png&quot; caption=&quot;The save data dialogue box.&quot; %}You&#39;ll have a file that looks something like this:</p>
<pre><code># Of Voices, Text Area Name, Text Area Data
1,morphBox,
,areaPitch1,80 128 128 128 1 40 77 495 48 2 21 19 1 1 500 1 3 190 115 13 5 1 3
,dAreaMap1,2 7 1 8 2 8 1 8 2 8 4 5 9 0 4 5 2 3 5 3 6 0 2
,mapArea1,31 34 34 34 25 28 30 60 28 25 26 26 25 25 60 25 25 38 33 26 25 25 25
,dMapArea1,1 5 1 5 1 5 1 5 1 5 3 3 6 0 3 3 1 2 3 2 4 0 1
,so_text_area1,32 35 35 35 25 28 30 59 28 25 27 27 25 25 59 25 25 39 33 27 25 25 25
</code></pre>
<p>You can see your original data in the &#39;areaPitch1&#39; field, and your subsequent mappings. The site allows you to generate up to four voices at a time into a single MIDI file; depending on how you want to add instrumentation subsequently, you might wish to generate one MIDI file at a time. Let&#39;s play the music - click on &#39;Play&#39;. You can select the tempo here, and an instrument. You can listen to your data in the browser, or save as a MIDI file by clicking the blue &#39;Save MIDI file&#39;.</p>
<p>Go back to the beginning, and load both columns of data into this template:</p>
<pre><code># Of Voices, Text Area Name, Text Area Data
2,morphBox,
,areaPitch1,
,areaPitch2,
</code></pre>
<p>{% include figure.html filename=&quot;sonification-2voices-7.png&quot; caption=&quot;Put 2 into the voices box at the top of the interface. When you then go to any of the option pages - here, we&#39;re at &#39;pitch input&#39; - two displays open up to show you the data for two voices. Load your csv data as before, but have your csv formatted to have &#39;areaPitch1&#39; and &#39;areaPitch2&#39; as described in the main text. The data for voice one will appear on the left, and for voice two on the right.&quot; %}</p>
<p>When you have multiple voices of data, what stands out? Note that in this approach, the distance between points in the real world is not factored into our sonification. This distance, if it were, might be crucical. Distance, of course, does not have to be geographic - it can be temporal. The next tool we&#39;ll explore allows us to factor that into our sonification explicitly.</p>
<h1 id="a-quick-word-about-getting-python-set-up">A quick word about getting Python set up</h1>
<p>The next section of this tutorial requires Python. If you haven&#39;t experimented with Python yet, you will need to spend some time <a href="/lessons/intro-to-bash">becoming familiar with the command line (PC) or terminal (OS)</a>. You might find this quick <a href="/lessons/installing-python-modules-pip">guide to installing python &#39;modules&#39;</a> handy (but come back to it after you read the rest of this section).</p>
<p>Mac users will already have Python installed on their machine. You can test this by holding down the COMMAND button and the spacebar; in the search window, type <code>terminal</code> and click on the terminal application. At the prompt, eg, the cursor blinking at <code>$</code> type <code>python --version</code> and the computer will respond with what version of python you have. <em>This next section of the tutorial assumes Python 2.7; it has not been tested on Python 3</em>.</p>
<p>For Windows users, Python is not installed by default on your machine so <a href="http://docs.python-guide.org/en/latest/starting/install/win/">this page</a> will help you get started, though things are a bit more complicated than that page makes out. First, download the <code>.msi</code> file that that page recommends (Python 2.7). Double click the file, and it should install itself in a new directory, eg <code>C:\Python27\</code>. Then, we have to tell Windows the location of where to look for Python whenever you run a python program; that is, you put the location of that directory into your &#39;path&#39;, or the environment variable that windows always checks when confronted with a new command. There are a couple ways of doing this, but perhaps the easiest is to search your computer for the program <code>Powershell</code> (type &#39;powershell&#39; into your windows computer search). Open Powershell, and at the <code>&gt;</code> prompt, paste this entire line:</p>
<p><code>[Environment]::SetEnvironmentVariable(&quot;Path&quot;, &quot;$env:Path;C:\Python27\;C:\Python27\Scripts\&quot;, &quot;User&quot;)</code></p>
<p>You can close powershell when you&#39;re done. You&#39;ll know it worked if nothing very much happens once you&#39;ve pressed &#39;enter&#39;. To test that everything is okay, open a command prompt (here are <a href="http://www.howtogeek.com/235101/10-ways-to-open-the-command-prompt-in-windows-10/">10 ways to do this</a>) and type at the <code>&gt;</code> prompt <code>python --version</code>. It should tell you <code>Python 2.7.10</code> or similar.</p>
<p>The last piece of the puzzle that all users will need is a program called <code>Pip</code>. Mac users can install it by typing at the terminal :<code>sudo easy_install pip</code>. Windows users have a bit of a harder time. First, right-click and save-as this link: <a href="https://bootstrap.pypa.io/get-pip.py">https://bootstrap.pypa.io/get-pip.py</a> (If you just click on the link, it will show you the code in your browser). Save it somewhere handy. Open a command prompt in the directory where you saved <code>get-pip.py</code>. Then, type at the command prompt <code>python get-pip.py</code>. Conventionally, in tutorials, you will see <code>&gt;</code> or <code>$</code> at points where you are required to enter something at the command prompt or the terminal. You don&#39;t ever have to type those two characters.</p>
<p>Finally, when you have python code you want to run, you can enter it in your text editor and save it with the <code>.py</code> extension. Your file is a text file, but the file <strong>extension</strong> tells your computer to use Python to interpret it; but remember, type <code>python</code> at the prompt first, eg: <code>$ python my-cool-script.py</code>.</p>
<h1 id="miditime">MIDITime</h1>
<p>MIDITime is a python package developed by <a href="https://www.revealnews.org/">Reveal News (formerly, the Centre for Investigative Reporting)</a>. Its <a href="https://github.com/cirlabs/miditime">Github repository is here</a>. Miditime is built explicitly for time series data (that is, a sequence of observations collected over time).</p>
<p>While the Musicalgorithms tool has a more-or-less intuitive interface, the investigator sacrifices the ability to know what, exactly, is going on under the hood. In principle, one could examine the underlying code for the MIDITime package to see exactly what&#39;s going on. More importantly, the previous tool had no ability to account for data where the points are distant from one another in clock-time. MIDITime lets us take into account that our data might be clustering in time.</p>
<p>Let us assume that you have a historic diary to which you&#39;ve fitted a <a href="/lessons/topic-modeling-and-mallet">topic model</a>. The resulting output might have diary entries as rows, and the percentage composition each topic contributes to that entry as the columns. In which case, <em>listening</em> to these values might help you understand the patterns of thought in the diary in a way that visualizing as a graph might not. Outliers or recurrent musical patterns could stand out to the ear in a way the grammar of graphs obscures.</p>
<h3 id="installing-miditime">Installing MIDITime</h3>
<p>Installing miditime is straightforward using <a href="/lessons/installing-python-modules-pip">pip</a>:</p>
<p><code>$ pip install miditime</code> or <code>$ sudo pip install miditime</code> for a Mac or Linux machine;
<code>&gt; python pip install miditime</code> on a Windows machine. (Windows users, if the instructions above didn&#39;t quite work for you, you might want to try <a href="https://sites.google.com/site/pydatalog/python/pip-for-windows">this helper program</a> instead to get Pip working properly on your machine).</p>
<h3 id="practice-1">Practice</h3>
<p>Let us look at the sample script provided. Open your text editor, and copy and paste the sample script in:</p>
<pre><code class="language-python">#!/usr/bin/python

from miditime.miditime import MIDITime
# NOTE: this import works at least as of v1.1.3; for older versions or forks of miditime, you may need to use
# from miditime.MIDITime import MIDITime

# Instantiate the class with a tempo (120bpm is the default) and an output file destination.
mymidi = MIDITime(120, &#39;myfile.mid&#39;)

# Create a list of notes. Each note is a list: [time, pitch, attack, duration]
midinotes = [
    [0, 60, 200, 3],  #At 0 beats (the start), Middle C with attack 200, for 3 beats
    [10, 61, 200, 4]  #At 10 beats (12 seconds from start), C#5 with attack 200, for 4 beats
]

# Add a track with those notes
mymidi.add_track(midinotes)

# Output the .mid file
mymidi.save_midi()
</code></pre>
<p>Save this script as <code>music1.py</code>. At your terminal or command prompt, run the script:</p>
<p><code>$ python music1.py</code></p>
<p>A new file, <code>myfile.mid</code> will be written to your directory. To hear this file, you can open it with Quicktime or Windows Media Player. (You can add instrumentation to it by opening it in Garageband or <a href="https://lmms.io/">LMMS</a>).</p>
<p><code>Music1.py</code> imports miditime (remember, you must do <code>pip install miditime</code> before running the script). Then, it creates an output file destination and sets the tempo. The notes are all listed individually, where the first number is the time when the note should be played, the pitch of the note (ie, the actual note!), how hard or rythmically the note is hit (the <a href="#attack">attack</a>), and then how long the note lasts. The notes are then written to the track, and then the track is written to <code>myfile.mid</code>.</p>
<p>Play with this script now, and add more notes. The notes for &#39;Baa Baa Black Sheep&#39; are:</p>
<pre><code>D, D, A, A, B, B, B, B, A
Baa, Baa, black, sheep, have, you, any, wool?
</code></pre>
<p>Can you make your computer play this song? (This <a href="https://web.archive.org/web/20171211192102/http://www.electronics.dit.ie/staff/tscarff/Music_technology/midi/midi_note_numbers_for_octaves.htm">chart</a> will help).</p>
<p><strong>By the way</strong> There is a text file specification for describing music called &#39;<a href="http://abcnotation.com/wiki/abc:standard:v2.1">ABC Notation</a>&#39;. It is beyond us for now, but one could write a sonification script in say a spreadsheet, mapping values to note names in the ABC specification (if you&#39;ve ever used an IF - THEN in Excel to convert percentage grades to letter grades, you&#39;ll have a sense of how this might be done) and then using a site like <a href="http://trillian.mit.edu/~jc/music/abc/ABCcontrib.html">this one</a> to convert the ABC notation into a .mid file.</p>
<h3 id="getting-your-own-data-in">Getting your own data in</h3>
<p><a href="/assets/sonification-diary.csv">This file</a> is a selection from the topic model fitted to John Adams&#39; Diaries for<a href="http://themacroscope.org">The Macroscope</a>. Only the strongest signals have been preserved by rounding the values in the columns to two decimal places (remembering that .25 for instance would indicate that that topic is contributing to a quarter of that diary entry&#39;s composition). To get this data into your python script, it has to be formatted in a particular away. The tricky bit is getting the date field right.</p>
<p><em>For the purposes of this tutorial, we are going to leave the names of variables and so on unchanged from the sample script. The sample script was developed with earthquake data in mind; so where it says &#39;magnitude&#39; we can think of it as equating to &#39;% topic composition.&#39;</em></p>
<pre><code>my_data = [
    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 3.4},
    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 3.2},
    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 3.6},
    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 3.0},
    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 5.6},
    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 4.0}
]
</code></pre>
<p>One could approach the problem of getting our data into that format using regular expressions; it might be easier to just open our topic model in a spreadsheet. Copy the topic data to a new sheet, and leave columns to the left and to the right of the data. In the example below, I put it in column D, and then filled in the rest of the data around it, like so:</p>
<table>
<thead>
<tr>
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>{&#39;event_date&#39;: datetime</td>
<td>(1753,6,8)</td>
<td>, &#39;magnitude&#39;:</td>
<td>0.0024499630</td>
<td>},</td>
</tr>
<tr>
<td>2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>Then copy and paste the elements that do not change to fill up the entire column. The date element has to be (year,month,day). Once you&#39;ve filled up the table, you can copy and paste it into your text editor so that it becomes part of the <code>my_data</code> array, like so:</p>
<pre><code>my_data = [
{&#39;event_date&#39;: datetime(1753,6,8), &#39;magnitude&#39;:0.0024499630},
{&#39;event_date&#39;: datetime(1753,6,9), &#39;magnitude&#39;:0.0035766320},
{&#39;event_date&#39;: datetime(1753,6,10), &#39;magnitude&#39;:0.0022171550},
{&#39;event_date&#39;: datetime(1753,6,11), &#39;magnitude&#39;:0.0033220150},
{&#39;event_date&#39;: datetime(1753,6,12), &#39;magnitude&#39;:0.0046445900},
{&#39;event_date&#39;: datetime(1753,6,13), &#39;magnitude&#39;:0.0035766320},
{&#39;event_date&#39;: datetime(1753,6,14), &#39;magnitude&#39;:0.0042241550}
]
</code></pre>
<p>Note that the last row does not have a comma at the end of the line.</p>
<p>Your final script will look something like this, using the example from the Miditime page itself (the code sections below have been interrupted by commentary, but should be pasted into your text editor as a single file):</p>
<pre><code class="language-python">from miditime.MIDITime import MIDITime
from datetime import datetime
import random

mymidi = MIDITime(108, &#39;johnadams1.mid&#39;, 3, 4, 1)
</code></pre>
<p>The values after MIDITime, <code>MIDITime(108, &#39;johnadams1.mid&#39;, 3, 4, 1)</code> set</p>
<ul>
<li>the beats per minute (108),</li>
<li>the output file (&#39;johnadams1.mid&#39;),</li>
<li>the number of seconds to represent a year in the music (3 seconds to a calendar year, so all of the notes for diary entries from 1753 will be scaled against 3 seconds; there are 50 years in the data, so the final song will be 50 x 3 seconds long, or a bit over two minutes),</li>
<li>the base octave for the music (middle C is conventionally represented as C5, so here 4 represents one octave below middle C),</li>
<li>and how many octaves to map the pitches against.</li>
</ul>
<p>Now we pass our data into the script by feeding it into the <code>my_data</code> array (this gets pasted in next):</p>
<pre><code class="language-python">my_data = [
{&#39;event_date&#39;: datetime(1753,6,8), &#39;magnitude&#39;:0.0024499630},
{&#39;event_date&#39;: datetime(1753,6,9), &#39;magnitude&#39;:0.0035766320},
</code></pre>
<p>...have your data in here, remembering to end the final event_date line <strong>without</strong> a comma, and finishing the data with a <code>]</code> on its own line, eg</p>
<pre><code class="language-python">{&#39;event_date&#39;: datetime(1753,6,14), &#39;magnitude&#39;:0.0042241550}
]
</code></pre>
<p>and then paste:</p>
<pre><code class="language-python">my_data_epoched = [{&#39;days_since_epoch&#39;: mymidi.days_since_epoch(d[&#39;event_date&#39;]), &#39;magnitude&#39;: d[&#39;magnitude&#39;]} for d in my_data]

my_data_timed = [{&#39;beat&#39;: mymidi.beat(d[&#39;days_since_epoch&#39;]), &#39;magnitude&#39;: d[&#39;magnitude&#39;]} for d in my_data_epoched]

start_time = my_data_timed[0][&#39;beat&#39;]
</code></pre>
<p>This part works out the timing between your different diary entries; diaries that are close together in time will therefore have their notes sounding closer together. Finally, we define how the data get mapped against the pitch. Remembering that our data are percentages ranging from 0.01 (ie 1%) to 0.99 (99%), we <code>scale_pct</code> between 0 and 1. If you weren&#39;t dealing with percentages, you&#39;d use your lowest value and your highest value (if for instance your data were counts of some element of interest, as in the archaology data used earlier). Thus, we paste in:</p>
<pre><code class="language-python">def mag_to_pitch_tuned(magnitude):
    scale_pct = mymidi.linear_scale_pct(0, 1, magnitude)
    # Pick a range of notes. This allows you to play in a key.
    c_major = [&#39;C&#39;, &#39;C#&#39;, &#39;D&#39;, &#39;D#&#39;, &#39;E&#39;, &#39;E#&#39;, &#39;F&#39;, &#39;F#&#39;, &#39;G&#39;, &#39;G#&#39;, &#39;A&#39;, &#39;A#&#39;, &#39;B&#39;, &#39;B#&#39;]

    #Find the note that matches your data point
    note = mymidi.scale_to_note(scale_pct, c_major)

    #Translate that note to a MIDI pitch
    midi_pitch = mymidi.note_to_midi_pitch(note)

    return midi_pitch

note_list = []

for d in my_data_timed:
    note_list.append([
        d[&#39;beat&#39;] - start_time,
        mag_to_pitch_tuned(d[&#39;magnitude&#39;]),
        random.randint(0,200),  # attack
        random.randint(1,4)  # duration, in beats
    ])
</code></pre>
<p>and then paste in this final bit of code to write your sound values to file:</p>
<pre><code># Add a track with those notes
mymidi.add_track(midinotes)

# Output the .mid file
mymidi.save_midi()
</code></pre>
<p>Save this file with a new name and the <code>.py</code> file extension.</p>
<p>For each column of data in your original data, <strong>have a unique script and remember to change the output file name!</strong> Otherwise you will overwrite your data. Then, you can load the individual midi files into Garageband or LMMS for instrumentation. Here&#39;s the full <a href="https://www.youtube.com/watch?v=ikqRXtI3JeA">John Adams Diary</a>.</p>
<h1 id="sonic-pi">Sonic Pi</h1>
<p>Having unique midifiles that you arrange (in Garageband or some other music composition program) moves you from &#39;sonifying&#39; towards composition and sound art. In this final section, I do not offer you a full tutorial on using <a href="http://sonic-pi.net">Sonic Pi</a>, but rather point you towards this environment that allows for the actual live-coding and performance of your data (see <a href="https://www.youtube.com/watch?v=oW-3HVOeUQA">this video</a> for an actual live-coding performance). Sonic Pi&#39;s built-in tutorials will show you something of the potential of using your computer as an actual musical instrument (where you type Ruby code into its built-in editor while the interpreter plays what you encode).</p>
<p>Why would you want to do this? As has progressively become clear in tutorial, when you sonify your data you begin to make choices about how the data maps into sound, and these choices reflect implicit or explicit decisions about which data matter. There is a continuum of &#39;objectivity&#39;, if you will. At one end, a sonification that supports an argument about the past; at the other, a performance about the past as riveting and personal as any well-done public lecture. Sonification moves our data off the page and into the ears of our listeners: it is a kind of public history. Performing our data... imagine that!</p>
<p>Here, I offer simply a code snippet that will allow you to import your data, where your data is simply a list of values saved as csv. I am indebted to George Washington University librarian Laura Wrubel who posted to <a href="https://gist.github.com/lwrubel">gist.github.com</a> her experiments in sonifying her library&#39;s circulation transactions.</p>
<p>In this <a href="/assets/sonification-jesuittopics.csv">sample file</a>(a topic model generated from the <a href="http://puffin.creighton.edu/jesuit/relations/">Jesuit Relations</a>), there are two topics. The first row contains the headers: topic1, topic2.</p>
<h3 id="practice-2">Practice</h3>
<p>Follow the initial tutorials that Sonic Pi provides until you get a feel for the interface and some of the possibilities. (These tutorials are also concatenated <a href="https://gist.github.com/jwinder/e59be201082cca694df9">here</a>; you can also listen to an interview with Sam Aaron, the creator of Sonic Pi, <a href="https://devchat.tv/ruby-rogues/215-rr-sonic-pi-with-sam-aaron">here</a>). Then, in a new buffer (editor window), copy the following (again, the code snippets that follow will eventually be collated into a single script in your Sonic Pi buffer window):</p>
<pre><code>require &#39;csv&#39;
data = CSV.parse(File.read(&quot;/path/to/your/directory/data.csv&quot;), {:headers =&gt; true, :header_converters =&gt; :symbol})
use_bpm 100
</code></pre>
<p>Remember, <code>path/to/your/directory/</code> is the actual location of your data on your machine. Make sure it is either called <code>data.csv</code> or that you change the line above so that it actually loads your file!</p>
<p>Now, let&#39;s load that data into our music:</p>
<pre><code>#this bit of code will run only once, unless you comment out the line with
#&#39;live_loop&#39;, and also comment out the final &#39;end&#39; at the bottom
# of this code block
#&#39;commenting out&#39; means removing the # sign.

# live_loop :jesuit do
data.each do |line|
  topic1 = line[:topic1].to_f
  topic2 = line[:topic2].to_f

  use_synth :piano
  play topic1*100, attack: rand(0.5), decay: rand(1), amp: rand(0.25)
  use_synth :piano
  play topic2*100, attack: rand(0.5), decay: rand(1), amp: rand(0.25)
  sleep (0.5)
end
</code></pre>
<p>The first few lines load the columns of data in; then we say which sound sample we wish to use (piano) and then tell Sonic Pi to play topic 1 according to the following criteria (a random value less than 0.5 for the attack; a decay using a random value less than 1; and an <a href="#amplitude">amplitude</a> using a random value less than 0.25). See the x 100 in the line? That takes our data value (which is a decimal, remember) and turns it into a whole number. In this piece of code (the way I&#39;ve written it), that number equates directly with a note. If 88 is the lowest note and 1 is the highest, you can see that this approach is a bit problematic: we haven&#39;t actually done any pitch mapping here! In which case, you could use Musicalgorithms to do your pitch mapping, and then feed those values back into Sonic Pi. Alternatively, since this code is more or less Ruby, you could look up how to normalize the data and then do a linear mapping of your values against the range 1 - 88. A good place to start would be to study <a href="https://github.com/stevelloyd/Learn-sonification-with-Sonic-Pi">this worksheet by Steve Lloyd</a> on sonifying weather data with the Sonic Pi. Finally, the other thing to notice here is that the &#39;rand&#39; value (random) allows us to add a bit of &#39;humanity&#39; into the music in terms of the dynamics. Then we do the same thing again for topic2.</p>
<p>You can then add beats, loops, samples, and the whole parephernalia that Sonic Pi permits. Where you put code chunks affects the playback; if you put a loop before the data block above, the loop will play first. For instance, if you insert the following after the <code>use_bpm 100</code> line,</p>
<pre><code>#intro bit
sleep 2
sample :ambi_choir, attack: 2, sustain: 4, rate: 0.25, release: 1
sleep 6
</code></pre>
<p>You&#39;ll get a bit of an introductory ta-da for your piece. It waits 2 seconds, plays the &#39;ambi_choir&#39; sample, then waits 6 more seconds before playing our data. If you wanted to add a bit of an ominous drum sound that played throughout your piece, you&#39;d insert this bit next (and before your own data):</p>
<pre><code>#bit that keeps going throughout the music
live_loop :boom do
  with_fx :reverb, room: 0.5 do
    sample :bd_boom, rate: 1, amp: 1
  end
  sleep 2
end
</code></pre>
<p>The code is pretty clear: loop the &#39;bd_boom&#39; sample with the reverb sound effect, at a particular rate. Sleep 2 seconds between loops.</p>
<p>By the way, &#39;live-coding&#39;? What makes this a &#39;live-coding&#39; environment is that you can make changes to the code <em>while Sonic Pi is turning it into music</em>. Don&#39;t like what you&#39;re hearing? Change the code up on the fly!</p>
<p>For more on Sonic Pi, <a href="https://www.miskatonic.org/music/access2015/">this workshop website</a> is a good place to start. See also Laura Wrubel&#39;s <a href="http://library.gwu.edu/scholarly-technology-group/posts/sound-library-work">report on attending that workshop, and her and her colleague&#39;s work in this area</a>.</p>
<h1 id="nihil-novi-sub-sole">Nihil Novi Sub Sole</h1>
<p>Again, lest we think that we are at the cutting edge in our algorithmic generation of music, a salutary reminder was published in 1978 on &#39;dice music games&#39; of the eighteenth century, where rolls of the dice determined the recombination of pre-written snippets of music. <a href="https://rbnrpi.wordpress.com/project-list/mozart-dice-generated-waltz-revisited-with-sonic-pi/">Some of these games have been explored and re-coded for the Sonic-Pi by Robin Newman</a>. Newman also uses a tool that could be described as Markdown+Pandoc for musical notation, <a href="http://www.lilypond.org/">Lilypond</a> to score these compositions. The antecedents for everything you will find at <em>The Programming Historian</em> are deeper than you might suspect!</p>
<h1 id="conclusion">Conclusion</h1>
<p>Sonifying our data forces us to confront the ways our data are often not so much about the past, but rather our constructed versions of it. It does so partly by virtue of its novelty and the art and artifice required to map data to sound. But it does so also by its contrast with our received notions of visualization of data. It may be that the sounds one generates never rise to the level of &#39;music&#39;; but if it helps transform how we encounter the past, and how others engage with the past, then the effort will be worth it. As Trevor Owens might have put it, &#39;Sonfication is about <a href="http://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/">discovery, not justification&#39;</a>.</p>
<h2 id="terms">Terms</h2>
<ul>
<li><strong>MIDI</strong>,<a name="midi"></a>musical instrument digital interface. It is a description of a note&#39;s value and timing, not of its dynamics or how one might play it (this is an important distinction). It allows computers and instruments to talk to each other; one can apply different instrumentation to a MIDI file much the same way one would change the font on a piece of text (or run a markdown file through Pandoc).</li>
<li><strong>MP3</strong>,<a name="mp3"></a> a compression format for sound that is <em>lossy</em> in that it strips out data as part of its compression routine.</li>
<li><strong>Pitch</strong>,<a name="pitch"></a> the actual note itself (middle C, etc)</li>
<li><strong>Attack</strong>,<a name="attack"></a> how the note is played or hit</li>
<li><strong>Duration</strong>,<a name="duration"></a> how long the note lasts (whole notes, quarter notes, eighth notes etc)</li>
<li><strong>Pitch Mapping &amp; Duration Mapping</strong>, <a name="pitch mapping"></a> scaling data values against a range of notes or the length of the note</li>
<li><strong>Amplitude</strong>, <a name="amplitude"></a>roughly, the loudness of the note</li>
</ul>
<h1 id="references">References</h1>
<p><a name="Baio"></a>Baio, Andy. 2015. &#39;If Drake Was Born A Piano&#39;. Waxy. <a href="http://waxy.org/2015/12/if_drake_was_born_a_piano/">http://waxy.org/2015/12/if_drake_was_born_a_piano/</a></p>
<p><a name="Drucker"></a>Drucker, Johanna. 2011. Humanities Approaches to Graphical Display. DHQ 5.1 <a href="http://web.archive.org/web/20190203083307/http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html">http://web.archive.org/web/20190203083307/http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html</a></p>
<p><a name="Hedges"></a>Hedges, Stephen A. 1978. “Dice Music in the Eighteenth Century”. Music &amp; Letters 59 (2). Oxford University Press: 180–87. <a href="http://www.jstor.org/stable/734136">http://www.jstor.org/stable/734136</a>.</p>
<p><a name="Hermann"></a>Hermann, T. 2008. &quot;Taxonomy and definitions for sonification and auditory display&quot;. In P. Susini and O. Warusfel (eds.) Proceedings of the 14th international conference on auditory display (ICAD 2008). IRCAM, Paris. <a href="http://www.icad.org/Proceedings/2008/Hermann2008.pdf">http://www.icad.org/Proceedings/2008/Hermann2008.pdf</a></p>
<p><a name="Koebler"></a>Koebler, Jason. 2015. &quot;The Strange Acoustic Phenomenon Behind These Wacked-Out Versions of Pop Songs&quot; Motherboard, Dec 18. <a href="https://web.archive.org/web/20161023223029/http://motherboard.vice.com/read/the-strange-acoustic-phenomenon-behind-these-wacked-out-versions-of-pop-songs">https://web.archive.org/web/20161023223029/http://motherboard.vice.com/read/the-strange-acoustic-phenomenon-behind-these-wacked-out-versions-of-pop-songs</a></p>
<p><a name="Last"></a>Last and Usyskin, 2015. &quot;Listen to the Sound of Data&quot;. In Aaron K. Baughman et al. (eds.) Multimedia Data Mining and Analytics. Springer: Heidelberg. Pp. 419-446 <a href="https://www.researchgate.net/publication/282504359_Listen_to_the_Sound_of_Data">https://www.researchgate.net/publication/282504359_Listen_to_the_Sound_of_Data</a></p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="sonification/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"The Sound of Data (a gentle introduction to sonification for historians)\",\"layout\":\"lesson\",\"date\":\"2016-06-07T00:00:00.000Z\",\"authors\":[\"Shawn Graham\"],\"reviewers\":[\"Jeff Veitch\",\"Tim Compeau\"],\"editors\":[\"Ian Milligan\"],\"difficulty\":2,\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F4\",\"activity\":\"transforming\",\"topics\":[\"distant-reading\"],\"abstract\":\"There are any number of guides that will help you visualize the past, but this lesson will help you hear the past.\",\"redirect_from\":\"\u002Flessons\u002Fsonification\",\"avatar_alt\":\"A violin\",\"doi\":\"10.46430\u002Fphen0057\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"introduction\\\"\u003EIntroduction\u003C\u002Fh1\u003E\\n\u003Cp\u003Eποίησις - fabrication, creation, production\u003C\u002Fp\u003E\\n\u003Cp\u003EI am too tired of seeing the past. There are any number of guides that will help you \u003Cem\u003Evisualize\u003C\u002Fem\u003E that past which cannot be seen, but often we forget what a creative act visualization is. We are perhaps too tied to our screens, too much invested in ‘seeing’. Let me hear something of the past instead.\u003C\u002Fp\u003E\\n\u003Cp\u003EWhile there is a deep history and literature on archaeoacoustics and soundscapes that try to capture the sound of a place \u003Cem\u003Eas it was\u003C\u002Fem\u003E (\u003Ca href=\\\"https:\u002F\u002Fwww.digitalstudies.org\u002Farticles\u002F10.16995\u002Fdscn.58\\\"\u003Esee for instance the Virtual St. Paul&#39;s\u003C\u002Fa\u003E or the work of \u003Ca href=\\\"https:\u002F\u002Fjeffdveitch.wordpress.com\u002F\\\"\u003EJeff Veitch on ancient Ostia\u003C\u002Fa\u003E), I am interested instead to ’sonify&#39; what I have \u003Cem\u003Eright now\u003C\u002Fem\u003E, the data themselves. I want to figure out a grammar for representing data in sound that is appropriate for history. \u003Ca href=\\\"#Drucker\\\"\u003EDrucker\u003C\u002Fa\u003E \u003Ca href=\\\"http:\u002F\u002Fweb.archive.org\u002Fweb\u002F20190203083307\u002Fhttp:\u002F\u002Fwww.digitalhumanities.org\u002Fdhq\u002Fvol\u002F5\u002F1\u002F000091\u002F000091.html\\\"\u003Efamously reminds us\u003C\u002Fa\u003E that ‘data’ are not really things given, but rather things captured, things transformed: that is to say, ‘capta’. In sonifying data, I literally perform the past in the present, and so the assumptions, the transformations, I make are foregrounded. The resulting aural experience is a literal ‘deformance’ (portmanteau of ‘deform’ and ‘perform’) that makes us hear modern layers of the past in a new way.\u003C\u002Fp\u003E\\n\u003Cp\u003EI want to hear the meaning of the past, but I know that I can’t. Nevertheless, when I hear an instrument, I can imagine the physicality of the player playing it; in its echoes and resonances I can discern the physical space. I can feel the bass; I can move to the rhythm. The music engages my whole body, my whole imagination. Its associations with sounds, music, and tones I’ve heard before create a deep temporal experience, a system of embodied relationships between myself and the past. Visual? We have had visual representations of the past for so long, we have almost forgotten the artistic and performative aspect of those grammars of expression.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this tutorial, you will learn to make some noise from your data about the past. The \u003Cem\u003Emeaning\u003C\u002Fem\u003E of that noise, well... that&#39;s up to you. Part of the point of this tutorial is to make your data unfamiliar again. By translating it, transcoding it, \u003Ca href=\\\"http:\u002F\u002Fblog.taracopplestone.co.uk\u002Fmaking-things-photobashing-as-archaeological-remediation\u002F\\\"\u003E\u003Cem\u003Eremediating\u003C\u002Fem\u003E\u003C\u002Fa\u003E it, we begin to see elements of the data that our familiarity with visual modes of expression have blinded us to. This deformation, this deformance, is in keeping with arguments made by for instance Mark Sample on \u003Ca href=\\\"http:\u002F\u002Fwww.samplereality.com\u002F2012\u002F05\u002F02\u002Fnotes-towards-a-deformed-humanities\u002F\\\"\u003Ebreaking things\u003C\u002Fa\u003E, or Bethany Nowviskie on the &#39;\u003Ca href=\\\"http:\u002F\u002Fnowviskie.org\u002F2013\u002Fresistance-in-the-materials\u002F\\\"\u003Eresistance in the materials\u003C\u002Fa\u003E&#39;. Sonification moves us along the continuum from data to capta, social science to art, \u003Ca href=\\\"http:\u002F\u002Fnooart.org\u002Fpost\u002F73353953758\u002Ftemkin-glitchhumancomputerinteraction\\\"\u003Eglitch to aesthetic\u003C\u002Fa\u003E. So let&#39;s see what this all sounds like.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"objectives\\\"\u003EObjectives\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn this tutorial, I will introduce you to three different ways of generating sound or music from your data.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the first, we will use a freely available and free-to-use system developed by Jonathan Middleton called \u003Cem\u003EMusicalgorithms\u003C\u002Fem\u003E, to introduce some of the issues and key terms involved. In the second, we will use a small python library to &#39;parameter map&#39; our data against the 88 key keyboard, and introduce some artistry into our work. Finally, we will learn how to load our data into the open source live-coding environment for sound and music, \u003Cem\u003ESonic Pi\u003C\u002Fem\u003E, at which time I will leave you to explore that project&#39;s copious tutorials and resources.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou will see that &#39;sonification&#39; moves us along the spectrum from mere &#39;visualization\u002Fauralization&#39; to actual performance.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"tools\\\"\u003ETools\u003C\u002Fh3\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EMusicalgorithms \u003Ca href=\\\"http:\u002F\u002Fmusicalgorithms.org\u002F\\\"\u003Ehttp:\u002F\u002Fmusicalgorithms.org\u002F\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EMIDITime \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fcirlabs\u002Fmiditime\\\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fcirlabs\u002Fmiditime\u003C\u002Fa\u003E (I have forked a copy \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fshawngraham\u002Fmiditime\\\"\u003Ehere\u003C\u002Fa\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003ESonic Pi \u003Ca href=\\\"http:\u002F\u002Fsonic-pi.net\u002F\\\"\u003Ehttp:\u002F\u002Fsonic-pi.net\u002F\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch3 id=\\\"example-data\\\"\u003EExample Data\u003C\u002Fh3\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ca href=\\\"\u002Fassets\u002Fsonification-roman-data.csv\\\"\u003ERoman artefact data\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"\u002Fassets\u002Fsonification-diary.csv\\\"\u003EExcerpt from the Topic model of John Adams&#39; Diary\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"\u002Fassets\u002Fsonification-jesuittopics.csv\\\"\u003EExcerpt from the Topic model of the Jesuit Relations\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch1 id=\\\"some-background-on-sonification\\\"\u003ESome Background on Sonification\u003C\u002Fh1\u003E\\n\u003Cp\u003ESonification is the practice of mapping aspects of the data to produce sound signals. In general, a technique can be called ‘sonification’ if it meets certain conditions. These include reproducibility (the same data can be transformed the same ways by other researchers and produce the same results) and what might be called intelligibility - that the ‘objective’ elements of the original data are reflected systematically in the resulting sound (see \u003Ca href=\\\"#Hermann\\\"\u003EHermann\u003C\u002Fa\u003E \u003Ca href=\\\"http:\u002F\u002Fwww.icad.org\u002FProceedings\u002F2008\u002FHermann2008.pdf\\\"\u003E2008\u003C\u002Fa\u003E for a taxonomy of sonification). \u003Ca href=\\\"#Last\\\"\u003ELast and Usyskin\u003C\u002Fa\u003E \u003Ca href=\\\"https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F282504359_Listen_to_the_Sound_of_Data\\\"\u003E(2015)\u003C\u002Fa\u003E designed a series of experiments to determine what kinds of data-analytic tasks could be performed when the data were sonified. Their experimental results (Last and Usyskin 2015) have shown that even untrained listeners (listeners with no formal training in music) can make useful distinctions in the data. They found listeners could discriminate in the sonified data common data exploration tasks such as classification and clustering. (Their sonified outputs mapped the underlying data to the Western musical scale.)\u003C\u002Fp\u003E\\n\u003Cp\u003ELast and Usyskin focused on time-series data.  They argue that time-series data are particularly well suited to sonification because there are natural parallels with musical sound. Music is sequential, it has duration, and it evolves over time; so too with time-series data \u003Ca href=\\\"https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F282504359_Listen_to_the_Sound_of_Data\\\"\u003E(Last and Usyskin 2015: 424)\u003C\u002Fa\u003E. It becomes a problem of matching the data to the appropriate sonic outputs. In many applications of sonification, a technique called ‘parameter mapping’ is used to marry aspects of the data along various auditory dimensions such as \u003Ca href=\\\"#pitch\\\"\u003Epitch\u003C\u002Fa\u003E, variation, brilliance, and onset. The problem with this approach is that where there is no temporal relationship (or rather, no non-linear relationship) between the original data points, the resulting sound can be ‘confusing’ (2015: 422).\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"hearing-the-gaps\\\"\u003EHearing the Gaps\u003C\u002Fh2\u003E\\n\u003Cp\u003EThere is also the way that we fill in gaps in the sound with our expectations. Consider this video where the \u003Ca href=\\\"#mp3\\\"\u003Emp3\u003C\u002Fa\u003E has been converted to \u003Ca href=\\\"#midi\\\"\u003EMIDI\u003C\u002Fa\u003E back to mp3; the music has been &#39;flattened&#39; so that all sonic information is being played by one instrument. (Generating this effect is rather like saving a webpage as .txt, opening it in Word, and then resaving it as .html). All sounds (including vocals) have been translated to their corresponding note values, and then turned back into an mp3.\u003C\u002Fp\u003E\\n\u003Cp\u003EIt is noisy; yet we perceive meaning. Consider the video below:\u003C\u002Fp\u003E\\n\u003Ciframe src=\\\"https:\u002F\u002Fplayer.vimeo.com\u002Fvideo\u002F149070596\\\" width=\\\"640\\\" height=\\\"360\\\" frameborder=\\\"0\\\" webkitallowfullscreen mozallowfullscreen allowfullscreen\u003E\u003C\u002Fiframe\u003E\\n\\n\u003Cp\u003EWhat&#39;s going on here? If that song was already known to you, you probably heard the actual &#39;words&#39;. Yet, no words are present in the song! If the song was not already familiar to you, it sounded like garbled nonsense (see more examples on \u003Ca href=\\\"#Baio\\\"\u003EAndy Baio&#39;s\u003C\u002Fa\u003E \u003Ca href=\\\"http:\u002F\u002Fwaxy.org\u002F2015\u002F12\u002Fif_drake_was_born_a_piano\u002F\\\"\u003Ewebsite\u003C\u002Fa\u003E). This effect is sometimes called an &#39;auditory hallucination&#39;(cf. \u003Ca href=\\\"#Koebler\\\"\u003EKoebler, 2015\u003C\u002Fa\u003E). This example shows how in any representation of data we can hear\u002Fsee what is not, strictly speaking, there. We fill the holes with our own expectations.\u003C\u002Fp\u003E\\n\u003Cp\u003EConsider the implications for history. If we sonify our data, and begin to hear patterns in the sound, or odd outliers, our cultural expectations about how music works (our memories of similar snippets of music, heard in particular contexts) are going to colour our interpretation. This I would argue is true about all representations of the past, but sonifying is just odd enough to our regular methods that this self-awareness will help us identify or communicate the critical patterns in the (data of the) past.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe will progress through three different tools for sonifying data, noting how choices in one tool affect the output, and can be mitigated by reimagining the data via another tool. Ultimately, there is nothing any more objective in &#39;sonification&#39; than there is in &#39;visualization&#39;, and so the investigator has to be prepared to justify her choices, and to make these choices transparent and reproducible for others. (And lest we think that sonification and algorithmically generated music is somehow a &#39;new&#39; thing, I direct the interested reader to \u003Ca href=\\\"#hedges\\\"\u003EHedges, (1978)\u003C\u002Fa\u003E.)\u003C\u002Fp\u003E\\n\u003Cp\u003EIn each section, I will give a conceptual introduction, followed by a walkthrough using sample archaeological or historical data.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"musicalgorithms\\\"\u003EMusicalgorithms\u003C\u002Fh1\u003E\\n\u003Cp\u003EThere are a wide variety of tools out there to sonify data. Some for instance are packages for the widely-used \u003Ca href=\\\"https:\u002F\u002Fcran.r-project.org\u002F\\\"\u003ER statistical environment\u003C\u002Fa\u003E, such as ‘\u003Ca href=\\\"https:\u002F\u002Fcran.r-project.org\u002Fweb\u002Fpackages\u002Fplayitbyr\u002Findex.html\\\"\u003EplayitbyR\u003C\u002Fa\u003E’ and ‘\u003Ca href=\\\"https:\u002F\u002Fcran.r-project.org\u002Fweb\u002Fpackages\u002FaudiolyzR\u002Findex.html\\\"\u003EAudiolyzR\u003C\u002Fa\u003E’. The first of these however has not been maintained or updated to work with the current version of R (its last update was a number of years ago), and the second requires considerable configuration of extra software to make it work properly.\u003C\u002Fp\u003E\\n\u003Cp\u003EBy contrast, the \u003Ca href=\\\"http:\u002F\u002Fmusicalgorithms.org\u002F\\\"\u003EMusicalgorithms\u003C\u002Fa\u003E site is quite easy to use. The Musicalgorithms site has been online for over a decade. Though it is not open source, it represents a long-term research project in computational music by its creator, Jonathan Middleton. It is currently in its third major iteration (earlier iterations remain usable online). We will begin with Musicalalgorithms because it allows us to quickly enter and tweak our data to produce a MIDI file representation. Make sure to select &#39;\u003Ca href=\\\"http:\u002F\u002Fmusicalgorithms.org\u002F3.0\u002Findex.html\\\"\u003EVersion 3\u003C\u002Fa\u003E.&#39;\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;sonification-musicalgorithms-main-site-1.png&quot; caption=&quot;The Musicalgorithms Website as it appeared on February 2nd, 2016&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EMusicalgorithms effects a number of transformations on the data. In the sample data below (the default from the site itself), there is but one row of data, even if it looks like several rows. The sample data is comprised of comma separated fields that are themselves space delimited.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E# Of Voices, Text Area Name, Text Area Data\\n1,morphBox,\\n,areaPitch1,2 7 1 8 2 8 1 8 2 8 4 5 9 0 4 5 2 3 5 3 6 0 2 8\\n,dAreaMap1,2 7 1 8 2 8 1 8 2 8 4 5 9 0 4 5 2 3 5 3 6 0 2 8\\n,mapArea1,20 69 11 78 20 78 11 78 20 78 40 49 88 1 40 49 20 30 49 30 59 1 20 78\\n,dMapArea1,1 5 1 5 1 5 1 5 1 5 3 3 6 0 3 3 1 2 3 2 4 0 1 5\\n,so_text_area1,20 69 11 78 20 78 11 78 20 78 40 49 88 1 40 49 20 30 49 30 59 1 20 78\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis data represents the source data and its transformations; sharing this data would allow another investigator to replicate or extend the sonification using other tools. When one begins however, only the basic data below is needed (a list of data points):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E# Of Voices, Text Area Name, Text Area Data\\n1,morphBox,\\n,areaPitch1,24 72 12 84 21 81 14 81 24 81 44 51 94 01 44 51 24 31 5 43 61 04 21 81\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe key field for us is ‘areaPitch1,’ which contains the space-delimited input data.  The other fields will become filled as we move through Musicalgorithms&#39; various settings. In the data above (eg 24 72 12 84 etc), the values are raw counts of inscriptions from a series of sites along a Roman road in Britain. (We will practice with other data in a moment below).\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;sonification-musicalgorithms-pitch-mapping-2.png&quot; caption=&quot;After you load your data, you can select the different operations across the top menu bar of the site. In the screenshot, the information mouseover is explaining what happens to the scaling of your data if you select the division operation to scale your data to the range of notes selected.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ENow, as you page across the various tabs in the interface (‘\u003Ca href=\\\"#duration\\\"\u003Eduration\u003C\u002Fa\u003E input’, ‘[pitch mapping](#pitch mapping)’, ‘duration mapping’, ‘scale options’) you can effect various transformations. In ‘pitch mapping’, there are a number of mathematical options for mapping the data against the full 88 keys\u002Fpitches of a piano keyboard (in a linear mapping, the \u003Cem\u003Emean\u003C\u002Fem\u003E of one’s data would be mapped to middle C, or 40). One can also choose the kind of scale, whether it is a minor or major and so on. At this point, once you&#39;ve selected your various transformations, you should save the text file. On the file tab, ‘play’, one can download a midi file. Your default audio program can play midi files (often defaulting to a piano tone). More complicated instrumentation can be assigned by opening the midi file in music mixing programs such as GarageBand (Mac) or \u003Ca href=\\\"https:\u002F\u002Flmms.io\u002F\\\"\u003ELMMS\u003C\u002Fa\u003E (Windows, Mac, Linux). (Using Garageband or LMMS are outside the scope of this tutorial. A video tutorial on LMMS is available \u003Ca href=\\\"https:\u002F\u002Fyoutu.be\u002F4dYxV3tqTUc\\\"\u003Ehere\u003C\u002Fa\u003E, while Garageband tutorials proliferate online. Lynda.com has \u003Ca href=\\\"http:\u002F\u002Fwww.lynda.com\u002FGarageBand-tutorials\u002FImporting-audio-tracks\u002F156620\u002F164050-4.html\\\"\u003Ean excellent one\u003C\u002Fa\u003E)\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you had several columns of data for the same points - say, in our example from Roman Britain, we also wanted to sonify counts of a pottery type for those same towns - you can reload your next data series, effect the transformations and mappings, and generate another MIDI file. Since Garageband and LMMS allow for overlaying of voices, you can begin to build up complicated sequences of music.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;sonification-garageband-john-adams-3.png&quot; caption=&quot;Screenshot of Garageband, where the midi files are sonified topics from John Adams&#39; Diary. In the Garageband interface (LMMS is similar), each midi file is dragged-and-dropped into place. The instrumentation for each midi file (ie track) can be selected from Garageband&#39;s menus. The labels for each track have here been changed to reflect the key words in each topic. The green area to the right represents a visualization of the notes in each track. You can watch this interface in action and listen to the music \u003Ca href=\\\"https:\u002F\u002Fyoutu.be\u002FikqRXtI3JeA\\\"\u003Ehere\u003C\u002Fa\u003E.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EWhich transformations should you use? If you had two columns of data, you have two voices. It might make sense, in our hypothetical data, to play the first voice loud, in a major key: inscriptions &#39;speak&#39; to us, in a manner of speaking, after all. (Roman inscriptions do address the reader, the passerby, literally: &#39;O you who passes by...&#39;). Then, perhaps since the pottery you are interested in are humble wares, perhaps they would be mapped against the lower end of the scale, or given longer duration notes to represent their ubiquity across classes in this region.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003EThere is no &#39;right&#39; way to represent your data as sound\u003C\u002Fem\u003E, at least not yet: but even with this simple example, we begin to see how shades of meaning and interpretation can be inflected into our data and into our experience of that data.\u003C\u002Fp\u003E\\n\u003Cp\u003EBut what about time? Historical data often has a punctuation point, a distinct &#39;time when&#39; something occured. Thus, the amount of time between two data points has to be taken into account. This is where our next tool becomes quite useful, for when our data points have a relationship to one another in temporal space. We begin to move from sonfication (data points) to music (relationships between points).\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"practice\\\"\u003EPractice\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe \u003Ca href=\\\"\u002Fassets\u002Fsonification-roman-data.csv\\\"\u003Esample dataset\u003C\u002Fa\u003E provided contains counts of Roman coins in its first column and counts of other Roman materials from the same locations, as contained in the Portable Antiquities Scheme database from the British Museum. A sonification of this data might reveal or highlight aspects of the economic situation along Watling street, a major route through Roman Britain. The data points are organized geographically from North West to South East; thus as the sound plays out, we are hearing movement over space. Each note represents another stop along the way.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Cp\u003EOpen the\u003Ca href=\\\"\u002Fassets\u002Fsonification-roman-data.csv\\\"\u003Esonification-roman-data.csv\u003C\u002Fa\u003E in a spreadsheet. Copy the first column into a text editor. Delete the line endings so that the data is all in a single row.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EAdd the following column information like so:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E# Of Voices, Text Area Name, Text Area Data\\n1,morphBox,\\n,areaPitch1,\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E...so that your data follows immediately after that last comma (as like \u003Ca href=\\\"\u002Fassets\u002Fsonification-romancoin-data-music.csv\\\"\u003Ethis\u003C\u002Fa\u003E). Save the file with a useful name like \u003Ccode\u003Ecoinsounds1.csv\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EGo to the \u003Ca href=\\\"http:\u002F\u002Fmusicalgorithms.org\u002F3.0\u002Findex.html\\\"\u003EMusicalgorithms\u003C\u002Fa\u003E site (version 3), and hit the load button. In the pop-up, click the blue &#39;load&#39; button and select the file saved in step 2. The site will load your materials and display a green check mark if it loaded successfully. If it did not, make sure that your values are separated by spaces, and that they follow immediately the last comma in the code block in step 2. You may also try loading up the \u003Ca href=\\\"\u002Fassets\u002Fsonification-romancoin-data-music.csv\\\"\u003Edemo file for this tutorial\u003C\u002Fa\u003E instead.{% include figure.html filename=&quot;sonification-musicalgorithms-upload-4.png&quot; caption=&quot;Click &#39;load&#39; on the main screen to get this dialogue box. Then &#39;load csv&#39;. Select your file; it will appear in the box. Then click the bottom load button.&quot; %}\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EClick on &#39;Pitch Input&#39;. You&#39;ll see the values of your data. For now, \u003Cstrong\u003Edo not select\u003C\u002Fstrong\u003E any further options on this page (thus using the site&#39;s default values).\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EClick on &#39;Duration Input&#39;. \u003Cstrong\u003EDo not select any options here for now\u003C\u002Fstrong\u003E. The options here will map various transformations against your data that will alter the duration for each note. Do not worry about these options for now; move on.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EClick on &#39;Pitch Mapping&#39;. This is the most crucial choice, as it will transform (that is, scale) your raw data to a mapping against the keys of the keyboard. Leave the \u003Ccode\u003Emapping\u003C\u002Fcode\u003E set to &#39;division&#39;.  (The other options are modulo or logarithmic). The option \u003Ccode\u003ERange\u003C\u002Fcode\u003E 1 to 88 uses the full 88 keys of the keyboard; thus your lowest value would accord to the deepest note on the piano and your highest value with the highest note. You might wish instead to constrain your music around middle C, so enter 25 to 60 as your range. The output should change to: \u003Ccode\u003E31,34,34,34,25,28,30,60,28,25,26,26,25,25,60,25,25,38,33,26,25,25,25\u003C\u002Fcode\u003E These are no longer your counts; they are notes on the keyboard.{% include figure.html filename=&quot;sonification-musicalgorithms-settings-for-pitch-mapping-5.png&quot; caption=&quot;Click into the &#39;range&#39; box and set it to 25. The values underneath will change automatically. Click into the &#39;to&#39; box and set it to 60. Click back into the other box; the values will update.&quot; %}\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EClick on &#39;Duration Mapping&#39;. Like Pitch Mapping, this takes a range of times that you specify and uses the various mathematical options to map that range of possibilities against your notes. If you mouse over the \u003Ccode\u003Ei\u003C\u002Fcode\u003E you will see how the numbers correspond with whole notes, quarter notes, eigth notes, and so on. Leave the default values for now.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EClick on &#39;Scale Options&#39;. Here we can begin to select something of what might be called the &#39;emotional&#39; aspect to sound. We commonly think of major scales being &#39;happy&#39; while minor scales are &#39;sad&#39;; for an accessible discussion see \u003Ca href=\\\"http:\u002F\u002Fwww.ethanhein.com\u002Fwp\u002F2010\u002Fscales-and-emotions\u002F\\\"\u003Ethis blog post\u003C\u002Fa\u003E. For now, select &#39;scale by: major&#39;. Leave the &#39;scale&#39; as \u003Ccode\u003EC\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003EYou have now sonified one column of data! Click on the &#39;save&#39; button, then &#39;save csv&#39;. {% include figure.html filename=&quot;sonification-musicalgorithms-save-6.png&quot; caption=&quot;The save data dialogue box.&quot; %}You&#39;ll have a file that looks something like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E# Of Voices, Text Area Name, Text Area Data\\n1,morphBox,\\n,areaPitch1,80 128 128 128 1 40 77 495 48 2 21 19 1 1 500 1 3 190 115 13 5 1 3\\n,dAreaMap1,2 7 1 8 2 8 1 8 2 8 4 5 9 0 4 5 2 3 5 3 6 0 2\\n,mapArea1,31 34 34 34 25 28 30 60 28 25 26 26 25 25 60 25 25 38 33 26 25 25 25\\n,dMapArea1,1 5 1 5 1 5 1 5 1 5 3 3 6 0 3 3 1 2 3 2 4 0 1\\n,so_text_area1,32 35 35 35 25 28 30 59 28 25 27 27 25 25 59 25 25 39 33 27 25 25 25\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EYou can see your original data in the &#39;areaPitch1&#39; field, and your subsequent mappings. The site allows you to generate up to four voices at a time into a single MIDI file; depending on how you want to add instrumentation subsequently, you might wish to generate one MIDI file at a time. Let&#39;s play the music - click on &#39;Play&#39;. You can select the tempo here, and an instrument. You can listen to your data in the browser, or save as a MIDI file by clicking the blue &#39;Save MIDI file&#39;.\u003C\u002Fp\u003E\\n\u003Cp\u003EGo back to the beginning, and load both columns of data into this template:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E# Of Voices, Text Area Name, Text Area Data\\n2,morphBox,\\n,areaPitch1,\\n,areaPitch2,\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;sonification-2voices-7.png&quot; caption=&quot;Put 2 into the voices box at the top of the interface. When you then go to any of the option pages - here, we&#39;re at &#39;pitch input&#39; - two displays open up to show you the data for two voices. Load your csv data as before, but have your csv formatted to have &#39;areaPitch1&#39; and &#39;areaPitch2&#39; as described in the main text. The data for voice one will appear on the left, and for voice two on the right.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EWhen you have multiple voices of data, what stands out? Note that in this approach, the distance between points in the real world is not factored into our sonification. This distance, if it were, might be crucical. Distance, of course, does not have to be geographic - it can be temporal. The next tool we&#39;ll explore allows us to factor that into our sonification explicitly.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"a-quick-word-about-getting-python-set-up\\\"\u003EA quick word about getting Python set up\u003C\u002Fh1\u003E\\n\u003Cp\u003EThe next section of this tutorial requires Python. If you haven&#39;t experimented with Python yet, you will need to spend some time \u003Ca href=\\\"\u002Flessons\u002Fintro-to-bash\\\"\u003Ebecoming familiar with the command line (PC) or terminal (OS)\u003C\u002Fa\u003E. You might find this quick \u003Ca href=\\\"\u002Flessons\u002Finstalling-python-modules-pip\\\"\u003Eguide to installing python &#39;modules&#39;\u003C\u002Fa\u003E handy (but come back to it after you read the rest of this section).\u003C\u002Fp\u003E\\n\u003Cp\u003EMac users will already have Python installed on their machine. You can test this by holding down the COMMAND button and the spacebar; in the search window, type \u003Ccode\u003Eterminal\u003C\u002Fcode\u003E and click on the terminal application. At the prompt, eg, the cursor blinking at \u003Ccode\u003E$\u003C\u002Fcode\u003E type \u003Ccode\u003Epython --version\u003C\u002Fcode\u003E and the computer will respond with what version of python you have. \u003Cem\u003EThis next section of the tutorial assumes Python 2.7; it has not been tested on Python 3\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor Windows users, Python is not installed by default on your machine so \u003Ca href=\\\"http:\u002F\u002Fdocs.python-guide.org\u002Fen\u002Flatest\u002Fstarting\u002Finstall\u002Fwin\u002F\\\"\u003Ethis page\u003C\u002Fa\u003E will help you get started, though things are a bit more complicated than that page makes out. First, download the \u003Ccode\u003E.msi\u003C\u002Fcode\u003E file that that page recommends (Python 2.7). Double click the file, and it should install itself in a new directory, eg \u003Ccode\u003EC:\\\\Python27\\\\\u003C\u002Fcode\u003E. Then, we have to tell Windows the location of where to look for Python whenever you run a python program; that is, you put the location of that directory into your &#39;path&#39;, or the environment variable that windows always checks when confronted with a new command. There are a couple ways of doing this, but perhaps the easiest is to search your computer for the program \u003Ccode\u003EPowershell\u003C\u002Fcode\u003E (type &#39;powershell&#39; into your windows computer search). Open Powershell, and at the \u003Ccode\u003E&gt;\u003C\u002Fcode\u003E prompt, paste this entire line:\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ccode\u003E[Environment]::SetEnvironmentVariable(&quot;Path&quot;, &quot;$env:Path;C:\\\\Python27\\\\;C:\\\\Python27\\\\Scripts\\\\&quot;, &quot;User&quot;)\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EYou can close powershell when you&#39;re done. You&#39;ll know it worked if nothing very much happens once you&#39;ve pressed &#39;enter&#39;. To test that everything is okay, open a command prompt (here are \u003Ca href=\\\"http:\u002F\u002Fwww.howtogeek.com\u002F235101\u002F10-ways-to-open-the-command-prompt-in-windows-10\u002F\\\"\u003E10 ways to do this\u003C\u002Fa\u003E) and type at the \u003Ccode\u003E&gt;\u003C\u002Fcode\u003E prompt \u003Ccode\u003Epython --version\u003C\u002Fcode\u003E. It should tell you \u003Ccode\u003EPython 2.7.10\u003C\u002Fcode\u003E or similar.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe last piece of the puzzle that all users will need is a program called \u003Ccode\u003EPip\u003C\u002Fcode\u003E. Mac users can install it by typing at the terminal :\u003Ccode\u003Esudo easy_install pip\u003C\u002Fcode\u003E. Windows users have a bit of a harder time. First, right-click and save-as this link: \u003Ca href=\\\"https:\u002F\u002Fbootstrap.pypa.io\u002Fget-pip.py\\\"\u003Ehttps:\u002F\u002Fbootstrap.pypa.io\u002Fget-pip.py\u003C\u002Fa\u003E (If you just click on the link, it will show you the code in your browser). Save it somewhere handy. Open a command prompt in the directory where you saved \u003Ccode\u003Eget-pip.py\u003C\u002Fcode\u003E. Then, type at the command prompt \u003Ccode\u003Epython get-pip.py\u003C\u002Fcode\u003E. Conventionally, in tutorials, you will see \u003Ccode\u003E&gt;\u003C\u002Fcode\u003E or \u003Ccode\u003E$\u003C\u002Fcode\u003E at points where you are required to enter something at the command prompt or the terminal. You don&#39;t ever have to type those two characters.\u003C\u002Fp\u003E\\n\u003Cp\u003EFinally, when you have python code you want to run, you can enter it in your text editor and save it with the \u003Ccode\u003E.py\u003C\u002Fcode\u003E extension. Your file is a text file, but the file \u003Cstrong\u003Eextension\u003C\u002Fstrong\u003E tells your computer to use Python to interpret it; but remember, type \u003Ccode\u003Epython\u003C\u002Fcode\u003E at the prompt first, eg: \u003Ccode\u003E$ python my-cool-script.py\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"miditime\\\"\u003EMIDITime\u003C\u002Fh1\u003E\\n\u003Cp\u003EMIDITime is a python package developed by \u003Ca href=\\\"https:\u002F\u002Fwww.revealnews.org\u002F\\\"\u003EReveal News (formerly, the Centre for Investigative Reporting)\u003C\u002Fa\u003E. Its \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fcirlabs\u002Fmiditime\\\"\u003EGithub repository is here\u003C\u002Fa\u003E. Miditime is built explicitly for time series data (that is, a sequence of observations collected over time).\u003C\u002Fp\u003E\\n\u003Cp\u003EWhile the Musicalgorithms tool has a more-or-less intuitive interface, the investigator sacrifices the ability to know what, exactly, is going on under the hood. In principle, one could examine the underlying code for the MIDITime package to see exactly what&#39;s going on. More importantly, the previous tool had no ability to account for data where the points are distant from one another in clock-time. MIDITime lets us take into account that our data might be clustering in time.\u003C\u002Fp\u003E\\n\u003Cp\u003ELet us assume that you have a historic diary to which you&#39;ve fitted a \u003Ca href=\\\"\u002Flessons\u002Ftopic-modeling-and-mallet\\\"\u003Etopic model\u003C\u002Fa\u003E. The resulting output might have diary entries as rows, and the percentage composition each topic contributes to that entry as the columns. In which case, \u003Cem\u003Elistening\u003C\u002Fem\u003E to these values might help you understand the patterns of thought in the diary in a way that visualizing as a graph might not. Outliers or recurrent musical patterns could stand out to the ear in a way the grammar of graphs obscures.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"installing-miditime\\\"\u003EInstalling MIDITime\u003C\u002Fh3\u003E\\n\u003Cp\u003EInstalling miditime is straightforward using \u003Ca href=\\\"\u002Flessons\u002Finstalling-python-modules-pip\\\"\u003Epip\u003C\u002Fa\u003E:\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ccode\u003E$ pip install miditime\u003C\u002Fcode\u003E or \u003Ccode\u003E$ sudo pip install miditime\u003C\u002Fcode\u003E for a Mac or Linux machine;\\n\u003Ccode\u003E&gt; python pip install miditime\u003C\u002Fcode\u003E on a Windows machine. (Windows users, if the instructions above didn&#39;t quite work for you, you might want to try \u003Ca href=\\\"https:\u002F\u002Fsites.google.com\u002Fsite\u002Fpydatalog\u002Fpython\u002Fpip-for-windows\\\"\u003Ethis helper program\u003C\u002Fa\u003E instead to get Pip working properly on your machine).\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"practice-1\\\"\u003EPractice\u003C\u002Fh3\u003E\\n\u003Cp\u003ELet us look at the sample script provided. Open your text editor, and copy and paste the sample script in:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E#!\u002Fusr\u002Fbin\u002Fpython\\n\\nfrom miditime.miditime import MIDITime\\n# NOTE: this import works at least as of v1.1.3; for older versions or forks of miditime, you may need to use\\n# from miditime.MIDITime import MIDITime\\n\\n# Instantiate the class with a tempo (120bpm is the default) and an output file destination.\\nmymidi = MIDITime(120, &#39;myfile.mid&#39;)\\n\\n# Create a list of notes. Each note is a list: [time, pitch, attack, duration]\\nmidinotes = [\\n    [0, 60, 200, 3],  #At 0 beats (the start), Middle C with attack 200, for 3 beats\\n    [10, 61, 200, 4]  #At 10 beats (12 seconds from start), C#5 with attack 200, for 4 beats\\n]\\n\\n# Add a track with those notes\\nmymidi.add_track(midinotes)\\n\\n# Output the .mid file\\nmymidi.save_midi()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESave this script as \u003Ccode\u003Emusic1.py\u003C\u002Fcode\u003E. At your terminal or command prompt, run the script:\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ccode\u003E$ python music1.py\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EA new file, \u003Ccode\u003Emyfile.mid\u003C\u002Fcode\u003E will be written to your directory. To hear this file, you can open it with Quicktime or Windows Media Player. (You can add instrumentation to it by opening it in Garageband or \u003Ca href=\\\"https:\u002F\u002Flmms.io\u002F\\\"\u003ELMMS\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ccode\u003EMusic1.py\u003C\u002Fcode\u003E imports miditime (remember, you must do \u003Ccode\u003Epip install miditime\u003C\u002Fcode\u003E before running the script). Then, it creates an output file destination and sets the tempo. The notes are all listed individually, where the first number is the time when the note should be played, the pitch of the note (ie, the actual note!), how hard or rythmically the note is hit (the \u003Ca href=\\\"#attack\\\"\u003Eattack\u003C\u002Fa\u003E), and then how long the note lasts. The notes are then written to the track, and then the track is written to \u003Ccode\u003Emyfile.mid\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EPlay with this script now, and add more notes. The notes for &#39;Baa Baa Black Sheep&#39; are:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003ED, D, A, A, B, B, B, B, A\\nBaa, Baa, black, sheep, have, you, any, wool?\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ECan you make your computer play this song? (This \u003Ca href=\\\"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20171211192102\u002Fhttp:\u002F\u002Fwww.electronics.dit.ie\u002Fstaff\u002Ftscarff\u002FMusic_technology\u002Fmidi\u002Fmidi_note_numbers_for_octaves.htm\\\"\u003Echart\u003C\u002Fa\u003E will help).\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003EBy the way\u003C\u002Fstrong\u003E There is a text file specification for describing music called &#39;\u003Ca href=\\\"http:\u002F\u002Fabcnotation.com\u002Fwiki\u002Fabc:standard:v2.1\\\"\u003EABC Notation\u003C\u002Fa\u003E&#39;. It is beyond us for now, but one could write a sonification script in say a spreadsheet, mapping values to note names in the ABC specification (if you&#39;ve ever used an IF - THEN in Excel to convert percentage grades to letter grades, you&#39;ll have a sense of how this might be done) and then using a site like \u003Ca href=\\\"http:\u002F\u002Ftrillian.mit.edu\u002F~jc\u002Fmusic\u002Fabc\u002FABCcontrib.html\\\"\u003Ethis one\u003C\u002Fa\u003E to convert the ABC notation into a .mid file.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"getting-your-own-data-in\\\"\u003EGetting your own data in\u003C\u002Fh3\u003E\\n\u003Cp\u003E\u003Ca href=\\\"\u002Fassets\u002Fsonification-diary.csv\\\"\u003EThis file\u003C\u002Fa\u003E is a selection from the topic model fitted to John Adams&#39; Diaries for\u003Ca href=\\\"http:\u002F\u002Fthemacroscope.org\\\"\u003EThe Macroscope\u003C\u002Fa\u003E. Only the strongest signals have been preserved by rounding the values in the columns to two decimal places (remembering that .25 for instance would indicate that that topic is contributing to a quarter of that diary entry&#39;s composition). To get this data into your python script, it has to be formatted in a particular away. The tricky bit is getting the date field right.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003EFor the purposes of this tutorial, we are going to leave the names of variables and so on unchanged from the sample script. The sample script was developed with earthquake data in mind; so where it says &#39;magnitude&#39; we can think of it as equating to &#39;% topic composition.&#39;\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Emy_data = [\\n    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 3.4},\\n    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 3.2},\\n    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 3.6},\\n    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 3.0},\\n    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 5.6},\\n    {&#39;event_date&#39;: &lt;datetime object&gt;, &#39;magnitude&#39;: 4.0}\\n]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EOne could approach the problem of getting our data into that format using regular expressions; it might be easier to just open our topic model in a spreadsheet. Copy the topic data to a new sheet, and leave columns to the left and to the right of the data. In the example below, I put it in column D, and then filled in the rest of the data around it, like so:\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003E\u003C\u002Fth\u003E\\n\u003Cth\u003EA\u003C\u002Fth\u003E\\n\u003Cth\u003EB\u003C\u002Fth\u003E\\n\u003Cth\u003EC\u003C\u002Fth\u003E\\n\u003Cth\u003ED\u003C\u002Fth\u003E\\n\u003Cth\u003EE\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E{&#39;event_date&#39;: datetime\u003C\u002Ftd\u003E\\n\u003Ctd\u003E(1753,6,8)\u003C\u002Ftd\u003E\\n\u003Ctd\u003E, &#39;magnitude&#39;:\u003C\u002Ftd\u003E\\n\u003Ctd\u003E0.0024499630\u003C\u002Ftd\u003E\\n\u003Ctd\u003E},\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003E\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EThen copy and paste the elements that do not change to fill up the entire column. The date element has to be (year,month,day). Once you&#39;ve filled up the table, you can copy and paste it into your text editor so that it becomes part of the \u003Ccode\u003Emy_data\u003C\u002Fcode\u003E array, like so:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Emy_data = [\\n{&#39;event_date&#39;: datetime(1753,6,8), &#39;magnitude&#39;:0.0024499630},\\n{&#39;event_date&#39;: datetime(1753,6,9), &#39;magnitude&#39;:0.0035766320},\\n{&#39;event_date&#39;: datetime(1753,6,10), &#39;magnitude&#39;:0.0022171550},\\n{&#39;event_date&#39;: datetime(1753,6,11), &#39;magnitude&#39;:0.0033220150},\\n{&#39;event_date&#39;: datetime(1753,6,12), &#39;magnitude&#39;:0.0046445900},\\n{&#39;event_date&#39;: datetime(1753,6,13), &#39;magnitude&#39;:0.0035766320},\\n{&#39;event_date&#39;: datetime(1753,6,14), &#39;magnitude&#39;:0.0042241550}\\n]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENote that the last row does not have a comma at the end of the line.\u003C\u002Fp\u003E\\n\u003Cp\u003EYour final script will look something like this, using the example from the Miditime page itself (the code sections below have been interrupted by commentary, but should be pasted into your text editor as a single file):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efrom miditime.MIDITime import MIDITime\\nfrom datetime import datetime\\nimport random\\n\\nmymidi = MIDITime(108, &#39;johnadams1.mid&#39;, 3, 4, 1)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe values after MIDITime, \u003Ccode\u003EMIDITime(108, &#39;johnadams1.mid&#39;, 3, 4, 1)\u003C\u002Fcode\u003E set\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003Ethe beats per minute (108),\u003C\u002Fli\u003E\\n\u003Cli\u003Ethe output file (&#39;johnadams1.mid&#39;),\u003C\u002Fli\u003E\\n\u003Cli\u003Ethe number of seconds to represent a year in the music (3 seconds to a calendar year, so all of the notes for diary entries from 1753 will be scaled against 3 seconds; there are 50 years in the data, so the final song will be 50 x 3 seconds long, or a bit over two minutes),\u003C\u002Fli\u003E\\n\u003Cli\u003Ethe base octave for the music (middle C is conventionally represented as C5, so here 4 represents one octave below middle C),\u003C\u002Fli\u003E\\n\u003Cli\u003Eand how many octaves to map the pitches against.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003ENow we pass our data into the script by feeding it into the \u003Ccode\u003Emy_data\u003C\u002Fcode\u003E array (this gets pasted in next):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Emy_data = [\\n{&#39;event_date&#39;: datetime(1753,6,8), &#39;magnitude&#39;:0.0024499630},\\n{&#39;event_date&#39;: datetime(1753,6,9), &#39;magnitude&#39;:0.0035766320},\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E...have your data in here, remembering to end the final event_date line \u003Cstrong\u003Ewithout\u003C\u002Fstrong\u003E a comma, and finishing the data with a \u003Ccode\u003E]\u003C\u002Fcode\u003E on its own line, eg\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E{&#39;event_date&#39;: datetime(1753,6,14), &#39;magnitude&#39;:0.0042241550}\\n]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003Eand then paste:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Emy_data_epoched = [{&#39;days_since_epoch&#39;: mymidi.days_since_epoch(d[&#39;event_date&#39;]), &#39;magnitude&#39;: d[&#39;magnitude&#39;]} for d in my_data]\\n\\nmy_data_timed = [{&#39;beat&#39;: mymidi.beat(d[&#39;days_since_epoch&#39;]), &#39;magnitude&#39;: d[&#39;magnitude&#39;]} for d in my_data_epoched]\\n\\nstart_time = my_data_timed[0][&#39;beat&#39;]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis part works out the timing between your different diary entries; diaries that are close together in time will therefore have their notes sounding closer together. Finally, we define how the data get mapped against the pitch. Remembering that our data are percentages ranging from 0.01 (ie 1%) to 0.99 (99%), we \u003Ccode\u003Escale_pct\u003C\u002Fcode\u003E between 0 and 1. If you weren&#39;t dealing with percentages, you&#39;d use your lowest value and your highest value (if for instance your data were counts of some element of interest, as in the archaology data used earlier). Thus, we paste in:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edef mag_to_pitch_tuned(magnitude):\\n    scale_pct = mymidi.linear_scale_pct(0, 1, magnitude)\\n    # Pick a range of notes. This allows you to play in a key.\\n    c_major = [&#39;C&#39;, &#39;C#&#39;, &#39;D&#39;, &#39;D#&#39;, &#39;E&#39;, &#39;E#&#39;, &#39;F&#39;, &#39;F#&#39;, &#39;G&#39;, &#39;G#&#39;, &#39;A&#39;, &#39;A#&#39;, &#39;B&#39;, &#39;B#&#39;]\\n\\n    #Find the note that matches your data point\\n    note = mymidi.scale_to_note(scale_pct, c_major)\\n\\n    #Translate that note to a MIDI pitch\\n    midi_pitch = mymidi.note_to_midi_pitch(note)\\n\\n    return midi_pitch\\n\\nnote_list = []\\n\\nfor d in my_data_timed:\\n    note_list.append([\\n        d[&#39;beat&#39;] - start_time,\\n        mag_to_pitch_tuned(d[&#39;magnitude&#39;]),\\n        random.randint(0,200),  # attack\\n        random.randint(1,4)  # duration, in beats\\n    ])\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003Eand then paste in this final bit of code to write your sound values to file:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E# Add a track with those notes\\nmymidi.add_track(midinotes)\\n\\n# Output the .mid file\\nmymidi.save_midi()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESave this file with a new name and the \u003Ccode\u003E.py\u003C\u002Fcode\u003E file extension.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor each column of data in your original data, \u003Cstrong\u003Ehave a unique script and remember to change the output file name!\u003C\u002Fstrong\u003E Otherwise you will overwrite your data. Then, you can load the individual midi files into Garageband or LMMS for instrumentation. Here&#39;s the full \u003Ca href=\\\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=ikqRXtI3JeA\\\"\u003EJohn Adams Diary\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"sonic-pi\\\"\u003ESonic Pi\u003C\u002Fh1\u003E\\n\u003Cp\u003EHaving unique midifiles that you arrange (in Garageband or some other music composition program) moves you from &#39;sonifying&#39; towards composition and sound art. In this final section, I do not offer you a full tutorial on using \u003Ca href=\\\"http:\u002F\u002Fsonic-pi.net\\\"\u003ESonic Pi\u003C\u002Fa\u003E, but rather point you towards this environment that allows for the actual live-coding and performance of your data (see \u003Ca href=\\\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=oW-3HVOeUQA\\\"\u003Ethis video\u003C\u002Fa\u003E for an actual live-coding performance). Sonic Pi&#39;s built-in tutorials will show you something of the potential of using your computer as an actual musical instrument (where you type Ruby code into its built-in editor while the interpreter plays what you encode).\u003C\u002Fp\u003E\\n\u003Cp\u003EWhy would you want to do this? As has progressively become clear in tutorial, when you sonify your data you begin to make choices about how the data maps into sound, and these choices reflect implicit or explicit decisions about which data matter. There is a continuum of &#39;objectivity&#39;, if you will. At one end, a sonification that supports an argument about the past; at the other, a performance about the past as riveting and personal as any well-done public lecture. Sonification moves our data off the page and into the ears of our listeners: it is a kind of public history. Performing our data... imagine that!\u003C\u002Fp\u003E\\n\u003Cp\u003EHere, I offer simply a code snippet that will allow you to import your data, where your data is simply a list of values saved as csv. I am indebted to George Washington University librarian Laura Wrubel who posted to \u003Ca href=\\\"https:\u002F\u002Fgist.github.com\u002Flwrubel\\\"\u003Egist.github.com\u003C\u002Fa\u003E her experiments in sonifying her library&#39;s circulation transactions.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this \u003Ca href=\\\"\u002Fassets\u002Fsonification-jesuittopics.csv\\\"\u003Esample file\u003C\u002Fa\u003E(a topic model generated from the \u003Ca href=\\\"http:\u002F\u002Fpuffin.creighton.edu\u002Fjesuit\u002Frelations\u002F\\\"\u003EJesuit Relations\u003C\u002Fa\u003E), there are two topics. The first row contains the headers: topic1, topic2.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"practice-2\\\"\u003EPractice\u003C\u002Fh3\u003E\\n\u003Cp\u003EFollow the initial tutorials that Sonic Pi provides until you get a feel for the interface and some of the possibilities. (These tutorials are also concatenated \u003Ca href=\\\"https:\u002F\u002Fgist.github.com\u002Fjwinder\u002Fe59be201082cca694df9\\\"\u003Ehere\u003C\u002Fa\u003E; you can also listen to an interview with Sam Aaron, the creator of Sonic Pi, \u003Ca href=\\\"https:\u002F\u002Fdevchat.tv\u002Fruby-rogues\u002F215-rr-sonic-pi-with-sam-aaron\\\"\u003Ehere\u003C\u002Fa\u003E). Then, in a new buffer (editor window), copy the following (again, the code snippets that follow will eventually be collated into a single script in your Sonic Pi buffer window):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Erequire &#39;csv&#39;\\ndata = CSV.parse(File.read(&quot;\u002Fpath\u002Fto\u002Fyour\u002Fdirectory\u002Fdata.csv&quot;), {:headers =&gt; true, :header_converters =&gt; :symbol})\\nuse_bpm 100\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ERemember, \u003Ccode\u003Epath\u002Fto\u002Fyour\u002Fdirectory\u002F\u003C\u002Fcode\u003E is the actual location of your data on your machine. Make sure it is either called \u003Ccode\u003Edata.csv\u003C\u002Fcode\u003E or that you change the line above so that it actually loads your file!\u003C\u002Fp\u003E\\n\u003Cp\u003ENow, let&#39;s load that data into our music:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E#this bit of code will run only once, unless you comment out the line with\\n#&#39;live_loop&#39;, and also comment out the final &#39;end&#39; at the bottom\\n# of this code block\\n#&#39;commenting out&#39; means removing the # sign.\\n\\n# live_loop :jesuit do\\ndata.each do |line|\\n  topic1 = line[:topic1].to_f\\n  topic2 = line[:topic2].to_f\\n\\n  use_synth :piano\\n  play topic1*100, attack: rand(0.5), decay: rand(1), amp: rand(0.25)\\n  use_synth :piano\\n  play topic2*100, attack: rand(0.5), decay: rand(1), amp: rand(0.25)\\n  sleep (0.5)\\nend\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe first few lines load the columns of data in; then we say which sound sample we wish to use (piano) and then tell Sonic Pi to play topic 1 according to the following criteria (a random value less than 0.5 for the attack; a decay using a random value less than 1; and an \u003Ca href=\\\"#amplitude\\\"\u003Eamplitude\u003C\u002Fa\u003E using a random value less than 0.25). See the x 100 in the line? That takes our data value (which is a decimal, remember) and turns it into a whole number. In this piece of code (the way I&#39;ve written it), that number equates directly with a note. If 88 is the lowest note and 1 is the highest, you can see that this approach is a bit problematic: we haven&#39;t actually done any pitch mapping here! In which case, you could use Musicalgorithms to do your pitch mapping, and then feed those values back into Sonic Pi. Alternatively, since this code is more or less Ruby, you could look up how to normalize the data and then do a linear mapping of your values against the range 1 - 88. A good place to start would be to study \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fstevelloyd\u002FLearn-sonification-with-Sonic-Pi\\\"\u003Ethis worksheet by Steve Lloyd\u003C\u002Fa\u003E on sonifying weather data with the Sonic Pi. Finally, the other thing to notice here is that the &#39;rand&#39; value (random) allows us to add a bit of &#39;humanity&#39; into the music in terms of the dynamics. Then we do the same thing again for topic2.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou can then add beats, loops, samples, and the whole parephernalia that Sonic Pi permits. Where you put code chunks affects the playback; if you put a loop before the data block above, the loop will play first. For instance, if you insert the following after the \u003Ccode\u003Euse_bpm 100\u003C\u002Fcode\u003E line,\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E#intro bit\\nsleep 2\\nsample :ambi_choir, attack: 2, sustain: 4, rate: 0.25, release: 1\\nsleep 6\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EYou&#39;ll get a bit of an introductory ta-da for your piece. It waits 2 seconds, plays the &#39;ambi_choir&#39; sample, then waits 6 more seconds before playing our data. If you wanted to add a bit of an ominous drum sound that played throughout your piece, you&#39;d insert this bit next (and before your own data):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E#bit that keeps going throughout the music\\nlive_loop :boom do\\n  with_fx :reverb, room: 0.5 do\\n    sample :bd_boom, rate: 1, amp: 1\\n  end\\n  sleep 2\\nend\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe code is pretty clear: loop the &#39;bd_boom&#39; sample with the reverb sound effect, at a particular rate. Sleep 2 seconds between loops.\u003C\u002Fp\u003E\\n\u003Cp\u003EBy the way, &#39;live-coding&#39;? What makes this a &#39;live-coding&#39; environment is that you can make changes to the code \u003Cem\u003Ewhile Sonic Pi is turning it into music\u003C\u002Fem\u003E. Don&#39;t like what you&#39;re hearing? Change the code up on the fly!\u003C\u002Fp\u003E\\n\u003Cp\u003EFor more on Sonic Pi, \u003Ca href=\\\"https:\u002F\u002Fwww.miskatonic.org\u002Fmusic\u002Faccess2015\u002F\\\"\u003Ethis workshop website\u003C\u002Fa\u003E is a good place to start. See also Laura Wrubel&#39;s \u003Ca href=\\\"http:\u002F\u002Flibrary.gwu.edu\u002Fscholarly-technology-group\u002Fposts\u002Fsound-library-work\\\"\u003Ereport on attending that workshop, and her and her colleague&#39;s work in this area\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"nihil-novi-sub-sole\\\"\u003ENihil Novi Sub Sole\u003C\u002Fh1\u003E\\n\u003Cp\u003EAgain, lest we think that we are at the cutting edge in our algorithmic generation of music, a salutary reminder was published in 1978 on &#39;dice music games&#39; of the eighteenth century, where rolls of the dice determined the recombination of pre-written snippets of music. \u003Ca href=\\\"https:\u002F\u002Frbnrpi.wordpress.com\u002Fproject-list\u002Fmozart-dice-generated-waltz-revisited-with-sonic-pi\u002F\\\"\u003ESome of these games have been explored and re-coded for the Sonic-Pi by Robin Newman\u003C\u002Fa\u003E. Newman also uses a tool that could be described as Markdown+Pandoc for musical notation, \u003Ca href=\\\"http:\u002F\u002Fwww.lilypond.org\u002F\\\"\u003ELilypond\u003C\u002Fa\u003E to score these compositions. The antecedents for everything you will find at \u003Cem\u003EThe Programming Historian\u003C\u002Fem\u003E are deeper than you might suspect!\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"conclusion\\\"\u003EConclusion\u003C\u002Fh1\u003E\\n\u003Cp\u003ESonifying our data forces us to confront the ways our data are often not so much about the past, but rather our constructed versions of it. It does so partly by virtue of its novelty and the art and artifice required to map data to sound. But it does so also by its contrast with our received notions of visualization of data. It may be that the sounds one generates never rise to the level of &#39;music&#39;; but if it helps transform how we encounter the past, and how others engage with the past, then the effort will be worth it. As Trevor Owens might have put it, &#39;Sonfication is about \u003Ca href=\\\"http:\u002F\u002Fwww.trevorowens.org\u002F2012\u002F11\u002Fdiscovery-and-justification-are-different-notes-on-sciencing-the-humanities\u002F\\\"\u003Ediscovery, not justification&#39;\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"terms\\\"\u003ETerms\u003C\u002Fh2\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Cstrong\u003EMIDI\u003C\u002Fstrong\u003E,\u003Ca name=\\\"midi\\\"\u003E\u003C\u002Fa\u003Emusical instrument digital interface. It is a description of a note&#39;s value and timing, not of its dynamics or how one might play it (this is an important distinction). It allows computers and instruments to talk to each other; one can apply different instrumentation to a MIDI file much the same way one would change the font on a piece of text (or run a markdown file through Pandoc).\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cstrong\u003EMP3\u003C\u002Fstrong\u003E,\u003Ca name=\\\"mp3\\\"\u003E\u003C\u002Fa\u003E a compression format for sound that is \u003Cem\u003Elossy\u003C\u002Fem\u003E in that it strips out data as part of its compression routine.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cstrong\u003EPitch\u003C\u002Fstrong\u003E,\u003Ca name=\\\"pitch\\\"\u003E\u003C\u002Fa\u003E the actual note itself (middle C, etc)\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cstrong\u003EAttack\u003C\u002Fstrong\u003E,\u003Ca name=\\\"attack\\\"\u003E\u003C\u002Fa\u003E how the note is played or hit\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cstrong\u003EDuration\u003C\u002Fstrong\u003E,\u003Ca name=\\\"duration\\\"\u003E\u003C\u002Fa\u003E how long the note lasts (whole notes, quarter notes, eighth notes etc)\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cstrong\u003EPitch Mapping &amp; Duration Mapping\u003C\u002Fstrong\u003E, \u003Ca name=\\\"pitch mapping\\\"\u003E\u003C\u002Fa\u003E scaling data values against a range of notes or the length of the note\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cstrong\u003EAmplitude\u003C\u002Fstrong\u003E, \u003Ca name=\\\"amplitude\\\"\u003E\u003C\u002Fa\u003Eroughly, the loudness of the note\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch1 id=\\\"references\\\"\u003EReferences\u003C\u002Fh1\u003E\\n\u003Cp\u003E\u003Ca name=\\\"Baio\\\"\u003E\u003C\u002Fa\u003EBaio, Andy. 2015. &#39;If Drake Was Born A Piano&#39;. Waxy. \u003Ca href=\\\"http:\u002F\u002Fwaxy.org\u002F2015\u002F12\u002Fif_drake_was_born_a_piano\u002F\\\"\u003Ehttp:\u002F\u002Fwaxy.org\u002F2015\u002F12\u002Fif_drake_was_born_a_piano\u002F\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ca name=\\\"Drucker\\\"\u003E\u003C\u002Fa\u003EDrucker, Johanna. 2011. Humanities Approaches to Graphical Display. DHQ 5.1 \u003Ca href=\\\"http:\u002F\u002Fweb.archive.org\u002Fweb\u002F20190203083307\u002Fhttp:\u002F\u002Fwww.digitalhumanities.org\u002Fdhq\u002Fvol\u002F5\u002F1\u002F000091\u002F000091.html\\\"\u003Ehttp:\u002F\u002Fweb.archive.org\u002Fweb\u002F20190203083307\u002Fhttp:\u002F\u002Fwww.digitalhumanities.org\u002Fdhq\u002Fvol\u002F5\u002F1\u002F000091\u002F000091.html\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ca name=\\\"Hedges\\\"\u003E\u003C\u002Fa\u003EHedges, Stephen A. 1978. “Dice Music in the Eighteenth Century”. Music &amp; Letters 59 (2). Oxford University Press: 180–87. \u003Ca href=\\\"http:\u002F\u002Fwww.jstor.org\u002Fstable\u002F734136\\\"\u003Ehttp:\u002F\u002Fwww.jstor.org\u002Fstable\u002F734136\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ca name=\\\"Hermann\\\"\u003E\u003C\u002Fa\u003EHermann, T. 2008. &quot;Taxonomy and definitions for sonification and auditory display&quot;. In P. Susini and O. Warusfel (eds.) Proceedings of the 14th international conference on auditory display (ICAD 2008). IRCAM, Paris. \u003Ca href=\\\"http:\u002F\u002Fwww.icad.org\u002FProceedings\u002F2008\u002FHermann2008.pdf\\\"\u003Ehttp:\u002F\u002Fwww.icad.org\u002FProceedings\u002F2008\u002FHermann2008.pdf\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ca name=\\\"Koebler\\\"\u003E\u003C\u002Fa\u003EKoebler, Jason. 2015. &quot;The Strange Acoustic Phenomenon Behind These Wacked-Out Versions of Pop Songs&quot; Motherboard, Dec 18. \u003Ca href=\\\"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20161023223029\u002Fhttp:\u002F\u002Fmotherboard.vice.com\u002Fread\u002Fthe-strange-acoustic-phenomenon-behind-these-wacked-out-versions-of-pop-songs\\\"\u003Ehttps:\u002F\u002Fweb.archive.org\u002Fweb\u002F20161023223029\u002Fhttp:\u002F\u002Fmotherboard.vice.com\u002Fread\u002Fthe-strange-acoustic-phenomenon-behind-these-wacked-out-versions-of-pop-songs\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ca name=\\\"Last\\\"\u003E\u003C\u002Fa\u003ELast and Usyskin, 2015. &quot;Listen to the Sound of Data&quot;. In Aaron K. Baughman et al. (eds.) Multimedia Data Mining and Analytics. Springer: Heidelberg. Pp. 419-446 \u003Ca href=\\\"https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F282504359_Listen_to_the_Sound_of_Data\\\"\u003Ehttps:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F282504359_Listen_to_the_Sound_of_Data\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
