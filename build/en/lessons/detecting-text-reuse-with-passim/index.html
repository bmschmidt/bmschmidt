<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-c09d08cd.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-c09d08cd.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/detecting-text-reuse-with-passim"),
					params: {lang:"en",lessons:"lessons",slug:"detecting-text-reuse-with-passim"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Detecting Text Reuse with Passim</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<p>In this lesson you will be introduced to the automatic detection of text reuse with the Passim library. You will learn how to install and run Passim and its dependencies, how to prepare your texts as input files suitable for use with Passim and, finally, how to process the output generated by Passim to carry out basic analyses.</p>
<p>This lesson targets digital humanities (DH) practitioners without any prior knowledge of text reuse, but with a working knowledge of <a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)">bash scripting</a> and Python as well as some data manipulation. For tutorials on bash scripting and <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a>, you can refer to the Programming Historian <a href="/en/lessons/intro-to-bash">“Introduction to the Bash Command Line</a> tutorial and the <a href="/en/lessons/?topic=python">library of current Python lessons</a> on the <em>Programming Historian</em> website.</p>
<p>This lesson includes an overview of <a href="https://github.com/dasmiq/Passim">Passim</a>, an open source tool for automatic text reuse detection. While the tool has been used in a number of small and large DH projects, it lacks a user-friendly documentation with examples and set up instructions, a gap that we aim to fill with this <em>Programming Historian</em> lesson.</p>
<h1 id="introduction-to-text-reuse">Introduction to Text Reuse</h1>
<p>Text reuse can be defined as &quot;the meaningful reiteration of text, usually beyond the simple repetition of common language&quot; (Romanello et al. 2014). It is such a broad concept that it can be understood at different levels and studied in a large variety of contexts. In a publishing or teaching context, for example, instances of text reuse can constitute plagiarism should portions of someone else’s text be repeated without appropriate attribution. In the context of literary studies, text reuse is often just a synonym for literary phenomena like allusions, paraphrases and direct quotations.</p>
<p>The following list includes just some of the libraries available that perform automatic text reuse detection:</p>
<ul>
<li>The <a href="https://docs.ropensci.org/textreuse/">R textreuse package</a> (R) written by Lincoln Mullen</li>
<li><a href="https://www.etrap.eu/research/tracer/">TRACER</a> (Java) developed by Marco Büchler and colleagues</li>
<li><a href="https://blast.ncbi.nlm.nih.gov/Blast.cgi">Basic Local Alignment Search Tool (BLAST)</a></li>
<li><a href="https://github.com/tesserae/tesserae">Tesserae</a> (PHP, Perl)</li>
<li><a href="https://github.com/ARTFL-Project/text-pair">TextPAIR (Pairwise Alignment for Intertextual Relations)</a></li>
<li><a href="https://github.com/dasmiq/Passim">Passim</a> (Scala) developed by <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a> (Northeastern University)</li>
</ul>
<p>For this tutorial we chose the Passim library for three main reasons. Firstly, it can be adapted to a variety of use cases as it works well on a small text collection as well as on a large-scale corpus. Secondly, while the documentation for Passim is extensive, because of its relatively advanced user audience, a more user-centered step-by-step tutorial about detecting text reuse with Passim would be beneficial to the user community. Lastly, the following examples illustrate the variety of scenarios in which text reuse is a useful methodology:</p>
<ul>
<li>To determine whether a digital library contains multiple editions of the same work(s)</li>
<li>To find quotations in a text, provided that the target works are known (e.g. find quotations of the Bible within 17c English literature)  </li>
<li>To study the virality and spread of texts (e.g. <a href="https://viraltexts.org/">Viral Texts</a> by Cordell and Smith for historical newspapers)</li>
<li>To identify (and possibly filter out) duplicate documents within a text collection before performing further processing steps (e.g. topic modelling as illustrated by Schofield et al. (2017))</li>
</ul>
<p>For these reasons, Passim is usually a great choice. It will help you automate the search for repeated text passages in a corpus — whether these are running ads in newspapers, multiple copies of the same poem, or direct (and slightly indirect) quotations in someone else&#39;s book.
Text reuse detection as implemented in Passim aims at identifying these copies and repetitions automatically, and yields clusters of passages that were deemed to be related with one another. Ultimately, what a cluster contains can vary a lot and will depend on your research question. For example, Passim can group together copies of the same article that differ only with respect to optical character recognition (OCR) errors, but it can also help to retrieve texts that share the same journalistic template, such as horoscopes or advertisements.</p>
<h1 id="prerequisites">Prerequisites</h1>
<p>This tutorial requires the following:</p>
<ul>
<li>A basic understanding of Bash scripts. For readers needing a review on Bash scripts, read the <em>Programming Historian</em> lesson <a href="/en/lessons/intro-to-bash">&quot;Introduction to the Bash Command Line&quot;</a>.</li>
<li>Knowledge of JSON. To learn more about JSON, read the <em>Programming Historian</em> lesson <a href="/en/lessons/json-and-jq">&quot;Reshaping JSON with jq&quot;</a>.</li>
</ul>
<p>Moreover, while a basic understanding of Python — and a working Python installation — are not strictly needed to work with Passim, they are required to run some parts of this tutorial (e.g. the Jupyter notebook with data exploration, or the Early English Books Online (EEBO) data preparation script). If you are not familiar with Python, please read the <em>Programming Historian</em> lesson <a href="/en/lessons/introduction-and-installation">&quot;Python Introduction and Installation&quot;</a>.   </p>
<p>Note that installing Passim on Windows is more arduous than macOS or Linux. As a result, we recommend using macOS or Linux (or a virtual environment) for this lesson.</p>
<h1 id="installing-passim">Installing Passim</h1>
<p>Installing Passim requires installing the following software:</p>
<ul>
<li><a href="https://www.java.com/fr/download/">Java JDK (version 8)</a></li>
<li><a href="https://www.scala-sbt.org/">Scala Build Tool (SBT)</a></li>
<li><a href="https://spark.apache.org/">Apache Spark</a></li>
</ul>
<p>But why are all these dependencies needed?</p>
<p>Passim is written in a programming language called Scala. To execute a software written in Scala, its sources need to be compiled into an executable JAR file, which is performed by <code>sbt</code>, Scala&#39;s interactive build tool. Finally, since Passim is designed to work also on large-scale text collections (with several thousands or millions of documents), behind the scenes it uses Spark, a cluster-computing framework written in Java. Using Spark allows Passim to handle the distributed processing of certain parts of the code, which is useful when handling large amounts of data. The <a href="https://spark.apache.org/docs/latest/cluster-overview.html#glossary">Spark glossary</a> is a useful resource to learn basic Spark terminology (words like &quot;driver&quot;, &quot;executor&quot;, etc.) but learning this terminology may not be necessary if you are running Passim on a small dataset.</p>
<p>Before installing this set of software, you&#39;ll need to download the Passim source code from GitHub:</p>
<pre><code class="language-bash">&gt;&gt;&gt; git clone https://github.com/dasmiq/Passim.git
</code></pre>
<p>If you are not familiar with Git and GitHub, we recommend reading the <em>Programming Historian</em> lesson <a href="https://doi.org/10.46430/phen0051">&quot;An Introduction to Version Control Using GitHub Desktop&quot;</a>.</p>
<h2 id="macos-instructions">macOS instructions</h2>
<p>These instructions are aimed at users of Apple&#39;s macOS and were tested under version 10.13.4 (a.k.a. High Sierra).</p>
<h3 id="check-java-installation">Check Java Installation</h3>
<p>Ensure that you have Java Development Kit 8 by typing the following command in a new Terminal window:</p>
<pre><code class="language-bash">&gt;&gt;&gt; java -version
</code></pre>
<p>If the output of this command looks similar to the following example, then Java 8 is installed on your machine.</p>
<pre><code>openjdk version &quot;1.8.0_262&quot;
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_262-b10)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.262-b10, mixed mode)
</code></pre>
<h3 id="installing-java-8">Installing Java 8</h3>
<p>In case another version of Java is installed on your machine, follow the following steps to install Java 8 alongside the existing Java version.</p>
<p>This is important so as not to break already installed software that needs more recent Java versions.</p>
<ol>
<li><p>Install the <code>brew</code> package manager by following installation instructions on the <a href="https://brew.sh/">Brew.sh</a> website. Once the installation is completed, run <code>brew --help</code> to verify it works.</p>
</li>
<li><p>Use <code>brew</code> to install Java 8.</p>
</li>
</ol>
<pre><code class="language-bash">&gt;&gt;&gt; brew cask install adoptopenjdk/openjdk/adoptopenjdk8
</code></pre>
<p>Verify that Java 8 is installed.</p>
<pre><code class="language-bash">&gt;&gt;&gt; /usr/libexec/java_home -V
</code></pre>
<p>This command should output something similar to the following:</p>
<pre><code class="language-bash">Matching Java Virtual Machines (2):
    13.0.2, x86_64:    &quot;Java SE 13.0.2&quot;    /Library/Java/JavaVirtualMachines/jdk-13.0.2.jdk/Contents/Home
    1.8.0_262, x86_64:    &quot;AdoptOpenJDK 8&quot;    /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home

/Library/Java/JavaVirtualMachines/jdk-13.0.2.jdk/Contents/Home
</code></pre>
<ol start="3">
<li>Install <code>jenv</code>, a tool that allows you to manage multiple Java versions installed on the same machine, and to easily switch between them.</li>
</ol>
<pre><code class="language-bash">&gt;&gt;&gt; brew install jenv
</code></pre>
<p>To be able to call <code>jenv</code> without specifying the executable&#39;s full path don&#39;t forget to add <code>jenv</code> to your <code>$PATH</code> environment variable by opening the file <code>~/.bashrc</code> with your favorite text editor and adding the following lines at the end of the file:</p>
<pre><code class="language-bash"># activate jenv
export PATH=&quot;$HOME/.jenv/bin:$PATH&quot;
eval &quot;$(jenv init -)&quot;
</code></pre>
<p>After adding these lines, you need to open another terminal window or run the following line so that the <code>$PATH</code> variable is updated with the change you just made (the command <code>source</code> triggers the reload of your <code>bash</code> configuration).</p>
<pre><code class="language-bash">&gt;&gt;&gt; source ~/.bashrc
</code></pre>
<p>Once installed, add the existing Java versions to <code>jenv</code> (i.e. those listed by the command <code>/usr/libexec/java_home -V</code>):</p>
<pre><code class="language-bash"># your mileage may vary, so make sure you replace this path
# with the actual path to the JAVA_HOME in your machine
&gt;&gt;&gt; jenv add /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home
</code></pre>
<p>Now you can set the default version of Java for this project by running the following:</p>
<pre><code class="language-bash">&gt;&gt;&gt; jenv local 1.8

# verify
&gt;&gt;&gt; java -version
</code></pre>
<h3 id="compiling-passim-from-the-sources-macos">Compiling Passim From the Sources (macOS)</h3>
<p>Passim is written in a programming language called Scala. Before being able to execute a software written in Scala, its sources need to be compiled. This task is performed by <code>sbt</code>, the Interactive Build Tool.</p>
<p>To determine whether <code>sbt</code> is installed on your machine, run the following command:</p>
<pre><code class="language-bash">&gt;&gt;&gt; sbt about
</code></pre>
<p>If this command prints <code>bash: sbt: command not found</code> it means <code>sbt</code> is not installed.
However, Passim comes with a useful script (<code>build/sbt</code>) that will download and install SBT automatically before compiling the sources from Passim.</p>
<p><strong>NB</strong>: Using an external (i.e. already installed) SBT may lead to issues, we recommend the following method for compiling Passim.</p>
<p>To compile the program, run the following command from the directory where you&#39;ve previously cloned Passim&#39;s GH repository:</p>
<pre><code class="language-bash">&gt;&gt;&gt; cd Passim/
&gt;&gt;&gt; build/sbt package
</code></pre>
<p>This command will take some time (around 3 minutes on a modern connection), but will let you know of the progress. As your computer starts downloading required files, a log will be printed on screen. At the end of this process, <code>sbt</code> will have created a <code>.jar</code> archive contaning the compiled sources for Passim. This file is found in the <code>target</code> directory: <code>target/scala-2.11/Passim_2.11-0.2.0.jar</code>. Depending on the version of Scala and Passim, the actual path might be slightly different on your computer.</p>
<p>The <code>bin</code> directory contains a Passim file: this is the executable that will launch Passim. In order for your computer the location of this file, and thus for it to recognise the Passim command, we need to add the path to the <code>PATH</code> environment variable.</p>
<pre><code class="language-bash"># replace /home/simon/Passim for the directory where you installed Passim
&gt;&gt;&gt; export PATH=&quot;/home/simon/Passim/bin:$PATH&quot;
</code></pre>
<p>To add the path permanently to the <code>PATH</code> environment variable, open the file <code>~/.bashrc</code> with your favorite text editor and add the following line anywhere in the file (then execute <code>source ~/.bashrc</code> to apply this change):</p>
<pre><code class="language-bash"># replace &quot;/home/simon/Passim&quot; for the directory where you installed Passim
export PATH=&quot;/home/simon/Passim/bin:$PATH&quot;
</code></pre>
<h3 id="installing-spark">Installing Spark</h3>
<ol>
<li><p>Navigate to the <a href="http://spark.apache.org/downloads">download section</a> of the Spark website and select Spark release version &#39;3.x.x&#39; (where &#39;<em>x</em>&#39; means any version that starts with &#39;3.&#39;), and package type &#39;Pre-built for Apache Hadoop 2.7&#39; from the dropdown menus.</p>
</li>
<li><p>Extract the compressed binaries to a directory of your choice (e.g. <code>/Applications</code>):</p>
<pre><code class="language-bash">&gt;&gt;&gt; cd /Applications/
&gt;&gt;&gt; tar -xvf ~/Downloads/spark-3.1.x-bin-hadoop2.7.tgz
</code></pre>
</li>
<li><p>Add the directory where you installed Spark to your <code>PATH</code> environment variable. To do so temporarily run the following command:</p>
</li>
</ol>
<pre><code class="language-bash">&gt;&gt;&gt; export PATH=&quot;/Applications/spark-3.1.x-bin-hadoop2.7:$PATH&quot;
</code></pre>
<p>To add the path installation directory permanently to your <code>PATH</code> environment variable, open the file <code>~/.bashrc</code> with your favorite text editor and add the following line anywhere in the file:</p>
<pre><code class="language-bash">export PATH=&quot;/Applications/spark-3.1.x-bin-hadoop2.7:$PATH&quot;
</code></pre>
<p>After editing <code>~/.bashrc</code>, open another terminal window or run the following command:</p>
<pre><code class="language-bash">&gt;&gt;&gt; source ~/.bashrc
</code></pre>
<h2 id="linux-instructions">Linux instructions</h2>
<p>These instructions are aimed at Debian-based distributions (Debian, Ubuntu, Linux Mint, etc.). If you run another type of distribution (Fedora, Gentoo, etc.), replace the distribution-specific commands (eg <code>apt</code>) with those used by your specific distribution.</p>
<h3 id="check-java-installation-1">Check Java Installation</h3>
<p>To ensure that you have the Java Development Kit 8 installed, run the following command:</p>
<pre><code class="language-bash">&gt;&gt;&gt; java -version
</code></pre>
<p>If the command above returns <code>1.8.0_252</code> or similar, then you have Java Development Kit 8 installed (the <code>8</code> lets you know you have correct kit installed and selected by default). If your output looks different, choose one of the following commands accordingly:</p>
<pre><code class="language-bash"># If you don&#39;t, install it
&gt;&gt;&gt;&gt; apt install openjdk-8-jdk
</code></pre>
<pre><code class="language-bash"># if your *default* JDK is not version 8
&gt;&gt;&gt; sudo update-alternatives --config java
</code></pre>
<h3 id="compiling-passim-from-the-sources">Compiling Passim from the Sources</h3>
<p>Refer to the <a href="#compiling-passim-from-the-sources-(macOS)">compilation instructions for macOS</a>, as they are the same for the Linux environment.</p>
<h3 id="installing-spark-1">Installing Spark</h3>
<ol>
<li>Download the Spark binaries by using <code>wget</code>:<pre><code class="language-bash">&gt;&gt;&gt; wget -P /tmp/ http://apache.mirrors.spacedump.net/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz
</code></pre>
</li>
<li>Extract the compressed binaries to a directory of your choice:<pre><code class="language-bash">&gt;&gt;&gt; tar -xvf /tmp/spark-3.1.2-bin-hadoop2.7.tgz -C /usr/local/
</code></pre>
</li>
<li>Add the directory where you installed Spark to your <code>PATH</code> environment variable. To add the directory to your <code>PATH</code> environment variable temporarily, run the following command:<pre><code class="language-bash">&gt;&gt;&gt; export PATH=&quot;/usr/local/spark-3.1.2-bin-hadoop2.7/bin:$PATH&quot;  # note that &quot;/usr/local/&quot; is the directory specified above, if you specified another directory change this accordingly
</code></pre>
To add the directory to your <code>PATH</code> environment variable permanently, open the file <code>~/.bashrc</code> with your favorite text editor and add the following line anywhere in the file:<pre><code class="language-bash">&gt;&gt;&gt; export PATH=&quot;/usr/local/spark-3.1.2-bin-hadoop2.7/bin:$PATH&quot;
</code></pre>
After editing <code>~/.bashrc</code>, you need to open another terminal window or run the following line so that your <code>PATH</code> variable is updated with the change you just made.<pre><code class="language-bash">&gt;&gt;&gt; source ~/.bashrc
</code></pre>
</li>
</ol>
<h2 id="verify-the-installation">Verify the Installation</h2>
<p>At this point you have installed Passim and all required packages on your machine. If you type <code>Passim --help</code> in the command line, you should see output similar to the following:</p>
<pre><code class="language-bash">Ivy Default Cache set to: /Users/matteo/.ivy2/cache
The jars for the packages stored in: /Users/matteo/.ivy2/jars
:: loading settings :: url = jar:file:/Applications/spark-2.4.6-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
com.github.scopt#scopt_2.11 added as a dependency
graphframes#graphframes added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-bb5bd11f-ba3c-448e-8f69-5693cc073428;1.0
    confs: [default]
    found com.github.scopt#scopt_2.11;3.5.0 in spark-list
    found graphframes#graphframes;0.7.0-spark2.4-s_2.11 in spark-list
    found org.slf4j#slf4j-api;1.7.16 in spark-list
:: resolution report :: resolve 246ms :: artifacts dl 4ms
    :: modules in use:
    com.github.scopt#scopt_2.11;3.5.0 from spark-list in [default]
    graphframes#graphframes;0.7.0-spark2.4-s_2.11 from spark-list in [default]
    org.slf4j#slf4j-api;1.7.16 from spark-list in [default]
    ---------------------------------------------------------------------
    |                  |            modules            ||   artifacts   |
    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
    ---------------------------------------------------------------------
    |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
    ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-bb5bd11f-ba3c-448e-8f69-5693cc073428
    confs: [default]
    0 artifacts copied, 3 already retrieved (0kB/6ms)
20/07/17 15:23:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties
20/07/17 15:23:19 INFO SparkContext: Running Spark version 2.4.6
20/07/17 15:23:19 INFO SparkContext: Submitted application: Passim.PassimApp
20/07/17 15:23:19 INFO SecurityManager: Changing view acls to: matteo
20/07/17 15:23:19 INFO SecurityManager: Changing modify acls to: matteo
20/07/17 15:23:19 INFO SecurityManager: Changing view acls groups to:
20/07/17 15:23:19 INFO SecurityManager: Changing modify acls groups to:
20/07/17 15:23:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(matteo); groups with view permissions: Set(); users  with modify permissions: Set(matteo); groups with modify permissions: Set()
20/07/17 15:23:20 INFO Utils: Successfully started service &#39;sparkDriver&#39; on port 62254.
20/07/17 15:23:20 INFO SparkEnv: Registering MapOutputTracker
20/07/17 15:23:20 INFO SparkEnv: Registering BlockManagerMaster
20/07/17 15:23:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/07/17 15:23:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/07/17 15:23:20 INFO DiskBlockManager: Created local directory at /private/var/folders/8s/rnkbnf8549qclh_gcb_qj_yw0000gv/T/blockmgr-f42fca4e-0a6d-4751-8d3b-36db57896aa4
20/07/17 15:23:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/07/17 15:23:20 INFO SparkEnv: Registering OutputCommitCoordinator
20/07/17 15:23:20 INFO Utils: Successfully started service &#39;SparkUI&#39; on port 4040.
20/07/17 15:23:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.24:4040
20/07/17 15:23:20 INFO SparkContext: Added JAR file:///Users/matteo/.ivy2/jars/com.github.scopt_scopt_2.11-3.5.0.jar at spark://192.168.0.24:62254/jars/com.github.scopt_scopt_2.11-3.5.0.jar with timestamp 1594992200488
20/07/17 15:23:20 INFO SparkContext: Added JAR file:///Users/matteo/.ivy2/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar at spark://192.168.0.24:62254/jars/graphframes_graphframes-0.7.0-spark2.4-s_2.11.jar with timestamp 1594992200489
20/07/17 15:23:20 INFO SparkContext: Added JAR file:///Users/matteo/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://192.168.0.24:62254/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1594992200489
20/07/17 15:23:20 INFO SparkContext: Added JAR file:/Users/matteo/Documents/Passim/target/scala-2.11/Passim_2.11-0.2.0.jar at spark://192.168.0.24:62254/jars/Passim_2.11-0.2.0.jar with timestamp 1594992200489
20/07/17 15:23:20 INFO Executor: Starting executor ID driver on host localhost
20/07/17 15:23:20 INFO Utils: Successfully started service &#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; on port 62255.
20/07/17 15:23:20 INFO NettyBlockTransferService: Server created on 192.168.0.24:62255
20/07/17 15:23:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/07/17 15:23:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.24, 62255, None)
20/07/17 15:23:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.24:62255 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.24, 62255, None)
20/07/17 15:23:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.24, 62255, None)
20/07/17 15:23:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.24, 62255, None)
Usage: Passim [options] &lt;path&gt;,&lt;path&gt;,... &lt;path&gt;

  --boilerplate            Detect boilerplate within groups.
  --labelPropagation       Cluster with label propagation.
  -n, --n &lt;value&gt;          index n-gram features; default=5
  -l, --minDF &lt;value&gt;      Lower limit on document frequency; default=2
  -u, --maxDF &lt;value&gt;      Upper limit on document frequency; default=100
  -m, --min-match &lt;value&gt;  Minimum number of n-gram matches between documents; default=5
  -a, --min-align &lt;value&gt;  Minimum length of alignment; default=20
  -L, --min-lines &lt;value&gt;  Minimum number of lines in boilerplate and docwise alignments; default=5
  -g, --gap &lt;value&gt;        Minimum size of the gap that separates passages; default=100
  -c, --context &lt;value&gt;    Size of context for aligned passages; default=0
  -o, --relative-overlap &lt;value&gt;
                           Minimum relative overlap to merge passages; default=0.8
  -M, --merge-diverge &lt;value&gt;
                           Maximum length divergence for merging extents; default=0.3
  -r, --max-repeat &lt;value&gt;
                           Maximum repeat of one series in a cluster; default=10
  -p, --pairwise           Output pairwise alignments
  -d, --docwise            Output docwise alignments
  --linewise               Output linewise alignments
  -N, --names              Output names and exit
  -P, --postings           Output postings and exit
  -i, --id &lt;value&gt;         Field for unique document IDs; default=id
  -t, --text &lt;value&gt;       Field for document text; default=text
  -s, --group &lt;value&gt;      Field to group documents into series; default=series
  -f, --filterpairs &lt;value&gt;
                           Constraint on posting pairs; default=gid &lt; gid2
  --fields &lt;value&gt;         Semicolon-delimited list of fields to index
  --input-format &lt;value&gt;   Input format; default=json
  --schema-path &lt;value&gt;    Input schema path in json format
  --output-format &lt;value&gt;  Output format; default=json
  --aggregate              Output aggregate alignments of consecutive seqs
  -w, --word-length &lt;value&gt;
                           Minimum average word length to match; default=2
  --help                   prints usage text
  &lt;path&gt;,&lt;path&gt;,...        Comma-separated input paths
  &lt;path&gt;                   Output path
20/07/17 15:23:20 INFO SparkContext: Invoking stop() from shutdown hook
20/07/17 15:23:20 INFO SparkUI: Stopped Spark web UI at http://192.168.0.24:4040
20/07/17 15:23:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/07/17 15:23:21 INFO MemoryStore: MemoryStore cleared
20/07/17 15:23:21 INFO BlockManager: BlockManager stopped
20/07/17 15:23:21 INFO BlockManagerMaster: BlockManagerMaster stopped
20/07/17 15:23:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/07/17 15:23:21 INFO SparkContext: Successfully stopped SparkContext
20/07/17 15:23:21 INFO ShutdownHookManager: Shutdown hook called
20/07/17 15:23:21 INFO ShutdownHookManager: Deleting directory /private/var/folders/8s/rnkbnf8549qclh_gcb_qj_yw0000gv/T/spark-dbeee326-7f37-475a-9379-74da31d72117
20/07/17 15:23:21 INFO ShutdownHookManager: Deleting directory /private/var/folders/8s/rnkbnf8549qclh_gcb_qj_yw0000gv/T/spark-9ae8a384-b1b3-49fa-aaff-94ae2f37b2d9
</code></pre>
<h1 id="preparing-data-for-passim">Preparing Data for Passim</h1>
<p>The goal of using Passim is to automate the search for repeated text passages in a corpus. For example, a newspaper corpus contains multiple copies of the same article, identical or with slight differences from one another, as well as repetitions of smaller portions of a newspaper page (e.g. advertisement, event listings, etc.).</p>
<p>As the documentation for Passim specifies &quot;the input to Passim is a set of documents. Depending on the kind of data you have, you might choose documents to be whole books, pages of books, whole issues of newspapers, individual newspaper articles, etc. Minimally, a document consists of an identifier string and a single string of text content&quot; (Refer to the minimal JSON input example in the next section for more information about the structure of input for Passim).</p>
<p>Figure 1 gives a schematic representation of input and output data for Passim. Given an input set of documents, divided into document series, Passim will attempt to identify reuse of text from documents in different series, and not within these series. In the case of a newspaper corpus, articles from the same newspaper will belong to the same document series, as we are not interested in detecting reuse within the same newspaper, but across different newspapers.</p>
<p>Ultimately, what constitutes a document, and how these documents should be divided into series, are the choices you&#39;ll need to make when preparing your data for Passim.  Naturally, the decision on what constitutes a <em>series</em> of documents is directly dependent on your goals or research questions. Finding quotations of the Bible in a corpus of books is a &quot;one-to-many&quot; case of text reuse detection, which requires documents to be grouped into two series (<code>bible</code> and <code>non_bible</code>). Instead, the comparison between multiple editions of the Bible (also known as collation) can be seen a &quot;many-to-many&quot; case, where each edition will correspond to and constitute a series of documents (e.g. pages).  If your research questions change at some point, thus requiring a re-definition of document series, you will need also to produce new input data for Passim to reflect this change.</p>
<p>{% include figure.html filename=&quot;textreuse-generic.png&quot; caption=&quot;Figure 1. Schematic representation of text reuse clusters; each cluster consists of similar passages found in several series of documents.&quot; %}</p>
<h2 id="basic-json-format">Basic JSON format</h2>
<p>The input format for Passim consists of JSON documents in the <a href="http://jsonlines.org/">JSON lines format</a> (i.e. each line of text contains a single JSON document).</p>
<p>The following file content for a file named <code>test.json</code> illustrates a minimal example of the input format for Passim: </p>
<pre><code class="language-json">{&quot;id&quot;: &quot;d1&quot;, &quot;series&quot;: &quot;abc&quot;, &quot;text&quot;: &quot;This is the text of a document.&quot;}
{&quot;id&quot;: &quot;d2&quot;, &quot;series&quot;: &quot;def&quot;, &quot;text&quot;: &quot;This is the text of another document.&quot;}
</code></pre>
<p>The fields <code>id</code>, <code>series</code> and <code>text</code> are the only fields required by Passim. Given this file as input, the software will try to detect text reuse between documents in the series <code>abc</code> and those in the series <code>def</code>, on the basis of the contents in <code>text</code>.</p>
<p>Throughout this tutorial we will be using the command-line tool <a href="https://stedolan.github.io/jq/"><code>jq</code></a> to inspect and do some basic process on both input and output JSON data. Note that, if you don&#39;t have <code>jq</code> installed, you&#39;ll need to execute <code>sudo apt-get install jq</code> under Ubuntu or <code>brew install jq</code> under macOS (for other operating systems <a href="https://stedolan.github.io/jq/download/">refer to the official JQ installation page</a>).</p>
<p>For example, to select and print the field <code>series</code> of your input <code>test.json</code>, run the following command:</p>
<pre><code class="language-bash">&gt;&gt;&gt; jq &#39;.series&#39; test.json

# this will print
&quot;abc&quot;
&quot;def&quot;
</code></pre>
<p>Note: If you are using <code>jq</code> to look at your JSON data, you need to use the <code>--slurp</code> parameter whenever you want to treat the content of one or more JSON line files as a single array of JSON documents and apply some filters to it (e.g. to select and print only one document, use the following command <code>jq --slurp &#39;.[-1]&#39; test.json</code>). Otherwise <code>jq</code> will treat each document separately thus causing the following error:</p>
<pre><code class="language-bash">&gt;&gt;&gt; jq &#39;.[0]&#39; test.json

jq: error (at &lt;stdin&gt;:1): Cannot index string with string &quot;series&quot;
jq: error (at &lt;stdin&gt;:2): Cannot index string with string &quot;series&quot;
</code></pre>
<h2 id="a-note-on-packaging-data">A Note on Packaging Data</h2>
<p>Depending one the total size of your data, it may be a good idea to store Passim input files as compressed archives. Passim supports several compression schemes like .gzip and .bzip2. Note that a compressed datastream will be slower to process than an uncompressed one, so using this option will only be beneficial if your data is large (i.e. gigabytes of text), if you have access to many computing cores, or have a limited amount of disk space.</p>
<p>This command (or, better, chain of commands) will output the first document in a bzip2-compressed JSON lines file (some fields have been truncated for the sake of readability):</p>
<pre><code class="language-bash">&gt;&gt;&gt; bzcat impresso/GDL-1900.jsonl.bz2 | jq --slurp &#39;.[0]&#39;
</code></pre>
<p>And will output the following:</p>
<pre><code class="language-json">{
  &quot;series&quot;: &quot;GDL&quot;,
  &quot;date&quot;: &quot;1900-12-12&quot;,
  &quot;id&quot;: &quot;GDL-1900-12-12-a-i0001&quot;,
  &quot;cc&quot;: true,
  &quot;pages&quot;: [
    {
      &quot;id&quot;: &quot;GDL-1900-12-12-a-p0001&quot;,
      &quot;seq&quot;: 1,
      &quot;regions&quot;: [
        {
          &quot;start&quot;: 0,
          &quot;length&quot;: 13,
          &quot;coords&quot;: {
            &quot;x&quot;: 471,
            &quot;y&quot;: 1240,
            &quot;w&quot;: 406,
            &quot;h&quot;: 113
          }
        },
        {
          &quot;start&quot;: 13,
          &quot;length&quot;: 2,
          &quot;coords&quot;: {
            &quot;x&quot;: 113,
            &quot;y&quot;: 1233,
            &quot;w&quot;: 15,
            &quot;h&quot;: 54
          }
        },
        ...
      ]
    }
  ],
  &quot;title&quot;: &quot;gratuitement ,la §azette seia envoyée&quot;,
  &quot;text&quot;: &quot;gratuitement\n, la § azette\nseia envoyée\ndès ce jour au 31 décembre, aux personnes\nqui s&#39;abonneront pour l&#39;année 1901.\nLes abonnements sont reçus par l&#39;admi-\nnistration de la Gazette de Lausanne et dans\ntous les bureaux de poste.\n&quot;
}
</code></pre>
<h2 id="custom-json-format">Custom JSON format</h2>
<p>(Note: This subsection is not strictly necessary to run Passim, as the second case study will showcase. Nonetheless, these steps may be useful to readers with advanced needs with the regards to the format and structure of input data.)</p>
<p>There are cases where you may want to include additional information (i.e. JSON fields) in each input document, in addition to the required ones (<code>id</code>, <code>series</code>, <code>text</code>). As an example, when working with OCR data you may want to pass image coordinate information alongside the article text. Passim does support the use of input data that follow a custom JSON format as behind the scenes it relies on Spark to infer the structure of the input data (i.e. the JSON schema). Passim will not directly use these fields, but it will keep them in the produced output.</p>
<p>However, there may be cases where Spark fails to infer the correct structure of input data (e.g. by inferring a wrong data type for a given field). In these cases, you need to inform Passim about the correct schema of the input data. </p>
<p>The following example illustrates a step-by-step approach to troubleshooting this relatively rare situation where one needs to correct the inferred JSON schema. Passim comes with the command <code>json-df-schema</code>, which runs a (Python) script to infer the schema from any JSON input. The following steps are necessary to infer the structure from any JSON data:</p>
<ol>
<li>Install the necessary Python libraries.<pre><code class="language-bash">&gt;&gt;&gt; pip install pyspark
</code></pre>
</li>
<li>Extract an input example from one of our compressed input files.<pre><code class="language-bash"># here we take the 3rd document in the .bz2 file
# and save it to a new local file
&gt;&gt;&gt; bzcat impresso/data/GDL-1900.jsonl.bz2 | head | jq --slurp &quot;.[2]&quot; &gt; impresso/data/impresso-sample-document.json
</code></pre>
</li>
<li>Ask <code>json-df-schema</code> to infer the schema of our data from our sample file.<pre><code class="language-bash">&gt;&gt;&gt; json-df-schema impresso/data/impresso-sample-document.json &gt; impresso/schema/Passim.schema.orig
</code></pre>
</li>
</ol>
<p><code>json-df-schema</code> will try to guess the JSON schema of input data and output it to a file. The following example is what the schema generated by Passim (<code>Passim.schema.orig</code>) looks like:</p>
<pre><code class="language-json">{
  &quot;fields&quot;: [
    {
      &quot;metadata&quot;: {},
      &quot;name&quot;: &quot;cc&quot;,
      &quot;nullable&quot;: true,
      &quot;type&quot;: &quot;boolean&quot;
    },
    {
      &quot;metadata&quot;: {},
      &quot;name&quot;: &quot;date&quot;,
      &quot;nullable&quot;: true,
      &quot;type&quot;: &quot;string&quot;
    },
    {
      &quot;metadata&quot;: {},
      &quot;name&quot;: &quot;id&quot;,
      &quot;nullable&quot;: true,
      &quot;type&quot;: &quot;string&quot;
    },
    {
      &quot;metadata&quot;: {},
      &quot;name&quot;: &quot;pages&quot;,
      &quot;nullable&quot;: true,
      &quot;type&quot;: {
        &quot;containsNull&quot;: true,
        &quot;elementType&quot;: {
          &quot;fields&quot;: [
            {
              &quot;metadata&quot;: {},
              &quot;name&quot;: &quot;id&quot;,
              &quot;nullable&quot;: true,
              &quot;type&quot;: &quot;string&quot;
            },
            {
              &quot;metadata&quot;: {},
              &quot;name&quot;: &quot;regions&quot;,
              &quot;nullable&quot;: true,
              &quot;type&quot;: {
                &quot;containsNull&quot;: true,
                &quot;elementType&quot;: {
                  &quot;fields&quot;: [
                    {
                      &quot;metadata&quot;: {},
                      &quot;name&quot;: &quot;coords&quot;,
                      &quot;nullable&quot;: true,
                      &quot;type&quot;: {
                        &quot;fields&quot;: [
                          {
                            &quot;metadata&quot;: {},
                            &quot;name&quot;: &quot;h&quot;,
                            &quot;nullable&quot;: true,
                            &quot;type&quot;: &quot;long&quot;
                          },
                          {
                            &quot;metadata&quot;: {},
                            &quot;name&quot;: &quot;w&quot;,
                            &quot;nullable&quot;: true,
                            &quot;type&quot;: &quot;long&quot;
                          },
                          {
                            &quot;metadata&quot;: {},
                            &quot;name&quot;: &quot;x&quot;,
                            &quot;nullable&quot;: true,
                            &quot;type&quot;: &quot;long&quot;
                          },
                          {
                            &quot;metadata&quot;: {},
                            &quot;name&quot;: &quot;y&quot;,
                            &quot;nullable&quot;: true,
                            &quot;type&quot;: &quot;long&quot;
                          }
                        ],
                        &quot;type&quot;: &quot;struct&quot;
                      }
                    },
                    {
                      &quot;metadata&quot;: {},
                      &quot;name&quot;: &quot;length&quot;,
                      &quot;nullable&quot;: true,
                      &quot;type&quot;: &quot;long&quot;
                    },
                    {
                      &quot;metadata&quot;: {},
                      &quot;name&quot;: &quot;start&quot;,
                      &quot;nullable&quot;: true,
                      &quot;type&quot;: &quot;long&quot;
                    }
                  ],
                  &quot;type&quot;: &quot;struct&quot;
                },
                &quot;type&quot;: &quot;array&quot;
              }
            },
            {
              &quot;metadata&quot;: {},
              &quot;name&quot;: &quot;seq&quot;,
              &quot;nullable&quot;: true,
              &quot;type&quot;: &quot;long&quot;
            }
          ],
          &quot;type&quot;: &quot;struct&quot;
        },
        &quot;type&quot;: &quot;array&quot;
      }
    },
    {
      &quot;metadata&quot;: {},
      &quot;name&quot;: &quot;series&quot;,
      &quot;nullable&quot;: true,
      &quot;type&quot;: &quot;string&quot;
    },
    {
      &quot;metadata&quot;: {},
      &quot;name&quot;: &quot;text&quot;,
      &quot;nullable&quot;: true,
      &quot;type&quot;: &quot;string&quot;
    },
    {
      &quot;metadata&quot;: {},
      &quot;name&quot;: &quot;title&quot;,
      &quot;nullable&quot;: true,
      &quot;type&quot;: &quot;string&quot;
    }
  ],
  &quot;type&quot;: &quot;struct&quot;
}
</code></pre>
<p>Passim has failed to recognize the coordinate field as containing integer values and it has interpreted as a long data type.  At this point, we need to change the type of the sub-fields of <code>coords</code> (i.e. <code>h</code>, <code>w</code>, <code>x</code>, and <code>y</code>) from <code>&quot;type&quot;: &quot;long&quot;</code> to <code>&quot;type&quot;: &quot;integer&quot;</code>. This type mismatch needs to be fixed, otherwise Passim will treat <code>int</code> values as if they were <code>long</code>, thus potentially leading to issues or inconsistencies in the generated output.</p>
<p>We can now save the schema for later into a new file (<code>passim.schema</code>) for later use. This schema is needed when processing the input data provided for <a href="#case-study-2:-text-reuse-in-a-large-corpus-of-historical-newspapers">the second case study</a> presented in this lesson.</p>
<h1 id="running-passim">Running Passim</h1>
<p>In this section we illustrate the usage of Passim with two separate case studies: 1) detecting Bible quotes in seventeent century texts and 2) detecting text reuse in a large corpus of historical newspapers. The first case study highlights some of the basics of using Passim,  while the second case study contains many details and best practices that would be helpful for a large-scale text reuse project.</p>
<p>In the following table, we build on the original Passim documentation and explain some of the more useful parameters that this library offers. The case studies do not require you to master these parameters, so feel free to skip directly to the <a href="#downloading-the-data">Downloading the Data</a> section and come back to this section once you are comfortable enough to use Passim on your own data.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Default value</th>
<th>Description</th>
<th>Explanation</th>
</tr>
</thead>
<tbody><tr>
<td><code>--n</code></td>
<td>5</td>
<td>N-gram order for text-reuse detection</td>
<td>N-grams are chains of words of length N. This setting allows you to decide what type of n-gram (unigram, bigram, trigram...) Passim should use when creating a list of possible text reuse candidates.<br /><br />Setting this parameter to a lower value can help in the case of very noisy texts (i.e. when many words in a text are affected by one or more OCR errors). In fact, the longer the n-gram, the more likely it is to contain OCR mistakes.</td>
</tr>
<tr>
<td><code>--minDF</code> (<code>-l</code>)</td>
<td>2</td>
<td>Lower limit on document frequency of n-grams used</td>
<td>Since n-grams are used in Passim to retrieve document candidate pairs, an n-gram occurring only once is not useful as it will retrieve only one document (and not a pair). For this reason <code>--minDF</code> defaults to <code>2</code>.</td>
</tr>
<tr>
<td><code>--maxDF</code> (<code>-u</code>)</td>
<td>100</td>
<td>Upper limit on document frequency of n-grams used.</td>
<td>This parameter will filter out n-grams that are too common, thus occurring many times in a given document. <br /><br />This value has an impact on the performances as it will reduce the number of document pairs retrieved by Passim that will need to be compared.</td>
</tr>
<tr>
<td><code>--min-match</code> (<code>-m</code>)</td>
<td>5</td>
<td>Minimum number of matching n-grams between two documents</td>
<td>This parameter allows you to decide how many n-grams must be found between two documents.</td>
</tr>
<tr>
<td><code>--relative-overlap</code> (<code>-o</code>)</td>
<td>0.8</td>
<td>Proportion that two different aligned passages from the same document must overlap to be clustered together, as measured on the longer passage <!-- TODO SH: Current mismatch between official doc and code, see what is going to be changed after David answers to this issue https://github.com/dasmiq/Passim/issues/10 --></td>
<td>This parameter determines the degree of string similarity two passages need to have in order to be clustered together.<br /><br />In the case of very noisy texts, it may be desirable to set this parameter to a  smaller value.</td>
</tr>
<tr>
<td><code>--max-repeat</code> (<code>-r</code>)</td>
<td>10</td>
<td>Maximum repeat of one series in a cluster</td>
<td>This paramter allows you to specify how much a given series can be present in a cluster.</td>
</tr>
</tbody></table>
<h2 id="downloading-the-data">Downloading the data</h2>
<p>Sample data needed to run the command examples in the two case studies can be downloaded from the <a href="https://github.com/impresso/PH-Passim-tutorial">dedicated GitHub repository</a>. Before continuing with the case studies, download a local copy of the data by cloning the repository.</p>
<pre><code class="language-bash">&gt;&gt;&gt; git clone https://github.com/impresso/PH-Passim-tutorial.git
</code></pre>
<p>Alternatively, it is possible to download the data for this lesson from Zenodo at the address <a href="https://zenodo.org/badge/latestdoi/250229057">https://zenodo.org/badge/latestdoi/250229057</a>.</p>
<h2 id="case-study-1-bible-quotes-in-seventeenth-century-texts">Case study 1: Bible Quotes in Seventeenth Century Texts</h2>
<p>In this first case study, we will look at text reuse using texts taken from <a href="https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/">EEBO-TCP</a> Phase I, the publicly available keyed-in version of Early English Books Online provided by the Text Creation Partnership. This case study is a special case of text reuse, as we are not focusing at inter-authors text reuse, but rather at the influence a single book — in this case, the Bible in its published-in-1611 King James version — had on several authors. Can we detect what documents contain extracts from the Bible?</p>
<p>As this is a small-scale example of what an actual research question making use of text reuse methods could look like, we will only use some of the 25,368 works available in EEBO-TCP, taken randomly. This smaller selection size should also allow anyone reading this tutorial to run this example on their personal laptop. Ideally, we recommend using a corpus such as <a href="https://www.uantwerpen.be/en/projects/mind-bending-grammars/emma-corpus/">Early Modern Multiloquent Authors (EMMA)</a>, compiled by the University of Antwerp&#39;s <a href="https://www.uantwerpen.be/en/projects/mind-bending-grammars/">Mind Bending Grammars</a> project, should someone want to properly study the use of Bible quotes in seventeenth century texts. This corpus has the advantage of providing hand-curated metadata in an easily parseable format, allowing any researcher to focus on specific authors, periods, etc.</p>
<h3 id="extracting-the-data">Extracting the Data</h3>
<p>At the root of the newly-created directory is a JSON file: <code>passim_in.json</code>. This file contains all our data, in the format described above: one document per line (<code>text</code>), structured with the bare minimum of required metadata (<code>id</code>, <code>series</code>). As this is a small file, we encourage you to open the file using a text editor such as Notepad++ on Windows or Sublime on Linux/macOS to familiarise yourself with how the data is formatted. Because our case study focuses on the detection of Bible passages in several documents and not on text reuse within all documents, we have formatted the data so that the <code>series</code> field contains <code>bible</code> for the Bible (last line of our JSON file), and <code>not_bible</code> for all other documents. Passim does not analyse documents that belong to the same series, so this effectively tells the software to only compare all documents with the Bible — not with each other.</p>
<p>The <a href="https://github.com/impresso/PH-Passim-tutorial/">accompanying Github repository</a> contains a <a href="https://github.com/impresso/PH-Passim-tutorial/blob/master/eebo/code/main.py">Python script</a> to transform EEBO-TCP into the JSON format required by Passim and used in this lesson. We encourage the readers to reuse it and adapt it to their needs.</p>
<h3 id="running-passim-1">Running Passim</h3>
<p>Create a directory where you want to store the output of Passim (we use <code>Passim_output_bible</code> but any name will work). If you decide to use the default <code>Passim_output_bible</code> directory, ensure you remove all of its content (i.e. pre-computed Passim output) either manually or by running <code>rm -r ./eebo/Passim_output_bible/*</code>.</p>
<p>As we will see in more detail in the second use case, Passim, through Spark, allows for many options. By default Java does not allocate much memory to its processes, and running Passim even on very little datasets will cause Passim to crash because of an <code>OutOfMemory</code> error — even if you have a machine with a lot of RAM. To avoid this,  when calling Passim we add some additional parameters that will tell Spark to use more RAM for its processes.</p>
<p>You are now ready to go forward with your first text reuse project. </p>
<ol>
<li><p>Move to the sub-directory <code>eebo</code> by executing the command <code>cd eebo/</code>, starting from the directory where, earlier on, you cloned the repository <a href="https://github.com/impresso/PH-Passim-tutorial/"><code>PH-Passim-tutorial</code></a>.</p>
</li>
<li><p>Run the following command and go have a cup of your favorite hot beverage:</p>
<pre><code class="language-bash">&gt;&gt;&gt; SPARK_SUBMIT_ARGS=&#39;--master local[12] --driver-memory 8G --executor-memory 4G&#39; passim passim_in.json passim_output_bible/
</code></pre>
</li>
</ol>
<p>For now, do not worry about the additional arguments <code>SPARK_SUBMIT_ARGS=&#39;--master local[12] --driver-memory 8G --executor-memory 4G&#39;</code>; in the section <a href="#case-study-2:-text-reuse-in-a-large-corpus-of-historical-newspapers">&quot;Case Study 2&quot;</a> we will explain them in detail.</p>
<p>This test case takes approximatively eight minutes on a recent laptop with eight threads. You can also follow the progress of the detection at <a href="http://localhost:4040">http://localhost:4040</a> — an interactive dashboard created by Spark (Note: the dashboard will shut down as soon as Passim has finished running).</p>
<h2 id="case-study-2-text-reuse-in-a-large-corpus-of-historical-newspapers">Case study 2: Text Reuse in a large corpus of historical newspapers</h2>
<p>The second case study is drawn from <a href="https://impresso-project.ch/">impresso</a>, a recent research project aimed at enabling critical text mining of newspaper archives with the implementation of a technological framework to extract, process, link, and explore data from print media archives.</p>
<p>In this project, we use Passim to detect text reuse at scale. The extracted text reuse clusters are then integrated into the <a href="https://impresso-project.ch/app">impresso tool</a> in two ways. First, in the main article reading view users can readily see which portions of an article were reused by other articles in the corpus. Second, users can browse through all clusters in a dedicated page (currently more than 6 million), perform full-text searches on their contents, and filter the results according to a number of criteria (cluster size, time span covered, lexical overlap, etc.).</p>
<p>More generally, detecting text reuse in a large-scale newspaper corpus can be useful in many of the following ways:</p>
<ul>
<li>Identify (and possibly filter out) duplicated documents before performing further processing steps (e.g. topic modelling)</li>
<li>Study the virality and spread of news</li>
<li>Study information flows, both within and across national borders</li>
<li>to allow users discover which contents, within in their own collections, generated text reuse (e.g. famous political speeches, portions of national constitutions, etc.)</li>
</ul>
<p>For this case study we consider a tiny fraction of the <em>impresso</em> corpus, consisting of one year&#39;s worth of newspaper data (i.e. 1900) for a sample of four newspapers. The corpus contains 76 newspapers from Switzerland and Luxembourg, covering a time span of 200 years. The sample data necessary to run step by step this case study are contained in the folder <a href="https://github.com/impresso/PH-Passim-tutorial/tree/master/impresso"><code>impresso/</code></a>.</p>
<h3 id="data-preparation">Data preparation</h3>
<p>The format used in impresso to store newspapers data is slightly different from Passim&#39;s input format so we need a script to take care of transforming the former into the latter. While discussing how this script works goes well beyond the scope of this lesson, you can find the conversion script on the <a href="https://github.com/impresso/impresso-pycommons/blob/master/impresso_commons/text/rebuilder.py">impresso GitHub repository</a> should you be interested. The output of this script is one JSON line file per newspaper per year, compressed into a <code>.bz2</code> archive for the sake of efficient storage. Examples of this format can be found in the directory <code>impresso/data</code> and shown in the following example:</p>
<pre><code>&gt;&gt;&gt; ls -la impresso/data/
EXP-1900.jsonl.bz2
GDL-1900.jsonl.bz2
IMP-1900.jsonl.bz2
JDG-1900.jsonl.bz2
</code></pre>
<p>Each newspaper archive is named after the newspaper identifier: for example, <code>GDL</code> stands for <em>Gazette de Lausanne</em>. In total, these four <code>.bz2</code> files contain 92,000 articles through Passim, corresponding to all articles published in 1900 in the four sampled newspapers.</p>
<p>Sometimes it&#39;s not easy to inspect data packaged in this way. But some Bash commands like <code>bzcat</code> and <code>jq</code> can help us. For example, with the following chain of commands we can find out how many documents (newspaper articles) are contained in each of the input files by counting their IDs:</p>
<pre><code>&gt;&gt;&gt; bzcat impresso/data/GDL-1900.jsonl.bz2 | jq --slurp &#39;[.[] |del(.pages)| .id]|length&#39;
28380
</code></pre>
<p>And similarly, in all input files:</p>
<pre><code>&gt;&gt;&gt; bzcat impresso/data/*-1900.jsonl.bz2 | jq --slurp &#39;[.[] |del(.pages)| .id]|length&#39;
92514
</code></pre>
<p>What these commands do is to read the content of the <code>.bz2</code> file by means of <code>bzcat</code> and then <em>pipe</em> this content into <code>jq</code> which</p>
<ul>
<li>iterates through all docouments in the JSON line file</li>
<li>for each document it removes the <code>pages</code> field as it&#39;s not needed and selects only the <code>id</code> field</li>
<li>finally, with <code>length</code> <code>jq</code> computes the size of the list of IDs created by the previous expression</li>
</ul>
<h3 id="running-passim-2">Running Passim</h3>
<p>To run the impresso data through Passim, execute the following command in a <code>Terminal</code> window:</p>
<pre><code>SPARK_SUBMIT_ARGS=&#39;--master local[12] --driver-memory 10G --executor-memory 10G --conf spark.local.dir=/scratch/matteo/spark-tmp/&#39; Passim --schema-path=&quot;impresso/schema/Passim.schema&quot; &quot;impresso/data/*.jsonl.bz2&quot; &quot;impresso/Passim-output/&quot;
</code></pre>
<p>This command is made up of the following parameters:</p>
<ul>
<li><strong><code>SPARK_SUBMIT_ARGS</code></strong> passes some configuration parameters to Spark, the library that takes care of parallel execution of processes.<ul>
<li><code>--master local[10]</code>: <code>local</code> means we are running Spark in single machine-mode; <code>[10]</code> specifies the number of workers (or threads, in this specific case) over which processes should be distributed (<code>local [*]</code> will make use of the maximum number of threads);  </li>
<li><code>--executor-memory 4G</code>: The equivalent of the maximum heap size when running a regular JAVA application. It&#39;s the amount of memory that Spark allocates to each executor.</li>
<li><code>--conf spark.local.dir=/scratch/matteo/spark-tmp/</code>: A directory where Spark stores temporary data. When working with large datasets, it is important to specify a location with sufficient free disk space.</li>
</ul>
</li>
<li><strong><code>--schema-path</code></strong>: Specifies the path to the JSON schema describing the input data to be ran through Passim (see section <a href="#custom-json-format">&quot;Custom JSON format&quot;</a> for more information about how to generate such schema).</li>
<li><strong><code>impresso/data/*.jsonl.bz2</code></strong>: Specifies the input files (i.e. all files contained in <code>impresso/data/</code> with <code>.jsonl.bz2</code> in the file name);</li>
<li><strong><code>impresso/Passim-output/</code></strong>: Specifies where Passim should write its output</li>
</ul>
<p>If you want to limit the processing to a couple of input files — for example to limit memory usage — you can specify the input using the following command:</p>
<pre><code>impresso/data/{EXP-1900.jsonl.bz2,GDL-1900.jsonl.bz2}.jsonl.bz2
</code></pre>
<p>You can monitor Passim&#39;s progress while running by pointing your browser to the address <code>localhost:4040</code> where the Spark dashboard can be accessed (Figure 2).</p>
<p>{% include figure.html filename=&quot;spark-dashboard.png&quot; caption=&quot;Figure 2. Screenshot of the Spark dashboard while running Passim.&quot; %}</p>
<p>Running Passim with eight workers (and 4 Gb of executor memory) takes about five minutes to process the 92,514 articles published in 1900 in the newspapers GDL, JDG, EXP, IMP (but your mileage may vary).</p>
<p>If you provide as input a folder with <code>*.bz2</code> files, ensure these files are not found within subdirectories or Passim will not be able to find them automatically.</p>
<p>It is important that the output folder where Passim will write its output is empty. Especially when running the first experiments and getting familiar with the software it can very easily happen to specify a non-empty output folder. Specifying a non-empty output folder usually leads to an error as Passim processes the folder content and does not simply overwrite it.</p>
<h3 id="inspecting-passims-output">Inspecting Passim&#39;s Output</h3>
<p>Once Passim has finished running, the output folder <code>impresso/Passim-output/</code> will contain a sub-folder <code>out.json/</code> with the extracted text reuse clusters. If you specified <code>--output=parquet</code> instead of <code>--output=json</code>, this sub-folder will be named <code>out.parquet</code>.</p>
<p>In the JSON output each document corresponds to a text reuse passage. Since passages are aggregated into clusters, each passage contains a field <code>cluster</code> with the ID of the cluster to which it belongs.</p>
<p>To obtain the total number of cluster, we can count the number of unique cluster IDs with the following one-line command:</p>
<pre><code class="language-bash">&gt;&gt;&gt; cat impresso/Passim-output/out.json/*.json | jq --slurp &#39;[.[] | .cluster] | unique | length&#39;

2721
</code></pre>
<p>Similarly, we can print the 100th cluster ID:</p>
<pre><code class="language-bash">&gt;&gt;&gt; cat impresso/Passim-output/out.json/*.json | jq --slurp &#39;[.[] | .cluster] | unique | .[100]&#39;

77309411592
</code></pre>
<p>And with a simple <code>jq</code> query we can print all passages belonging to this text reuse cluster:</p>
<pre><code>&gt;&gt;&gt; cat impresso/Passim-output/out.json/*.json | jq --slurp &#39;.[] | select(.cluster==77309411592)|del(.pages)&#39;
</code></pre>
<pre><code class="language-json">{
  &quot;uid&quot;: -6695317871595380000,
  &quot;cluster&quot;: 77309411592,
  &quot;size&quot;: 2,
  &quot;bw&quot;: 8,
  &quot;ew&quot;: 96,
  &quot;cc&quot;: true,
  &quot;date&quot;: &quot;1900-07-30&quot;,
  &quot;id&quot;: &quot;EXP-1900-07-30-a-i0017&quot;,
  &quot;series&quot;: &quot;EXP&quot;,
  &quot;text&quot;: &quot;nouvel accident de\nmontagne : Le fils dû guide Wyss, de\nWilderswil, âgé de 17 ans, accompagnait\nvendredi un touriste italien dans l&#39;as-\ncension du Petersgrat En descendant sur\nle glacier de Tschingel, le jeune guide\ntomba dans une crevasse profonde de\n25 mètres. La corde était trop courte\npour l&#39;en retirer, et des guides appelés\nà son secours ne parvinrent pas non\nplus à le dégager. Le jeune homme crie\nqu&#39;il n&#39;est pas blessé. Une nouvelle co-\nlonne de secours est partie samedi de\nLauterbrunnen.\nAarau, 28 juillet.\n&quot;,
  &quot;title&quot;: &quot;DERNIÈRES NOUVELLES&quot;,
  &quot;gid&quot;: -8329671890893709000,
  &quot;begin&quot;: 53,
  &quot;end&quot;: 572
}
{
  &quot;uid&quot;: -280074845860282140,
  &quot;cluster&quot;: 77309411592,
  &quot;size&quot;: 2,
  &quot;bw&quot;: 2,
  &quot;ew&quot;: 93,
  &quot;cc&quot;: true,
  &quot;date&quot;: &quot;1900-07-30&quot;,
  &quot;id&quot;: &quot;GDL-1900-07-30-a-i0016&quot;,
  &quot;series&quot;: &quot;GDL&quot;,
  &quot;text&quot;: &quot;NOUVEAUX ACCIOENTS\nInterlaken. 29 juillet.\nLe fils du guide Wyss, de Wilderswil, âgé\nde dix-sept ans, accompagnait, vendredi, un\ntouriste italien dans l&#39;ascension du Peters-\ngrat.\nEn descendant sur le glacier de Tschingel,\nU jeune guide tomba dans une crevasse pro-\nfonde de vingt-cinq mètres. La corde était trop\ncourte pour l&#39;en retirer, et des guides appelés\nà son secours ne parvinrent pas non plus à le\ndégager. Le jeune homme crie qu&#39;il n&#39;est pas\nblessé. Une nouvelle colonne de secours est\npartie samedi de Lauterbrunnen.\nChamonix, 28 juillet.\n&quot;,
  &quot;title&quot;: &quot;(Chronique alpestre&quot;,
  &quot;gid&quot;: 2328324961100034600,
  &quot;begin&quot;: 20,
  &quot;end&quot;: 571
}
</code></pre>
<p>As you can see from the output above, this cluster contains the same piece of news — a mountain accident which happened in Interlaken on 30 July 1900 — reported by two different newspapers on the very same day with slightly different words.</p>
<h1 id="using-passims-output">Using Passim&#39;s Output</h1>
<p>Since the usage of text reuse data ultimately depends on the research questions at hand — and there many possible applications of text reuse, as we have seen above — covering how to use Passim&#39;s output falls beyond the scope of this lesson.</p>
<p>Code that &#39;does something&#39; with the data output by Passim can be written in many different programming languages. Extracted clusters can be used to deduplicate documents in a corpus, or even collate together multiple witnesses of the same text, but this will entirely depend on the research context and specific use case.</p>
<p>To given an example of where to go next, for those who want to manipulate and further analyse text reuse data in Python, we provide a Jupyter notebook (<a href="https://github.com/impresso/PH-Passim-tutorial/blob/master/explore-Passim-output.ipynb"><code>explore-Passim-output.ipynb</code></a>) that shows how to import Passim&#39;s JSON output into a <code>pandas.DataFrame</code> and how to analyse the distribution of text reuse clusters in both uses cases presented above. For readers that are not familair with the Python library <code>pandas</code>, the <em>Programming Historian</em> lesson written by Charlie Harper on <a href="/en/lessons/visualizing-with-bokeh"><em>Visualizing Data with Bokeh and Pandas</em></a> is a nice (and required) introductory reading.</p>
<p>The code contained and explained in the notebook will produce the two plots of Figures 3 and 4, showing how the sizes of text reuse clusters are distributed in the impresso and Bible data respectively.</p>
<p>{% include figure.html filename=&quot;plot-impresso.png&quot; caption=&quot;Figure 3. Distribution of text reuse cluster sizes in the impresso sample data.&quot; %}</p>
<p>{% include figure.html filename=&quot;plot-bible.png&quot; caption=&quot;Figure 4. Distribution of text reuse cluster sizes in the Bible sample data.&quot; %}</p>
<p>As you can see from the plots, in both cases the majority of text reuse clusters contains at most two passages. In the impresso sample data, however, there is much more variance in the size of clusters, with 10% of them having a size comprised between 6 and 296 passages, as opposed to the Bible data where the maximum cluster size is 3.</p>
<h1 id="further-readings">Further readings</h1>
<p><strong>Passim</strong></p>
<ul>
<li>Smith et al. (2015) introduce in detail the text reuse detection algorithm implemented in Passim</li>
<li>Cordell (2015) applied Passim to study text reuse within a large corpus of American newspapers</li>
</ul>
<p><strong>textreuse</strong></p>
<ul>
<li>Vogler et al. (2020) apply the <code>textreuse</code> R package (Mullen 2016) to study the phenomenon of <em>media concentration</em> in contemporary journalism</li>
</ul>
<p><strong>TRACER</strong></p>
<ul>
<li>Büchler et al. (2014) explain the algorithms for text reuse detection that are implemented in TRACER;</li>
<li>Franzini et al. (2018) use and evaluate TRACER for the extraction of quotations from a Latin text (the <em>Summa contra Gentiles</em> of Thomas Aquinas)</li>
</ul>
<p><strong>BLAST</strong></p>
<ul>
<li>Vierthaler et al. (2019) use the BLAST alignment algorithm to detect reuse in Chinese texts</li>
<li>Vesanto et al. (2017) and Salmi et al. (2019) apply BLAST to a comprehensive corpus of newspapers published in Finland</li>
</ul>
<h1 id="acknowledgements">Acknowledgements</h1>
<p>A sincere thanks goes to Marco Büchler and Ryan Muther for reviewing this lesson, as well as to our colleagues Marten Düring and David Smith for their constructive feedback on an early version of this tutorial. Additional thanks go to Anna-Maria Sichani for serving as editor.</p>
<p>The authors warmly thank the newspaper <a href="https://letemps.ch/">Le Temps</a> — owner of <em>La Gazette de Lausanne</em> (GDL) and the <em>Journal de Genève</em> (JDG) — and the group <a href="https://www.arcinfo.ch/">ArcInfo</a> — owner of <em>L’Impartial</em> (IMP) and <em>L’Express</em> (EXP) —  for accepting to share their data for academic purposes.</p>
<p>MR gratefully acknowledges the financial support of the Swiss National Science Foundation (SNSF) for the project <a href="https://impresso-project.ch/"><em>impresso – Media Monitoring of the Past</em></a> under grant number CR-SII5_173719. SH&#39;s work was supported by the European Union’s Horizon 2020 research and innovation programme under grant 770299 (<a href="https://www.newseye.eu/">NewsEye</a>). SH was affiliated with the University of Helsinki and the University of Geneva for most of this work, and is currently funded by the project <em>Towards Computational Lexical Semantic Change Detection</em> supported by the Swedish Research Council (20192022; dnr 2018-01184).</p>
<h1 id="bibliography">Bibliography</h1>
<ol>
<li>Greta Franzini, Maria Moritz, Marco Büchler, Marco Passarotti. Using and evaluating TRACER for an Index fontium computatus of the Summa contra Gentiles of Thomas Aquinas. In <em>Proceedings of the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018)</em>. (2018). <a href="http://ceur-ws.org/Vol-2253/paper22.pdf">Link</a></li>
<li>David A. Smith, Ryan Cordell, Abby Mullen. Computational Methods for Uncovering Reprinted Texts in Antebellum Newspapers. <em>American Literary History</em> <strong>27</strong>, E1–E15 Oxford University Press, 2015. <a href="http://dx.doi.org/10.1093/alh/ajv029">Link</a></li>
<li>Ryan Cordell. Reprinting Circulation, and the Network Author in Antebellum Newspapers. <em>American Literary History</em> <strong>27</strong>, 417–445 Oxford University Press (OUP), 2015. <a href="http://dx.doi.org/10.1093/alh/ajv028">Link</a></li>
<li>Daniel Vogler, Linards Udris, Mark Eisenegger. Measuring Media Content Concentration at a Large Scale Using Automated Text Comparisons. <em>Journalism Studies</em> <strong>0</strong>, 1–20 Taylor &amp; Francis, 2020. <a href="http://dx.doi.org/10.1080/1461670x.2020.1761865">Link</a></li>
<li>Lincoln Mullen. textreuse: Detect Text Reuse and Document Similarity. (2016). <a href="https://github.com/ropensci/textreuse">Link</a></li>
<li>Marco Büchler, Philip R. Burns, Martin Müller, Emily Franzini, Greta Franzini. Towards a Historical Text Re-use Detection. 221–238 In <em>Text Mining: From Ontology Learning to Automated Text Processing Applications</em>. Springer International Publishing, 2014. <a href="http://dx.doi.org/10.1007/978-3-319-12655-5_11">Link</a></li>
<li>Paul Vierthaler, Meet Gelein. A BLAST-based, Language-agnostic Text Reuse Algorithm with a MARKUS Implementation and Sequence Alignment Optimized for Large Chinese Corpora. <em>Journal of Cultural Analytics</em> (2019). <a href="http://dx.doi.org/10.22148/16.034">Link</a></li>
<li>Aleksi Vesanto, Asko Nivala, Heli Rantala, Tapio Salakoski, Hannu Salmi, Filip Ginter. Applying BLAST to Text Reuse Detection in Finnish Newspapers and Journals, 1771-1910. 54–58 In <em>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</em>. Linköping University Electronic Press, 2017. <a href="https://www.aclweb.org/anthology/W17-0510">Link</a></li>
<li>Hannu Salmi, Heli Rantala, Aleksi Vesanto, Filip Ginter. The long-term reuse of text in the Finnish press, 1771–1920. <strong>2364</strong>, 394–544 In <em>CEUR Workshop Proceedings</em>. (2019).</li>
<li>Axel J Soto, Abidalrahman Mohammad, Andrew Albert, Aminul Islam, Evangelos Milios, Michael Doyle, Rosane Minghim, Maria Cristina de Oliveira. Similarity-Based Support for Text Reuse in Technical Writing. 97–106 In <em>Proceedings of the 2015 ACM Symposium on Document Engineering</em>. ACM, 2015. <a href="http://dx.doi.org/10.1145/2682571.2797068">Link</a></li>
<li>Alexandra Schofield, Laure Thompson, David Mimno. Quantifying the Effects of Text Duplication on Semantic Models. 2737–2747 In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics, 2017. <a href="http://dx.doi.org/10.18653/v1/D17-1290">Link</a></li>
<li>Matteo Romanello, Aurélien Berra, Alexandra Trachsel. Rethinking Text Reuse as Digital Classicists. <em>Digital Humanities conference</em>, 2014. <a href="https://wiki.digitalclassicist.org/Text_Reuse">Link</a></li>
</ol>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="detecting-text-reuse-with-passim/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Detecting Text Reuse with Passim\",\"collection\":\"lessons\",\"layout\":\"lesson\",\"slug\":\"detecting-text-reuse-with-Passim\",\"date\":\"2021-05-16T00:00:00.000Z\",\"authors\":[\"Matteo Romanello\",\"Simon Hengchen\"],\"editors\":\"Anna-Maria Sichani\",\"reviewers\":[\"Ryan Muther\",\"Marco Büchler\"],\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F305\",\"difficulty\":3,\"activity\":\"transforming\",\"topics\":[\"data-manipulation\"],\"abstract\":\"In this lesson you will learn about text reuse detection -- the automatic identification of reused passages in texts -- and why you might want to use it in your research. Through a detailed installation guide and two case studies, this lesson will teach you the ropes of Passim, an open source and scalable tool for text reuse detection.\",\"avatar_alt\":\"Stack of newspapers surrounded by quills and telegraph wires\",\"doi\":\"10.46430\u002Fphen0092\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this lesson you will be introduced to the automatic detection of text reuse with the Passim library. You will learn how to install and run Passim and its dependencies, how to prepare your texts as input files suitable for use with Passim and, finally, how to process the output generated by Passim to carry out basic analyses.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis lesson targets digital humanities (DH) practitioners without any prior knowledge of text reuse, but with a working knowledge of \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBash_(Unix_shell)\\\"\u003Ebash scripting\u003C\u002Fa\u003E and Python as well as some data manipulation. For tutorials on bash scripting and \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPython_(programming_language)\\\"\u003EPython\u003C\u002Fa\u003E, you can refer to the Programming Historian \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fintro-to-bash\\\"\u003E“Introduction to the Bash Command Line\u003C\u002Fa\u003E tutorial and the \u003Ca href=\\\"\u002Fen\u002Flessons\u002F?topic=python\\\"\u003Elibrary of current Python lessons\u003C\u002Fa\u003E on the \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E website.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis lesson includes an overview of \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fdasmiq\u002FPassim\\\"\u003EPassim\u003C\u002Fa\u003E, an open source tool for automatic text reuse detection. While the tool has been used in a number of small and large DH projects, it lacks a user-friendly documentation with examples and set up instructions, a gap that we aim to fill with this \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E lesson.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"introduction-to-text-reuse\\\"\u003EIntroduction to Text Reuse\u003C\u002Fh1\u003E\\n\u003Cp\u003EText reuse can be defined as &quot;the meaningful reiteration of text, usually beyond the simple repetition of common language&quot; (Romanello et al. 2014). It is such a broad concept that it can be understood at different levels and studied in a large variety of contexts. In a publishing or teaching context, for example, instances of text reuse can constitute plagiarism should portions of someone else’s text be repeated without appropriate attribution. In the context of literary studies, text reuse is often just a synonym for literary phenomena like allusions, paraphrases and direct quotations.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe following list includes just some of the libraries available that perform automatic text reuse detection:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EThe \u003Ca href=\\\"https:\u002F\u002Fdocs.ropensci.org\u002Ftextreuse\u002F\\\"\u003ER textreuse package\u003C\u002Fa\u003E (R) written by Lincoln Mullen\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.etrap.eu\u002Fresearch\u002Ftracer\u002F\\\"\u003ETRACER\u003C\u002Fa\u003E (Java) developed by Marco Büchler and colleagues\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fblast.ncbi.nlm.nih.gov\u002FBlast.cgi\\\"\u003EBasic Local Alignment Search Tool (BLAST)\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Ftesserae\u002Ftesserae\\\"\u003ETesserae\u003C\u002Fa\u003E (PHP, Perl)\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FARTFL-Project\u002Ftext-pair\\\"\u003ETextPAIR (Pairwise Alignment for Intertextual Relations)\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fdasmiq\u002FPassim\\\"\u003EPassim\u003C\u002Fa\u003E (Scala) developed by \u003Ca href=\\\"http:\u002F\u002Fwww.ccs.neu.edu\u002Fhome\u002Fdasmith\u002F\\\"\u003EDavid Smith\u003C\u002Fa\u003E (Northeastern University)\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EFor this tutorial we chose the Passim library for three main reasons. Firstly, it can be adapted to a variety of use cases as it works well on a small text collection as well as on a large-scale corpus. Secondly, while the documentation for Passim is extensive, because of its relatively advanced user audience, a more user-centered step-by-step tutorial about detecting text reuse with Passim would be beneficial to the user community. Lastly, the following examples illustrate the variety of scenarios in which text reuse is a useful methodology:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ETo determine whether a digital library contains multiple editions of the same work(s)\u003C\u002Fli\u003E\\n\u003Cli\u003ETo find quotations in a text, provided that the target works are known (e.g. find quotations of the Bible within 17c English literature)  \u003C\u002Fli\u003E\\n\u003Cli\u003ETo study the virality and spread of texts (e.g. \u003Ca href=\\\"https:\u002F\u002Fviraltexts.org\u002F\\\"\u003EViral Texts\u003C\u002Fa\u003E by Cordell and Smith for historical newspapers)\u003C\u002Fli\u003E\\n\u003Cli\u003ETo identify (and possibly filter out) duplicate documents within a text collection before performing further processing steps (e.g. topic modelling as illustrated by Schofield et al. (2017))\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EFor these reasons, Passim is usually a great choice. It will help you automate the search for repeated text passages in a corpus — whether these are running ads in newspapers, multiple copies of the same poem, or direct (and slightly indirect) quotations in someone else&#39;s book.\\nText reuse detection as implemented in Passim aims at identifying these copies and repetitions automatically, and yields clusters of passages that were deemed to be related with one another. Ultimately, what a cluster contains can vary a lot and will depend on your research question. For example, Passim can group together copies of the same article that differ only with respect to optical character recognition (OCR) errors, but it can also help to retrieve texts that share the same journalistic template, such as horoscopes or advertisements.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"prerequisites\\\"\u003EPrerequisites\u003C\u002Fh1\u003E\\n\u003Cp\u003EThis tutorial requires the following:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EA basic understanding of Bash scripts. For readers needing a review on Bash scripts, read the \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E lesson \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fintro-to-bash\\\"\u003E&quot;Introduction to the Bash Command Line&quot;\u003C\u002Fa\u003E.\u003C\u002Fli\u003E\\n\u003Cli\u003EKnowledge of JSON. To learn more about JSON, read the \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E lesson \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fjson-and-jq\\\"\u003E&quot;Reshaping JSON with jq&quot;\u003C\u002Fa\u003E.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EMoreover, while a basic understanding of Python — and a working Python installation — are not strictly needed to work with Passim, they are required to run some parts of this tutorial (e.g. the Jupyter notebook with data exploration, or the Early English Books Online (EEBO) data preparation script). If you are not familiar with Python, please read the \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E lesson \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fintroduction-and-installation\\\"\u003E&quot;Python Introduction and Installation&quot;\u003C\u002Fa\u003E.   \u003C\u002Fp\u003E\\n\u003Cp\u003ENote that installing Passim on Windows is more arduous than macOS or Linux. As a result, we recommend using macOS or Linux (or a virtual environment) for this lesson.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"installing-passim\\\"\u003EInstalling Passim\u003C\u002Fh1\u003E\\n\u003Cp\u003EInstalling Passim requires installing the following software:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.java.com\u002Ffr\u002Fdownload\u002F\\\"\u003EJava JDK (version 8)\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.scala-sbt.org\u002F\\\"\u003EScala Build Tool (SBT)\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fspark.apache.org\u002F\\\"\u003EApache Spark\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EBut why are all these dependencies needed?\u003C\u002Fp\u003E\\n\u003Cp\u003EPassim is written in a programming language called Scala. To execute a software written in Scala, its sources need to be compiled into an executable JAR file, which is performed by \u003Ccode\u003Esbt\u003C\u002Fcode\u003E, Scala&#39;s interactive build tool. Finally, since Passim is designed to work also on large-scale text collections (with several thousands or millions of documents), behind the scenes it uses Spark, a cluster-computing framework written in Java. Using Spark allows Passim to handle the distributed processing of certain parts of the code, which is useful when handling large amounts of data. The \u003Ca href=\\\"https:\u002F\u002Fspark.apache.org\u002Fdocs\u002Flatest\u002Fcluster-overview.html#glossary\\\"\u003ESpark glossary\u003C\u002Fa\u003E is a useful resource to learn basic Spark terminology (words like &quot;driver&quot;, &quot;executor&quot;, etc.) but learning this terminology may not be necessary if you are running Passim on a small dataset.\u003C\u002Fp\u003E\\n\u003Cp\u003EBefore installing this set of software, you&#39;ll need to download the Passim source code from GitHub:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; git clone https:\u002F\u002Fgithub.com\u002Fdasmiq\u002FPassim.git\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf you are not familiar with Git and GitHub, we recommend reading the \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E lesson \u003Ca href=\\\"https:\u002F\u002Fdoi.org\u002F10.46430\u002Fphen0051\\\"\u003E&quot;An Introduction to Version Control Using GitHub Desktop&quot;\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"macos-instructions\\\"\u003EmacOS instructions\u003C\u002Fh2\u003E\\n\u003Cp\u003EThese instructions are aimed at users of Apple&#39;s macOS and were tested under version 10.13.4 (a.k.a. High Sierra).\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"check-java-installation\\\"\u003ECheck Java Installation\u003C\u002Fh3\u003E\\n\u003Cp\u003EEnsure that you have Java Development Kit 8 by typing the following command in a new Terminal window:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; java -version\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf the output of this command looks similar to the following example, then Java 8 is installed on your machine.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Eopenjdk version &quot;1.8.0_262&quot;\\nOpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_262-b10)\\nOpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.262-b10, mixed mode)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch3 id=\\\"installing-java-8\\\"\u003EInstalling Java 8\u003C\u002Fh3\u003E\\n\u003Cp\u003EIn case another version of Java is installed on your machine, follow the following steps to install Java 8 alongside the existing Java version.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis is important so as not to break already installed software that needs more recent Java versions.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Cp\u003EInstall the \u003Ccode\u003Ebrew\u003C\u002Fcode\u003E package manager by following installation instructions on the \u003Ca href=\\\"https:\u002F\u002Fbrew.sh\u002F\\\"\u003EBrew.sh\u003C\u002Fa\u003E website. Once the installation is completed, run \u003Ccode\u003Ebrew --help\u003C\u002Fcode\u003E to verify it works.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EUse \u003Ccode\u003Ebrew\u003C\u002Fcode\u003E to install Java 8.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; brew cask install adoptopenjdk\u002Fopenjdk\u002Fadoptopenjdk8\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EVerify that Java 8 is installed.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; \u002Fusr\u002Flibexec\u002Fjava_home -V\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis command should output something similar to the following:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003EMatching Java Virtual Machines (2):\\n    13.0.2, x86_64:    &quot;Java SE 13.0.2&quot;    \u002FLibrary\u002FJava\u002FJavaVirtualMachines\u002Fjdk-13.0.2.jdk\u002FContents\u002FHome\\n    1.8.0_262, x86_64:    &quot;AdoptOpenJDK 8&quot;    \u002FLibrary\u002FJava\u002FJavaVirtualMachines\u002Fadoptopenjdk-8.jdk\u002FContents\u002FHome\\n\\n\u002FLibrary\u002FJava\u002FJavaVirtualMachines\u002Fjdk-13.0.2.jdk\u002FContents\u002FHome\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Col start=\\\"3\\\"\u003E\\n\u003Cli\u003EInstall \u003Ccode\u003Ejenv\u003C\u002Fcode\u003E, a tool that allows you to manage multiple Java versions installed on the same machine, and to easily switch between them.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; brew install jenv\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ETo be able to call \u003Ccode\u003Ejenv\u003C\u002Fcode\u003E without specifying the executable&#39;s full path don&#39;t forget to add \u003Ccode\u003Ejenv\u003C\u002Fcode\u003E to your \u003Ccode\u003E$PATH\u003C\u002Fcode\u003E environment variable by opening the file \u003Ccode\u003E~\u002F.bashrc\u003C\u002Fcode\u003E with your favorite text editor and adding the following lines at the end of the file:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# activate jenv\\nexport PATH=&quot;$HOME\u002F.jenv\u002Fbin:$PATH&quot;\\neval &quot;$(jenv init -)&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAfter adding these lines, you need to open another terminal window or run the following line so that the \u003Ccode\u003E$PATH\u003C\u002Fcode\u003E variable is updated with the change you just made (the command \u003Ccode\u003Esource\u003C\u002Fcode\u003E triggers the reload of your \u003Ccode\u003Ebash\u003C\u002Fcode\u003E configuration).\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; source ~\u002F.bashrc\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EOnce installed, add the existing Java versions to \u003Ccode\u003Ejenv\u003C\u002Fcode\u003E (i.e. those listed by the command \u003Ccode\u003E\u002Fusr\u002Flibexec\u002Fjava_home -V\u003C\u002Fcode\u003E):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# your mileage may vary, so make sure you replace this path\\n# with the actual path to the JAVA_HOME in your machine\\n&gt;&gt;&gt; jenv add \u002FLibrary\u002FJava\u002FJavaVirtualMachines\u002Fadoptopenjdk-8.jdk\u002FContents\u002FHome\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENow you can set the default version of Java for this project by running the following:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; jenv local 1.8\\n\\n# verify\\n&gt;&gt;&gt; java -version\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch3 id=\\\"compiling-passim-from-the-sources-macos\\\"\u003ECompiling Passim From the Sources (macOS)\u003C\u002Fh3\u003E\\n\u003Cp\u003EPassim is written in a programming language called Scala. Before being able to execute a software written in Scala, its sources need to be compiled. This task is performed by \u003Ccode\u003Esbt\u003C\u002Fcode\u003E, the Interactive Build Tool.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo determine whether \u003Ccode\u003Esbt\u003C\u002Fcode\u003E is installed on your machine, run the following command:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; sbt about\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf this command prints \u003Ccode\u003Ebash: sbt: command not found\u003C\u002Fcode\u003E it means \u003Ccode\u003Esbt\u003C\u002Fcode\u003E is not installed.\\nHowever, Passim comes with a useful script (\u003Ccode\u003Ebuild\u002Fsbt\u003C\u002Fcode\u003E) that will download and install SBT automatically before compiling the sources from Passim.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003ENB\u003C\u002Fstrong\u003E: Using an external (i.e. already installed) SBT may lead to issues, we recommend the following method for compiling Passim.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo compile the program, run the following command from the directory where you&#39;ve previously cloned Passim&#39;s GH repository:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; cd Passim\u002F\\n&gt;&gt;&gt; build\u002Fsbt package\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis command will take some time (around 3 minutes on a modern connection), but will let you know of the progress. As your computer starts downloading required files, a log will be printed on screen. At the end of this process, \u003Ccode\u003Esbt\u003C\u002Fcode\u003E will have created a \u003Ccode\u003E.jar\u003C\u002Fcode\u003E archive contaning the compiled sources for Passim. This file is found in the \u003Ccode\u003Etarget\u003C\u002Fcode\u003E directory: \u003Ccode\u003Etarget\u002Fscala-2.11\u002FPassim_2.11-0.2.0.jar\u003C\u002Fcode\u003E. Depending on the version of Scala and Passim, the actual path might be slightly different on your computer.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe \u003Ccode\u003Ebin\u003C\u002Fcode\u003E directory contains a Passim file: this is the executable that will launch Passim. In order for your computer the location of this file, and thus for it to recognise the Passim command, we need to add the path to the \u003Ccode\u003EPATH\u003C\u002Fcode\u003E environment variable.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# replace \u002Fhome\u002Fsimon\u002FPassim for the directory where you installed Passim\\n&gt;&gt;&gt; export PATH=&quot;\u002Fhome\u002Fsimon\u002FPassim\u002Fbin:$PATH&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ETo add the path permanently to the \u003Ccode\u003EPATH\u003C\u002Fcode\u003E environment variable, open the file \u003Ccode\u003E~\u002F.bashrc\u003C\u002Fcode\u003E with your favorite text editor and add the following line anywhere in the file (then execute \u003Ccode\u003Esource ~\u002F.bashrc\u003C\u002Fcode\u003E to apply this change):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# replace &quot;\u002Fhome\u002Fsimon\u002FPassim&quot; for the directory where you installed Passim\\nexport PATH=&quot;\u002Fhome\u002Fsimon\u002FPassim\u002Fbin:$PATH&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch3 id=\\\"installing-spark\\\"\u003EInstalling Spark\u003C\u002Fh3\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Cp\u003ENavigate to the \u003Ca href=\\\"http:\u002F\u002Fspark.apache.org\u002Fdownloads\\\"\u003Edownload section\u003C\u002Fa\u003E of the Spark website and select Spark release version &#39;3.x.x&#39; (where &#39;\u003Cem\u003Ex\u003C\u002Fem\u003E&#39; means any version that starts with &#39;3.&#39;), and package type &#39;Pre-built for Apache Hadoop 2.7&#39; from the dropdown menus.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EExtract the compressed binaries to a directory of your choice (e.g. \u003Ccode\u003E\u002FApplications\u003C\u002Fcode\u003E):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; cd \u002FApplications\u002F\\n&gt;&gt;&gt; tar -xvf ~\u002FDownloads\u002Fspark-3.1.x-bin-hadoop2.7.tgz\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EAdd the directory where you installed Spark to your \u003Ccode\u003EPATH\u003C\u002Fcode\u003E environment variable. To do so temporarily run the following command:\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; export PATH=&quot;\u002FApplications\u002Fspark-3.1.x-bin-hadoop2.7:$PATH&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ETo add the path installation directory permanently to your \u003Ccode\u003EPATH\u003C\u002Fcode\u003E environment variable, open the file \u003Ccode\u003E~\u002F.bashrc\u003C\u002Fcode\u003E with your favorite text editor and add the following line anywhere in the file:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Eexport PATH=&quot;\u002FApplications\u002Fspark-3.1.x-bin-hadoop2.7:$PATH&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAfter editing \u003Ccode\u003E~\u002F.bashrc\u003C\u002Fcode\u003E, open another terminal window or run the following command:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; source ~\u002F.bashrc\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"linux-instructions\\\"\u003ELinux instructions\u003C\u002Fh2\u003E\\n\u003Cp\u003EThese instructions are aimed at Debian-based distributions (Debian, Ubuntu, Linux Mint, etc.). If you run another type of distribution (Fedora, Gentoo, etc.), replace the distribution-specific commands (eg \u003Ccode\u003Eapt\u003C\u002Fcode\u003E) with those used by your specific distribution.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"check-java-installation-1\\\"\u003ECheck Java Installation\u003C\u002Fh3\u003E\\n\u003Cp\u003ETo ensure that you have the Java Development Kit 8 installed, run the following command:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; java -version\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf the command above returns \u003Ccode\u003E1.8.0_252\u003C\u002Fcode\u003E or similar, then you have Java Development Kit 8 installed (the \u003Ccode\u003E8\u003C\u002Fcode\u003E lets you know you have correct kit installed and selected by default). If your output looks different, choose one of the following commands accordingly:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# If you don&#39;t, install it\\n&gt;&gt;&gt;&gt; apt install openjdk-8-jdk\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# if your *default* JDK is not version 8\\n&gt;&gt;&gt; sudo update-alternatives --config java\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch3 id=\\\"compiling-passim-from-the-sources\\\"\u003ECompiling Passim from the Sources\u003C\u002Fh3\u003E\\n\u003Cp\u003ERefer to the \u003Ca href=\\\"#compiling-passim-from-the-sources-(macOS)\\\"\u003Ecompilation instructions for macOS\u003C\u002Fa\u003E, as they are the same for the Linux environment.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"installing-spark-1\\\"\u003EInstalling Spark\u003C\u002Fh3\u003E\\n\u003Col\u003E\\n\u003Cli\u003EDownload the Spark binaries by using \u003Ccode\u003Ewget\u003C\u002Fcode\u003E:\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; wget -P \u002Ftmp\u002F http:\u002F\u002Fapache.mirrors.spacedump.net\u002Fspark\u002Fspark-3.1.2\u002Fspark-3.1.2-bin-hadoop2.7.tgz\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EExtract the compressed binaries to a directory of your choice:\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; tar -xvf \u002Ftmp\u002Fspark-3.1.2-bin-hadoop2.7.tgz -C \u002Fusr\u002Flocal\u002F\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EAdd the directory where you installed Spark to your \u003Ccode\u003EPATH\u003C\u002Fcode\u003E environment variable. To add the directory to your \u003Ccode\u003EPATH\u003C\u002Fcode\u003E environment variable temporarily, run the following command:\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; export PATH=&quot;\u002Fusr\u002Flocal\u002Fspark-3.1.2-bin-hadoop2.7\u002Fbin:$PATH&quot;  # note that &quot;\u002Fusr\u002Flocal\u002F&quot; is the directory specified above, if you specified another directory change this accordingly\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\nTo add the directory to your \u003Ccode\u003EPATH\u003C\u002Fcode\u003E environment variable permanently, open the file \u003Ccode\u003E~\u002F.bashrc\u003C\u002Fcode\u003E with your favorite text editor and add the following line anywhere in the file:\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; export PATH=&quot;\u002Fusr\u002Flocal\u002Fspark-3.1.2-bin-hadoop2.7\u002Fbin:$PATH&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\nAfter editing \u003Ccode\u003E~\u002F.bashrc\u003C\u002Fcode\u003E, you need to open another terminal window or run the following line so that your \u003Ccode\u003EPATH\u003C\u002Fcode\u003E variable is updated with the change you just made.\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; source ~\u002F.bashrc\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch2 id=\\\"verify-the-installation\\\"\u003EVerify the Installation\u003C\u002Fh2\u003E\\n\u003Cp\u003EAt this point you have installed Passim and all required packages on your machine. If you type \u003Ccode\u003EPassim --help\u003C\u002Fcode\u003E in the command line, you should see output similar to the following:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003EIvy Default Cache set to: \u002FUsers\u002Fmatteo\u002F.ivy2\u002Fcache\\nThe jars for the packages stored in: \u002FUsers\u002Fmatteo\u002F.ivy2\u002Fjars\\n:: loading settings :: url = jar:file:\u002FApplications\u002Fspark-2.4.6-bin-hadoop2.7\u002Fjars\u002Fivy-2.4.0.jar!\u002Forg\u002Fapache\u002Fivy\u002Fcore\u002Fsettings\u002Fivysettings.xml\\ncom.github.scopt#scopt_2.11 added as a dependency\\ngraphframes#graphframes added as a dependency\\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-bb5bd11f-ba3c-448e-8f69-5693cc073428;1.0\\n    confs: [default]\\n    found com.github.scopt#scopt_2.11;3.5.0 in spark-list\\n    found graphframes#graphframes;0.7.0-spark2.4-s_2.11 in spark-list\\n    found org.slf4j#slf4j-api;1.7.16 in spark-list\\n:: resolution report :: resolve 246ms :: artifacts dl 4ms\\n    :: modules in use:\\n    com.github.scopt#scopt_2.11;3.5.0 from spark-list in [default]\\n    graphframes#graphframes;0.7.0-spark2.4-s_2.11 from spark-list in [default]\\n    org.slf4j#slf4j-api;1.7.16 from spark-list in [default]\\n    ---------------------------------------------------------------------\\n    |                  |            modules            ||   artifacts   |\\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\\n    ---------------------------------------------------------------------\\n    |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\\n    ---------------------------------------------------------------------\\n:: retrieving :: org.apache.spark#spark-submit-parent-bb5bd11f-ba3c-448e-8f69-5693cc073428\\n    confs: [default]\\n    0 artifacts copied, 3 already retrieved (0kB\u002F6ms)\\n20\u002F07\u002F17 15:23:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\nUsing Spark&#39;s default log4j profile: org\u002Fapache\u002Fspark\u002Flog4j-defaults.properties\\n20\u002F07\u002F17 15:23:19 INFO SparkContext: Running Spark version 2.4.6\\n20\u002F07\u002F17 15:23:19 INFO SparkContext: Submitted application: Passim.PassimApp\\n20\u002F07\u002F17 15:23:19 INFO SecurityManager: Changing view acls to: matteo\\n20\u002F07\u002F17 15:23:19 INFO SecurityManager: Changing modify acls to: matteo\\n20\u002F07\u002F17 15:23:19 INFO SecurityManager: Changing view acls groups to:\\n20\u002F07\u002F17 15:23:19 INFO SecurityManager: Changing modify acls groups to:\\n20\u002F07\u002F17 15:23:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(matteo); groups with view permissions: Set(); users  with modify permissions: Set(matteo); groups with modify permissions: Set()\\n20\u002F07\u002F17 15:23:20 INFO Utils: Successfully started service &#39;sparkDriver&#39; on port 62254.\\n20\u002F07\u002F17 15:23:20 INFO SparkEnv: Registering MapOutputTracker\\n20\u002F07\u002F17 15:23:20 INFO SparkEnv: Registering BlockManagerMaster\\n20\u002F07\u002F17 15:23:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\\n20\u002F07\u002F17 15:23:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\\n20\u002F07\u002F17 15:23:20 INFO DiskBlockManager: Created local directory at \u002Fprivate\u002Fvar\u002Ffolders\u002F8s\u002Frnkbnf8549qclh_gcb_qj_yw0000gv\u002FT\u002Fblockmgr-f42fca4e-0a6d-4751-8d3b-36db57896aa4\\n20\u002F07\u002F17 15:23:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\\n20\u002F07\u002F17 15:23:20 INFO SparkEnv: Registering OutputCommitCoordinator\\n20\u002F07\u002F17 15:23:20 INFO Utils: Successfully started service &#39;SparkUI&#39; on port 4040.\\n20\u002F07\u002F17 15:23:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http:\u002F\u002F192.168.0.24:4040\\n20\u002F07\u002F17 15:23:20 INFO SparkContext: Added JAR file:\u002F\u002F\u002FUsers\u002Fmatteo\u002F.ivy2\u002Fjars\u002Fcom.github.scopt_scopt_2.11-3.5.0.jar at spark:\u002F\u002F192.168.0.24:62254\u002Fjars\u002Fcom.github.scopt_scopt_2.11-3.5.0.jar with timestamp 1594992200488\\n20\u002F07\u002F17 15:23:20 INFO SparkContext: Added JAR file:\u002F\u002F\u002FUsers\u002Fmatteo\u002F.ivy2\u002Fjars\u002Fgraphframes_graphframes-0.7.0-spark2.4-s_2.11.jar at spark:\u002F\u002F192.168.0.24:62254\u002Fjars\u002Fgraphframes_graphframes-0.7.0-spark2.4-s_2.11.jar with timestamp 1594992200489\\n20\u002F07\u002F17 15:23:20 INFO SparkContext: Added JAR file:\u002F\u002F\u002FUsers\u002Fmatteo\u002F.ivy2\u002Fjars\u002Forg.slf4j_slf4j-api-1.7.16.jar at spark:\u002F\u002F192.168.0.24:62254\u002Fjars\u002Forg.slf4j_slf4j-api-1.7.16.jar with timestamp 1594992200489\\n20\u002F07\u002F17 15:23:20 INFO SparkContext: Added JAR file:\u002FUsers\u002Fmatteo\u002FDocuments\u002FPassim\u002Ftarget\u002Fscala-2.11\u002FPassim_2.11-0.2.0.jar at spark:\u002F\u002F192.168.0.24:62254\u002Fjars\u002FPassim_2.11-0.2.0.jar with timestamp 1594992200489\\n20\u002F07\u002F17 15:23:20 INFO Executor: Starting executor ID driver on host localhost\\n20\u002F07\u002F17 15:23:20 INFO Utils: Successfully started service &#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; on port 62255.\\n20\u002F07\u002F17 15:23:20 INFO NettyBlockTransferService: Server created on 192.168.0.24:62255\\n20\u002F07\u002F17 15:23:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\\n20\u002F07\u002F17 15:23:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.24, 62255, None)\\n20\u002F07\u002F17 15:23:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.24:62255 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.24, 62255, None)\\n20\u002F07\u002F17 15:23:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.24, 62255, None)\\n20\u002F07\u002F17 15:23:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.24, 62255, None)\\nUsage: Passim [options] &lt;path&gt;,&lt;path&gt;,... &lt;path&gt;\\n\\n  --boilerplate            Detect boilerplate within groups.\\n  --labelPropagation       Cluster with label propagation.\\n  -n, --n &lt;value&gt;          index n-gram features; default=5\\n  -l, --minDF &lt;value&gt;      Lower limit on document frequency; default=2\\n  -u, --maxDF &lt;value&gt;      Upper limit on document frequency; default=100\\n  -m, --min-match &lt;value&gt;  Minimum number of n-gram matches between documents; default=5\\n  -a, --min-align &lt;value&gt;  Minimum length of alignment; default=20\\n  -L, --min-lines &lt;value&gt;  Minimum number of lines in boilerplate and docwise alignments; default=5\\n  -g, --gap &lt;value&gt;        Minimum size of the gap that separates passages; default=100\\n  -c, --context &lt;value&gt;    Size of context for aligned passages; default=0\\n  -o, --relative-overlap &lt;value&gt;\\n                           Minimum relative overlap to merge passages; default=0.8\\n  -M, --merge-diverge &lt;value&gt;\\n                           Maximum length divergence for merging extents; default=0.3\\n  -r, --max-repeat &lt;value&gt;\\n                           Maximum repeat of one series in a cluster; default=10\\n  -p, --pairwise           Output pairwise alignments\\n  -d, --docwise            Output docwise alignments\\n  --linewise               Output linewise alignments\\n  -N, --names              Output names and exit\\n  -P, --postings           Output postings and exit\\n  -i, --id &lt;value&gt;         Field for unique document IDs; default=id\\n  -t, --text &lt;value&gt;       Field for document text; default=text\\n  -s, --group &lt;value&gt;      Field to group documents into series; default=series\\n  -f, --filterpairs &lt;value&gt;\\n                           Constraint on posting pairs; default=gid &lt; gid2\\n  --fields &lt;value&gt;         Semicolon-delimited list of fields to index\\n  --input-format &lt;value&gt;   Input format; default=json\\n  --schema-path &lt;value&gt;    Input schema path in json format\\n  --output-format &lt;value&gt;  Output format; default=json\\n  --aggregate              Output aggregate alignments of consecutive seqs\\n  -w, --word-length &lt;value&gt;\\n                           Minimum average word length to match; default=2\\n  --help                   prints usage text\\n  &lt;path&gt;,&lt;path&gt;,...        Comma-separated input paths\\n  &lt;path&gt;                   Output path\\n20\u002F07\u002F17 15:23:20 INFO SparkContext: Invoking stop() from shutdown hook\\n20\u002F07\u002F17 15:23:20 INFO SparkUI: Stopped Spark web UI at http:\u002F\u002F192.168.0.24:4040\\n20\u002F07\u002F17 15:23:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\\n20\u002F07\u002F17 15:23:21 INFO MemoryStore: MemoryStore cleared\\n20\u002F07\u002F17 15:23:21 INFO BlockManager: BlockManager stopped\\n20\u002F07\u002F17 15:23:21 INFO BlockManagerMaster: BlockManagerMaster stopped\\n20\u002F07\u002F17 15:23:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\\n20\u002F07\u002F17 15:23:21 INFO SparkContext: Successfully stopped SparkContext\\n20\u002F07\u002F17 15:23:21 INFO ShutdownHookManager: Shutdown hook called\\n20\u002F07\u002F17 15:23:21 INFO ShutdownHookManager: Deleting directory \u002Fprivate\u002Fvar\u002Ffolders\u002F8s\u002Frnkbnf8549qclh_gcb_qj_yw0000gv\u002FT\u002Fspark-dbeee326-7f37-475a-9379-74da31d72117\\n20\u002F07\u002F17 15:23:21 INFO ShutdownHookManager: Deleting directory \u002Fprivate\u002Fvar\u002Ffolders\u002F8s\u002Frnkbnf8549qclh_gcb_qj_yw0000gv\u002FT\u002Fspark-9ae8a384-b1b3-49fa-aaff-94ae2f37b2d9\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch1 id=\\\"preparing-data-for-passim\\\"\u003EPreparing Data for Passim\u003C\u002Fh1\u003E\\n\u003Cp\u003EThe goal of using Passim is to automate the search for repeated text passages in a corpus. For example, a newspaper corpus contains multiple copies of the same article, identical or with slight differences from one another, as well as repetitions of smaller portions of a newspaper page (e.g. advertisement, event listings, etc.).\u003C\u002Fp\u003E\\n\u003Cp\u003EAs the documentation for Passim specifies &quot;the input to Passim is a set of documents. Depending on the kind of data you have, you might choose documents to be whole books, pages of books, whole issues of newspapers, individual newspaper articles, etc. Minimally, a document consists of an identifier string and a single string of text content&quot; (Refer to the minimal JSON input example in the next section for more information about the structure of input for Passim).\u003C\u002Fp\u003E\\n\u003Cp\u003EFigure 1 gives a schematic representation of input and output data for Passim. Given an input set of documents, divided into document series, Passim will attempt to identify reuse of text from documents in different series, and not within these series. In the case of a newspaper corpus, articles from the same newspaper will belong to the same document series, as we are not interested in detecting reuse within the same newspaper, but across different newspapers.\u003C\u002Fp\u003E\\n\u003Cp\u003EUltimately, what constitutes a document, and how these documents should be divided into series, are the choices you&#39;ll need to make when preparing your data for Passim.  Naturally, the decision on what constitutes a \u003Cem\u003Eseries\u003C\u002Fem\u003E of documents is directly dependent on your goals or research questions. Finding quotations of the Bible in a corpus of books is a &quot;one-to-many&quot; case of text reuse detection, which requires documents to be grouped into two series (\u003Ccode\u003Ebible\u003C\u002Fcode\u003E and \u003Ccode\u003Enon_bible\u003C\u002Fcode\u003E). Instead, the comparison between multiple editions of the Bible (also known as collation) can be seen a &quot;many-to-many&quot; case, where each edition will correspond to and constitute a series of documents (e.g. pages).  If your research questions change at some point, thus requiring a re-definition of document series, you will need also to produce new input data for Passim to reflect this change.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;textreuse-generic.png&quot; caption=&quot;Figure 1. Schematic representation of text reuse clusters; each cluster consists of similar passages found in several series of documents.&quot; %}\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"basic-json-format\\\"\u003EBasic JSON format\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe input format for Passim consists of JSON documents in the \u003Ca href=\\\"http:\u002F\u002Fjsonlines.org\u002F\\\"\u003EJSON lines format\u003C\u002Fa\u003E (i.e. each line of text contains a single JSON document).\u003C\u002Fp\u003E\\n\u003Cp\u003EThe following file content for a file named \u003Ccode\u003Etest.json\u003C\u002Fcode\u003E illustrates a minimal example of the input format for Passim: \u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-json\\\"\u003E{&quot;id&quot;: &quot;d1&quot;, &quot;series&quot;: &quot;abc&quot;, &quot;text&quot;: &quot;This is the text of a document.&quot;}\\n{&quot;id&quot;: &quot;d2&quot;, &quot;series&quot;: &quot;def&quot;, &quot;text&quot;: &quot;This is the text of another document.&quot;}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe fields \u003Ccode\u003Eid\u003C\u002Fcode\u003E, \u003Ccode\u003Eseries\u003C\u002Fcode\u003E and \u003Ccode\u003Etext\u003C\u002Fcode\u003E are the only fields required by Passim. Given this file as input, the software will try to detect text reuse between documents in the series \u003Ccode\u003Eabc\u003C\u002Fcode\u003E and those in the series \u003Ccode\u003Edef\u003C\u002Fcode\u003E, on the basis of the contents in \u003Ccode\u003Etext\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EThroughout this tutorial we will be using the command-line tool \u003Ca href=\\\"https:\u002F\u002Fstedolan.github.io\u002Fjq\u002F\\\"\u003E\u003Ccode\u003Ejq\u003C\u002Fcode\u003E\u003C\u002Fa\u003E to inspect and do some basic process on both input and output JSON data. Note that, if you don&#39;t have \u003Ccode\u003Ejq\u003C\u002Fcode\u003E installed, you&#39;ll need to execute \u003Ccode\u003Esudo apt-get install jq\u003C\u002Fcode\u003E under Ubuntu or \u003Ccode\u003Ebrew install jq\u003C\u002Fcode\u003E under macOS (for other operating systems \u003Ca href=\\\"https:\u002F\u002Fstedolan.github.io\u002Fjq\u002Fdownload\u002F\\\"\u003Erefer to the official JQ installation page\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\\n\u003Cp\u003EFor example, to select and print the field \u003Ccode\u003Eseries\u003C\u002Fcode\u003E of your input \u003Ccode\u003Etest.json\u003C\u002Fcode\u003E, run the following command:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; jq &#39;.series&#39; test.json\\n\\n# this will print\\n&quot;abc&quot;\\n&quot;def&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENote: If you are using \u003Ccode\u003Ejq\u003C\u002Fcode\u003E to look at your JSON data, you need to use the \u003Ccode\u003E--slurp\u003C\u002Fcode\u003E parameter whenever you want to treat the content of one or more JSON line files as a single array of JSON documents and apply some filters to it (e.g. to select and print only one document, use the following command \u003Ccode\u003Ejq --slurp &#39;.[-1]&#39; test.json\u003C\u002Fcode\u003E). Otherwise \u003Ccode\u003Ejq\u003C\u002Fcode\u003E will treat each document separately thus causing the following error:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; jq &#39;.[0]&#39; test.json\\n\\njq: error (at &lt;stdin&gt;:1): Cannot index string with string &quot;series&quot;\\njq: error (at &lt;stdin&gt;:2): Cannot index string with string &quot;series&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"a-note-on-packaging-data\\\"\u003EA Note on Packaging Data\u003C\u002Fh2\u003E\\n\u003Cp\u003EDepending one the total size of your data, it may be a good idea to store Passim input files as compressed archives. Passim supports several compression schemes like .gzip and .bzip2. Note that a compressed datastream will be slower to process than an uncompressed one, so using this option will only be beneficial if your data is large (i.e. gigabytes of text), if you have access to many computing cores, or have a limited amount of disk space.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis command (or, better, chain of commands) will output the first document in a bzip2-compressed JSON lines file (some fields have been truncated for the sake of readability):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; bzcat impresso\u002FGDL-1900.jsonl.bz2 | jq --slurp &#39;.[0]&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAnd will output the following:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-json\\\"\u003E{\\n  &quot;series&quot;: &quot;GDL&quot;,\\n  &quot;date&quot;: &quot;1900-12-12&quot;,\\n  &quot;id&quot;: &quot;GDL-1900-12-12-a-i0001&quot;,\\n  &quot;cc&quot;: true,\\n  &quot;pages&quot;: [\\n    {\\n      &quot;id&quot;: &quot;GDL-1900-12-12-a-p0001&quot;,\\n      &quot;seq&quot;: 1,\\n      &quot;regions&quot;: [\\n        {\\n          &quot;start&quot;: 0,\\n          &quot;length&quot;: 13,\\n          &quot;coords&quot;: {\\n            &quot;x&quot;: 471,\\n            &quot;y&quot;: 1240,\\n            &quot;w&quot;: 406,\\n            &quot;h&quot;: 113\\n          }\\n        },\\n        {\\n          &quot;start&quot;: 13,\\n          &quot;length&quot;: 2,\\n          &quot;coords&quot;: {\\n            &quot;x&quot;: 113,\\n            &quot;y&quot;: 1233,\\n            &quot;w&quot;: 15,\\n            &quot;h&quot;: 54\\n          }\\n        },\\n        ...\\n      ]\\n    }\\n  ],\\n  &quot;title&quot;: &quot;gratuitement ,la §azette seia envoyée&quot;,\\n  &quot;text&quot;: &quot;gratuitement\\\\n, la § azette\\\\nseia envoyée\\\\ndès ce jour au 31 décembre, aux personnes\\\\nqui s&#39;abonneront pour l&#39;année 1901.\\\\nLes abonnements sont reçus par l&#39;admi-\\\\nnistration de la Gazette de Lausanne et dans\\\\ntous les bureaux de poste.\\\\n&quot;\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"custom-json-format\\\"\u003ECustom JSON format\u003C\u002Fh2\u003E\\n\u003Cp\u003E(Note: This subsection is not strictly necessary to run Passim, as the second case study will showcase. Nonetheless, these steps may be useful to readers with advanced needs with the regards to the format and structure of input data.)\u003C\u002Fp\u003E\\n\u003Cp\u003EThere are cases where you may want to include additional information (i.e. JSON fields) in each input document, in addition to the required ones (\u003Ccode\u003Eid\u003C\u002Fcode\u003E, \u003Ccode\u003Eseries\u003C\u002Fcode\u003E, \u003Ccode\u003Etext\u003C\u002Fcode\u003E). As an example, when working with OCR data you may want to pass image coordinate information alongside the article text. Passim does support the use of input data that follow a custom JSON format as behind the scenes it relies on Spark to infer the structure of the input data (i.e. the JSON schema). Passim will not directly use these fields, but it will keep them in the produced output.\u003C\u002Fp\u003E\\n\u003Cp\u003EHowever, there may be cases where Spark fails to infer the correct structure of input data (e.g. by inferring a wrong data type for a given field). In these cases, you need to inform Passim about the correct schema of the input data. \u003C\u002Fp\u003E\\n\u003Cp\u003EThe following example illustrates a step-by-step approach to troubleshooting this relatively rare situation where one needs to correct the inferred JSON schema. Passim comes with the command \u003Ccode\u003Ejson-df-schema\u003C\u002Fcode\u003E, which runs a (Python) script to infer the schema from any JSON input. The following steps are necessary to infer the structure from any JSON data:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EInstall the necessary Python libraries.\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; pip install pyspark\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EExtract an input example from one of our compressed input files.\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# here we take the 3rd document in the .bz2 file\\n# and save it to a new local file\\n&gt;&gt;&gt; bzcat impresso\u002Fdata\u002FGDL-1900.jsonl.bz2 | head | jq --slurp &quot;.[2]&quot; &gt; impresso\u002Fdata\u002Fimpresso-sample-document.json\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003EAsk \u003Ccode\u003Ejson-df-schema\u003C\u002Fcode\u003E to infer the schema of our data from our sample file.\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; json-df-schema impresso\u002Fdata\u002Fimpresso-sample-document.json &gt; impresso\u002Fschema\u002FPassim.schema.orig\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003E\u003Ccode\u003Ejson-df-schema\u003C\u002Fcode\u003E will try to guess the JSON schema of input data and output it to a file. The following example is what the schema generated by Passim (\u003Ccode\u003EPassim.schema.orig\u003C\u002Fcode\u003E) looks like:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-json\\\"\u003E{\\n  &quot;fields&quot;: [\\n    {\\n      &quot;metadata&quot;: {},\\n      &quot;name&quot;: &quot;cc&quot;,\\n      &quot;nullable&quot;: true,\\n      &quot;type&quot;: &quot;boolean&quot;\\n    },\\n    {\\n      &quot;metadata&quot;: {},\\n      &quot;name&quot;: &quot;date&quot;,\\n      &quot;nullable&quot;: true,\\n      &quot;type&quot;: &quot;string&quot;\\n    },\\n    {\\n      &quot;metadata&quot;: {},\\n      &quot;name&quot;: &quot;id&quot;,\\n      &quot;nullable&quot;: true,\\n      &quot;type&quot;: &quot;string&quot;\\n    },\\n    {\\n      &quot;metadata&quot;: {},\\n      &quot;name&quot;: &quot;pages&quot;,\\n      &quot;nullable&quot;: true,\\n      &quot;type&quot;: {\\n        &quot;containsNull&quot;: true,\\n        &quot;elementType&quot;: {\\n          &quot;fields&quot;: [\\n            {\\n              &quot;metadata&quot;: {},\\n              &quot;name&quot;: &quot;id&quot;,\\n              &quot;nullable&quot;: true,\\n              &quot;type&quot;: &quot;string&quot;\\n            },\\n            {\\n              &quot;metadata&quot;: {},\\n              &quot;name&quot;: &quot;regions&quot;,\\n              &quot;nullable&quot;: true,\\n              &quot;type&quot;: {\\n                &quot;containsNull&quot;: true,\\n                &quot;elementType&quot;: {\\n                  &quot;fields&quot;: [\\n                    {\\n                      &quot;metadata&quot;: {},\\n                      &quot;name&quot;: &quot;coords&quot;,\\n                      &quot;nullable&quot;: true,\\n                      &quot;type&quot;: {\\n                        &quot;fields&quot;: [\\n                          {\\n                            &quot;metadata&quot;: {},\\n                            &quot;name&quot;: &quot;h&quot;,\\n                            &quot;nullable&quot;: true,\\n                            &quot;type&quot;: &quot;long&quot;\\n                          },\\n                          {\\n                            &quot;metadata&quot;: {},\\n                            &quot;name&quot;: &quot;w&quot;,\\n                            &quot;nullable&quot;: true,\\n                            &quot;type&quot;: &quot;long&quot;\\n                          },\\n                          {\\n                            &quot;metadata&quot;: {},\\n                            &quot;name&quot;: &quot;x&quot;,\\n                            &quot;nullable&quot;: true,\\n                            &quot;type&quot;: &quot;long&quot;\\n                          },\\n                          {\\n                            &quot;metadata&quot;: {},\\n                            &quot;name&quot;: &quot;y&quot;,\\n                            &quot;nullable&quot;: true,\\n                            &quot;type&quot;: &quot;long&quot;\\n                          }\\n                        ],\\n                        &quot;type&quot;: &quot;struct&quot;\\n                      }\\n                    },\\n                    {\\n                      &quot;metadata&quot;: {},\\n                      &quot;name&quot;: &quot;length&quot;,\\n                      &quot;nullable&quot;: true,\\n                      &quot;type&quot;: &quot;long&quot;\\n                    },\\n                    {\\n                      &quot;metadata&quot;: {},\\n                      &quot;name&quot;: &quot;start&quot;,\\n                      &quot;nullable&quot;: true,\\n                      &quot;type&quot;: &quot;long&quot;\\n                    }\\n                  ],\\n                  &quot;type&quot;: &quot;struct&quot;\\n                },\\n                &quot;type&quot;: &quot;array&quot;\\n              }\\n            },\\n            {\\n              &quot;metadata&quot;: {},\\n              &quot;name&quot;: &quot;seq&quot;,\\n              &quot;nullable&quot;: true,\\n              &quot;type&quot;: &quot;long&quot;\\n            }\\n          ],\\n          &quot;type&quot;: &quot;struct&quot;\\n        },\\n        &quot;type&quot;: &quot;array&quot;\\n      }\\n    },\\n    {\\n      &quot;metadata&quot;: {},\\n      &quot;name&quot;: &quot;series&quot;,\\n      &quot;nullable&quot;: true,\\n      &quot;type&quot;: &quot;string&quot;\\n    },\\n    {\\n      &quot;metadata&quot;: {},\\n      &quot;name&quot;: &quot;text&quot;,\\n      &quot;nullable&quot;: true,\\n      &quot;type&quot;: &quot;string&quot;\\n    },\\n    {\\n      &quot;metadata&quot;: {},\\n      &quot;name&quot;: &quot;title&quot;,\\n      &quot;nullable&quot;: true,\\n      &quot;type&quot;: &quot;string&quot;\\n    }\\n  ],\\n  &quot;type&quot;: &quot;struct&quot;\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EPassim has failed to recognize the coordinate field as containing integer values and it has interpreted as a long data type.  At this point, we need to change the type of the sub-fields of \u003Ccode\u003Ecoords\u003C\u002Fcode\u003E (i.e. \u003Ccode\u003Eh\u003C\u002Fcode\u003E, \u003Ccode\u003Ew\u003C\u002Fcode\u003E, \u003Ccode\u003Ex\u003C\u002Fcode\u003E, and \u003Ccode\u003Ey\u003C\u002Fcode\u003E) from \u003Ccode\u003E&quot;type&quot;: &quot;long&quot;\u003C\u002Fcode\u003E to \u003Ccode\u003E&quot;type&quot;: &quot;integer&quot;\u003C\u002Fcode\u003E. This type mismatch needs to be fixed, otherwise Passim will treat \u003Ccode\u003Eint\u003C\u002Fcode\u003E values as if they were \u003Ccode\u003Elong\u003C\u002Fcode\u003E, thus potentially leading to issues or inconsistencies in the generated output.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe can now save the schema for later into a new file (\u003Ccode\u003Epassim.schema\u003C\u002Fcode\u003E) for later use. This schema is needed when processing the input data provided for \u003Ca href=\\\"#case-study-2:-text-reuse-in-a-large-corpus-of-historical-newspapers\\\"\u003Ethe second case study\u003C\u002Fa\u003E presented in this lesson.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"running-passim\\\"\u003ERunning Passim\u003C\u002Fh1\u003E\\n\u003Cp\u003EIn this section we illustrate the usage of Passim with two separate case studies: 1) detecting Bible quotes in seventeent century texts and 2) detecting text reuse in a large corpus of historical newspapers. The first case study highlights some of the basics of using Passim,  while the second case study contains many details and best practices that would be helpful for a large-scale text reuse project.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the following table, we build on the original Passim documentation and explain some of the more useful parameters that this library offers. The case studies do not require you to master these parameters, so feel free to skip directly to the \u003Ca href=\\\"#downloading-the-data\\\"\u003EDownloading the Data\u003C\u002Fa\u003E section and come back to this section once you are comfortable enough to use Passim on your own data.\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003EParameter\u003C\u002Fth\u003E\\n\u003Cth\u003EDefault value\u003C\u002Fth\u003E\\n\u003Cth\u003EDescription\u003C\u002Fth\u003E\\n\u003Cth\u003EExplanation\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003E\u003Ccode\u003E--n\u003C\u002Fcode\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd\u003EN-gram order for text-reuse detection\u003C\u002Ftd\u003E\\n\u003Ctd\u003EN-grams are chains of words of length N. This setting allows you to decide what type of n-gram (unigram, bigram, trigram...) Passim should use when creating a list of possible text reuse candidates.\u003Cbr \u002F\u003E\u003Cbr \u002F\u003ESetting this parameter to a lower value can help in the case of very noisy texts (i.e. when many words in a text are affected by one or more OCR errors). In fact, the longer the n-gram, the more likely it is to contain OCR mistakes.\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E\u003Ccode\u003E--minDF\u003C\u002Fcode\u003E (\u003Ccode\u003E-l\u003C\u002Fcode\u003E)\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003ELower limit on document frequency of n-grams used\u003C\u002Ftd\u003E\\n\u003Ctd\u003ESince n-grams are used in Passim to retrieve document candidate pairs, an n-gram occurring only once is not useful as it will retrieve only one document (and not a pair). For this reason \u003Ccode\u003E--minDF\u003C\u002Fcode\u003E defaults to \u003Ccode\u003E2\u003C\u002Fcode\u003E.\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E\u003Ccode\u003E--maxDF\u003C\u002Fcode\u003E (\u003Ccode\u003E-u\u003C\u002Fcode\u003E)\u003C\u002Ftd\u003E\\n\u003Ctd\u003E100\u003C\u002Ftd\u003E\\n\u003Ctd\u003EUpper limit on document frequency of n-grams used.\u003C\u002Ftd\u003E\\n\u003Ctd\u003EThis parameter will filter out n-grams that are too common, thus occurring many times in a given document. \u003Cbr \u002F\u003E\u003Cbr \u002F\u003EThis value has an impact on the performances as it will reduce the number of document pairs retrieved by Passim that will need to be compared.\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E\u003Ccode\u003E--min-match\u003C\u002Fcode\u003E (\u003Ccode\u003E-m\u003C\u002Fcode\u003E)\u003C\u002Ftd\u003E\\n\u003Ctd\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd\u003EMinimum number of matching n-grams between two documents\u003C\u002Ftd\u003E\\n\u003Ctd\u003EThis parameter allows you to decide how many n-grams must be found between two documents.\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E\u003Ccode\u003E--relative-overlap\u003C\u002Fcode\u003E (\u003Ccode\u003E-o\u003C\u002Fcode\u003E)\u003C\u002Ftd\u003E\\n\u003Ctd\u003E0.8\u003C\u002Ftd\u003E\\n\u003Ctd\u003EProportion that two different aligned passages from the same document must overlap to be clustered together, as measured on the longer passage \u003C!-- TODO SH: Current mismatch between official doc and code, see what is going to be changed after David answers to this issue https:\u002F\u002Fgithub.com\u002Fdasmiq\u002FPassim\u002Fissues\u002F10 --\u003E\u003C\u002Ftd\u003E\\n\u003Ctd\u003EThis parameter determines the degree of string similarity two passages need to have in order to be clustered together.\u003Cbr \u002F\u003E\u003Cbr \u002F\u003EIn the case of very noisy texts, it may be desirable to set this parameter to a  smaller value.\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E\u003Ccode\u003E--max-repeat\u003C\u002Fcode\u003E (\u003Ccode\u003E-r\u003C\u002Fcode\u003E)\u003C\u002Ftd\u003E\\n\u003Ctd\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd\u003EMaximum repeat of one series in a cluster\u003C\u002Ftd\u003E\\n\u003Ctd\u003EThis paramter allows you to specify how much a given series can be present in a cluster.\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Ch2 id=\\\"downloading-the-data\\\"\u003EDownloading the data\u003C\u002Fh2\u003E\\n\u003Cp\u003ESample data needed to run the command examples in the two case studies can be downloaded from the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fimpresso\u002FPH-Passim-tutorial\\\"\u003Ededicated GitHub repository\u003C\u002Fa\u003E. Before continuing with the case studies, download a local copy of the data by cloning the repository.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; git clone https:\u002F\u002Fgithub.com\u002Fimpresso\u002FPH-Passim-tutorial.git\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAlternatively, it is possible to download the data for this lesson from Zenodo at the address \u003Ca href=\\\"https:\u002F\u002Fzenodo.org\u002Fbadge\u002Flatestdoi\u002F250229057\\\"\u003Ehttps:\u002F\u002Fzenodo.org\u002Fbadge\u002Flatestdoi\u002F250229057\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"case-study-1-bible-quotes-in-seventeenth-century-texts\\\"\u003ECase study 1: Bible Quotes in Seventeenth Century Texts\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn this first case study, we will look at text reuse using texts taken from \u003Ca href=\\\"https:\u002F\u002Ftextcreationpartnership.org\u002Ftcp-texts\u002Feebo-tcp-early-english-books-online\u002F\\\"\u003EEEBO-TCP\u003C\u002Fa\u003E Phase I, the publicly available keyed-in version of Early English Books Online provided by the Text Creation Partnership. This case study is a special case of text reuse, as we are not focusing at inter-authors text reuse, but rather at the influence a single book — in this case, the Bible in its published-in-1611 King James version — had on several authors. Can we detect what documents contain extracts from the Bible?\u003C\u002Fp\u003E\\n\u003Cp\u003EAs this is a small-scale example of what an actual research question making use of text reuse methods could look like, we will only use some of the 25,368 works available in EEBO-TCP, taken randomly. This smaller selection size should also allow anyone reading this tutorial to run this example on their personal laptop. Ideally, we recommend using a corpus such as \u003Ca href=\\\"https:\u002F\u002Fwww.uantwerpen.be\u002Fen\u002Fprojects\u002Fmind-bending-grammars\u002Femma-corpus\u002F\\\"\u003EEarly Modern Multiloquent Authors (EMMA)\u003C\u002Fa\u003E, compiled by the University of Antwerp&#39;s \u003Ca href=\\\"https:\u002F\u002Fwww.uantwerpen.be\u002Fen\u002Fprojects\u002Fmind-bending-grammars\u002F\\\"\u003EMind Bending Grammars\u003C\u002Fa\u003E project, should someone want to properly study the use of Bible quotes in seventeenth century texts. This corpus has the advantage of providing hand-curated metadata in an easily parseable format, allowing any researcher to focus on specific authors, periods, etc.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"extracting-the-data\\\"\u003EExtracting the Data\u003C\u002Fh3\u003E\\n\u003Cp\u003EAt the root of the newly-created directory is a JSON file: \u003Ccode\u003Epassim_in.json\u003C\u002Fcode\u003E. This file contains all our data, in the format described above: one document per line (\u003Ccode\u003Etext\u003C\u002Fcode\u003E), structured with the bare minimum of required metadata (\u003Ccode\u003Eid\u003C\u002Fcode\u003E, \u003Ccode\u003Eseries\u003C\u002Fcode\u003E). As this is a small file, we encourage you to open the file using a text editor such as Notepad++ on Windows or Sublime on Linux\u002FmacOS to familiarise yourself with how the data is formatted. Because our case study focuses on the detection of Bible passages in several documents and not on text reuse within all documents, we have formatted the data so that the \u003Ccode\u003Eseries\u003C\u002Fcode\u003E field contains \u003Ccode\u003Ebible\u003C\u002Fcode\u003E for the Bible (last line of our JSON file), and \u003Ccode\u003Enot_bible\u003C\u002Fcode\u003E for all other documents. Passim does not analyse documents that belong to the same series, so this effectively tells the software to only compare all documents with the Bible — not with each other.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fimpresso\u002FPH-Passim-tutorial\u002F\\\"\u003Eaccompanying Github repository\u003C\u002Fa\u003E contains a \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fimpresso\u002FPH-Passim-tutorial\u002Fblob\u002Fmaster\u002Feebo\u002Fcode\u002Fmain.py\\\"\u003EPython script\u003C\u002Fa\u003E to transform EEBO-TCP into the JSON format required by Passim and used in this lesson. We encourage the readers to reuse it and adapt it to their needs.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"running-passim-1\\\"\u003ERunning Passim\u003C\u002Fh3\u003E\\n\u003Cp\u003ECreate a directory where you want to store the output of Passim (we use \u003Ccode\u003EPassim_output_bible\u003C\u002Fcode\u003E but any name will work). If you decide to use the default \u003Ccode\u003EPassim_output_bible\u003C\u002Fcode\u003E directory, ensure you remove all of its content (i.e. pre-computed Passim output) either manually or by running \u003Ccode\u003Erm -r .\u002Feebo\u002FPassim_output_bible\u002F*\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EAs we will see in more detail in the second use case, Passim, through Spark, allows for many options. By default Java does not allocate much memory to its processes, and running Passim even on very little datasets will cause Passim to crash because of an \u003Ccode\u003EOutOfMemory\u003C\u002Fcode\u003E error — even if you have a machine with a lot of RAM. To avoid this,  when calling Passim we add some additional parameters that will tell Spark to use more RAM for its processes.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou are now ready to go forward with your first text reuse project. \u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Cp\u003EMove to the sub-directory \u003Ccode\u003Eeebo\u003C\u002Fcode\u003E by executing the command \u003Ccode\u003Ecd eebo\u002F\u003C\u002Fcode\u003E, starting from the directory where, earlier on, you cloned the repository \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fimpresso\u002FPH-Passim-tutorial\u002F\\\"\u003E\u003Ccode\u003EPH-Passim-tutorial\u003C\u002Fcode\u003E\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003ERun the following command and go have a cup of your favorite hot beverage:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; SPARK_SUBMIT_ARGS=&#39;--master local[12] --driver-memory 8G --executor-memory 4G&#39; passim passim_in.json passim_output_bible\u002F\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003EFor now, do not worry about the additional arguments \u003Ccode\u003ESPARK_SUBMIT_ARGS=&#39;--master local[12] --driver-memory 8G --executor-memory 4G&#39;\u003C\u002Fcode\u003E; in the section \u003Ca href=\\\"#case-study-2:-text-reuse-in-a-large-corpus-of-historical-newspapers\\\"\u003E&quot;Case Study 2&quot;\u003C\u002Fa\u003E we will explain them in detail.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis test case takes approximatively eight minutes on a recent laptop with eight threads. You can also follow the progress of the detection at \u003Ca href=\\\"http:\u002F\u002Flocalhost:4040\\\"\u003Ehttp:\u002F\u002Flocalhost:4040\u003C\u002Fa\u003E — an interactive dashboard created by Spark (Note: the dashboard will shut down as soon as Passim has finished running).\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"case-study-2-text-reuse-in-a-large-corpus-of-historical-newspapers\\\"\u003ECase study 2: Text Reuse in a large corpus of historical newspapers\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe second case study is drawn from \u003Ca href=\\\"https:\u002F\u002Fimpresso-project.ch\u002F\\\"\u003Eimpresso\u003C\u002Fa\u003E, a recent research project aimed at enabling critical text mining of newspaper archives with the implementation of a technological framework to extract, process, link, and explore data from print media archives.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this project, we use Passim to detect text reuse at scale. The extracted text reuse clusters are then integrated into the \u003Ca href=\\\"https:\u002F\u002Fimpresso-project.ch\u002Fapp\\\"\u003Eimpresso tool\u003C\u002Fa\u003E in two ways. First, in the main article reading view users can readily see which portions of an article were reused by other articles in the corpus. Second, users can browse through all clusters in a dedicated page (currently more than 6 million), perform full-text searches on their contents, and filter the results according to a number of criteria (cluster size, time span covered, lexical overlap, etc.).\u003C\u002Fp\u003E\\n\u003Cp\u003EMore generally, detecting text reuse in a large-scale newspaper corpus can be useful in many of the following ways:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EIdentify (and possibly filter out) duplicated documents before performing further processing steps (e.g. topic modelling)\u003C\u002Fli\u003E\\n\u003Cli\u003EStudy the virality and spread of news\u003C\u002Fli\u003E\\n\u003Cli\u003EStudy information flows, both within and across national borders\u003C\u002Fli\u003E\\n\u003Cli\u003Eto allow users discover which contents, within in their own collections, generated text reuse (e.g. famous political speeches, portions of national constitutions, etc.)\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EFor this case study we consider a tiny fraction of the \u003Cem\u003Eimpresso\u003C\u002Fem\u003E corpus, consisting of one year&#39;s worth of newspaper data (i.e. 1900) for a sample of four newspapers. The corpus contains 76 newspapers from Switzerland and Luxembourg, covering a time span of 200 years. The sample data necessary to run step by step this case study are contained in the folder \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fimpresso\u002FPH-Passim-tutorial\u002Ftree\u002Fmaster\u002Fimpresso\\\"\u003E\u003Ccode\u003Eimpresso\u002F\u003C\u002Fcode\u003E\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"data-preparation\\\"\u003EData preparation\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe format used in impresso to store newspapers data is slightly different from Passim&#39;s input format so we need a script to take care of transforming the former into the latter. While discussing how this script works goes well beyond the scope of this lesson, you can find the conversion script on the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fimpresso\u002Fimpresso-pycommons\u002Fblob\u002Fmaster\u002Fimpresso_commons\u002Ftext\u002Frebuilder.py\\\"\u003Eimpresso GitHub repository\u003C\u002Fa\u003E should you be interested. The output of this script is one JSON line file per newspaper per year, compressed into a \u003Ccode\u003E.bz2\u003C\u002Fcode\u003E archive for the sake of efficient storage. Examples of this format can be found in the directory \u003Ccode\u003Eimpresso\u002Fdata\u003C\u002Fcode\u003E and shown in the following example:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E&gt;&gt;&gt; ls -la impresso\u002Fdata\u002F\\nEXP-1900.jsonl.bz2\\nGDL-1900.jsonl.bz2\\nIMP-1900.jsonl.bz2\\nJDG-1900.jsonl.bz2\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EEach newspaper archive is named after the newspaper identifier: for example, \u003Ccode\u003EGDL\u003C\u002Fcode\u003E stands for \u003Cem\u003EGazette de Lausanne\u003C\u002Fem\u003E. In total, these four \u003Ccode\u003E.bz2\u003C\u002Fcode\u003E files contain 92,000 articles through Passim, corresponding to all articles published in 1900 in the four sampled newspapers.\u003C\u002Fp\u003E\\n\u003Cp\u003ESometimes it&#39;s not easy to inspect data packaged in this way. But some Bash commands like \u003Ccode\u003Ebzcat\u003C\u002Fcode\u003E and \u003Ccode\u003Ejq\u003C\u002Fcode\u003E can help us. For example, with the following chain of commands we can find out how many documents (newspaper articles) are contained in each of the input files by counting their IDs:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E&gt;&gt;&gt; bzcat impresso\u002Fdata\u002FGDL-1900.jsonl.bz2 | jq --slurp &#39;[.[] |del(.pages)| .id]|length&#39;\\n28380\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAnd similarly, in all input files:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E&gt;&gt;&gt; bzcat impresso\u002Fdata\u002F*-1900.jsonl.bz2 | jq --slurp &#39;[.[] |del(.pages)| .id]|length&#39;\\n92514\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWhat these commands do is to read the content of the \u003Ccode\u003E.bz2\u003C\u002Fcode\u003E file by means of \u003Ccode\u003Ebzcat\u003C\u002Fcode\u003E and then \u003Cem\u003Epipe\u003C\u002Fem\u003E this content into \u003Ccode\u003Ejq\u003C\u002Fcode\u003E which\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003Eiterates through all docouments in the JSON line file\u003C\u002Fli\u003E\\n\u003Cli\u003Efor each document it removes the \u003Ccode\u003Epages\u003C\u002Fcode\u003E field as it&#39;s not needed and selects only the \u003Ccode\u003Eid\u003C\u002Fcode\u003E field\u003C\u002Fli\u003E\\n\u003Cli\u003Efinally, with \u003Ccode\u003Elength\u003C\u002Fcode\u003E \u003Ccode\u003Ejq\u003C\u002Fcode\u003E computes the size of the list of IDs created by the previous expression\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch3 id=\\\"running-passim-2\\\"\u003ERunning Passim\u003C\u002Fh3\u003E\\n\u003Cp\u003ETo run the impresso data through Passim, execute the following command in a \u003Ccode\u003ETerminal\u003C\u002Fcode\u003E window:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003ESPARK_SUBMIT_ARGS=&#39;--master local[12] --driver-memory 10G --executor-memory 10G --conf spark.local.dir=\u002Fscratch\u002Fmatteo\u002Fspark-tmp\u002F&#39; Passim --schema-path=&quot;impresso\u002Fschema\u002FPassim.schema&quot; &quot;impresso\u002Fdata\u002F*.jsonl.bz2&quot; &quot;impresso\u002FPassim-output\u002F&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis command is made up of the following parameters:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003ESPARK_SUBMIT_ARGS\u003C\u002Fcode\u003E\u003C\u002Fstrong\u003E passes some configuration parameters to Spark, the library that takes care of parallel execution of processes.\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003E--master local[10]\u003C\u002Fcode\u003E: \u003Ccode\u003Elocal\u003C\u002Fcode\u003E means we are running Spark in single machine-mode; \u003Ccode\u003E[10]\u003C\u002Fcode\u003E specifies the number of workers (or threads, in this specific case) over which processes should be distributed (\u003Ccode\u003Elocal [*]\u003C\u002Fcode\u003E will make use of the maximum number of threads);  \u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003E--executor-memory 4G\u003C\u002Fcode\u003E: The equivalent of the maximum heap size when running a regular JAVA application. It&#39;s the amount of memory that Spark allocates to each executor.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003E--conf spark.local.dir=\u002Fscratch\u002Fmatteo\u002Fspark-tmp\u002F\u003C\u002Fcode\u003E: A directory where Spark stores temporary data. When working with large datasets, it is important to specify a location with sufficient free disk space.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003E--schema-path\u003C\u002Fcode\u003E\u003C\u002Fstrong\u003E: Specifies the path to the JSON schema describing the input data to be ran through Passim (see section \u003Ca href=\\\"#custom-json-format\\\"\u003E&quot;Custom JSON format&quot;\u003C\u002Fa\u003E for more information about how to generate such schema).\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003Eimpresso\u002Fdata\u002F*.jsonl.bz2\u003C\u002Fcode\u003E\u003C\u002Fstrong\u003E: Specifies the input files (i.e. all files contained in \u003Ccode\u003Eimpresso\u002Fdata\u002F\u003C\u002Fcode\u003E with \u003Ccode\u003E.jsonl.bz2\u003C\u002Fcode\u003E in the file name);\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cstrong\u003E\u003Ccode\u003Eimpresso\u002FPassim-output\u002F\u003C\u002Fcode\u003E\u003C\u002Fstrong\u003E: Specifies where Passim should write its output\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EIf you want to limit the processing to a couple of input files — for example to limit memory usage — you can specify the input using the following command:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Eimpresso\u002Fdata\u002F{EXP-1900.jsonl.bz2,GDL-1900.jsonl.bz2}.jsonl.bz2\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EYou can monitor Passim&#39;s progress while running by pointing your browser to the address \u003Ccode\u003Elocalhost:4040\u003C\u002Fcode\u003E where the Spark dashboard can be accessed (Figure 2).\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;spark-dashboard.png&quot; caption=&quot;Figure 2. Screenshot of the Spark dashboard while running Passim.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ERunning Passim with eight workers (and 4 Gb of executor memory) takes about five minutes to process the 92,514 articles published in 1900 in the newspapers GDL, JDG, EXP, IMP (but your mileage may vary).\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you provide as input a folder with \u003Ccode\u003E*.bz2\u003C\u002Fcode\u003E files, ensure these files are not found within subdirectories or Passim will not be able to find them automatically.\u003C\u002Fp\u003E\\n\u003Cp\u003EIt is important that the output folder where Passim will write its output is empty. Especially when running the first experiments and getting familiar with the software it can very easily happen to specify a non-empty output folder. Specifying a non-empty output folder usually leads to an error as Passim processes the folder content and does not simply overwrite it.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"inspecting-passims-output\\\"\u003EInspecting Passim&#39;s Output\u003C\u002Fh3\u003E\\n\u003Cp\u003EOnce Passim has finished running, the output folder \u003Ccode\u003Eimpresso\u002FPassim-output\u002F\u003C\u002Fcode\u003E will contain a sub-folder \u003Ccode\u003Eout.json\u002F\u003C\u002Fcode\u003E with the extracted text reuse clusters. If you specified \u003Ccode\u003E--output=parquet\u003C\u002Fcode\u003E instead of \u003Ccode\u003E--output=json\u003C\u002Fcode\u003E, this sub-folder will be named \u003Ccode\u003Eout.parquet\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the JSON output each document corresponds to a text reuse passage. Since passages are aggregated into clusters, each passage contains a field \u003Ccode\u003Ecluster\u003C\u002Fcode\u003E with the ID of the cluster to which it belongs.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo obtain the total number of cluster, we can count the number of unique cluster IDs with the following one-line command:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; cat impresso\u002FPassim-output\u002Fout.json\u002F*.json | jq --slurp &#39;[.[] | .cluster] | unique | length&#39;\\n\\n2721\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESimilarly, we can print the 100th cluster ID:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E&gt;&gt;&gt; cat impresso\u002FPassim-output\u002Fout.json\u002F*.json | jq --slurp &#39;[.[] | .cluster] | unique | .[100]&#39;\\n\\n77309411592\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAnd with a simple \u003Ccode\u003Ejq\u003C\u002Fcode\u003E query we can print all passages belonging to this text reuse cluster:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E&gt;&gt;&gt; cat impresso\u002FPassim-output\u002Fout.json\u002F*.json | jq --slurp &#39;.[] | select(.cluster==77309411592)|del(.pages)&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-json\\\"\u003E{\\n  &quot;uid&quot;: -6695317871595380000,\\n  &quot;cluster&quot;: 77309411592,\\n  &quot;size&quot;: 2,\\n  &quot;bw&quot;: 8,\\n  &quot;ew&quot;: 96,\\n  &quot;cc&quot;: true,\\n  &quot;date&quot;: &quot;1900-07-30&quot;,\\n  &quot;id&quot;: &quot;EXP-1900-07-30-a-i0017&quot;,\\n  &quot;series&quot;: &quot;EXP&quot;,\\n  &quot;text&quot;: &quot;nouvel accident de\\\\nmontagne : Le fils dû guide Wyss, de\\\\nWilderswil, âgé de 17 ans, accompagnait\\\\nvendredi un touriste italien dans l&#39;as-\\\\ncension du Petersgrat En descendant sur\\\\nle glacier de Tschingel, le jeune guide\\\\ntomba dans une crevasse profonde de\\\\n25 mètres. La corde était trop courte\\\\npour l&#39;en retirer, et des guides appelés\\\\nà son secours ne parvinrent pas non\\\\nplus à le dégager. Le jeune homme crie\\\\nqu&#39;il n&#39;est pas blessé. Une nouvelle co-\\\\nlonne de secours est partie samedi de\\\\nLauterbrunnen.\\\\nAarau, 28 juillet.\\\\n&quot;,\\n  &quot;title&quot;: &quot;DERNIÈRES NOUVELLES&quot;,\\n  &quot;gid&quot;: -8329671890893709000,\\n  &quot;begin&quot;: 53,\\n  &quot;end&quot;: 572\\n}\\n{\\n  &quot;uid&quot;: -280074845860282140,\\n  &quot;cluster&quot;: 77309411592,\\n  &quot;size&quot;: 2,\\n  &quot;bw&quot;: 2,\\n  &quot;ew&quot;: 93,\\n  &quot;cc&quot;: true,\\n  &quot;date&quot;: &quot;1900-07-30&quot;,\\n  &quot;id&quot;: &quot;GDL-1900-07-30-a-i0016&quot;,\\n  &quot;series&quot;: &quot;GDL&quot;,\\n  &quot;text&quot;: &quot;NOUVEAUX ACCIOENTS\\\\nInterlaken. 29 juillet.\\\\nLe fils du guide Wyss, de Wilderswil, âgé\\\\nde dix-sept ans, accompagnait, vendredi, un\\\\ntouriste italien dans l&#39;ascension du Peters-\\\\ngrat.\\\\nEn descendant sur le glacier de Tschingel,\\\\nU jeune guide tomba dans une crevasse pro-\\\\nfonde de vingt-cinq mètres. La corde était trop\\\\ncourte pour l&#39;en retirer, et des guides appelés\\\\nà son secours ne parvinrent pas non plus à le\\\\ndégager. Le jeune homme crie qu&#39;il n&#39;est pas\\\\nblessé. Une nouvelle colonne de secours est\\\\npartie samedi de Lauterbrunnen.\\\\nChamonix, 28 juillet.\\\\n&quot;,\\n  &quot;title&quot;: &quot;(Chronique alpestre&quot;,\\n  &quot;gid&quot;: 2328324961100034600,\\n  &quot;begin&quot;: 20,\\n  &quot;end&quot;: 571\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAs you can see from the output above, this cluster contains the same piece of news — a mountain accident which happened in Interlaken on 30 July 1900 — reported by two different newspapers on the very same day with slightly different words.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"using-passims-output\\\"\u003EUsing Passim&#39;s Output\u003C\u002Fh1\u003E\\n\u003Cp\u003ESince the usage of text reuse data ultimately depends on the research questions at hand — and there many possible applications of text reuse, as we have seen above — covering how to use Passim&#39;s output falls beyond the scope of this lesson.\u003C\u002Fp\u003E\\n\u003Cp\u003ECode that &#39;does something&#39; with the data output by Passim can be written in many different programming languages. Extracted clusters can be used to deduplicate documents in a corpus, or even collate together multiple witnesses of the same text, but this will entirely depend on the research context and specific use case.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo given an example of where to go next, for those who want to manipulate and further analyse text reuse data in Python, we provide a Jupyter notebook (\u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fimpresso\u002FPH-Passim-tutorial\u002Fblob\u002Fmaster\u002Fexplore-Passim-output.ipynb\\\"\u003E\u003Ccode\u003Eexplore-Passim-output.ipynb\u003C\u002Fcode\u003E\u003C\u002Fa\u003E) that shows how to import Passim&#39;s JSON output into a \u003Ccode\u003Epandas.DataFrame\u003C\u002Fcode\u003E and how to analyse the distribution of text reuse clusters in both uses cases presented above. For readers that are not familair with the Python library \u003Ccode\u003Epandas\u003C\u002Fcode\u003E, the \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E lesson written by Charlie Harper on \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fvisualizing-with-bokeh\\\"\u003E\u003Cem\u003EVisualizing Data with Bokeh and Pandas\u003C\u002Fem\u003E\u003C\u002Fa\u003E is a nice (and required) introductory reading.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe code contained and explained in the notebook will produce the two plots of Figures 3 and 4, showing how the sizes of text reuse clusters are distributed in the impresso and Bible data respectively.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;plot-impresso.png&quot; caption=&quot;Figure 3. Distribution of text reuse cluster sizes in the impresso sample data.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;plot-bible.png&quot; caption=&quot;Figure 4. Distribution of text reuse cluster sizes in the Bible sample data.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EAs you can see from the plots, in both cases the majority of text reuse clusters contains at most two passages. In the impresso sample data, however, there is much more variance in the size of clusters, with 10% of them having a size comprised between 6 and 296 passages, as opposed to the Bible data where the maximum cluster size is 3.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"further-readings\\\"\u003EFurther readings\u003C\u002Fh1\u003E\\n\u003Cp\u003E\u003Cstrong\u003EPassim\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ESmith et al. (2015) introduce in detail the text reuse detection algorithm implemented in Passim\u003C\u002Fli\u003E\\n\u003Cli\u003ECordell (2015) applied Passim to study text reuse within a large corpus of American newspapers\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003E\u003Cstrong\u003Etextreuse\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EVogler et al. (2020) apply the \u003Ccode\u003Etextreuse\u003C\u002Fcode\u003E R package (Mullen 2016) to study the phenomenon of \u003Cem\u003Emedia concentration\u003C\u002Fem\u003E in contemporary journalism\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003E\u003Cstrong\u003ETRACER\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EBüchler et al. (2014) explain the algorithms for text reuse detection that are implemented in TRACER;\u003C\u002Fli\u003E\\n\u003Cli\u003EFranzini et al. (2018) use and evaluate TRACER for the extraction of quotations from a Latin text (the \u003Cem\u003ESumma contra Gentiles\u003C\u002Fem\u003E of Thomas Aquinas)\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003E\u003Cstrong\u003EBLAST\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EVierthaler et al. (2019) use the BLAST alignment algorithm to detect reuse in Chinese texts\u003C\u002Fli\u003E\\n\u003Cli\u003EVesanto et al. (2017) and Salmi et al. (2019) apply BLAST to a comprehensive corpus of newspapers published in Finland\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch1 id=\\\"acknowledgements\\\"\u003EAcknowledgements\u003C\u002Fh1\u003E\\n\u003Cp\u003EA sincere thanks goes to Marco Büchler and Ryan Muther for reviewing this lesson, as well as to our colleagues Marten Düring and David Smith for their constructive feedback on an early version of this tutorial. Additional thanks go to Anna-Maria Sichani for serving as editor.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe authors warmly thank the newspaper \u003Ca href=\\\"https:\u002F\u002Fletemps.ch\u002F\\\"\u003ELe Temps\u003C\u002Fa\u003E — owner of \u003Cem\u003ELa Gazette de Lausanne\u003C\u002Fem\u003E (GDL) and the \u003Cem\u003EJournal de Genève\u003C\u002Fem\u003E (JDG) — and the group \u003Ca href=\\\"https:\u002F\u002Fwww.arcinfo.ch\u002F\\\"\u003EArcInfo\u003C\u002Fa\u003E — owner of \u003Cem\u003EL’Impartial\u003C\u002Fem\u003E (IMP) and \u003Cem\u003EL’Express\u003C\u002Fem\u003E (EXP) —  for accepting to share their data for academic purposes.\u003C\u002Fp\u003E\\n\u003Cp\u003EMR gratefully acknowledges the financial support of the Swiss National Science Foundation (SNSF) for the project \u003Ca href=\\\"https:\u002F\u002Fimpresso-project.ch\u002F\\\"\u003E\u003Cem\u003Eimpresso – Media Monitoring of the Past\u003C\u002Fem\u003E\u003C\u002Fa\u003E under grant number CR-SII5_173719. SH&#39;s work was supported by the European Union’s Horizon 2020 research and innovation programme under grant 770299 (\u003Ca href=\\\"https:\u002F\u002Fwww.newseye.eu\u002F\\\"\u003ENewsEye\u003C\u002Fa\u003E). SH was affiliated with the University of Helsinki and the University of Geneva for most of this work, and is currently funded by the project \u003Cem\u003ETowards Computational Lexical Semantic Change Detection\u003C\u002Fem\u003E supported by the Swedish Research Council (20192022; dnr 2018-01184).\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"bibliography\\\"\u003EBibliography\u003C\u002Fh1\u003E\\n\u003Col\u003E\\n\u003Cli\u003EGreta Franzini, Maria Moritz, Marco Büchler, Marco Passarotti. Using and evaluating TRACER for an Index fontium computatus of the Summa contra Gentiles of Thomas Aquinas. In \u003Cem\u003EProceedings of the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018)\u003C\u002Fem\u003E. (2018). \u003Ca href=\\\"http:\u002F\u002Fceur-ws.org\u002FVol-2253\u002Fpaper22.pdf\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EDavid A. Smith, Ryan Cordell, Abby Mullen. Computational Methods for Uncovering Reprinted Texts in Antebellum Newspapers. \u003Cem\u003EAmerican Literary History\u003C\u002Fem\u003E \u003Cstrong\u003E27\u003C\u002Fstrong\u003E, E1–E15 Oxford University Press, 2015. \u003Ca href=\\\"http:\u002F\u002Fdx.doi.org\u002F10.1093\u002Falh\u002Fajv029\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003ERyan Cordell. Reprinting Circulation, and the Network Author in Antebellum Newspapers. \u003Cem\u003EAmerican Literary History\u003C\u002Fem\u003E \u003Cstrong\u003E27\u003C\u002Fstrong\u003E, 417–445 Oxford University Press (OUP), 2015. \u003Ca href=\\\"http:\u002F\u002Fdx.doi.org\u002F10.1093\u002Falh\u002Fajv028\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EDaniel Vogler, Linards Udris, Mark Eisenegger. Measuring Media Content Concentration at a Large Scale Using Automated Text Comparisons. \u003Cem\u003EJournalism Studies\u003C\u002Fem\u003E \u003Cstrong\u003E0\u003C\u002Fstrong\u003E, 1–20 Taylor &amp; Francis, 2020. \u003Ca href=\\\"http:\u002F\u002Fdx.doi.org\u002F10.1080\u002F1461670x.2020.1761865\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003ELincoln Mullen. textreuse: Detect Text Reuse and Document Similarity. (2016). \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fropensci\u002Ftextreuse\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EMarco Büchler, Philip R. Burns, Martin Müller, Emily Franzini, Greta Franzini. Towards a Historical Text Re-use Detection. 221–238 In \u003Cem\u003EText Mining: From Ontology Learning to Automated Text Processing Applications\u003C\u002Fem\u003E. Springer International Publishing, 2014. \u003Ca href=\\\"http:\u002F\u002Fdx.doi.org\u002F10.1007\u002F978-3-319-12655-5_11\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EPaul Vierthaler, Meet Gelein. A BLAST-based, Language-agnostic Text Reuse Algorithm with a MARKUS Implementation and Sequence Alignment Optimized for Large Chinese Corpora. \u003Cem\u003EJournal of Cultural Analytics\u003C\u002Fem\u003E (2019). \u003Ca href=\\\"http:\u002F\u002Fdx.doi.org\u002F10.22148\u002F16.034\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EAleksi Vesanto, Asko Nivala, Heli Rantala, Tapio Salakoski, Hannu Salmi, Filip Ginter. Applying BLAST to Text Reuse Detection in Finnish Newspapers and Journals, 1771-1910. 54–58 In \u003Cem\u003EProceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language\u003C\u002Fem\u003E. Linköping University Electronic Press, 2017. \u003Ca href=\\\"https:\u002F\u002Fwww.aclweb.org\u002Fanthology\u002FW17-0510\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EHannu Salmi, Heli Rantala, Aleksi Vesanto, Filip Ginter. The long-term reuse of text in the Finnish press, 1771–1920. \u003Cstrong\u003E2364\u003C\u002Fstrong\u003E, 394–544 In \u003Cem\u003ECEUR Workshop Proceedings\u003C\u002Fem\u003E. (2019).\u003C\u002Fli\u003E\\n\u003Cli\u003EAxel J Soto, Abidalrahman Mohammad, Andrew Albert, Aminul Islam, Evangelos Milios, Michael Doyle, Rosane Minghim, Maria Cristina de Oliveira. Similarity-Based Support for Text Reuse in Technical Writing. 97–106 In \u003Cem\u003EProceedings of the 2015 ACM Symposium on Document Engineering\u003C\u002Fem\u003E. ACM, 2015. \u003Ca href=\\\"http:\u002F\u002Fdx.doi.org\u002F10.1145\u002F2682571.2797068\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EAlexandra Schofield, Laure Thompson, David Mimno. Quantifying the Effects of Text Duplication on Semantic Models. 2737–2747 In \u003Cem\u003EProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\u003C\u002Fem\u003E. Association for Computational Linguistics, 2017. \u003Ca href=\\\"http:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FD17-1290\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EMatteo Romanello, Aurélien Berra, Alexandra Trachsel. Rethinking Text Reuse as Digital Classicists. \u003Cem\u003EDigital Humanities conference\u003C\u002Fem\u003E, 2014. \u003Ca href=\\\"https:\u002F\u002Fwiki.digitalclassicist.org\u002FText_Reuse\\\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\"}"}</script></div>
	</body>
</html>
