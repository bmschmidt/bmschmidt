{"metadata":{"title":"Cleaning OCR’d text with Regular Expressions","layout":"lesson","date":"2013-05-22T00:00:00.000Z","authors":["Laura Turner O'Hara"],"editors":["Fred Gibbs"],"difficulty":2,"activity":"transforming","topics":["data-manipulation"],"abstract":"Optical Character Recognition (OCR)—the conversion of scanned images to machine-encoded text—has proven a godsend for historical research. This lesson will help you clean up OCR'd text to make it more usable.","exclude_from_check":["reviewers","review-ticket"],"redirect_from":"/lessons/cleaning-ocrd-text-with-regular-expressions","avatar_alt":"A typesetter and inker at work on a printing press","doi":"10.46430/phen0024"},"html_body":"<p>{% include toc.html %}</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>Optical Character Recognition (OCR)—the conversion of scanned images to\nmachine-encoded text—has proven a godsend for historical research. This\nprocess allows texts to be searchable on one hand and more easily parsed\nand mined on the other. But we’ve all noticed that the OCR for historic\ntexts is far from perfect. Old type faces and formats make for unique\nOCR. Take for example, this page from the <em>Congressional Directory</em> from\nthe 50th Congress (1887). The PDF scan downloaded from <a href=\"http://home.heinonline.org/\" title=\"Source for Legal and Government-based documents\">HeinOnline</a>\nlooks organized:</p>\n<p>{% include figure.html filename=&quot;cd_pdf.png&quot; caption=&quot;This is a screenshot of the PDF page.&quot; %}</p>\n<p>However, the OCR layer (downloaded as a text file*) shows that the\nmachine-encoded text is not nearly as neat:</p>\n<p>{% include figure.html filename=&quot;cd_txt.png&quot; caption=&quot;This is a screenshot of the OCR.&quot; %}</p>\n<blockquote>\n<p>Note: If you do not have the option to download a text file, you can\nuse the <a href=\"http://www.unixuser.org/~euske/python/pdfminer/index.html\" title=\"PDF Miner Module\">pdfminer</a> module to extract text from the pdf.</p>\n</blockquote>\n<p>Since I want to use this to map the Washington residences for Members of\nthese late 19th-century Congresses, how might I make this data more\nuseable?</p>\n<p>The answer is Regular Expressions or “regex.” Here’s what regex did for\nme. Though this is not a “real” CSV file (the commas are not quite\nright), it can be easily viewed in Excel and prepped for geocoding. Much\nbetter than the text file from above, right?</p>\n<pre><code class=\"language-text\">Aldrich, N. W,Providence, R. I\nAllison, William B, Dubuque, Iowa,24Vermont avenue,\nBate, William,Nashville, Ten, Ebbitt House\nBeck, James B,Lexington, Ky\nBerry, James I, Bentonville, Ark, National Hotel,\nBlair, I lenry \\V, Manchester, N. H,2o East Capitol stree_._&#39;\nBlodgett, Rufus,Long Branch, N. J\nBowen, Thomas M,Del Norte, Colo\nBrown, Joseph E, Atlanta, Ga, Woodmont Flats,\nButler, M. C,Edgefield, S. C, 1751 P street NW\nCall, Wilkinson, Jacksonville, Fla, 1903 N street NW\nCameron, J. D,Harrisburg, Pa, 21 Lafayette Square,\nChace, Jonathan,Providence, R, I\nChandler, William E, Concord, N. H, 1421 I street NW\nCockrell, Francis M,Warrensburgh,Mo, I518 R street NW\nCoke, Richard,Waco, Tex, 419 Sixth street NW\nColquitt, Alfred I I,Atlanta, Ga, 920 New York avenue\nCullom, Shelby M,Springfield, Ill, 1402 Massachusetts avenue\nDaniel, John W,,Lynchburgh, Va, I7OO Nineteenth st. NW\nDavis, Cushman K, Saint Paul, Minn, 17oo Fifteenth street NW\nDawes, Henry L,Pittsfield, Mass, 1632Rhode Island avenue.\nDolph, Joseph N,Portland, Oregon, 8 Lafayette Square,\nEdmunds, George F, Burlington, Vt, 2111 Massachusetts avenue\nEustis, James B,,New Orleans, La, 1761 N street NW\nEvarts, William M,New York, N. Y, i6oi K street NW\nFarwell, Charles B, Chicago, Ill,\nFaulkner, Charles James, Martinsburgh, W. Va,\nFrye, William P,Lewiston, Me, Hamilton House,\nGeorge, James Z,Jackson, Miss, Metropolitan Hotel\nGibson, Randall Lee, New Orleans, La, 1723 Rhode Island avenue.\nGorman, Arthur P, Laurel, Md .,1403 K street NW\nGray, George,Wilmington, Del,\nHale, Eugene,Ellsworth, Me, 917 Sixthteenth st. NW\nHampton, Wade, Columbia, S. C,\nHarris, Isham G, Memphis,Tenn, 13 First street NE\nHawley, Joseph R,Hartford, Corn, 1514 K street NW\nHearst, George,San Francisco, Cal,\nHiscock, Frank, Syracuse, N. Y, Arlington Hotel\nHoar, George F, Worcester, Mass, 1325 K street NW\nIngalls, John James, Atchison, Kans, I B street NW\nJones, James K,Washington, Ark, 915 M street NW\nJones, John P,Gold Hill, Nev\nKenna, John E,Charleston, W. Va, 14o B street NW\nMcPherson, John ,Jersey City, N. J, 1014 Vermont avenue,\nManderson, CharlesF. Omaha, Nebr,The Portland\nMorgan, John T,.Selma, Ala,I 13 First street NE\nMorrill, Justin S, Stratford, Vt, x Thomas Circle\n</code></pre>\n<h2 id=\"regular-expressions-regex\">Regular Expressions (Regex)</h2>\n<p>Regex is not a programming language. Rather it follows a syntax used in\nmany different languages, employing a series of characters to find\nand/or replace precise patterns in texts. For example, using this sample\ntext:</p>\n<pre><code>Let&#39;s get all this bad OCR and $tuff. Gr8!\n</code></pre>\n<p>1. You could isolate all the capital letters (L, O, C, R, G) with this\nregex:</p>\n<pre><code>[A-Z]\n</code></pre>\n<p>2. You could isolate the first capital letter (L) with this regex:</p>\n<pre><code>^[A-Z]\n</code></pre>\n<p>3. You could isolate all characters BUT the capital letters with this\nregex:</p>\n<pre><code>[^A-Z]\n</code></pre>\n<p>4. You could isolate the acronym “OCR” with this regex:</p>\n<pre><code>[A-Z]{3}\n</code></pre>\n<p>5. You could isolate the punctuation using this regex:</p>\n<pre><code>[[:punct:]]\n</code></pre>\n<p>6. You could isolate all the punctuation, spaces, and numbers this way:</p>\n<pre><code>[[:punct:], ,0-9]\n</code></pre>\n<p>The character set is not that large, but the patterns can get\ncomplicated. Moreover, different characters can mean different things\ndepending on their placement. Take for example, the difference between\nexample 2 and example 3 above. In example 2, the caret (^) means\nisolate the pattern at the beginning of the line or document. However,\nwhen you put the caret inside the character class (demarcated by <code>[]</code>) it\nmeans “except” these sets of characters.</p>\n<p>The best way to understand Regular Expressions is to learn what the\ncharacters do in different positions and practice, practice, practice.\nAnd since experimentation is best way to learn, I suggest using a regex\ntester tool and experiment with the syntax. For Mac users, I had a lot\nof luck with the <a href=\"http://krillapps.com/patterns/\" title=\"Patterns App for RegEx Experimentation\">Patterns App</a> (Mac Store $2.99), which allowed me\nto see what the regular expressions were doing in real time. It also\ncomes with a built-in cheat sheet for the symbols, but I actually found\nthis generic (meaning it works across languages) <a href=\"https://cheatography.com/davechild/cheat-sheets/regular-expressions/\" title=\"Reg Ex Cheat Sheet\">cheat sheet</a> more\ncomprehensive.</p>\n<h2 id=\"python-and-regex\">Python and Regex</h2>\n<p>In this tutorial, I use the Regular Expressions Python module to extract\na “cleaner” version of the <em>Congressional Directory</em> text file. Though\nthe <a href=\"http://docs.python.org/2/library/re.html\" title=\"Re Module Documentation\">documentation</a> for this module is fairly comprehensive, beginners\nwill have more luck with the simpler <a href=\"http://docs.python.org/2/howto/regex.html#regex-howto\" title=\"Reuglar Expressions HOWTO\">Regular Expression HOWTO\ndocumentation</a>.</p>\n<h3 id=\"two-things-to-note-before-you-get-started\">Two things to note before you get started</h3>\n<ul>\n<li>From what I’ve observed, Python is <em>not</em> the most efficient way to\nuse Regular Expressions if you have to clean a single document.\nCommand Line programs like <a href=\"http://www.gnu.org/software/sed/\" title=\"GNU&#39;s sed editor\">sed</a> or <a href=\"http://www.gnu.org/software/grep/\" title=\"GNU&#39;s grep editor\">grep</a> appear to be more\nefficient for this process. (I will leave it to the better grep/sed\nusers to create tutorials on those tools.) I use Python for several\nreasons: 1) I understand the syntax best; 2) I appreciate seeing\neach step written out in a single file so I can easily backtrack\nmistakes; and 3) I want a program I could use over and over again,\nsince I am cleaning multiple pages from the <em>Congressional\nDirectory</em>.</li>\n<li>The OCR in this document is far from consistent (within a single\npage or across multiple pages). Thus, the results of this cleaning\ntutorial are not perfect. <strong>My goal is to let regex do the heavy\nlifting and export a document in my chosen format that is <em>more</em>\norganized than the document with which I started.</strong> This\nsignificantly reduces, but does not eliminate, any hand-cleaning I\nmight need to do before geocoding the address data.</li>\n</ul>\n<h3 id=\"my-example-python-file\">My example Python File</h3>\n<p>Here’s the Python file that I used to created to clean my document:</p>\n<pre><code class=\"language-python\">#cdocr.py\n#strip the punctuation and extra information from HeinOnline text document\n\n#import re module\nimport re\n\n#Open the text file, and read the text file into a list\nwith open(&#39;../../data/txt/50-1-p1.txt&#39;) as ocr:\n    Text = ocr.readlines()\n\n#Create an empty list to fill with lines of corrected text\nCleanText = []\n\n# checks each line in the imported text file for all the following patterns\nfor line in Text:\n    #lines with multi-dashes contain data - searches for those lines\n    # -- does not isolate intro text lines with one dash.\n    dashes = re.search(&#39;(--+)&#39;, line)\n\n    #isolates lines with dashes and cleans\n    if dashes:\n        #replaces dashes with my chosen delimiter\n        nodash = re.sub(&#39;.(-+)&#39;, &#39;,&#39;, line)\n        #strikes multiple periods\n        nodots = re.sub(&#39;.(\\.\\.+)&#39;, &#39;&#39;, nodash)\n        #strikes extra spaces\n        nospaces = re.sub(&#39;(  +)&#39;, &#39;,&#39;, nodots)\n        #strikes *\n        nostar = re.sub(&#39;.[*]&#39;, &#39;&#39;, nospaces)\n        #strikes new line and comma at the beginning of the line\n        flushleft = re.sub(&#39;^\\W&#39;, &#39;&#39;, nostar)\n        #getting rid of double commas (i.e. - Evarts)\n        comma = re.sub(&#39;,{2,3}&#39;, &#39;,&#39;, flushleft)\n        #cleaning up some words that are stuck together (i.e. -  Dawes, Manderson)\n        #skips double OO that was put in place of 00 in address\n        caps = re.sub(&#39;[A-N|P-Z]{2,}&#39;, &#39;,&#39;, comma)\n        #Clean up NE and NW quadrant indicators by removing periods\n        ne = re.sub(&#39;(\\,*? N\\. ?E.)&#39;, &#39; NE&#39;, caps)\n        nw = re.sub(&#39;(\\,*? N\\. ?W[\\.\\,]*?_?)$&#39;, &#39; NW&#39;, ne) #MAKE VERBOSE\n        #Replace periods with commas between last and first names (i.e. - Chace, Cockrell)\n        match = re.search(&#39;^([A-Z][a-z]+\\. )&#39;, nw) #MAKE VERBOSE\n        if match:\n            names = re.sub(&#39;\\.&#39;, &#39;,&#39;, nw)\n        else:\n            names = nw\n           #Append each line to CleanText list while it loops through\n        CleanText.append(names)\n\n#Saving into a &#39;fake&#39; csv file\nwith open(&#39;cdocr2/50-1p1.csv&#39;, &#39;w&#39;) as fcsv:\n    #Write each line in CleanText to a file\n    for line in CleanText:\n        fcsv.write(line)\n</code></pre>\n<p>I’ve commented it pretty extensively, so I will explain why I structured\nthe code the way I did. I will also demonstrate a different way to\nformat long regular expressions for better legibility.</p>\n<ul>\n<li><strong>Lines 16-22</strong> – Notice in my original text file that my data is\nall on lines with multiple dashes. This code effectively isolates\nthose lines. I use the <a href=\"http://docs.python.org/2/library/re.html#re.search\" title=\"Explanation of re.search() function\">re.search()</a> function to find all lines\nwith multiple dashes. The “if” statement on line 20 only works with\nthe lines with dashes in the rest of the code. (This eliminates all\nintroductory text and the rows of page numbers that follow the data\nI want.)</li>\n<li><strong>Lines 23-40</strong> – This is the long process by which I eliminate all\nof the extraneous punctuation and put the pieces of my data (last\nname, first name, home post office, washington address) into\ndifferent fields for a csv document. I use the <a href=\"http://docs.python.org/2/library/re.html#re.sub\" title=\"Explanation of re.sub() function\">re.sub()</a>\nfunction, which substitutes pattern with another character. I\ncomment extensively here, so you can see what each piece does. This\nmay not be the most efficient way of doing this, but by doing this\npiece by piece, I could check my work as I went. As I built loop, I\nchecked each step by printing the variable in the command line. So,\nfor example, after line 24 (when I eliminate the dashes), I would\nadd “print nodash” (inside the if loop) before I ran the file in the\ncommand line. I checked each step to make sure my patterns were only\nchanging the things I wanted and not changing things I did <em>not</em>\nwant changed.</li>\n<li><strong>Lines 41-46</strong> - I used a slightly different method here. The OCR\nin the text file separated some names with a period (for example,\nChace.Jonathan vs. Chase,Jonathan). I wanted to isolate the periods\nthat came up in this pattern and change those periods to commas. So\nI searched for the pattern <code>^([A-Z][a-z]+\\.)</code>, which looks at the\nbeginning of a line (^) and finds a pattern with one capital\nletter, multiple lowercase letters and a period. After I had\nisolated that pattern, I substitute the period those lines that fit\nthe pattern with a comma.</li>\n</ul>\n<h3 id=\"using-verbose-mode\">Using Verbose Mode</h3>\n<p>Most regular expressions are difficult to read. But lines 39 and 40 look\n<em>especially</em> bad. How might you clarify these patterns for people who\nmight look at your code (or for yourself when you are staring at them at\n2:00 AM someday)? You can use the module’s <a href=\"http://docs.python.org/2/library/re.html#re.VERBOSE\" title=\"Explanation of re.verbose mode\">verbose mode</a>. By putting\nyour patterns in verbose mode, python ignores white space and the #\ncharacter, so you can split the patterns across multiple lines and\ncomment each piece. <em><strong>Keep in mind that, because it ignores spaces, if\nspaces are part of your pattern, you need to escape them with a\nbackslash (\\). Also note that re.VERBOSE and re.X are the same\nthing.</strong></em></p>\n<p>Here are lines 39 and 40 in verbose mode:</p>\n<pre><code class=\"language-python\">#This is the same as (\\,*? N\\. ?E.)\n#All spaces need to be escaped in verbose mode.\nne_pattern = re.compile(r&#39;&#39;&#39;\n    (               #start group\n        \\,*?        #look for comma (escaped); *? = 0 or more commas with fewest results\n        \\ N\\.?      #look for (escaped) space + N that might have an (escaped) period after it\n        \\ ?E        #look for an E that may or may not have an space in front of it\n        .           #the E might be followed by another character.\n    )               #close group\n    $               #ONLY look at the end of a line\n&#39;&#39;&#39;, re.VERBOSE)\n\n#This is the same as (\\,*? N\\. ?W[\\.\\,]*?_?)$\nnw_pattern = re.compile(r&#39;&#39;&#39;\n    (                   #start group\n        \\,*?            #look for comma (escaped); *? = 0 or more commas with fewest results\n        \\ N\\.?          #look for (escaped) space + N that might have an (escaped) period after it\n        \\ ?W            #look for an W that may or may not have an space in front of it\n        [\\.\\,]*?        #look for commas or periods (both escaped) that might come after W\n        _?              #look for underscore that comes after one of these NW quadrant indicators\n    )                   #close group\n    $                   #ONLY look at the end of a line\n&#39;&#39;&#39;, re.X)\n</code></pre>\n<p>In above example, I use the <a href=\"http://docs.python.org/2/library/re.html#re.compile\" title=\"Explanation of re.compile() function\">re.compile()</a> function to save the\npattern for future use. So, adjusting my full python code to use verbose\nmode would look like the following. Note that I define my verbose\npatterns on lines 17-39 and store them in variables (ne_pattern and\nnw_pattern). I use them in my loop on lines 65 and 66.</p>\n<pre><code class=\"language-python\">#cdocrverbose.py\n#strip the punctuation and extra information from HeinOnline text document\n\n#import re module\nimport re\n\n#Open the text file, and read the text file into a list\nwith open(&#39;../../data/txt/50-1-p1.txt&#39;) as ocr:\n    Text = ocr.readlines()\n\n#Create an empty list to fill with lines of corrected text\nCleanText = []\n\n##Creating verbose patterns for the more complicated pieces that I use later on.##\n\n#This is the same as (\\,*? N\\. ?E.)\n#All spaces need to be escaped in verbose mode.\nne_pattern = re.compile(r&#39;&#39;&#39;\n    (               #start group\n        \\,*?        #look for comma (escaped); *? = 0 or more commas with fewest results\n        \\ N\\.?      #look for (escaped) space + N that might have an (escaped) period after it\n        \\ ?E        #look for an E that may or may not have an space in front of it\n        .           #the E might be followed by another character.\n    )               #close group\n    $               #ONLY look at the end of a line\n&#39;&#39;&#39;, re.VERBOSE)\n\n#This is the same as (\\,*? N\\. ?W[\\.\\,]*?_?)$\nnw_pattern = re.compile(r&#39;&#39;&#39;\n    (                   #start group\n        \\,*?            #look for comma (escaped); *? = 0 or more commas with fewest results\n        \\ N\\.?          #look for (escaped) space + N that might have an (escaped) period after it\n        \\ ?W            #look for an W that may or may not have an space in front of it\n        [\\.\\,]*?        #look for commas or periods (both escaped) that might come after W\n        _?              #look for underscore that comes after one of these NW quadrant indicators\n    )                   #close group\n    $                   #ONLY look at the end of a line\n&#39;&#39;&#39;, re.VERBOSE)\n\n# checks each line in the imported text file for all the following patterns\nfor line in Text:\n    #lines with multi-dashes contain data - searches for those lines\n    # -- does not isolate intro text lines with one dash.\n    dashes = re.search(&#39;(--+)&#39;, line)\n\n    #isolates lines with dashes and cleans\n    if dashes:\n        #replaces dashes with my chosen delimiter\n        nodash = re.sub(&#39;.(-+)&#39;, &#39;,&#39;, line)\n        #strikes multiple periods\n        nodots = re.sub(&#39;.(\\.\\.+)&#39;, &#39;&#39;, nodash)\n        #strikes extra spaces\n        nospaces = re.sub(&#39;(  +)&#39;, &#39;,&#39;, nodots)\n        #strikes *\n        nostar = re.sub(&#39;.[*]&#39;, &#39;&#39;, nospaces)\n        #strikes new line and comma at the beginning of the line\n        flushleft = re.sub(&#39;^\\W&#39;, &#39;&#39;, nostar)\n        #getting rid of double commas (i.e. - Evarts)\n        comma = re.sub(&#39;,{2,3}&#39;, &#39;,&#39;, flushleft)\n        #cleaning up some words that are stuck together (i.e. -  Dawes, Manderson)\n        #skips double OO that was put in place of 00 in address\n        caps = re.sub(&#39;[A-N|P-Z]{2,}&#39;, &#39;,&#39;, comma)\n        #Clean up NE and NW quadrant indicators by removing periods (using Verbose regex defined above)\n        ne = re.sub(ne_pattern, &#39; NE&#39;, caps)\n        nw = re.sub(nw_pattern, &#39; NW&#39;, ne)\n        #Replace periods with commas between last and first names (i.e. - Chace, Cockrell)\n        match = re.search(&#39;^([A-Z][a-z]+\\.)&#39;, nw)\n        if match:\n            names = re.sub(&#39;\\.&#39;, &#39;,&#39;, nw)\n        else:\n            names = nw\n         #Append each line to CleanText list while it loops through\n        CleanText.append(names)\n\n#Saving into a &#39;fake&#39; csv file\nwith open(&#39;cdocr2/50-1p1.csv&#39;, &#39;w&#39;) as fcsv:\n    #Write each line in CleanText to a file\n    for line in CleanText:\n        fcsv.write(line)\n</code></pre>\n<p>In conclusion, I will note that this is not for the faint of heart.\nRegular Expressions are powerful. Yes, they are powerful enough to\ncompletely destroy your data. So practice on copies and take it one itty\nbitty step at a time.</p>\n"}