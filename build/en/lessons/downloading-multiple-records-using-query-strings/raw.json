{"metadata":{"title":"Downloading Multiple Records Using Query Strings","layout":"lesson","date":"2012-11-11T00:00:00.000Z","authors":["Adam Crymble"],"reviewers":["Luke Bergmann","Sharon Howard","Frederik Elwert"],"editors":["Fred Gibbs"],"difficulty":2,"exclude_from_check":["review-ticket"],"activity":"acquiring","topics":["web-scraping"],"abstract":"Downloading a single record from a website is easy, but downloading many records at a time – an increasingly frequent need for a historian – is much more efficient using a programming language such as Python. In this lesson, we will write a program that will download a series of records from the Old Bailey Online using custom search criteria, and save them to a directory on our computer.","previous":"output-keywords-in-context-in-html-file","series_total":"15 lessons","sequence":15,"python_warning":false,"redirect_from":"/lessons/downloading-multiple-records-using-query-strings","avatar_alt":"Figures working in a mine, pushing carts","doi":"10.46430/phen0005"},"html_body":"<p>{% include toc.html %}</p>\n<h2 id=\"module-goals\">Module Goals</h2>\n<p>Downloading a single record from a website is easy, but downloading many\nrecords at a time – an increasingly frequent need for a historian – is\nmuch more efficient using a programming language such as Python. In this\nlesson, we will write a program that will download a series of records\nfrom the <a href=\"http://www.oldbaileyonline.org/\">Old Bailey Online</a> using custom search criteria, and save\nthem to a directory on our computer. This process involves interpreting\nand manipulating URL <em>Query Strings</em>. In this case, the tutorial will seek\nto download sources that contain references to people of African descent\nthat were published in the <em>Old Bailey Proceedings</em> between 1700 and 1750.</p>\n<div class=\"alert alert-warning\">\nThe examples in this lesson include historic racialized language that readers may find offensive. The author does not condone the use of this language but has tried to use it in its historic context, recognizing that it is otherwise impossible to find the desired materials of the case study. Anyone teaching with this material is advised to take a sensitive approach towards the language and to apply best practices when teaching about race. The author recommends the many resources from Teaching Tolerance (https://www.tolerance.org); Peggy McIntosh, ‘White Privilege: Unpacking the Invisible Knapsack’ *Peace and Freedom Magazine*, (1989), 10-12; Binyavanga Wainaina, ‘How to Write About Africa’, *Granta* (92): 2006.\n</div>\n\n<h2 id=\"for-whom-is-this-useful\">For Whom is this Useful?</h2>\n<p>Automating the process of downloading records from an online database\nwill be useful for anyone who works with historical sources that are\nstored online in an orderly and accessible fashion and who wishes to\nsave copies of those sources on their own computer. It is particularly\nuseful for someone who wants to download many specific records, rather\nthan just a handful. If you want to download <em>all</em> or <em>most</em> of the\nrecords in a particular database, you may find Ian Milligan’s tutorial\non <a href=\"/lessons/automated-downloading-with-wget\">Automated Downloading with WGET</a> more suitable.</p>\n<p>The present tutorial will allow you to download discriminately,\nisolating specific records that meet your needs. Downloading multiple\nsources automatically saves considerable time. What you do with the\ndownloaded sources depends on your research goals. You may wish to\ncreate visualizations or perform various data analysis methods, or\nsimply reformat them to make browsing easier. Or, you may just want to\nkeep a backup copy so you can access them without Internet access.</p>\n<p>This lesson is for intermediate Python users. If you have not already tried the <a href=\"/lessons/introduction-and-installation\">Python Programming Basics</a> lessons, you may find that a useful starting point.</p>\n<h2 id=\"applying-our-historical-knowledge\">Applying our Historical Knowledge</h2>\n<p>In this lesson, we are trying to create our own corpus of cases related\nto people of African descent. From <a href=\"http://www.oldbaileyonline.org/browse.jsp?id=t17800628-33&amp;div=t17800628-33\">Benjamin Bowsey’s case</a> at the Old Bailey in 1780, we might note that “black” can be a useful keyword for us to use for locating other\ncases involving defendants of African descent. However, when we search for “black” on the Old Bailey website, we\nfind it often refers to other uses of the word: black horses, or black\ncloth. The task of disambiguating this use of language will have to wait\nfor another lesson. For now, let’s turn to easier cases. As historians,\nwe can probably think of keywords of historic racialised terms related to African descendants that\nwould be worth pursuing. The infamous “n-word” of course is not useful,\nas this term did not come into regular usage until the mid-nineteenth\ncentury. Other racialised language such as “negro” and “mulatto” are however, much more relevant to the\nearly eighteenth century. These keywords are less ambiguous than “black”\nand are much more likely to be immediate references to people in our\ntarget demographic. If we try these two terms in separate simple\nsearches on the Old Bailey website, we get results like in these\nscreenshots:</p>\n<p>{% include figure.html filename=&quot;SearchResultsNegro.png&quot; caption=&quot;Search results for &#39;negro&#39; in the Old Bailey Online&quot; %}</p>\n<p>{% include figure.html filename=&quot;SearchResultsMulatto.png&quot; caption=&quot;Search results for &#39;mulatto&#39; in the Old Bailey Online&quot; %}</p>\n<p>After glancing through these search results, it seems clear that these\nare references to people, rather than horses or cloth or other things\nthat may be black. We want to download them all to use in our analysis.\nWe could, of course, download them one at a time, manually. But let’s\nfind a programmatic way to automate this task.</p>\n<h2 id=\"the-advanced-search-on-obo\">The Advanced Search on OBO</h2>\n<p>Every website’s search features work differently. While searches work\nsimilarly, the intricacies of database searches may not be entirely\nobvious. Therefore it’s important to think critically about database\nsearch options and, when available, read the documentation provided on\nthe website. Prudent historical researchers always interrogate their\nsources; the procedures behind your search boxes should receive the same\nattention. The Old Bailey Online’s <a href=\"http://www.oldbaileyonline.org/forms/formMain.jsp\">advanced search form</a> lets you\nrefine your searches based on ten different fields including simple\nkeywords, a date range, and a crime type. As each website’s search\nfeature is different it always pays to take a moment or two to play with\nand read about the search options available. Since\nwe have already done the simple searches for “negro” and “mulatto”, we\nknow there will be results. However, let’s use the advanced search to\nlimit our results to records published in the Old Bailey Proceedings\ntrial accounts from 1700 to 1750 only. You can of course change this to\nwhatever you like, but this will make the example easier to follow.\nPerform the search shown in the image below. Make sure you tick the\n“Advanced” radio button and include the <code>*</code> wildcards to include\npluralized entries or those with an extra “e” on the end.</p>\n<p>{% include figure.html filename=&quot;AdvancedSearchExample.png&quot; caption=&quot;Old Bailey Advanced Search Example&quot; %}</p>\n<p>Execute the search and then click on the “Calculate Total” link to\nsee how many entries there are. We now have 13 results (if you have a\ndifferent number go back and make sure you copied the example above\nexactly). What we want to do at this point is download all of these\ntrial documents and analyze them further. Again, for only 13 records,\nyou might as well download each record manually. But as more and more\ndata comes online, it becomes more common to need to download 1,300 or\neven 130,000 records, in which case downloading individual records\nbecomes impractical and an understanding of how to automate the process\nbecomes that much more valuable. To automate the download process, we\nneed to step back and learn how the search URLs are created on the Old\nBailey website, a method common to many online databases and websites.</p>\n<h2 id=\"understanding-url-queries\">Understanding URL Queries</h2>\n<p>Take a look at the URL produced with the last search results page. It\nshould look like this:</p>\n<pre><code>https://www.oldbaileyonline.org/search.jsp?gen=1&amp;form=searchHomePage&amp;_divs_fulltext=mulatto*+negro*&amp;kwparse=advanced&amp;_divs_div0Type_div1Type=sessionsPaper_trialAccount&amp;fromYear=1700&amp;fromMonth=00&amp;toYear=1750&amp;toMonth=99&amp;start=0&amp;count=0\n</code></pre>\n<p>We had a look at URLs in <a href=\"/lessons/viewing-html-files\">Viewing HTML Files</a>, but this looks a lot\nmore complex. Although longer, it is actually <em>not</em> that much more\ncomplex. But it is easier to understand by noticing how our search\ncriteria get represented in the URL.</p>\n<pre><code>https://www.oldbaileyonline.org/search.jsp\n?gen=1\n&amp;form=searchHomePage\n&amp;_divs_fulltext=mulatto*+negro*\n&amp;kwparse=advanced\n&amp;_divs_div0Type_div1Type=sessionsPaper_trialAccount\n&amp;fromYear=1700\n&amp;fromMonth=00\n&amp;toYear=1750\n&amp;toMonth=99\n&amp;start=0\n&amp;count=0\n</code></pre>\n<p>In this view, we see more clearly our 12 important pieces of information\nthat we need to perform our search (one per line). On the first is the\nOld Bailey’s base website URL, followed by a query: “?” (don’t worry\nabout the <code>gen=1</code> bit; the developers of the Old Bailey Online say that\nit does not do anything.) and a series of 10 <em>name/value pairs</em> put\ntogether with &amp; characters. Together these 10 name/value pairs comprise\nthe query string, which tells the search engine what variables to use in\nspecific stages of the search. Notice that each name/value pair contains\nboth a variable name: toYear, and then assigns that variable a value: 1750.\nThis works in exactly the same way as <em>Function Arguments</em> by\npassing certain information to specific variables. In this case, the\nmost important variable is <code>_divs_fulltext=</code> which has been given the\nvalue:</p>\n<pre><code>mulatto*+negro*\n</code></pre>\n<p>This holds the search term we have typed into the search box. The\nprogram has automatically added a + sign in place of a blank space (URLs\ncannot contain spaces); otherwise that’s exactly what we’ve asked the\nOld Bailey site to find for us. The other variables hold values that we\ndefined as well. <em>fromYear</em> and <em>toYear</em> contain our date range. Since no\nyear has 99 months as suggested in the <em>toMonth</em> variable, we can assume\nthis is how the search algorithm ensures all records from that year are\nincluded. There are no hard and fast rules for figuring out what each\nvariable does because the person who built the site gets to name them.\nOften you can make an educated guess. All of the possible search fields\non the Advanced Search page have their own name/value pair. If you’d\nlike to find out the name of the variable so you can use it, do a new\nsearch and make sure you put a value in the field in which you are\ninterested. After you submit your search, you’ll see your value and the\nname associated with it as part of the URL of the search results page.\nWith the Old Bailey Online, as with many other websites, the search form\n(advanced or not) essentially helps you to construct URLs that tell the\ndatabase what to search for. If you can understand how the search fields\nare represented in the URL – which is often quite straightforward – then\nit becomes relatively simple to programmatically construct these URLs\nand thus to automate the process of downloading records.</p>\n<p>Now try changing the “<strong>start=0</strong>” to “<strong>start=10</strong>” and hit enter. You\nshould now have results 11-13. The “start” variable tells the website\nwhich entry should be shown at the top of the search results list. We\nshould be able to use this knowledge to create a series of URLs that\nwill allow us to download all 13 files. Let’s turn to that now.</p>\n<h2 id=\"systematically-downloading-files\">Systematically Downloading Files</h2>\n<p>In <a href=\"/lessons/working-with-web-pages\">Working with Webpages</a> we learned that Python can download a\nwebpage as long as we have the URL. In that lesson we used the URL to\ndownload the trial transcript of Benjamin Bowsey. In this case, we’re\ntrying to download multiple trial transcripts that meet the search\ncriteria we outlined above without having to repeatedly re-run the\nprogram. Instead, we want a program that will download everything we\nneed in one go. At this point we have a URL to a search results page\nthat contains the first ten entries of our search. We also know that by\nchanging the “start” value in the URL we can sequentially call each\nsearch results page, and ultimately retrieve all of the trial documents\nfrom them. Of course the research results don’t give us the trial\ndocuments themselves, but only links to them. So we need to extract the\nlink to the underlying records from the search results. On the Old\nBailey Online website, the URLs for the individual records (the trial\ntranscript files) can be found as links on the search results pages. We\nknow that all trial transcript URLs contain a trial id that takes the\nform: “t” followed by at least 8 numbers (e.g. t17800628-33). By looking\nfor links that contain that pattern, we can identify trial transcript\nURLs. As in previous lessons, let’s develop an algorithm so that we can\nbegin tackling this problem in a manner that a computer can handle. It\nseems this task can be achieved in four steps. We will need to:</p>\n<ul>\n<li>Generate the URLs for each search results page by incrementing the\n“start” variable by a fixed amount an appropriate number of times.</li>\n<li>Download each search results page as an HTML file.</li>\n<li>Extract the URLs of each trial transcript (using the trial ID as\ndescribed above) from the search results HTML files.</li>\n<li>Cycle through those extracted URLs to download each trial transcript\nand save it to a directory on our computer</li>\n</ul>\n<p>You’ll recall that this is fairly similar to the tasks we achieved in\n<a href=\"/lessons/working-with-web-pages\">Working with Webpages</a> and <a href=\"/lessons/from-html-to-list-of-words-2\">From HTML to a List of Words 2</a>. First\nwe download, then we parse out the information we’re after. And in this\ncase, we download some more.</p>\n<h3 id=\"downloading-the-search-results-pages\">Downloading the search results pages</h3>\n<p>First we need to generate the URLs for downloading each search results\npage. We have already got the first one by using the form on the\nwebsite:</p>\n<pre><code>https://www.oldbaileyonline.org/search.jsp?gen=1&amp;form=searchHomePage&amp;_divs_fulltext=mulatto*+negro*&amp;kwparse=advanced&amp;_divs_div0Type_div1Type=sessionsPaper_trialAccount&amp;fromYear=1700&amp;fromMonth=00&amp;toYear=1750&amp;toMonth=99&amp;start=0&amp;count=0\n</code></pre>\n<p>We could type this URL out twice and alter the ‘<em>start</em>’ variable to get\nus all 13 entries, but let’s write a program that would work no matter\nhow many search results pages or records we had to download, and no\nmatter what we decide to search for. Study this code and then add this\nfunction to a module named <code>obo.py</code> (create a file with that name and save it to the directory where you want to do your work). The comments in the code are meant to\nhelp you decipher the various parts.</p>\n<pre><code class=\"language-python\"># obo.py\ndef getSearchResults(query, kwparse, fromYear, fromMonth, toYear, toMonth):\n\n    import urllib.request\n\n    startValue = 0\n\n    #each part of the URL. Split up to be easier to read.\n    url = &#39;https://www.oldbaileyonline.org/search.jsp?gen=1&amp;form=searchHomePage&amp;_divs_fulltext=&#39;\n    url += query\n    url += &#39;&amp;kwparse=&#39; + kwparse\n    url += &#39;&amp;_divs_div0Type_div1Type=sessionsPaper_trialAccount&#39;\n    url += &#39;&amp;fromYear=&#39; + fromYear\n    url += &#39;&amp;fromMonth=&#39; + fromMonth\n    url += &#39;&amp;toYear=&#39; + toYear\n    url += &#39;&amp;toMonth=&#39; + toMonth\n    url += &#39;&amp;start=&#39; + str(startValue)\n    url += &#39;&amp;count=0&#39;\n\n    #download the page and save the result.\n    response = urllib.request.urlopen(url)\n    webContent = response.read().decode(&#39;UTF-8&#39;)\n    filename = &#39;search-result&#39;\n    f = open(filename + &quot;.html&quot;, &#39;w&#39;)\n    f.write(webContent)\n    f.close\n</code></pre>\n<p>In this function we have split up the various Query String components\nand used Function Arguments so that this function can be reused beyond\nour specific needs right now. When we call this function we will replace\nthe arguments with the values we want to search for. We then download\nthe search results page in a similar manner as done in <a href=\"/lessons/working-with-web-pages\">Working with\nWebpages</a>. Now, make a new file: <code>download-searches.py</code> and copy into\nit the following code. Note, the values we have passed as arguments are\nexactly the same as those used in the example above. Feel free to play\nwith these to get different results or see how they work.</p>\n<pre><code class=\"language-python\">#download-searches.py\nimport obo\n\nquery = &#39;mulatto*+negro*&#39;\n\nobo.getSearchResults(query, &quot;advanced&quot;, &quot;1700&quot;, &quot;00&quot;, &quot;1750&quot;, &quot;99&quot;)\n</code></pre>\n<p>When you run this code you should find a new file:\n“<code>search-results.html</code>” in your <code>programming-historian directory</code>\ncontaining the first search results page for your search. Check that\nthis downloaded properly and then delete the file. We’re going to adapt\nour program to download the other page containing the other 3 entries at\nthe same time so we want to make sure we get both. Let’s refine our\n<code>getSearchResults</code> function by adding another function argument called\n“entries” so we can tell the program how many pages of search results we\nneed to download. We will use the value of entries and some simple math\nto determine how many search results pages there are. This is fairly\nstraightforward since we know there are ten trial transcripts listed per\npage. We can calculate the number of search results pages by dividing\nthe value of entries by 10. We will save this result to a\nvariable named <code>pageCount</code>. It looks like this:</p>\n<pre><code class=\"language-python\">#determine how many files need to be downloaded.\npageCount = entries / 10\n</code></pre>\n<p>However, in cases where the number of entries is not a multiple of 10,\nthis will result in a decimal number. You can test this by\nrunning this code in your Terminal (Mac &amp; Linux) / Python Command Line\n(Windows) and printing out the value held in <code>pageCount</code>. (Note, from here\non, we will use the word Terminal to refer to this program).</p>\n<pre><code class=\"language-python\">entries = 13\npageCount = entries / 10\nprint(pageCount)\n-&gt; 1.3\n</code></pre>\n<p>We know the page count should be 2 (one page containing entries 1-10, and one\npage containing entries 11-13). Since we always want the next higher integer,\nwe can round up the result of the division.</p>\n<pre><code class=\"language-python\">#determine how many files need to be downloaded.\nimport math\npageCount = entries / 10\npageCount = math.ceil(pageCount)\n</code></pre>\n<p>If we add this to our <code>getSearchResults</code> function just under the\n<em>startValue = 0</em> line, our program, the code can now calculate the number\nof pages that need to be downloaded. However, at this stage it will\nstill only download the first page since we have only told the\ndownloading section of the function to run once. To correct this, we can\nadd that downloading code to a for loop which will download once for\nevery number in the <code>pageCount</code> variable. If it reads 1, then it will\ndownload once; if it reads 5 it will download five times, and so on.\nImmediately after the if statement you have just written, add the\nfollowing line and indent everything down to <code>f.close</code> one additional\ntab so that it is all enclosed in the for loop:</p>\n<pre><code class=\"language-python\">for pages in range(1, pageCount+1):\n    print(pages)\n</code></pre>\n<p>Since this is a for loop, all of the code we want to run repeatedly\nneeds to be intended as well. You can see if you have done this\ncorrectly by looking at the finished code example below. This loop takes\nadvantage of Python’s <a href=\"https://docs.python.org/3/tutorial/controlflow.html#the-range-function\">range</a> funciton. To understand this for loop it\nis probably best to think of <code>pageCount</code> as equal to 2 as it is in the\nexample. This two lines of code then means: start running with an\ninitial loop value of 1, and each time you run, add 1 more to that\nvalue. When the loop value is the same as <code>pageCount</code>, run once more and\nthen stop. This is particularly valuable for us because it means we can\ntell our program to run exactly once for each search results page and\nprovides a flexible new skill for controlling how many times a for loop\nruns. If you would like to practice with this new and powerful way of\nwriting for loops, you can open your Terminal and play around.</p>\n<pre><code class=\"language-python\">pageCount = 2\nfor pages in range(1, pageCount+1):\n    print(pages)\n\n-&gt; 1\n-&gt; 2\n</code></pre>\n<p>Before we add all of this code together to our <code>getSearchResults</code>\nfunction, we have to make two final adjustments. At the end of the for\nloop (but still inside the loop), and after our downloading code has run\nwe will need to change the <code>startValue</code> variable, which is used in\nbuilding the URL of the page we want to download. If we forget to do\nthis, our program will repeatedly download the first search results page\nsince we are not actually changing anything in the initial URL. The\n<code>startValue</code> variable, as discussed above, is what controls which search\nresults page we want to download. Therefore, we can request the next\nsearch results page by increasing the value of <code>startValue</code> by 10 after\nthe initial download has completed. If you are not sure where to put\nthis line you can peek ahead to the finished code example below.</p>\n<p>Finally, we want to ensure that the name of the file we have downloaded\nis different for each file. Otherwise, each download will save over the\nprevious download, leaving us with only a single file of search results.\nTo solve this, we can adjust the contents of the <code>filename</code> variable to\ninclude the value held in <code>startValue</code> so that each time we download a new\npage, it gets a different name. Since <code>startValue</code> is an integer, we will\nhave to convert it to a string before we can add it to the filename\nvariable. Adjust the line in your program that pertains to the <code>filename</code>\nvariable to looks like this:</p>\n<pre><code class=\"language-python\">filename = &#39;search-result&#39; + str(startValue)\n</code></pre>\n<p>You should now be able to add these new lines of code to your\ngetSearchResults function. Recall we have made the following additions:</p>\n<ul>\n<li>Add entries as an additional function argument right after toMonth</li>\n<li>Calculate the number of search results pages and add this\nimmediately after the line that begins with startValue = 0 (before\nwe build the URL and start downloading)</li>\n<li>Follow this immediately with a for loop that will tell the program\nto run once for each search results page, and indent the rest of the\ncode in the function so that it is inside the new loop.</li>\n<li>The last line in the for loop should now increase the value of the\nstartValue variable each time the loop runs.</li>\n<li>Adjust the existing filename variable so that each time a search\nresults page is downloaded it gives the file a unique name.</li>\n</ul>\n<p>The finished function code in your <code>obo.py</code> file should look like this:</p>\n<pre><code class=\"language-python\">#create URLs for search results pages and save the files\ndef getSearchResults(query, kwparse, fromYear, fromMonth, toYear, toMonth, entries):\n\n    import urllib.request, math\n\n    startValue = 0\n\n    #this is new! Determine how many files need to be downloaded.\n    pageCount = entries / 10\n    pageCount = math.ceil(pageCount)\n\n    #this line is new!\n    for pages in range(1, pageCount +1):\n\n        #each part of the URL. Split up to be easier to read.\n        url = &#39;https://www.oldbaileyonline.org/search.jsp?gen=1&amp;form=searchHomePage&amp;_divs_fulltext=&#39;\n        url += query\n        url += &#39;&amp;kwparse=&#39; + kwparse\n        url += &#39;&amp;_divs_div0Type_div1Type=sessionsPaper_trialAccount&#39;\n        url += &#39;&amp;fromYear=&#39; + fromYear\n        url += &#39;&amp;fromMonth=&#39; + fromMonth\n        url += &#39;&amp;toYear=&#39; + toYear\n        url += &#39;&amp;toMonth=&#39; + toMonth\n        url += &#39;&amp;start=&#39; + str(startValue)\n        url += &#39;&amp;count=0&#39;\n\n        #download the page and save the result.\n        response = urllib.request.urlopen(url)\n        webContent = response.read().decode(&#39;UTF-8&#39;)\n        filename = &#39;search-result&#39; + str(startValue)\n        f = open(filename + &quot;.html&quot;, &#39;w&#39;)\n        f.write(webContent)\n        f.close\n\n        #this lines is new!\n        startValue = startValue + 10\n</code></pre>\n<p>To run this new function, add the extra argument to\n<code>download-searches.py</code> and run the program again:</p>\n<pre><code class=\"language-python\">#download-searches.py\nimport obo\n\nquery = &#39;mulatto*+negro*&#39;\n\nobo.getSearchResults(query, &quot;advanced&quot;, &quot;1700&quot;, &quot;00&quot;, &quot;1750&quot;, &quot;99&quot;, 13)\n</code></pre>\n<p>Great! Now we have both search results pages, called\n<code>search-result0.html</code> and <code>search-result10.html</code>. But before we move\nonto the next step in the algorithm, let’s take care of some\nhousekeeping. Our <code>programming-historian</code> directory will quickly become\nunwieldy if we download multiple search results pages and trial\ntranscripts. Let’s have Python make a new directory named after our\nsearch terms.</p>\n<p>We want to add this new functionality in <code>getSearchResults</code>, so that our\nsearch results pages are downloaded to a directory with the same name as\nour search query. This will keep our <code>programming-historian</code> directory\nmore organized. To do this we will create a new directory using the <code>os</code>\nlibrary, short for “operating system”. That library contains a function\ncalled <code>makedirs</code>, which, unsurprisingly, makes a new directory. You can\ntry this out using the Terminal.</p>\n<pre><code class=\"language-python\">import os\n\nquery = &quot;myNewDirectory&quot;\nif not os.path.exists(query):\n    os.makedirs(query)\n</code></pre>\n<p>This program will check to see if your computer already has a directory\nwith this name. If not, you should now have a directory called\n<code>myNewDirectory</code> on your computer. On a Mac this is probably located in\nyour <code>/Users/username/</code> directory, and on Windows you should be able to\nfind it in the <code>Python</code> directory on your computer, the same in which\nyou opened your command line program. If this worked you can delete the\ndirectory from your hard drive, since it was just for practice. Since we\nwant to create a new directory named after the query that we input into\nthe Old Bailey Online website, we will make direct use of the query\nfunction argument from the getSearchResults function. To do this, import\nthe <code>os</code> library after the others and then add the\ncode you have just written immediately below. Your <code>getSearchResults</code>\nfunction should now look like this:</p>\n<pre><code class=\"language-python\">#create URLs for search results pages and save the files\ndef getSearchResults(query, kwparse, fromYear, fromMonth, toYear, toMonth, entries):\n\n    import urllib.request, math, os\n\n    #This line is new! Create a new directory\n    if not os.path.exists(query):\n        os.makedirs(query)\n\n    startValue = 0\n\n    #Determine how many files need to be downloaded.\n    pageCount = entries / 10\n    pageCount = math.ceil(pageCount)\n\n    for pages in range(1, pageCount +1):\n\n        #each part of the URL. Split up to be easier to read.\n        url = &#39;https://www.oldbaileyonline.org/search.jsp?gen=1&amp;form=searchHomePage&amp;_divs_fulltext=&#39;\n        url += query\n        url += &#39;&amp;kwparse=&#39; + kwparse\n        url += &#39;&amp;_divs_div0Type_div1Type=sessionsPaper_trialAccount&#39;\n        url += &#39;&amp;fromYear=&#39; + fromYear\n        url += &#39;&amp;fromMonth=&#39; + fromMonth\n        url += &#39;&amp;toYear=&#39; + toYear\n        url += &#39;&amp;toMonth=&#39; + toMonth\n        url += &#39;&amp;start=&#39; + str(startValue)\n        url += &#39;&amp;count=0&#39;\n\n        #download the page and save the result.\n        response = urllib.request.urlopen(url)\n        webContent = response.read().decode(&#39;UTF-8&#39;)\n\n        #save the result to the new directory\n        filename = &#39;search-result&#39; + str(startValue)\n\n        f = open(filename + &quot;.html&quot;, &#39;w&#39;)\n        f.write(webContent)\n        f.close\n\n        startValue = startValue + 10\n</code></pre>\n<p>The last step for this function is to make sure that when we save our\nsearch results pages, we save them in this new directory. To do this we\ncan make a minor adjustment to the filename variable so that the file\nends up in the right place. There are many ways we can do this, the\neasiest of which is just to append the new directory name plus a slash\nto the name of the file:</p>\n<pre><code class=\"language-python\">filename = query + &#39;/&#39; + &#39;search-result&#39; + str(startValue)\n</code></pre>\n<p>If your computer is running Windows you will need to use a backslash\ninstead of a forward slash in the above example. Add the above line to\nyour <code>getSearchResults</code> page in lieu of the current <code>filename</code> description.</p>\n<p>If you are running Windows, chances are your <code>downloadSearches.py</code>\nprogram will now crash when you run it because you are trying to create\na directory with a * in it. Windows does not like this. To get around\nthis problem we can use <a href=\"https://docs.python.org/3/library/re.html\">regular expressions</a> to remove any\nnon-Windows-friendly characters. We used regular expressions previously\nin <a href=\"/lessons/counting-frequencies\">Counting Frequencies</a>. To remove non-alpha-numeric characters from\nthe query, first import the regular expressions library immediately\nafter you have imported the <code>os</code> library, then use the <code>re.sub()</code> function\nto create a new string named <code>cleanQuery</code> that contains only alphanumeric\ncharacters. You will then have to substitute <code>cleanQuery</code> as the variable\nused in the <code>os.path.exists()</code>, <code>os.makedirs()</code>, and <code>filename</code> declarations.</p>\n<pre><code class=\"language-python\">import urllib.request, math, os, re\ncleanQuery = re.sub(r&#39;\\W+&#39;, &#39;&#39;, query)\nif not os.path.exists(cleanQuery):\n    os.makedirs(cleanQuery)\n\n...\n\nfilename = cleanQuery + &#39;/&#39; + &#39;search-result&#39; + str(startValue)\n</code></pre>\n<p>The final version of your function should look like this:</p>\n<pre><code class=\"language-python\">#create URLs for search results pages and save the files\ndef getSearchResults(query, kwparse, fromYear, fromMonth, toYear, toMonth, entries):\n\n    import urllib.request, math, os, re\n\n    cleanQuery = re.sub(r&#39;\\W+&#39;, &#39;&#39;, query)\n    if not os.path.exists(cleanQuery):\n        os.makedirs(cleanQuery)\n\n    startValue = 0\n\n    # Determine how many files need to be downloaded.\n    pageCount = entries / 10\n    pageCount = math.ceil(pageCount)\n\n    for pages in range(1, pageCount +1):\n\n        #each part of the URL. Split up to be easier to read.\n        url = &#39;https://www.oldbaileyonline.org/search.jsp?gen=1&amp;form=searchHomePage&amp;_divs_fulltext=&#39;\n        url += query\n        url += &#39;&amp;kwparse=&#39; + kwparse\n        url += &#39;&amp;_divs_div0Type_div1Type=sessionsPaper_trialAccount&#39;\n        url += &#39;&amp;fromYear=&#39; + fromYear\n        url += &#39;&amp;fromMonth=&#39; + fromMonth\n        url += &#39;&amp;toYear=&#39; + toYear\n        url += &#39;&amp;toMonth=&#39; + toMonth\n        url += &#39;&amp;start=&#39; + str(startValue)\n        url += &#39;&amp;count=0&#39;\n\n        #download the page and save the result.\n        response = urllib.request.urlopen(url)\n        webContent = response.read().decode(&#39;UTF-8&#39;)\n        filename = cleanQuery + &#39;/&#39; + &#39;search-result&#39; + str(startValue)\n        f = open(filename + &quot;.html&quot;, &#39;w&#39;)\n        f.write(webContent)\n        f.close\n\n        startValue = startValue + 10\n</code></pre>\n<p>This time we tell the program to download the trials and put them in the\nnew directory rather than our <code>programming-historian</code> directory. Run\n<code>download-searches.py</code> once more to ensure this worked and you\nunderstand how to save files to a particular directory using Python.</p>\n<h3 id=\"downloading-the-individual-trial-entries\">Downloading the individual trial entries</h3>\n<p>At this stage we have created a function that can download all of the\nsearch results HTML files from the Old Bailey Online website for an\nadvanced search that we have defined, and have done so programmatically.\nNow for the next step in the algorithm: Extract the URLs of each trial\ntranscript from the search results HTML files. In the lessons that\nprecede this one (eg, <a href=\"/lessons/working-with-web-pages\">Working with Webpages</a>), we have worked with the printer friendly versions of\nthe trial transcripts, so we will continue to do so. We know that the\nprinter friendly version of Benjamin Bowsey’s trial is located at the\nURL:</p>\n<pre><code>http://www.oldbaileyonline.org/print.jsp?div=t17800628-33\n</code></pre>\n<p>In the same way that changing query strings in the URLs yields different\nsearch results, changing the URL for trial records – namely substituting\none trial ID for another – we will get the transcript for that new\ntrial. This means that to find and download the 13 matching files, all\nwe need are these trial IDs. Since we know that search results pages on\nwebsites generally contain a link to the pages described, there is a\ngood chance that we can find these links embedded in the HTML code. If\nwe can scrape this information from the downloaded search results pages,\nwe can then use that information to generate a URL that will allow us to\ndownload each trial transcript. This is a technique that you can use for\nmost search result pages, not just Old Bailey Online! To do this, we\nmust first find where the trial IDs are amidst the HTML code in the\ndownloaded files, and then determine a way to consistently isolate them\nusing code so that no matter which search results page we download from\nthe site we are able to find the trial transcripts. First, open\n<code>search-results0.html</code> in Komodo Edit and have a look for the list of\nthe trials. The first entry starts with “Anne Smith” so you can use the\n“find” feature in Komodo Edit to jump immediately to the right spot.\nNotice Anne’s name is part of a link:</p>\n<pre><code>browse.jsp?id=t17160113-18&amp;amp;div=t17160113-18&amp;amp;terms=mulatto*_negro*#highlight\n</code></pre>\n<p>Perfect, the link contains the trial ID! Scroll through the remaining\nentries and you’ll find the same is true. Lucky for us, the site is well\nformatted and it looks like each link starts with “browse.jsp?id=”\nfollowed by the trial ID and finished with an &amp;, in Anne’s case:\n“browse.jsp?id=t17160113-18&amp;”. We can write a few lines of code that can\nisolate those IDs. Take a look at the following function. This function\nalso uses the <code>os</code> library, in this case to list all of the files\nlocated in the directory created in the previous section. The <code>os</code>\nlibrary contains a range of useful functions that mirror the types of\ntasks you would expect to be able to do with your mouse in the Mac\nFinder or Windows such as opening, closing, creating, deleting, and\nmoving files and directories, and is a great library to master – or at\nleast familiarize yourself with.</p>\n<pre><code class=\"language-python\">def getIndivTrials(query):\n    import os, re\n\n    cleanQuery = re.sub(r&#39;\\W+&#39;, &#39;&#39;, query)\n    searchResults = os.listdir(cleanQuery)\n\n    print(searchResults)\n</code></pre>\n<p>Create and run a new program called <code>extract-trial-ids.py</code> with the\nfollowing code. Make sure you input the same value into the query\nargument as you did in the previous example:</p>\n<pre><code class=\"language-python\">import obo\n\nobo.getIndivTrials(&quot;mulatto*+negro*&quot;)\n</code></pre>\n<p>If everything went right, you should see a list containing the names of\nall the files in your new “mulatto*+negro*” directory, which at this\npoint should be the two search results pages. Ensure this worked before\nmoving forward. Since we saved all of the search results pages with a\nfilename that includes “search-results”, we now want to open each file\nwith a name containing “search-results”, and extract all trial IDs found\ntherein. In this case we know we have 2, but we want our code to be as\nreusable as possible (with reason, of course!) Restricting this action\nto files named “search-results” will mean that this program will work as\nintended even if the directory contains many other unrelated files\nbecause the program will skip over anything with a different name. Add\nthe following to your getIndivTrials() function, which will check if\neach file contains “search-results” in its name. If it does, the file\nwill be opened and the contents saved to a variable named text. That\ntext variable will then be parsed looking for the trial ID, which we\nknow always follows “browse.jsp?id=”. If and when that trial ID is found\nit will be saved to a list and printed to the command output, which\nleaves us with all of the information we need to then write a program\nthat will download the desired trials.</p>\n<pre><code class=\"language-python\">def getIndivTrials(query):\n    import os, re\n\n    cleanQuery = re.sub(r&#39;\\W+&#39;, &#39;&#39;, query)\n    searchResults = os.listdir(cleanQuery)\n\n    urls = []\n\n    #find search-results pages\n    for files in searchResults:\n        if files.find(&quot;search-result&quot;) != -1:\n            f = open(cleanQuery + &quot;/&quot; + files, &#39;r&#39;)\n            text = f.read().split(&quot; &quot;)\n            f.close()\n\n            #look for trial IDs\n            for words in text:\n                if words.find(&quot;browse.jsp?id=&quot;) != -1:\n                    #isolate the id\n                    urls.append(words[words.find(&quot;id=&quot;) +3: words.find(&quot;&amp;&quot;)])\n\n    print(urls)\n</code></pre>\n<p>That last line of the for loop may look tricky, but make sure you\nunderstand it before moving on. The words variable is checked to see if\nit contains the characters “id=” (without the quotes), which of course\nrefers to a specific trial transcript ID. If it does, we use the slice\nstring method to capture only the chunk between <em>id=</em> and <em>&amp;</em> and append it\nto the url list. If we knew the exact index positions of this substring\nwe could have used those numerical values instead. However, by using the\n<em>find()</em> string method we have created a much more flexible program. The\nfollowing code does exactly the same thing as that last line in a less\ncondensed manner.</p>\n<pre><code class=\"language-python\">idStart = words.find(&quot;id=&quot;) + 3\nidEnd = words.find(&quot;&amp;&quot;)\ntrialID = words[idStart: idEnd]\n\nurls.append(trialID)\n</code></pre>\n<p>When you re-run <code>extract-trial-ids.py</code>, you should now see a list of all\nthe trial IDs. We can add a couple extra lines to turn these into proper\nURLs and download the whole list to our new directory. We’ll also use\nthe <code>time</code> library to pause our program for three seconds between\ndownloads– a technique called throttling. It’s considered good form not\nto pound someone’s server with many requests per second; and the slight\ndelay makes it more likely that all the files will actually download\nrather than <a href=\"http://www.checkupdown.com/status/E408.html\">time out</a>. Add the following code to the end of your\n<code>getIndivTrials()</code> function. This code will generate the URL of each\nindividual page, download the page to your computer, place it in your\nnew directory, save the file, and pause for 3 seconds before moving on\nto the next trial. This work is all contained in a for loop, and will\nrun once for every trial in your url list.</p>\n<pre><code class=\"language-python\">def getIndivTrials(query):\n    #...\n    import urllib.request, time\n\n    #import built-in python functions for building file paths\n    from os.path import join as pjoin\n\n    for items in urls:\n        #generate the URL\n        url = &quot;http://www.oldbaileyonline.org/print.jsp?div=&quot; + items\n\n        #download the page\n        response = urllib.request.urlopen(url)\n        webContent = response.read().decode(&#39;UTF-8&#39;)\n\n        #create the filename and place it in the new directory\n        filename = items + &#39;.html&#39;\n        filePath = pjoin(cleanQuery, filename)\n\n        #save the file\n        f = open(filePath, &#39;w&#39;)\n        f.write(webContent)\n        f.close\n\n        #pause for 3 second\n        time.sleep(3)\n</code></pre>\n<p>If we put this all together into a single function it should look\nsomething like this. (Note, we’ve put all the “import” calls at the top\nto keep things cleaner).</p>\n<pre><code class=\"language-python\">def getIndivTrials(query):\n    import os, re, urllib.request, time\n\n    #import built-in python functions for building file paths\n    from os.path import join as pjoin\n\n    cleanQuery = re.sub(r&#39;\\W+&#39;, &#39;&#39;, query)\n    searchResults = os.listdir(cleanQuery)\n\n    urls = []\n\n    #find search-results pages\n    for files in searchResults:\n        if files.find(&quot;search-result&quot;) != -1:\n            f = open(cleanQuery + &quot;/&quot; + files, &#39;r&#39;)\n            text = f.read().split(&quot; &quot;)\n            f.close()\n\n            #look for trial IDs\n            for words in text:\n                if words.find(&quot;browse.jsp?id=&quot;) != -1:\n                    #isolate the id\n                    urls.append(words[words.find(&quot;id=&quot;) +3: words.find(&quot;&amp;&quot;)])\n\n    #new from here down!\n    for items in urls:\n        #generate the URL\n        url = &quot;http://www.oldbaileyonline.org/print.jsp?div=&quot; + items\n\n        #download the page\n        response = urllib.request.urlopen(url)\n        webContent = response.read().decode(&#39;UTF-8&#39;)\n\n        #create the filename and place it in the new directory\n        filename = items + &#39;.html&#39;\n        filePath = pjoin(cleanQuery, filename)\n\n        #save the file\n        f = open(filePath, &#39;w&#39;)\n        f.write(webContent)\n        f.close\n\n        #pause for 3 seconds\n        time.sleep(3)\n</code></pre>\n<p>Let’s add the same three-second pause to our <code>getSearchResults</code> function\nto be kind to the Old Bailey Online servers:</p>\n<pre><code class=\"language-python\">#create URLs for search results pages and save the files\ndef getSearchResults(query, kwparse, fromYear, fromMonth, toYear, toMonth, entries):\n\n    import urllib.request, math, os, re, time\n\n    cleanQuery = re.sub(r&#39;\\W+&#39;, &#39;&#39;, query)\n    if not os.path.exists(cleanQuery):\n        os.makedirs(cleanQuery)\n\n    startValue = 0\n\n    #Determine how many files need to be downloaded.\n    pageCount = entries / 10\n    pageCount = math.ceil(pageCount)\n\n    for pages in range(1, pageCount +1):\n\n        #each part of the URL. Split up to be easier to read.\n        url = &#39;https://www.oldbaileyonline.org/search.jsp?gen=1&amp;form=searchHomePage&amp;_divs_fulltext=&#39;\n        url += query\n        url += &#39;&amp;kwparse=&#39; + kwparse\n        url += &#39;&amp;_divs_div0Type_div1Type=sessionsPaper_trialAccount&#39;\n        url += &#39;&amp;fromYear=&#39; + fromYear\n        url += &#39;&amp;fromMonth=&#39; + fromMonth\n        url += &#39;&amp;toYear=&#39; + toYear\n        url += &#39;&amp;toMonth=&#39; + toMonth\n        url += &#39;&amp;start=&#39; + str(startValue)\n        url += &#39;&amp;count=0&#39;\n\n        #download the page and save the result.\n        response = urllib.request.urlopen(url)\n        webContent = response.read().decode(&#39;UTF-8&#39;)\n        filename = cleanQuery + &#39;/&#39; + &#39;search-result&#39; + str(startValue)\n        f = open(filename + &quot;.html&quot;, &#39;w&#39;)\n        f.write(webContent)\n        f.close\n\n        startValue = startValue + 10\n\n        #pause for 3 seconds\n        time.sleep(3)\n</code></pre>\n<p>Finally, call the function in the <code>download-searches.py</code> program.</p>\n<pre><code class=\"language-python\">#download-searches.py\nimport obo\n\nquery = &#39;mulatto*+negro*&#39;\n\nobo.getSearchResults(query, &quot;advanced&quot;, &quot;1700&quot;, &quot;00&quot;, &quot;1750&quot;, &quot;99&quot;, 13)\n\nobo.getIndivTrials(query)\n</code></pre>\n<p>Now, you’ve created a program that can request and download files from\nthe Old Bailey website based on search parameters you define, all\nwithout visiting the site!</p>\n<h3 id=\"in-case-a-file-does-not-download\">In case a file does not download</h3>\n<p>Check to make all thirteen files have downloaded properly. If that’s the\ncase for you, that’s great! However, there’s a possibility that this\nprogram stalled along the way. That’s because our program, though\nrunning on your own machine, relies on two factors outside of our\nimmediate control: the speed of the Internet, and the response time of\nthe Old Bailey Online server at that moment. It’s one thing to ask\nPython to download a single file, but when we start asking for a file\nevery 3 seconds there’s a greater chance the server will either time out\nor fail to send us the file we are after.</p>\n<p>If we were using a web browser to make these requests, we’d eventually\nget a message that the “connection had timed out” or something of the\nsort. We all see this from time to time. However, our program isn’t\nbuilt to handle or relay such error messages, so instead you’ll realize\nit when you discover that the program has not returned the expected\nnumber of files or just seemingly does nothing. To prevent frustration\nand uncertainty, we want a fail-safe in our program that will attempt to\ndownload each trial. If for whatever reason it fails, we’ll make a note\nof it and move on to the next trial.</p>\n<p>To do this, we will make use of the Python <a href=\"http://docs.python.org/tutorial/errors.html\">try / except</a> error\nhandling mechanism, as well as a new library: socket. Try and Except are\na lot like an <em>if / else</em> statement. When you ask Python to <em>try</em> something,\nit will attempt to run the code; if the code fails to achieve what you\nhave defined, it will run the <em>except</em> code. This is often used when\ndealing with errors, known as <em>error handling</em>. We can use this to our\nadvantage by telling our program to attempt downloading a page. If it\nfails, we’ll ask it to let us know which file failed and then move on.\nTo do this we need to use the <code>socket</code> library, which will allow us to put\na time limit on a download attempt before moving on. This involves\naltering the <code>getIndivTrials</code> function.</p>\n<p>First, we need to load the <code>socket</code> library, which should be done in the\nsame way as all of our previous library imports. Then, we need to import\nthe <code>urllib.error</code> library, which allows us to handle download errors.\nWe will also need to\nset the default socket timeout length – how long do we want to try to\ndownload a page before we give up. This should go immediately after the\ncomment that begins with <code>#download the page</code></p>\n<pre><code class=\"language-python\">    import os, re, urllib.request, urllib.error, time, socket\n\n    #...\n        #download the page\n        socket.setdefaulttimeout(10)\n</code></pre>\n<p>Then, we need a new python list that will hold all of the urls that\nfailed to download. We will call this <code>failedAttempts</code> and you can insert\nit immediately after the <code>import</code> instructions:</p>\n<pre><code class=\"language-python\">failedAttempts = []\n</code></pre>\n<p>Finally, we can add the <em>try / except</em> statement, which is added in much\nthe same way as an <em>if / else</em> statement would be. In this case, we will\nput all of the code designed to download and save the trials in the try\nstatement, and in the except statement we will tell the program what we\nwant it to do if that should fail. Here, we will append the url that\nfailed to download to our new list, <code>failedAttempts</code></p>\n<pre><code class=\"language-python\">#...\n\n        socket.setdefaulttimeout(10)\n\n        try:\n            response = urllib2.urlopen(url)\n            webContent = response.read().decode(&#39;UTF-8&#39;)\n\n            #create the filename and place it in the new &quot;trials&quot; directory\n            filename = items + &#39;.html&#39;\n            filePath = pjoin(newDir, filename)\n\n            #save the file\n            f = open(filePath, &#39;w&#39;)\n            f.write(webContent)\n            f.close\n        except urllib.error.URLError:\n            failedAttempts.append(url)\n</code></pre>\n<p>Finally, we will tell the program to print the contents of the list to\nthe command output so we know which files failed to download. This\nshould be added as the last line in the function.</p>\n<pre><code class=\"language-python\">print(&quot;failed to download: &quot; + str(failedAttempts))\n</code></pre>\n<p>Now when you run the program, should there be a problem downloading a\nparticular file, you will receive a message in the Command Output window\nof Komodo Edit. This message will contain any URLs of files that failed\nto download. If there are only one or two, it’s probably fastest just to\nvisit the pages manually and use the “Save As” feature of your browser.\nIf you are feeling adventurous, you could modify the program to\nautomatically download the remaining files. The final version of your\n<code>getSearchResults()</code> and <code>getIndivTrials()</code> functions should now\nlook like this:</p>\n<pre><code class=\"language-python\">#create URLs for search results pages and save the files\ndef getSearchResults(query, kwparse, fromYear, fromMonth, toYear, toMonth, entries):\n\n    import urllib.request, math, os, re, time\n\n    cleanQuery = re.sub(r&#39;\\W+&#39;, &#39;&#39;, query)\n    if not os.path.exists(cleanQuery):\n        os.makedirs(cleanQuery)\n\n    startValue = 0\n\n    #Determine how many files need to be downloaded.\n    pageCount = entries / 10\n    pageCount = math.ceil(pageCount)\n\n    for pages in range(1, pageCount +1):\n\n        #each part of the URL. Split up to be easier to read.\n        url = &#39;https://www.oldbaileyonline.org/search.jsp?gen=1&amp;form=searchHomePage&amp;_divs_fulltext=&#39;\n        url += query\n        url += &#39;&amp;kwparse=&#39; + kwparse\n        url += &#39;&amp;_divs_div0Type_div1Type=sessionsPaper_trialAccount&#39;\n        url += &#39;&amp;fromYear=&#39; + fromYear\n        url += &#39;&amp;fromMonth=&#39; + fromMonth\n        url += &#39;&amp;toYear=&#39; + toYear\n        url += &#39;&amp;toMonth=&#39; + toMonth\n        url += &#39;&amp;start=&#39; + str(startValue)\n        url += &#39;&amp;count=0&#39;\n\n        #download the page and save the result.\n        response = urllib.request.urlopen(url)\n        webContent = response.read().decode(&#39;UTF-8&#39;)\n        filename = cleanQuery + &#39;/&#39; + &#39;search-result&#39; + str(startValue)\n        f = open(filename + &quot;.html&quot;, &#39;w&#39;)\n        f.write(webContent)\n        f.close\n\n        startValue = startValue + 10\n\n        #pause for 3 seconds\n        time.sleep(3)\n\ndef getIndivTrials(query):\n    import os, re, urllib.request, urllib.error, time, socket\n\n    failedAttempts = []\n\n    #import built-in python functions for building file paths\n    from os.path import join as pjoin\n\n    cleanQuery = re.sub(r&#39;\\W+&#39;, &#39;&#39;, query)\n    searchResults = os.listdir(cleanQuery)\n\n    urls = []\n\n    #find search-results pages\n    for files in searchResults:\n        if files.find(&quot;search-result&quot;) != -1:\n            f = open(cleanQuery + &quot;/&quot; + files, &#39;r&#39;)\n            text = f.read().split(&quot; &quot;)\n            f.close()\n\n            #look for trial IDs\n            for words in text:\n                if words.find(&quot;browse.jsp?id=&quot;) != -1:\n                    #isolate the id\n                    urls.append(words[words.find(&quot;id=&quot;) +3: words.find(&quot;&amp;&quot;)])\n\n    for items in urls:\n        #generate the URL\n        url = &quot;http://www.oldbaileyonline.org/print.jsp?div=&quot; + items\n\n        #download the page\n        socket.setdefaulttimeout(10)\n        try:\n            response = urllib.request.urlopen(url)\n            webContent = response.read().decode(&#39;UTF-8&#39;)\n\n            #create the filename and place it in the new directory\n            filename = items + &#39;.html&#39;\n            filePath = pjoin(cleanQuery, filename)\n\n            #save the file\n            f = open(filePath, &#39;w&#39;)\n            f.write(webContent)\n            f.close\n        except urllib.error.URLError:\n            failedAttempts.append(url)\n\n        #pause for 3 seconds\n        time.sleep(3)\n\n    print(&quot;failed to download: &quot; + str(failedAttempts))\n</code></pre>\n<h2 id=\"further-reading\">Further Reading</h2>\n<p>For more advanced users, or to become a more advanced user, you may find\nit worthwhile to read about achieving this same process using\nApplication Programming Interfaces (API). A website with an API will\ngenerally provide instructions on how to request certain documents. It’s\na very similar process to what we just did by interpreting the URL Query\nStrings, but without the added detective work required to decipher what\neach variable does. If you are interested in the Old Bailey Online, they\nhave recently released an API and the documentation can be quite\nhelpful:</p>\n<ul>\n<li>Old Bailey Online API\n(<a href=\"http://www.oldbaileyonline.org/static/DocAPI.jsp\">http://www.oldbaileyonline.org/static/DocAPI.jsp</a>)</li>\n<li>Python Best way to create directory if it doesn’t exist for file write? (<a href=\"http://stackoverflow.com/questions/273192/python-best-way-to-create-directory-if-it-doesnt-exist-for-file-write\">http://stackoverflow.com/questions/273192/python-best-way-to-create-directory-if-it-doesnt-exist-for-file-write</a>)</li>\n</ul>\n"}