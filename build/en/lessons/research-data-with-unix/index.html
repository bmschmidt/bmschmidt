<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-c09d08cd.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-c09d08cd.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/research-data-with-unix"),
					params: {lang:"en",lessons:"lessons",slug:"research-data-with-unix"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Counting and mining research data with Unix</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h1 id="counting-and-mining-research-data-with-unix">Counting and mining research data with Unix</h1>
<h2 id="introduction">Introduction</h2>
<p>This lesson will look at how research data, when organised in a clear and predictable manner, can be counted and mined using the Unix shell. The lesson builds on the lessons &quot;<a href="/lessons/preserving-your-research-data">Preserving Your Research Data: Documenting and Structuring Data</a>&quot; and &quot;<a href="../lessons/intro-to-bash">Introduction to the Bash Command Line</a>&quot;. Depending on your confidence with the Unix shell, it can also be used as a standalone lesson or refresher.</p>
<p>Having accumulated research data for one project, a historian might ask different questions of that same data when returning to it during a subsequent project. If this data is spread across multiple files - a series of tabulated data, a set of transcribed text, a collection of images - it can be counted and mined using simple Unix commands.</p>
<p>The Unix shell gives you access to a range of powerful commands that can transform how you count and mine research data. This lesson will introduce you to a series of commands that use counting and mining of tabulated data, though they only scratch the surface of what the Unix shell can do. By learning just a few simple commands you will be able to undertake tasks that are impossible in Libre Office Calc, Microsoft Excel, or other similar spreadsheet programs. These commands can be easily extended for use with non-tabulated data.</p>
<p>This lesson will also demonstrate that the options for manipulating, counting and mining data available to you will often depend on the amount of metadata, or descriptive text, contained in the filenames of the data you are using as much as the range of Unix commands you have learnt to use. Thus, even if it is not a prerequisite of working with the Unix shell, taking the time to structure your research data and filenaming conventions in a consistent and predictable manner is certainly a significant step towards getting the most out of Unix commands and being able to count and mine your research data. For the value of taking the time to make your data consistent and predictable beyond matters of preservation, see &quot;<a href="../lessons/preserving-your-research-data">Preserving Your Research Data: Documenting and Structuring Data</a>&quot;.</p>
<hr>
<h2 id="software-and-setup">Software and setup</h2>
<p>Windows users will need to install Git Bash. This can be installed by downloading the most recent installer at the <a href="http://msysgit.github.io/">git for windows webpage</a>. Instructions for installation are available at <a href="https://web.archive.org/web/20190318191709/https://openhatch.org/missions/windows-setup/install-git-bash">Open Hatch</a>.</p>
<p>OS X and Linux users will need to use their terminal shells to follow this lesson, as discussed in &quot;<a href="../lessons/intro-to-bash">Introduction to the Bash Command Line</a>.&quot;</p>
<p>This lesson was written using Git Bash 1.9.0 and the Windows 7 operating system. Equivalent file paths for OS X/Linux have been included where possible. Nonetheless, as commands and flags can change slightly between operating systems OS X/Linux users are referred to Deborah S. Ray and Eric J. Ray, &quot;<a href="https://www.worldcat.org/title/unix-and-linux/oclc/308171076&amp;referer=brief_results"><em>Unix and Linux: Visual Quickstart Guide</em></a>&quot;, 4th edition (2009) which covers interoperability in greater detail.</p>
<p>The files used in this lesson are available on &quot;<a href="https://doi.org/10.6084/m9.figshare.1172094">Figshare</a>&quot;. The data contains the metadata for journal articles categorised under &#39;History&#39; in the British Library ESTAR database. The data is shared under a CC0 copyright waiver.</p>
<p>Download the required files, save them to your computer, and unzip them. If you do not have default software installed to interact with .zip files, we recommend <a href="http://www.7-zip.org/">7-zip</a> for this purpose. On Windows, we recommend unzipping the folder provided to your c: drive so the files are at <code>c:\proghist\</code>. However, any location will work fine, but you may have to adjust your commands as you are following along with this lesson if you use a different location. On OS X or Linux, we similarly recommend unzipping them to your user directory, so that they appear at <code>/user/USERNAME/proghist/</code>. In both cases, this means that when you open up a new terminal window, you can just type <code>cd proghist</code> to move to the correct directory.</p>
<hr>
<h2 id="counting-files">Counting files</h2>
<p>You will begin this lesson by counting the contents of files using the Unix shell. The Unix shell can be used to quickly generate counts from across files, something that is tricky to achieve using the graphical user interfaces (GUI) of standard office suites.</p>
<p>In Unix the <code>wc</code> command is used to count the contents of a file or of a series of files.</p>
<p>Open the Unix shell and navigate to the directory that contains our data, the <code>data</code> subdirectory of the <code>proghist</code> directory. Remember, if at any time you are not sure where you are in your directory structure, type <code>pwd</code> and use the <code>cd</code> command to move to where you need to be. The directory structure here is slightly different between OS X/Linux and Windows: on the former, the directory is in a format such as <code>~/users/USERNAME/proghist/data</code> and on Windows in a format such as <code>c:\proghist\data</code>.</p>
<p>Type <code>ls</code> and then hit enter. This prints, or displays, a list that includes two files and a subdirectory.</p>
<p>The files in this directory are the dataset <code>2014-01_JA.csv</code> that contains journal article metadata and a file containing documentation about <code>2014-01_JA.csv</code> called <code>2014-01_JA.txt</code>.</p>
<p>The subdirectory is named <code>derived_data</code>. It contains four <a href="http://en.wikipedia.org/wiki/Tab-separated_values">.tsv</a> files derived from <code>2014-01_JA.csv</code>. Each of these includes all data where a keyword such as <code>africa</code> or <code>america</code> appears in the &#39;Title&#39; field of <code>2014-01_JA.csv</code>. The <code>derived_data</code> directory also includes a subdirectory called <code>results</code>.</p>
<p><em>Note: <a href="http://en.wikipedia.org/wiki/Comma-separated_values">CSV</a> files are those in which the units of data (or cells) are separated by commas (comma-separated-values) and TSV files are those in which they are separated by tabs.  Both can be read in simple text editors or in spreadsheet programs such as Libre Office Calc or Microsoft Excel.</em></p>
<p>Before you begin working with these files, you should move into the directory in which they are stored. Navigate to <code>c:\proghist\data\derived_data</code> on Windows or <code>~/users/USERNAME/proghist/data/derived_data</code> on OS X.</p>
<p>Now that you are here you can count the contents of the files.</p>
<p>The Unix command for counting is <code>wc</code>. Type <code>wc -w 2014-01-31_JA_africa.tsv</code> and hit enter. The flag <code>-w</code> combined with <code>wc</code> instructs the computer to print a word count, and the name of the file that has been counted, into the shell.</p>
<p>As was seen in &quot;<a href="../lessons/intro-to-bash">Introduction to the Bash Command Line</a>&quot;, flags such as <code>-w</code> are an essential part of getting the most out of the Unix shell as they give you better control over commands.</p>
<p>If your research is more concerned with the number of entries (or lines) than the number of words, you can use the line count flag. Type <code>wc -l 2014-01-31_JA_africa.tsv</code> and hit enter. Combined with <code>wc</code> the flag <code>-l</code> prints a line count and the name of the file that has been counted.</p>
<p>Finally, type <code>wc -c 2014-01-31_JA_africa.tsv</code> and hit enter. This uses the flag <code>-c</code> in combination with the command <code>wc</code> to print a character count for <code>2014-01-31_JA_africa.tsv</code>.</p>
<p><em>Note: OS X and Linux users should replace the <code>-c</code> flag with <code>-m</code>.</em></p>
<p>With these three flags, the most obvious thing historians can use <code>wc</code> for is to quickly compare the shape of sources in digital format - for example word counts per page of a book, the distribution of characters per page across a collection of newspapers, the average line lengths used by poets. You can also use <code>wc</code> with a combination of wildcards and flags to build more complex queries. Type <code>wc -l 2014-01-31_JA_a*.tsv</code> and hit enter. This prints the line counts for <code>2014-01-31_JA_africa.tsv</code> and <code>2014-01-31_JA_america.tsv</code>, offering a simple means of comparing these two sets of research data. Of course, it may be faster to compare the line count for the two documents in Libre Office Calc, Microsoft Excel, or a similar spreadsheet program. But when wishing to compare the line count for tens, hundreds, or thousands of documents, the Unix shell has a clear speed advantage.</p>
<p>Moreover, as our datasets increase in size you can use the Unix shell to do more than copy these line counts by hand, by the use of print screen, or by copy and paste methods. Using the <code>&gt;</code> redirect operator you can export your query results to a new file. Type <code>wc -l 2014-01-31_JA_a*.tsv &gt; results/2014-01-31_JA_a_wc.txt</code> and hit enter. This runs the same query as before, but rather than print the results within the Unix shell it saves the results as <code>2014-01-31_JA_a_wc.txt</code>. By prefacing this with <code>results/</code> it moves the .txt file to the <code>results</code> sub-directory. To check this, navigate to the <code>results</code> subdirectory, hit enter, type <code>ls</code>, and hit enter again to see this file listed within <code>c:\proghist\data\derived_data\results</code> on Windows or <code>/users/USERNAME/proghist/data/derived_data/results</code> on OS X/Linux.</p>
<h2 id="mining-files">Mining files</h2>
<p>The Unix shell can do much more than count the words, characters, and lines within a file. The <code>grep</code> command (meaning &#39;global regular expression print&#39;) is used to search across multiple files for specific strings of characters. It is able to do so much faster than the graphical search interface offered by most operating systems or office suites. And combined with the <code>&gt;</code> operator, the <code>grep</code> command becomes a powerful research tool can be used to mine your data for characteristics or word clusters that appear across multiple files and then export that data to a new file. The only limitations here are your imagination, the shape of your data, and - when working with thousands or millions of files - the processing power at your disposal.</p>
<p>To begin using <code>grep</code>, first navigate to the <code>derived_data</code> directory (<code>cd ..</code>). Here type <code>grep 1999 *.tsv</code> and hit enter. This query looks across all files in the directory that fit the given criteria (the .tsv files) for instances of the string, or character cluster, &#39;1999&#39;. It then prints them within the shell.</p>
<p><em>Note: there is a large amount of data to print, so if you get bored hit <code>ctrl+c</code> to cancel the action. Ctrl+c is used to cancel any process in the Unix shell.</em></p>
<p>Press the up arrow once in order to cycle back to your most recent action. Amend <code>grep 1999 *.tsv</code> to <code>grep -c 1999 *.tsv</code> and hit enter. The shell now prints the number of times the string 1999 appeared in each .tsv file. Cycle to the previous line again and amend this to <code>grep -c 1999 2014-01-31_JA_*.tsv &gt; results/2014-01-31_JA_1999.txt</code> and hit enter. This query looks for instances of the string &#39;1999&#39; across all documents that fit the criteria and saves them as <code>2014-01-31_JA_1999.txt</code> in the <code>results</code> subdirectory.</p>
<p>Strings need not be numbers. <code>grep -c revolution 2014-01-31_JA_america.tsv 2014-02-02_JA_britain.tsv</code>, for example, counts the instances of the string <code>revolution</code> within the defined files and prints those counts to the shell. Run this and then amend it to <code>grep -ci revolution 2014-01-31_JA_america.tsv 2014-02-02_JA_britain.tsv</code>. This repeats the query, but prints a case insensitive count (including instances of both <code>revolution</code> and <code>Revolution</code>). Note how the count has increased nearly 30 fold for those journal article titles that contain the keyword &#39;revolution&#39;. As before, cycling back and adding <code>&gt; results/</code>, followed by a filename (ideally in .txt format), will save the results to a data file.</p>
<p>You can also use <code>grep</code> to create subsets of tabulated data. Type <code>grep -i revolution 2014-01-31_JA_america.tsv 2014-02-02_JA_britain.tsv &gt; YEAR-MONTH-DAY_JA_america_britain_i_revolution.tsv</code> (where <code>YEAR-MONTH-DAY</code> is the date you are completing this lesson) and hit enter. This command looks in both of the defined files and exports any lines containing <code>revolution</code> (without regard to case) to the specified .tsv file.</p>
<p>The data has not been saved to to the <code>results</code> directory because it isn&#39;t strictly a result; it is derived data. Depending on your research project it may be easier to save this to another subdirectory. For now have a look at this file to verify its contents and when you are happy, delete it using the <code>rm</code> command. <em>Note: the <code>rm</code> common is very powerful and should be used with caution. Please refer to &quot;<a href="../lessons/intro-to-bash">Introduction to the Bash Command Line</a>&quot; for instructions on how to use this command correctly.</em></p>
<p>Finally, you can use another flag, <code>-v</code>, to exclude data elements when using the <code>grep</code> command. Type <code>grep -iv revolution 2014*_JA_a*.tsv &gt; 2014_JA_iv_revolution.csv</code> and hit enter. This query looks in the defined files (three in total) and exports all lines that do not contain <code>revolution</code> or <code>Revolution</code> to <code>c:\proghist\data\derived_data\2014_JA_iv_revolution.csv</code>.</p>
<p>Note that you have transformed the data from one format to another - from .tsv to .csv. Often there is a loss of data structure when undertaking such transformations. To observe this for yourself, run <code>grep -iv revolution 2014*_JA_a*.tsv &gt; 2014_JA_iv_revolution.tsv</code> and open both the .csv and .tsv files in Libre Office Calc, Microsoft Excel, or a similar spreadsheet program. Note the differences in column delineation between the two files.</p>
<p><em>Summary</em></p>
<p>Within the Unix shell you can now:</p>
<ul>
<li>use the <code>wc</code> command with the flags <code>-w</code> and <code>-l</code> to count the words and lines in a file or a series of files.</li>
<li>use the redirector and structure <code>&gt; subdirectory/filename</code> to save results into a subdirectory.</li>
<li>use the <code>grep</code> command to search for instances of a string.</li>
<li>use with <code>grep</code> the <code>-c</code> flag to count instances of a string, the <code>-i</code> flag to return a case insensitive search for a string, and the <code>-v</code> flag to exclude a string from the results.</li>
<li>combine these commands and flags to build complex queries in a way that suggests the potential for using the Unix shell to count and mine your research data and research projects.</li>
</ul>
<hr>
<h4 id="conclusion">Conclusion</h4>
<p>In this lesson you have learnt to undertake some basic file counting, to query across research data for common strings, and to save results and derived data. Though this lesson is restricted to using the Unix shell to count and mine tabulated data, the processes can be easily extended to free text. For this we recommend two guides written by William Turkel:</p>
<ul>
<li>William Turkel, &#39;<a href="http://williamjturkel.net/2013/06/15/basic-text-analysis-with-command-line-tools-in-linux/">Basic Text Analysis with Command Line Tools in Linux</a>&#39; (15 June 2013)</li>
<li>William Turkel, &#39;<a href="http://williamjturkel.net/2013/06/20/pattern-matching-and-permuted-term-indexing-with-command-line-tools-in-linux/">Pattern Matching and Permuted Term Indexing with Command Line Tools in Linux</a>&#39; (20 June 2013)</li>
</ul>
<p>As these recommendations suggest, the present lesson only scratches the surface of what the Unix shell environment is capable of. It is hoped, however, that this lesson has provided a taster sufficient to prompt further investigation and productive play.</p>
<p>For many historians, the full potential of these tools may only emerge upon embedding these skills into a real research project. Once your research grows, and, with it, your research data, being able to manipulate, count and mine thousands of files will be extremely useful. For if you choose to build on this lesson and investigate the Unix shell further you will find that even a large collection of files which do not contain any alpha-numeric data elements, such as image files, can be easily sorted, selected and queried in the Unix shell.</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="research-data-with-unix/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Counting and mining research data with Unix\",\"layout\":\"lesson\",\"date\":\"2014-09-20T00:00:00.000Z\",\"authors\":[\"James Baker\",\"Ian Milligan\"],\"reviewers\":[\"M. H. Beals\",\"Allison Hegel\"],\"editors\":[\"Adam Crymble\"],\"difficulty\":2,\"exclude_from_check\":[\"review-ticket\"],\"activity\":\"transforming\",\"topics\":[\"data-manipulation\"],\"abstract\":\"This lesson will look at how research data, when organised in a clear and predictable manner, can be counted and mined using the Unix shell.\",\"previous\":\"intro-to-bash\",\"redirect_from\":\"\u002Flessons\u002Fresearch-data-with-unix\",\"avatar_alt\":\"A diagram of a miner sorting ore into an apparatus\",\"doi\":\"10.46430\u002Fphen0040\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"counting-and-mining-research-data-with-unix\\\"\u003ECounting and mining research data with Unix\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"introduction\\\"\u003EIntroduction\u003C\u002Fh2\u003E\\n\u003Cp\u003EThis lesson will look at how research data, when organised in a clear and predictable manner, can be counted and mined using the Unix shell. The lesson builds on the lessons &quot;\u003Ca href=\\\"\u002Flessons\u002Fpreserving-your-research-data\\\"\u003EPreserving Your Research Data: Documenting and Structuring Data\u003C\u002Fa\u003E&quot; and &quot;\u003Ca href=\\\"..\u002Flessons\u002Fintro-to-bash\\\"\u003EIntroduction to the Bash Command Line\u003C\u002Fa\u003E&quot;. Depending on your confidence with the Unix shell, it can also be used as a standalone lesson or refresher.\u003C\u002Fp\u003E\\n\u003Cp\u003EHaving accumulated research data for one project, a historian might ask different questions of that same data when returning to it during a subsequent project. If this data is spread across multiple files - a series of tabulated data, a set of transcribed text, a collection of images - it can be counted and mined using simple Unix commands.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe Unix shell gives you access to a range of powerful commands that can transform how you count and mine research data. This lesson will introduce you to a series of commands that use counting and mining of tabulated data, though they only scratch the surface of what the Unix shell can do. By learning just a few simple commands you will be able to undertake tasks that are impossible in Libre Office Calc, Microsoft Excel, or other similar spreadsheet programs. These commands can be easily extended for use with non-tabulated data.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis lesson will also demonstrate that the options for manipulating, counting and mining data available to you will often depend on the amount of metadata, or descriptive text, contained in the filenames of the data you are using as much as the range of Unix commands you have learnt to use. Thus, even if it is not a prerequisite of working with the Unix shell, taking the time to structure your research data and filenaming conventions in a consistent and predictable manner is certainly a significant step towards getting the most out of Unix commands and being able to count and mine your research data. For the value of taking the time to make your data consistent and predictable beyond matters of preservation, see &quot;\u003Ca href=\\\"..\u002Flessons\u002Fpreserving-your-research-data\\\"\u003EPreserving Your Research Data: Documenting and Structuring Data\u003C\u002Fa\u003E&quot;.\u003C\u002Fp\u003E\\n\u003Chr\u003E\\n\u003Ch2 id=\\\"software-and-setup\\\"\u003ESoftware and setup\u003C\u002Fh2\u003E\\n\u003Cp\u003EWindows users will need to install Git Bash. This can be installed by downloading the most recent installer at the \u003Ca href=\\\"http:\u002F\u002Fmsysgit.github.io\u002F\\\"\u003Egit for windows webpage\u003C\u002Fa\u003E. Instructions for installation are available at \u003Ca href=\\\"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20190318191709\u002Fhttps:\u002F\u002Fopenhatch.org\u002Fmissions\u002Fwindows-setup\u002Finstall-git-bash\\\"\u003EOpen Hatch\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EOS X and Linux users will need to use their terminal shells to follow this lesson, as discussed in &quot;\u003Ca href=\\\"..\u002Flessons\u002Fintro-to-bash\\\"\u003EIntroduction to the Bash Command Line\u003C\u002Fa\u003E.&quot;\u003C\u002Fp\u003E\\n\u003Cp\u003EThis lesson was written using Git Bash 1.9.0 and the Windows 7 operating system. Equivalent file paths for OS X\u002FLinux have been included where possible. Nonetheless, as commands and flags can change slightly between operating systems OS X\u002FLinux users are referred to Deborah S. Ray and Eric J. Ray, &quot;\u003Ca href=\\\"https:\u002F\u002Fwww.worldcat.org\u002Ftitle\u002Funix-and-linux\u002Foclc\u002F308171076&amp;referer=brief_results\\\"\u003E\u003Cem\u003EUnix and Linux: Visual Quickstart Guide\u003C\u002Fem\u003E\u003C\u002Fa\u003E&quot;, 4th edition (2009) which covers interoperability in greater detail.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe files used in this lesson are available on &quot;\u003Ca href=\\\"https:\u002F\u002Fdoi.org\u002F10.6084\u002Fm9.figshare.1172094\\\"\u003EFigshare\u003C\u002Fa\u003E&quot;. The data contains the metadata for journal articles categorised under &#39;History&#39; in the British Library ESTAR database. The data is shared under a CC0 copyright waiver.\u003C\u002Fp\u003E\\n\u003Cp\u003EDownload the required files, save them to your computer, and unzip them. If you do not have default software installed to interact with .zip files, we recommend \u003Ca href=\\\"http:\u002F\u002Fwww.7-zip.org\u002F\\\"\u003E7-zip\u003C\u002Fa\u003E for this purpose. On Windows, we recommend unzipping the folder provided to your c: drive so the files are at \u003Ccode\u003Ec:\\\\proghist\\\\\u003C\u002Fcode\u003E. However, any location will work fine, but you may have to adjust your commands as you are following along with this lesson if you use a different location. On OS X or Linux, we similarly recommend unzipping them to your user directory, so that they appear at \u003Ccode\u003E\u002Fuser\u002FUSERNAME\u002Fproghist\u002F\u003C\u002Fcode\u003E. In both cases, this means that when you open up a new terminal window, you can just type \u003Ccode\u003Ecd proghist\u003C\u002Fcode\u003E to move to the correct directory.\u003C\u002Fp\u003E\\n\u003Chr\u003E\\n\u003Ch2 id=\\\"counting-files\\\"\u003ECounting files\u003C\u002Fh2\u003E\\n\u003Cp\u003EYou will begin this lesson by counting the contents of files using the Unix shell. The Unix shell can be used to quickly generate counts from across files, something that is tricky to achieve using the graphical user interfaces (GUI) of standard office suites.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn Unix the \u003Ccode\u003Ewc\u003C\u002Fcode\u003E command is used to count the contents of a file or of a series of files.\u003C\u002Fp\u003E\\n\u003Cp\u003EOpen the Unix shell and navigate to the directory that contains our data, the \u003Ccode\u003Edata\u003C\u002Fcode\u003E subdirectory of the \u003Ccode\u003Eproghist\u003C\u002Fcode\u003E directory. Remember, if at any time you are not sure where you are in your directory structure, type \u003Ccode\u003Epwd\u003C\u002Fcode\u003E and use the \u003Ccode\u003Ecd\u003C\u002Fcode\u003E command to move to where you need to be. The directory structure here is slightly different between OS X\u002FLinux and Windows: on the former, the directory is in a format such as \u003Ccode\u003E~\u002Fusers\u002FUSERNAME\u002Fproghist\u002Fdata\u003C\u002Fcode\u003E and on Windows in a format such as \u003Ccode\u003Ec:\\\\proghist\\\\data\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EType \u003Ccode\u003Els\u003C\u002Fcode\u003E and then hit enter. This prints, or displays, a list that includes two files and a subdirectory.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe files in this directory are the dataset \u003Ccode\u003E2014-01_JA.csv\u003C\u002Fcode\u003E that contains journal article metadata and a file containing documentation about \u003Ccode\u003E2014-01_JA.csv\u003C\u002Fcode\u003E called \u003Ccode\u003E2014-01_JA.txt\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe subdirectory is named \u003Ccode\u003Ederived_data\u003C\u002Fcode\u003E. It contains four \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FTab-separated_values\\\"\u003E.tsv\u003C\u002Fa\u003E files derived from \u003Ccode\u003E2014-01_JA.csv\u003C\u002Fcode\u003E. Each of these includes all data where a keyword such as \u003Ccode\u003Eafrica\u003C\u002Fcode\u003E or \u003Ccode\u003Eamerica\u003C\u002Fcode\u003E appears in the &#39;Title&#39; field of \u003Ccode\u003E2014-01_JA.csv\u003C\u002Fcode\u003E. The \u003Ccode\u003Ederived_data\u003C\u002Fcode\u003E directory also includes a subdirectory called \u003Ccode\u003Eresults\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003ENote: \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FComma-separated_values\\\"\u003ECSV\u003C\u002Fa\u003E files are those in which the units of data (or cells) are separated by commas (comma-separated-values) and TSV files are those in which they are separated by tabs.  Both can be read in simple text editors or in spreadsheet programs such as Libre Office Calc or Microsoft Excel.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EBefore you begin working with these files, you should move into the directory in which they are stored. Navigate to \u003Ccode\u003Ec:\\\\proghist\\\\data\\\\derived_data\u003C\u002Fcode\u003E on Windows or \u003Ccode\u003E~\u002Fusers\u002FUSERNAME\u002Fproghist\u002Fdata\u002Fderived_data\u003C\u002Fcode\u003E on OS X.\u003C\u002Fp\u003E\\n\u003Cp\u003ENow that you are here you can count the contents of the files.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe Unix command for counting is \u003Ccode\u003Ewc\u003C\u002Fcode\u003E. Type \u003Ccode\u003Ewc -w 2014-01-31_JA_africa.tsv\u003C\u002Fcode\u003E and hit enter. The flag \u003Ccode\u003E-w\u003C\u002Fcode\u003E combined with \u003Ccode\u003Ewc\u003C\u002Fcode\u003E instructs the computer to print a word count, and the name of the file that has been counted, into the shell.\u003C\u002Fp\u003E\\n\u003Cp\u003EAs was seen in &quot;\u003Ca href=\\\"..\u002Flessons\u002Fintro-to-bash\\\"\u003EIntroduction to the Bash Command Line\u003C\u002Fa\u003E&quot;, flags such as \u003Ccode\u003E-w\u003C\u002Fcode\u003E are an essential part of getting the most out of the Unix shell as they give you better control over commands.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf your research is more concerned with the number of entries (or lines) than the number of words, you can use the line count flag. Type \u003Ccode\u003Ewc -l 2014-01-31_JA_africa.tsv\u003C\u002Fcode\u003E and hit enter. Combined with \u003Ccode\u003Ewc\u003C\u002Fcode\u003E the flag \u003Ccode\u003E-l\u003C\u002Fcode\u003E prints a line count and the name of the file that has been counted.\u003C\u002Fp\u003E\\n\u003Cp\u003EFinally, type \u003Ccode\u003Ewc -c 2014-01-31_JA_africa.tsv\u003C\u002Fcode\u003E and hit enter. This uses the flag \u003Ccode\u003E-c\u003C\u002Fcode\u003E in combination with the command \u003Ccode\u003Ewc\u003C\u002Fcode\u003E to print a character count for \u003Ccode\u003E2014-01-31_JA_africa.tsv\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003ENote: OS X and Linux users should replace the \u003Ccode\u003E-c\u003C\u002Fcode\u003E flag with \u003Ccode\u003E-m\u003C\u002Fcode\u003E.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EWith these three flags, the most obvious thing historians can use \u003Ccode\u003Ewc\u003C\u002Fcode\u003E for is to quickly compare the shape of sources in digital format - for example word counts per page of a book, the distribution of characters per page across a collection of newspapers, the average line lengths used by poets. You can also use \u003Ccode\u003Ewc\u003C\u002Fcode\u003E with a combination of wildcards and flags to build more complex queries. Type \u003Ccode\u003Ewc -l 2014-01-31_JA_a*.tsv\u003C\u002Fcode\u003E and hit enter. This prints the line counts for \u003Ccode\u003E2014-01-31_JA_africa.tsv\u003C\u002Fcode\u003E and \u003Ccode\u003E2014-01-31_JA_america.tsv\u003C\u002Fcode\u003E, offering a simple means of comparing these two sets of research data. Of course, it may be faster to compare the line count for the two documents in Libre Office Calc, Microsoft Excel, or a similar spreadsheet program. But when wishing to compare the line count for tens, hundreds, or thousands of documents, the Unix shell has a clear speed advantage.\u003C\u002Fp\u003E\\n\u003Cp\u003EMoreover, as our datasets increase in size you can use the Unix shell to do more than copy these line counts by hand, by the use of print screen, or by copy and paste methods. Using the \u003Ccode\u003E&gt;\u003C\u002Fcode\u003E redirect operator you can export your query results to a new file. Type \u003Ccode\u003Ewc -l 2014-01-31_JA_a*.tsv &gt; results\u002F2014-01-31_JA_a_wc.txt\u003C\u002Fcode\u003E and hit enter. This runs the same query as before, but rather than print the results within the Unix shell it saves the results as \u003Ccode\u003E2014-01-31_JA_a_wc.txt\u003C\u002Fcode\u003E. By prefacing this with \u003Ccode\u003Eresults\u002F\u003C\u002Fcode\u003E it moves the .txt file to the \u003Ccode\u003Eresults\u003C\u002Fcode\u003E sub-directory. To check this, navigate to the \u003Ccode\u003Eresults\u003C\u002Fcode\u003E subdirectory, hit enter, type \u003Ccode\u003Els\u003C\u002Fcode\u003E, and hit enter again to see this file listed within \u003Ccode\u003Ec:\\\\proghist\\\\data\\\\derived_data\\\\results\u003C\u002Fcode\u003E on Windows or \u003Ccode\u003E\u002Fusers\u002FUSERNAME\u002Fproghist\u002Fdata\u002Fderived_data\u002Fresults\u003C\u002Fcode\u003E on OS X\u002FLinux.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"mining-files\\\"\u003EMining files\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe Unix shell can do much more than count the words, characters, and lines within a file. The \u003Ccode\u003Egrep\u003C\u002Fcode\u003E command (meaning &#39;global regular expression print&#39;) is used to search across multiple files for specific strings of characters. It is able to do so much faster than the graphical search interface offered by most operating systems or office suites. And combined with the \u003Ccode\u003E&gt;\u003C\u002Fcode\u003E operator, the \u003Ccode\u003Egrep\u003C\u002Fcode\u003E command becomes a powerful research tool can be used to mine your data for characteristics or word clusters that appear across multiple files and then export that data to a new file. The only limitations here are your imagination, the shape of your data, and - when working with thousands or millions of files - the processing power at your disposal.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo begin using \u003Ccode\u003Egrep\u003C\u002Fcode\u003E, first navigate to the \u003Ccode\u003Ederived_data\u003C\u002Fcode\u003E directory (\u003Ccode\u003Ecd ..\u003C\u002Fcode\u003E). Here type \u003Ccode\u003Egrep 1999 *.tsv\u003C\u002Fcode\u003E and hit enter. This query looks across all files in the directory that fit the given criteria (the .tsv files) for instances of the string, or character cluster, &#39;1999&#39;. It then prints them within the shell.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003ENote: there is a large amount of data to print, so if you get bored hit \u003Ccode\u003Ectrl+c\u003C\u002Fcode\u003E to cancel the action. Ctrl+c is used to cancel any process in the Unix shell.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EPress the up arrow once in order to cycle back to your most recent action. Amend \u003Ccode\u003Egrep 1999 *.tsv\u003C\u002Fcode\u003E to \u003Ccode\u003Egrep -c 1999 *.tsv\u003C\u002Fcode\u003E and hit enter. The shell now prints the number of times the string 1999 appeared in each .tsv file. Cycle to the previous line again and amend this to \u003Ccode\u003Egrep -c 1999 2014-01-31_JA_*.tsv &gt; results\u002F2014-01-31_JA_1999.txt\u003C\u002Fcode\u003E and hit enter. This query looks for instances of the string &#39;1999&#39; across all documents that fit the criteria and saves them as \u003Ccode\u003E2014-01-31_JA_1999.txt\u003C\u002Fcode\u003E in the \u003Ccode\u003Eresults\u003C\u002Fcode\u003E subdirectory.\u003C\u002Fp\u003E\\n\u003Cp\u003EStrings need not be numbers. \u003Ccode\u003Egrep -c revolution 2014-01-31_JA_america.tsv 2014-02-02_JA_britain.tsv\u003C\u002Fcode\u003E, for example, counts the instances of the string \u003Ccode\u003Erevolution\u003C\u002Fcode\u003E within the defined files and prints those counts to the shell. Run this and then amend it to \u003Ccode\u003Egrep -ci revolution 2014-01-31_JA_america.tsv 2014-02-02_JA_britain.tsv\u003C\u002Fcode\u003E. This repeats the query, but prints a case insensitive count (including instances of both \u003Ccode\u003Erevolution\u003C\u002Fcode\u003E and \u003Ccode\u003ERevolution\u003C\u002Fcode\u003E). Note how the count has increased nearly 30 fold for those journal article titles that contain the keyword &#39;revolution&#39;. As before, cycling back and adding \u003Ccode\u003E&gt; results\u002F\u003C\u002Fcode\u003E, followed by a filename (ideally in .txt format), will save the results to a data file.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou can also use \u003Ccode\u003Egrep\u003C\u002Fcode\u003E to create subsets of tabulated data. Type \u003Ccode\u003Egrep -i revolution 2014-01-31_JA_america.tsv 2014-02-02_JA_britain.tsv &gt; YEAR-MONTH-DAY_JA_america_britain_i_revolution.tsv\u003C\u002Fcode\u003E (where \u003Ccode\u003EYEAR-MONTH-DAY\u003C\u002Fcode\u003E is the date you are completing this lesson) and hit enter. This command looks in both of the defined files and exports any lines containing \u003Ccode\u003Erevolution\u003C\u002Fcode\u003E (without regard to case) to the specified .tsv file.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe data has not been saved to to the \u003Ccode\u003Eresults\u003C\u002Fcode\u003E directory because it isn&#39;t strictly a result; it is derived data. Depending on your research project it may be easier to save this to another subdirectory. For now have a look at this file to verify its contents and when you are happy, delete it using the \u003Ccode\u003Erm\u003C\u002Fcode\u003E command. \u003Cem\u003ENote: the \u003Ccode\u003Erm\u003C\u002Fcode\u003E common is very powerful and should be used with caution. Please refer to &quot;\u003Ca href=\\\"..\u002Flessons\u002Fintro-to-bash\\\"\u003EIntroduction to the Bash Command Line\u003C\u002Fa\u003E&quot; for instructions on how to use this command correctly.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EFinally, you can use another flag, \u003Ccode\u003E-v\u003C\u002Fcode\u003E, to exclude data elements when using the \u003Ccode\u003Egrep\u003C\u002Fcode\u003E command. Type \u003Ccode\u003Egrep -iv revolution 2014*_JA_a*.tsv &gt; 2014_JA_iv_revolution.csv\u003C\u002Fcode\u003E and hit enter. This query looks in the defined files (three in total) and exports all lines that do not contain \u003Ccode\u003Erevolution\u003C\u002Fcode\u003E or \u003Ccode\u003ERevolution\u003C\u002Fcode\u003E to \u003Ccode\u003Ec:\\\\proghist\\\\data\\\\derived_data\\\\2014_JA_iv_revolution.csv\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ENote that you have transformed the data from one format to another - from .tsv to .csv. Often there is a loss of data structure when undertaking such transformations. To observe this for yourself, run \u003Ccode\u003Egrep -iv revolution 2014*_JA_a*.tsv &gt; 2014_JA_iv_revolution.tsv\u003C\u002Fcode\u003E and open both the .csv and .tsv files in Libre Office Calc, Microsoft Excel, or a similar spreadsheet program. Note the differences in column delineation between the two files.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003ESummary\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EWithin the Unix shell you can now:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003Euse the \u003Ccode\u003Ewc\u003C\u002Fcode\u003E command with the flags \u003Ccode\u003E-w\u003C\u002Fcode\u003E and \u003Ccode\u003E-l\u003C\u002Fcode\u003E to count the words and lines in a file or a series of files.\u003C\u002Fli\u003E\\n\u003Cli\u003Euse the redirector and structure \u003Ccode\u003E&gt; subdirectory\u002Ffilename\u003C\u002Fcode\u003E to save results into a subdirectory.\u003C\u002Fli\u003E\\n\u003Cli\u003Euse the \u003Ccode\u003Egrep\u003C\u002Fcode\u003E command to search for instances of a string.\u003C\u002Fli\u003E\\n\u003Cli\u003Euse with \u003Ccode\u003Egrep\u003C\u002Fcode\u003E the \u003Ccode\u003E-c\u003C\u002Fcode\u003E flag to count instances of a string, the \u003Ccode\u003E-i\u003C\u002Fcode\u003E flag to return a case insensitive search for a string, and the \u003Ccode\u003E-v\u003C\u002Fcode\u003E flag to exclude a string from the results.\u003C\u002Fli\u003E\\n\u003Cli\u003Ecombine these commands and flags to build complex queries in a way that suggests the potential for using the Unix shell to count and mine your research data and research projects.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Chr\u003E\\n\u003Ch4 id=\\\"conclusion\\\"\u003EConclusion\u003C\u002Fh4\u003E\\n\u003Cp\u003EIn this lesson you have learnt to undertake some basic file counting, to query across research data for common strings, and to save results and derived data. Though this lesson is restricted to using the Unix shell to count and mine tabulated data, the processes can be easily extended to free text. For this we recommend two guides written by William Turkel:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EWilliam Turkel, &#39;\u003Ca href=\\\"http:\u002F\u002Fwilliamjturkel.net\u002F2013\u002F06\u002F15\u002Fbasic-text-analysis-with-command-line-tools-in-linux\u002F\\\"\u003EBasic Text Analysis with Command Line Tools in Linux\u003C\u002Fa\u003E&#39; (15 June 2013)\u003C\u002Fli\u003E\\n\u003Cli\u003EWilliam Turkel, &#39;\u003Ca href=\\\"http:\u002F\u002Fwilliamjturkel.net\u002F2013\u002F06\u002F20\u002Fpattern-matching-and-permuted-term-indexing-with-command-line-tools-in-linux\u002F\\\"\u003EPattern Matching and Permuted Term Indexing with Command Line Tools in Linux\u003C\u002Fa\u003E&#39; (20 June 2013)\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EAs these recommendations suggest, the present lesson only scratches the surface of what the Unix shell environment is capable of. It is hoped, however, that this lesson has provided a taster sufficient to prompt further investigation and productive play.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor many historians, the full potential of these tools may only emerge upon embedding these skills into a real research project. Once your research grows, and, with it, your research data, being able to manipulate, count and mine thousands of files will be extremely useful. For if you choose to build on this lesson and investigate the Unix shell further you will find that even a large collection of files which do not contain any alpha-numeric data elements, such as image files, can be easily sorted, selected and queried in the Unix shell.\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
