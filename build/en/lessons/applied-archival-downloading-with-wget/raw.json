{"metadata":{"title":"Applied Archival Downloading with Wget","layout":"lesson","date":"2013-09-13T00:00:00.000Z","authors":["Kellen Kurschinski"],"reviewers":["Nick Ruest","Konrad Lawson"],"editors":["Ian Milligan"],"difficulty":2,"exclude_from_check":["review-ticket"],"activity":"acquiring","topics":["web-scraping"],"abstract":"Now that you have learned how Wget can be used to mirror or download specific files from websites via the command line, it's time to expand your web-scraping skills through a few more lessons that focus on other uses for Wget's recursive retrieval function.","previous":"automated-downloading-with-wget","redirect_from":"/lessons/applied-archival-downloading-with-wget","avatar_alt":"Diagram of a well-drilling aparatus","doi":"10.46430/phen0022"},"html_body":"<p>{% include toc.html %}</p>\n<h2 id=\"background-and-lesson-goals\">Background and Lesson Goals</h2>\n<p>Now that you have learned how Wget can be used to mirror or download\nspecific files from websites like <a href=\"http://www.activehistory.ca\">ActiveHistory.ca</a> via the command\nline, it&#39;s time to expand your web-scraping skills through a few more\nlessons that focus on other uses for Wget&#39;s recursive retrieval\nfunction. The following tutorial provides three examples of how Wget can\nbe used to download large collections of documents from archival\nwebsites with assistance from the Python programing language. It will\nteach you how to parse and generate a list of URLs using a simple Python\nscript, and will also introduce you to a few of Wget&#39;s other useful\nfeatures. Similar functions to the ones demonstrated in this lesson can\nbe achieved using <a href=\"http://chronicle.com/blogs/profhacker/download-a-sequential-range-of-urls-with-curl/41055\">curl</a>, an open-source software capable of\nperforming automated downloads from the command line. For this lesson,\nhowever, we will focus on Wget and building your Python skills.</p>\n<p>Archival websites offer a wealth of resources to historians, but\nincreased accessibility does not always translate into increased\nutility. In other words, while online collections often allow historians\nto access hitherto unavailable or cost-prohibitive materials, they can\nalso be limited by the manner in which content is presented and\norganized. Take for example the <a href=\"http://www.collectionscanada.gc.ca/databases/indianaffairs/index-e.html\">Indian Affairs Annual Reports\ndatabase</a> hosted on the Library and Archives Canada [LAC] website. Say\nyou wanted to download an entire report, or reports for several decades.\nThe current system allows a user the option to read a plaintext version\nof each page, or click on the &quot;View a scanned page of original\nReport&quot; link, which will take the user to a page with LAC&#39;s embedded\nimage viewer. This allows you to see the original document, but it is\nalso cumbersome because it requires you to scroll through each\nindividual page. Moreover, if you want the document for offline viewing,\nthe only option is to <em>right click</em> –&gt; <em>save as</em> each image to a\ndirectory on your computer. If you want several decades&#39; worth of annual\nreports, you can see the limits to the current means of presentation\npretty easily. This lesson will allow you to overcome such an obstacle.</p>\n<h2 id=\"recursive-retrieval-and-sequential-urls-the-library-and-archives-canada-example\">Recursive Retrieval and Sequential URLs: The Library and Archives Canada Example</h2>\n<p>Let&#39;s get started. The first step involves building a script to generate\nsequential URLs using Python&#39;s ForLoop function. First, you&#39;ll need to\nidentify the beginning URL in the series of documents that you want to\ndownload. Because of its smaller size we&#39;re going to use the online war\ndiary for <a href=\"https://www.bac-lac.gc.ca/eng/CollectionSearch/Pages/record.aspx?app=fonandcol&amp;IdNumber=2005110&amp;new=-8585971893141232328\">No. 14 Canadian General Hospital</a> as our example. The\nentire war diary is 80 pages long. The URL for page 1 is\n<a href=\"http://data2.archives.ca/e/e061/e001518029.jpg\">http://data2.archives.ca/e/e061/e001518029.jpg</a> and the URL for page\n80 is &#39;<a href=\"http://data2.archives.ca/e/e061/e001518109.jpg\">http://data2.archives.ca/e/e061/e001518109.jpg</a>. Note that\nthey are in sequential order. We want to download the .jpeg images for\n<em>all</em> of the pages in the diary. To do this, we need to design a script\nto generate all of the URLs for the pages in between (and including) the\nfirst and last page of the diary.</p>\n<p>Open your preferred text editor (such as Komodo Edit) and enter the code\nbelow. Where it says &#39;integer 1′ type in &#39;8029′, where it says &#39;integer\n2′, type &#39;8110&#39;. The ForLoop will generate a list of numbers between\n&#39;8029&#39; and &#39;8110&#39;, but it will not print the last number in the range\n(i.e. 8110). To download all 80 pages in the diary you must add one to\nthe top-value of the range because it is at this integer where the\nForLoop is told to stop. This applies for any sequence of numbers you\ngenerate with this function. Additionally, the script will not properly\nexecute if <a href=\"http://en.wikipedia.org/wiki/Leading_zero\">leading zeros</a> are included in the range of integers, so\nyou must exclude them by leaving them in the string (the URL). In this\nexample I have parsed the URL so that only the last four digits of the\nstring are being manipulated by the ForLoop.</p>\n<pre><code class=\"language-python\">#URL-Generator.py\n\nurls = &#39;&#39;;\nf=open(&#39;urls.txt&#39;,&#39;w&#39;)\nfor x in range(&#39;integer1&#39;, &#39;integer2&#39;):\n    urls = &#39;http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\\n&#39; % (x)\n    f.write(urls)\nf.close\n</code></pre>\n<p>Now replace &#39;integer1′ and &#39;integer2′ with the bottom and top ranges of\nURLs  you want to download. The final product should look like this:</p>\n<pre><code class=\"language-python\">#URL-Generator.py\n\nurls = &#39;&#39;;\nf=open(&#39;urls.txt&#39;,&#39;w&#39;)\nfor x in range(8029, 8110):\n    urls = &#39;http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\\n&#39; % (x)\n    f.write(urls)\nf.close\n</code></pre>\n<p>Save the program as a .py file, and then click run the Python script.</p>\n<p>The ForLoop will automatically generate a sequential list of URLs\nbetween the range of two integers that you specified in the brackets,\nand will write them to a .txt file that will be saved in your\nProgramming Historian directory. The <code>%d</code> appends each sequential number\ngenerated by the ForLoop to the exact position you place it in the\nstring. Adding <code>\\n</code> to the end of the string removes line-breaks,\nallowing Wget to read the .txt file.</p>\n<p>You do not need to use all of the digits in the URL to specify the range\n– just the ones between the beginning and end of the sequence you are\ninterested in. This is why only the last 4 digits of the string were\nselected and <code>00151</code> was left intact.</p>\n<p>Before moving on to the next stage of the downloading process, make sure\nyou have created a directory where you would like to save your files,\nand, for ease of use, locate it in the main directory where you keep\nyour documents. For both Mac and Windows users this will normally be the\n&#39;Documents&#39; folder. For this example, we&#39;ll call our folder &#39;LAC&#39;. You\nshould move the urls.txt file your Python script created in to this\ndirectory.  To save time on future downloads, it is advisable to simply\nrun the program from the directory you plan to download to. This can be\nachieved by saving the URL-Generator.py file to your &#39;LAC&#39; folder.</p>\n<p>For Mac users, under your applications list, select <em>Utilities -&gt;\nTerminal</em>. For Windows Users, you will need to open your system&#39;s\nCommand Line utility.</p>\n<p>Once you have a shell open, you need to &#39;call&#39; the directory you want to\nsave your downloaded .jpeg files to. Type:</p>\n<pre><code class=\"language-bash\">cd ~/Documents\n</code></pre>\n<p>and hit enter. Then type:</p>\n<pre><code class=\"language-bash\">cd &#39;LAC&#39;\n</code></pre>\n<p>and press enter again. You now have the directory selected and are ready\nto begin downloading.</p>\n<p>Based on what you have learned from <a href=\"../lessons/automated-downloading-with-wget\">Ian Milligan&#39;s Wget\nlesson</a>, enter the following into\nthe command line (note you can choose whatever you like for your &#39;limit rate&#39;,\nbut be a responsible internet citizen and keep it under 200kb/s!):</p>\n<pre><code class=\"language-bash\">wget -i urls.txt -r --no-parent -nd -w 2 --limit-rate=100k\n</code></pre>\n<p><em>(Note: including &#39;-nd&#39; in the command line will keep Wget from\nautomatically mirroring the website&#39;s directories, making your files\neasier to access and organize).</em></p>\n<p>Within a few moments you should have all 80 pages of the war diary\ndownloaded to this directory. You can copy and move them into a new\nfolder as you please.</p>\n<h2 id=\"a-second-example-the-national-archives-of-australia\">A Second Example: The National Archives of Australia</h2>\n<p>{% include alert.html text=&#39;After this lesson was originally published, the National Archvies of Australia changed their URL patterns and broke the links provided here. We are preserving the original text for reference, however you may wish to <a href=\"#recursive-retrieval-and-wgets-accept--a-function\">skip to the next section</a>.&#39; %}</p>\n<p>Let&#39;s try one more example using this method of recursive retrieval.\nThis lesson can be broadly applied to numerous archives, not just\nCanadian ones!</p>\n<p>Say you wanted to download a manuscript from the National Archives of\nAustralia, which has a much more aesthetically pleasing online viewer\nthan LAC, but is still limited by only being able to scroll through one\nimage at a time. We&#39;ll use William Bligh&#39;s &quot;Notebook and List of\nMutineers, 1789&quot; which provides an account of the mutiny aboard the HMS\n<em>Bounty</em>. <a href=\"http://www.nla.gov.au/apps/cdview/?pi=nla.ms-ms5393-1\">On the viewer page</a> you&#39;ll note that there are 131 &#39;items&#39;\n(pages) to the notebook. This is somewhat misleading. Click on the first\nthumbnail in the top right to view the whole page. Now, <em>right-click -&gt;\nview image</em>. The URL should be\n&#39;<a href=\"http://nla.gov.au/nla.ms-ms5393-1-s1-v.jpg\">http://nla.gov.au/nla.ms-ms5393-1-s1-v.jpg</a>&#39;. If you browse through\nthe thumbnails, the last one is &#39;Part 127&#39;, which is located at\n&#39;<a href=\"http://nla.gov.au/nla.ms-ms5393-1-s127-v.jpg\">http://nla.gov.au/nla.ms-ms5393-1-s127-v.jpg</a>&#39;. The discrepancy\nbetween the range of URLs and the total number of files means that you\nmay miss a page or two in the automated download – in this case there\nare a few URLs that include a letter in the name of the .jpeg\n(&#39;s126a.v.jpg&#39; or &#39;s126b.v.jpg&#39; for example). This is going to happen\nfrom time to time when downloading from archives, so do not be surprised\nif you miss a page or two during an automated download.</p>\n<p>Note that a potential workaround\ncould include using regular expressions to make more complicated queries if appropriate\n(for more, see the <a href=\"/lessons/understanding-regular-expressions\">Understanding Regular Expressions</a>\nlesson).</p>\n<p>Let&#39;s run the script and Wget command once more:</p>\n<pre><code class=\"language-python\">#Bligh.py\n\nurls = &#39;&#39;;\nf=open(&#39;urls.txt&#39;,&#39;w&#39;)\nfor x in range(1, 128):\n    urls = &#39;http://www.nla.gov.au/apps/cdview/?pi=nla.ms-ms5393-1-s%d-v.jpg\\n&#39; % (x)\n    f.write(urls)\nf.close\n</code></pre>\n<p>And:</p>\n<pre><code class=\"language-bash\">wget -i urls.txt -r --no-parent -nd -w 2 --limit-rate=100k\n</code></pre>\n<p>You now have a (mostly) full copy of William Bligh&#39;s notebook. The\nmissing pages can be downloaded manually using <em>right-click -&gt; save\nimage as</em>.</p>\n<h2 id=\"recursive-retrieval-and-wgets-accept--a-function\">Recursive Retrieval and Wget&#39;s &#39;Accept&#39; (-A) Function</h2>\n<p>Sometimes automated downloading requires working around coding barriers.\nIt is common to encounter URLs that contain multiple sets of leading\nzeros, or URLs which may be too complex for someone with a limited\nbackground in coding to design a Python script for. Thankfully, Wget has\na built-in function called &#39;Accept&#39; (expressed as &#39;-A&#39;) that allows you\nto define what type of files you would like to download from a specific\nwebpage or an open directory.</p>\n<p>For this example we will use one of the many great collections available\nthrough the Library of Congress website: The Thomas Jefferson Papers. As\nwith LAC, the viewer for these files is outdated and requires you to\nnavigate page by page. We&#39;re going to download a selection from <a href=\"http://memory.loc.gov/cgi-bin/ampage?collId=mtj1&amp;fileName=mtj1page001.db&amp;recNum=1&amp;itemLink=/ammem/collections/jefferson_papers/mtjser1.html&amp;linkText=6\">Series\n1: General Correspondence. 1651-1827</a>. Open the link and then click on\nthe image (the .jpeg viewer looks awful familiar doesn&#39;t it?) The URL\nfor the image also follows a similar pattern to the war diary from LAC\nthat we downloaded earlier in the lesson, but the leading zeros\ncomplicate matters and do not permit us to easily generate URLs with the\nfirst script we used. Here&#39;s a workaround. Click on this\nlink:</p>\n<p><a href=\"http://memory.loc.gov/master/mss/mtj/mtj1/001/0000/\">http://memory.loc.gov/master/mss/mtj/mtj1/001/0000/</a></p>\n<p>The page you just opened is a sub-directory of the website that lists\nthe .jpeg files for a selection of the Jefferson Papers. This means that\nwe can use Wget&#39;s &#39;–A&#39; function to download all of the .jpeg images (100\nof them) listed on that page. But say you want to go further and\ndownload the whole range of files for this set of dates in Series 1 –\nthat&#39;s 1487 images. For a task like this where there are relatively few\nURLs you do not actually need to write a script (although you could\nusing my final example, which discusses the problem of leading zeros).\nInstead, simply manipulate the URLs in a .txt file as follows:</p>\n<p><a href=\"http://memory.loc.gov/master/mss/mtj/mtj1/001/0000/\">http://memory.loc.gov/master/mss/mtj/mtj1/001/0000/</a></p>\n<p><a href=\"http://memory.loc.gov/master/mss/mtj/mtj1/001/0100/\">http://memory.loc.gov/master/mss/mtj/mtj1/001/0100/</a></p>\n<p><a href=\"http://memory.loc.gov/master/mss/mtj/mtj1/001/0200/\">http://memory.loc.gov/master/mss/mtj/mtj1/001/0200/</a></p>\n<p>... all the way up to</p>\n<p><a href=\"http://memory.loc.gov/master/mss/mtj/mtj1/001/1400\">http://memory.loc.gov/master/mss/mtj/mtj1/001/1400</a></p>\n<p>This is the last sub-directory on the Library of Congress site for\nthese dates in Series 1. This last URL contains images 1400-1487.</p>\n<p>Your completed .txt file should have 15 URLs total. Before going any\nfurther, save the file as &#39;Jefferson.txt&#39; in the directory you plan to\nstore your downloaded files in.</p>\n<p>Now, run the following Wget command:</p>\n<pre><code class=\"language-bash\">wget -i Jefferson.txt -r --no-parent -nd -w 2 -A .jpg, .jpeg --limit-rate=100k\n</code></pre>\n<p>Voila, after a bit of waiting, you will have 1487 pages of presidential\npapers right at your fingertips!</p>\n<h2 id=\"more-complicated-recursive-retrieval-a-python-script-for-leading-zeros\">More Complicated Recursive Retrieval: A Python Script for Leading Zeros</h2>\n<p>The Library of Congress, like many online repositories, organizes their\ncollections using a numbering system that incorporates leading zeros\nwithin each URL. If the directory is open, Wget&#39;s –A function is a great\nway to get around this without having to do any coding. But what if the\ndirectory is closed and you can only access one image at a time? This\nfinal example will illustrate how to use a Python script to incorporate\nleading into a list of URLs. For this example we will be using the\n<a href=\"http://cushing.med.yale.edu/gsdl/collect/mdposter/\">Historical Medical Poster Collection</a>, available from the Harvey\nCushing/Jack Hay Whitney Medical Library (Yale University).</p>\n<p>First, we&#39;ll need to identify the URL of the first and last files we\nwant to download. We also want the high-resolution versions of each\nposter. To locate the URL for the high res image click on the first\nthumbnail (top left) then look below the poster for the link that says\n&#39;Click HERE for Full Image&#39;. If you follow the link, a high-resolution\nimage with a complex URL will appear. As was the case in the Australian\nArchives example, to get the simplified URL you must <em>right-click -&gt;\nview image</em> using your web-browser. The URL for the first poster should\nbe:</p>\n<p><a href=\"http://cushing.med.yale.edu/images/mdposter/full/poster0001.jpg\">http://cushing.med.yale.edu/images/mdposter/full/poster0001.jpg</a></p>\n<p>Follow the same steps for the last poster in the gallery – the URL\nshould be:</p>\n<p><a href=\"http://cushing.med.yale.edu/images/mdposter/full/poster0637.jpg\">http://cushing.med.yale.edu/images/mdposter/full/poster0637.jpg</a>.</p>\n<p>The script we used to download from LAC will not work because the range\nfunction cannot comprehend leading zeros. The script below provides an\neffective workaround that runs three different ForLoops and exports the\nURLs to a .txt file in much the same way as our original script. This\napproach would also work with the Jefferson Papers, but I chose to use\nthe –A function to demonstrate its utility and effectiveness as a less\ncomplicated alternative.</p>\n<p>In this script the poster URL is treated in much the same way as the URL\nin our LAC example. The key difference is that the leading zeros are\nincluded as part of the string. For each loop, the number of zeros in\nthe string decreases as the digits increase from single, to double, to\ntriple. The script can be expanded or shortened as needed. In this case\nwe needed to repeat the process three times because we were moving from\nthree leading zeros to one leading zero. To ensure that the script\niterates properly, a &#39;+&#39; should be added to each ForLoop as in the\nexample below.</p>\n<p>We do not recommend actually performing this download because of the\nsize and extent of the files. This example is merely intended to\nillustrate the how to build and execute the Python script.</p>\n<pre><code class=\"language-python\">#Leading-Zeros.py\n\nurls = &#39;&#39;;\nf=open(&#39;leading-zeros.txt&#39;,&#39;w&#39;)\n\nfor x in range(1,10):\n    urls += &#39;http://cushing.med.yale.edu/images/mdposter/full/poster000%d.jpg\\n&#39; % (x)\n\nfor y in range(10,100):\n    urls += &#39;http://cushing.med.yale.edu/images/mdposter/full/poster00%d.jpg\\n&#39; % (y)\n\nfor z in range(100,638):\n    urls += &#39;http://cushing.med.yale.edu/images/mdposter/full/poster0%d.jpg\\n&#39; % (z)\n\nf.write(urls)\nf.close\n</code></pre>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>These three examples only scratch the surface of Wget&#39;s potential.\nDigital archives organize, store, and present their content in a variety\nof ways, some of which are more accessible than others. Indeed, many\ndigital repositories store files using URLs that must be manipulated in\nseveral different ways to utilize a program like Wget. Wherever your\ndownloading may take you, new challenges and opportunities await. This\ntutorial has provided you with the core skills for further work in the\ndigital archive and, hopefully, will lead you to undertake your own\nexperiments in an effort to add new tools to the digital historian&#39;s\ntoolkit. As new methods for scraping online repositories become\navailable, we will continue to update this lesson with additional\nexamples of Wget&#39;s power and potential.</p>\n"}