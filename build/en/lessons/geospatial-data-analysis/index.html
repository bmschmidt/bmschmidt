<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-c09d08cd.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-c09d08cd.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/geospatial-data-analysis"),
					params: {lang:"en",lessons:"lessons",slug:"geospatial-data-analysis"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Using Geospatial Data to Inform Historical Research in R
</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h2 id="introduction">Introduction</h2>
<p>One primary focus of historical scholarship is explaining the complex relationships that influence change in the past. More often then not, patterns of change are understood and dependent on geographic understandings—or variations over space. The Great Migration, early development of the European economy, and the development of the American West are just a few topics that are both historical and geographic in nature. Combining knowledge of both space and time can provide a fuller understanding of events and transformations. This is even more evident when we are able to discern characteristics about the people/populations who are part of historical transformations/events.</p>
<p>One way this can be done is through an array of maps and visualizations that provide snapshots of geographic realities. Historians have long used maps spread over time to explain historical transformations such as economic developments and migration patterns. With the onset of digital technologies, more complex maps have developed, and scholars have integrated more complex information into maps alongside new geographic visualizations. [^1a]</p>
<p>The availability of historical Geographic Information Systems has expanded this further by providing scholars opportunities to analyze more data points using digital and statistical tools such as R. With these new tools, scholars can study a multitude of population characteristics that can be visualized and analyzed across variable regions and time periods. Essentially, this has allowed researchers to configure geographic data around their areas of inquiry, accelerating the growth of “spatial history.” With this data, scholars can inquire not only about how historical change relates to place, but also how it relates to a multitude of population characteristics, testing theories and questions in rapid fashion. Was there a possible link between railroad development and change in gender makeup of surrounding populations? Were there other relationships? What does this say about the development of these regions? We are limited only by the available data and the questions we choose to ask. These techniques offer scholars the opportunity to test assumptions and build understandings that may not be readily understood in traditional research or mapping techniques.</p>
<p>This tutorial will introduce scholars to some of these techniques for processing geospatial data, focusing on how geospatial data can be analyzed statistically as well as visualized. We will be comparing spatial regions and leveraging the variability of populations across defined spatial regions such as counties. This variability can provide insights into a broad range of social movements. Moreover, it can be used to assess significant variations in regions using some basic statistical models. Using these methods, we assume that there is a level of continuity or homogeneity within the defined regions. [^2] We can leverage these understandings to assess historical events and movements. For example, if a large proportion of members of a particular group come from a group of counties, the characteristics of these counties can provide insight into the nature of that movement; [^3] or if a set of events happen in particular counties, we can ask if there is something particular about these counties that could be linked to the event. In some cases, analysis can also reveal hidden realities about social movements or events based on their geographic nature. For example, if we step through characteristics of an area near a spatial event, we could discover an unknown characteristic we never realized correlated with the particular event, leading to new research possibilities. You can hopefully discover trends that may be surprising or some that we find are not as strong as has been assumed.</p>
<p>Specifically, this tutorial is going to use a membership list--with addresses--from a para-religious organization in America (PTL Ministries) and downloadable geographic data to assess population characteristics that could provide insights into an organization that is often characterized as more rural and less wealthy, alongside a host of other characteristics. The tutorial will then visualize and analyze this data to assess possible insights. This process will provide the basic tools and understandings that will allow scholars to assess other events and organizations that have geographic data. From this, you should be able to discover or challenge understandings of historical events using geospatial analysis.</p>
<h2 id="pre-requisites">Pre-requisites</h2>
<p>The work for this lesson will be done in R and R Studio, an open source statistical package used by data scientists, statisticians and other researchers. We are using R, because it is a widely-used open source tool that will allow us to both visualize and analyze our data using a multitude of methods that can be expanded upon quite easily. Some background knowledge of the software and statistics will be helpful. For introductions to R, I recommend the <a href="/lessons/r-basics-with-tabular-data">r-basics</a> tutorial  and the more comprehensive <a href="https://dh-r.lincolnmullen.com">Computational Historical Thinking</a> as starting points. There are many other services such as this <a href="https://www.coursera.org/learn/r-programming">MOOC</a> and <a href="https://www.datacamp.com/">DataCamp</a> that can introduce beginners to R&#39;s broader functionality. <a href="http://www.ats.ucla.edu/stat/r/default.htm">UCLA</a> also has a nice introduction.[^1] While this tutorial will attempt to step through the entire process in R, basic knowledge of R is needed. The tutorial also assumes users will have some knowledge about the event you are observing which you will use later as a means to test and contest assumptions.</p>
<h2 id="lesson-goals">Lesson Goals</h2>
<ul>
<li>Provide basic knowledge on how to use geographic data to analyze historical movements, especially movements where we have datasets or lists that are geographic in nature</li>
<li>Demonstrate how to merge geographic points or membership lists with geospatial data for further analysis</li>
<li>Demonstrate how to visualize this data geographically for analysis using choropleths</li>
<li>Highlight some statistical visualizations and models that can provide further insights</li>
</ul>
<h2 id="getting-started">Getting Started</h2>
<p>This tutorial will assume that you have <a href="https://www.youtube.com/watch?v=Ohnk9hcxf9M">set up R and R studio</a> or <a href="https://www.youtube.com/watch?v=ICGkG7Gg6j0">R studio for Mac</a>. Once setup, you should open up the program. I recommend creating a new project and setting up a working directory where you will store all of your information.</p>
<p>The first steps in R will be to load the necessary libraries that will allow R to perform the geographic functions used for this type of analysis:</p>
<pre><code class="language-r">install.packages(&quot;sf&quot;) # helps us work with spatial data
## for os specific requirments and directions see https://github.com/r-spatial/sf
install.packages(&quot;tmap&quot;) #helps create simple choropleths
install.packages(&quot;plotly&quot;) #helps create scatterplots
library(sf)
library(tmap)
library(plotly)
</code></pre>
<h2 id="the-data">The Data</h2>
<p>We are using two sources of data in this tutorial: our list of address from the organization, and the downloaded geospatial data that contains the demographic and geographic data that will aid our analysis. We are going to begin with geospatial data. This data is formatted as a <a href="https://www.esri.com/library/whitepapers/pdfs/shapefile.pdf">shapefile</a>. Shapefiles are data files that represent geographic regions and can also contain characteristics about that region. The U.S. census contains a bevy of information in shapefile format.</p>
<p>But, in order to get this information from historic censuses we are going to use data from the <a href="https://www.nhgis.org">National Historical Geographic Information System (NHGIS)</a> which is managed by the Minnesota Population Center at the University of Minnesota. NHGIS is a warehouse of historical census data covering the span of U.S. history. In order to use their services, you must first register and create an account. Once completed you can use their <a href="https://data2.nhgis.org/main">datafinder</a> to select the geographic level, time period, and the data that interests you. This <a href="#">lesson</a> provides detailed instructions on how to use their services to extract county-level census information, but the current lesson will provide the downloaded data.</p>
<p>If you are looking nationally prior to 1990, the county-level data is often your best bet as more precise geographic levels had not been standardized. For some regions and cities, however, there are more precise levels and in some cases smaller than zip codes. For this tutorial, we will use county level data from a time-appropriate decennial census. In general, it is best to use the smallest geographic region possible, but for historical research that often ends up being at the county level. In general, larger population centers have more detailed historical data, but rural areas were not completely covered until the 1990 census. For a more detailed description of the census regions and an interactive map see <a href="https://www.nhgis.org/user-resources/data-availability#table-data">NHGIS&#39;s discussion</a>.</p>
<h2 id="reading-the-data">Reading the Data</h2>
<p>We start by loading in the selected data. The data for this tutorial can be <a href="/assets/geospatial-data-analysis/data.zip">dowloaded here</a>. Once downloaded place all the files in a folder labeled data inside your working directory in R. We are going to create a variable and read in our data from our variable directory to it. Once run, the <code>County_Aggregate_Data</code> variable will contain the data and geographic information that we will analyze:</p>
<pre><code class="language-r">County_Aggregate_Data &lt;- st_read(&quot;./data/County1990ussm/&quot;)
</code></pre>
<p>We should now have a data object loaded with attached data:</p>
<p><img src="/images/geospatial-data-analysis/DataLoaded.png" alt="DataLoaded.png" title="Data Loaded in R"></p>
<p>If you are only interested in looking at particular states, I recommend filtering the results to speed up processing and data analysis. To accomplish this, use the following commands:</p>
<pre><code class="language-r">County_Aggregate_Data &lt;- County_Aggregate_Data[which(County_Aggregate_Data$STATENAM %in% c(&quot;North Carolina&quot;,&quot;South Carolina&quot;)),]
</code></pre>
<p>Following this command, I usually take a look at the distribution of the data using the summary command to ensure I am looking at the newly filtered data:</p>
<pre><code class="language-r">summary(County_Aggregate_Data)
</code></pre>
<p>This will return a bunch of summary data but most importantly it is showing that I have data only for the states I am filtering on:</p>
<p><img src="/images/geospatial-data-analysis/Data2.png" alt="Data2.png" title="Data Loaded in R Two"></p>
<p>Optionally, you can also plot the results to view a map of the data that you have downloaded. This could take some time, especially if you are not filtering the data. As above, this helps confirm that you are looking at the right geographic areas as only the filtered areas should be drawn. Below we will use R&#39;s basic graphing function to do this:</p>
<pre><code class="language-r">plot(County_Aggregate_Data$geometry,axes=TRUE)
</code></pre>
<p><img src="/images/geospatial-data-analysis/NCSC.png" alt="NCSC.png" title="FIRST DATA PLOT"></p>
<h2 id="merging-census-data">Merging Census Data</h2>
<p>Currently, our <code>County_Aggregate_Data</code> variable has the necessary geographic boundaries for our analysis (as the above plot highlighted), but not the demographic information that will allow us to assess characteristics of our membership list. Although the demographic data came along with the geographic data, it needs to be merged into our <code>County_Aggregate_Data</code> variable which is a SpatialDataFrame. The next step is to begin merging <code>County_Aggregate_Data</code> with NHGIS table data in the downloaded data directory.</p>
<p>Let&#39;s read in the NHGIS data and merge it on the common field. The <code>by.x</code> and <code>by.y</code> parameters indicate what fields the data is being joined on:</p>
<pre><code class="language-r">Census_Data &lt;- read.csv(&quot;./data/County1990_Data/nhgis0027_ts_nominal_county.csv&quot;, as.is=TRUE)
County_Aggregate_Data &lt;- merge(County_Aggregate_Data,Census_Data,by.x=&quot;GISJOIN&quot;, by.y=&quot;GISJOIN&quot;)
</code></pre>
<p>The number of variables in <code>County_Aggregate_Data</code> should now increase as all of table data is brought into this one object. We now have one large <code>SpatialDataFrame</code> that has all of the geographic and statistical data we downloaded. We could stop and analyze this data as it undoubtedly contains many insights but it is only the raw census data and not yet connected to the historical event or data we are analyzing.</p>
<h2 id="merging-external-data">Merging External Data</h2>
<p>The next step is to merge our list with our <code>SpatialDataFrame</code> so we can perform our analysis. While we are using a membership list, it can be any list that is geographic in nature. For example, you may have a list of events that happened during a particular time period; or a list of places an individual chooses to visit. This type of data will come in two basic formats. The first is information such as locations, address, or incident locations--which will be converted to geographic coordinates. The second will be a table that lists the same information alongside the county (or geographic region) where it occurred. We can handle either.</p>
<h2 id="geocoding">Geocoding</h2>
<p>In the first case we have raw addresses of the members of our organization which will necessitate some additional steps. The address will need be transformed into geographical points in a process called <a href="https://en.wikipedia.org/wiki/Geocoding">geocoding</a>. This will create geographic points--from addresses--that can be linked to spatial regions in our downloaded census data so that we can analyze it to help us discover trends related to geographic location of these addresses. R can do some of this work but if you have a large number of addresses, you will need to use an external service because the free services R uses (such as google) will cap how many address you can geocode in a day. One popular outside service is hosted by <a href="http://geoservices.tamu.edu/Services/Geocode/">Texas A&amp;M Geocoding Services</a> and can handle large batches at a reasonable price. In the end, our address will be transformed into a list of latitudes and longitudes. This is the data R needs.</p>
<p>If you have less than 2,500 addresses this can be handled in R using Google&#39;s geocoder. In R, you must first gather the address from whatever dataset you have, and then transform it. In our example, the data has already been geocoded, but below is an example of the commands used when processing a list of address and turning them into a list of geographic coordinates:</p>
<pre><code class="language-r">Addresses &lt;- data$Address
Member_Coordinates &lt;- geocode(Addresses)
</code></pre>
<p>In our example, we already have a list of geographic coordinates. But we still need to merge it with our <code>SpatialDataFrame</code> (<code>County_Aggregate_Data</code>) so we can analyze it in relation to the census and geographic data we have downloaded. First, we either get the externally geocoded data or the newly geocoded data. Since our data has been geocoded, we will use the first command below to pull in that data.</p>
<pre><code class="language-r">geocoded_addresses &lt;- read.csv(&quot;./data/GeocodedAddresses.csv&quot;, as.is=TRUE)
#or
geocoded_addresses &lt;- Member_Coordinates  ##if you have just geocoded these address
</code></pre>
<p>We now need to remove the records with empty data that represent addresses that could not be geocoded:</p>
<pre><code class="language-r">#Now remove empty data or rows that failed to geocode
geocoded_addresses &lt;- geocoded_addresses[!is.na(geocoded_addresses$Latitude) &amp; !is.na(geocoded_addresses$Longitude),]
</code></pre>
<p>Then we convert the data into a <code>SpatialDataFrame</code> so that can be merged[^7] with <code>County_Aggregate_Data</code> which contains previously imported data. We can see the process below:</p>
<pre><code class="language-r">#Now create the dataframe with geographic information for the merge
points &lt;- st_as_sf(geocoded_addresses, coords= c(&quot;Longitude&quot;,&quot;Latitude&quot;),crs = 4326, agr = &quot;constant&quot;)
</code></pre>
<p>Before we do the actual merge, we should ensure both objects are using the same coordinate systems otherwise the points and counties will not match up throwing everything off. To do that we transform our census data to our current system.</p>
<pre><code class="language-r">County_Aggregate_Data &lt;- st_transform(County_Aggregate_Data, st_crs(points))
</code></pre>
<p>Then I like to glimpse at the distribution of the point data within the census. We do this for a couple of reasons: first to verify that the merge will function correctly; secondly, to begin to look at the data distribution. We should see a list of numbers where each list represents the points that intersected with a particular county. Many of the insights we gain will come from this distribution. If counties with particular characteristics show a higher distribution, that can provide insights into our membership.  We will be looking at this more in depth as we proceed, but we are beginning to see some information here:</p>
<pre><code class="language-r">st_intersects(County_Aggregate_Data,points) # show which counties each point falls into
</code></pre>
<p>We can also place the data points on top of our map for a quick visual of our data again using plot and <a href="https://www.statmethods.net/advgraphs/parameters.html">some parameters</a> for better visualization:</p>
<pre><code class="language-r">plot(County_Aggregate_Data$geometry,axes=TRUE)
plot(points[which(points$State %in% c(&quot;NC&quot;,&quot;SC&quot;)),]$geometry,col = &quot;green&quot;, pch=20,cex=.5, axes=TRUE,add=TRUE)
</code></pre>
<p>Now we do the merge. This merge is a bit different than the earlier merge because we are going to create a new field that represents the number of &#39;hits&#39; within a county. Essentially, the <code>CountMembers</code> variable will now represent the number of members in a particular county which will, like the distribution data above, allow us to begin to get insights from the data. We are using the above st_intersects alongside R&#39;s <code>sapply</code> to compute this value. In essence, we are transforming our lists into count data so we can visualize and analyze the data:</p>
<pre><code class="language-r">County_Aggregate_Data$CountMembers &lt;- sapply(st_intersects(County_Aggregate_Data,points), function(z) if (length(z)==0) NA_integer_ else length(z))
</code></pre>
<p>Now we have a large dataframe called <code>County_Aggregate_Data</code> which has our count data and our census data by county. CountMembers now contains the count of members for their respective counties. But we may also want to merge data that is not a geographic point but rather a count of events/members and associated counties. Essentially, this data that is already tallied for the geographic regions we are interested in. This data should come from roughly the same timeframe as the spatial data for accuracy. In our example, we have a list of churches by denomination, which will hopefully give us additional insight into our data as we can assess if counties with a high number of churches of particular denomination also tend to be high in membership to our organization. To do this, merge we need to load the list:</p>
<pre><code class="language-r">religion &lt;- read.csv(&quot;./data/Religion/Churches.csv&quot;, as.is=TRUE)
</code></pre>
<p>Depending on the state of the data you may need to do some data transformations in order to merge it back with the DataFrame. For complex transformations, see tutorials in R on working with data such as <a href="/en/lessons/data_wrangling_and_management_in_R">Data Wrangling and Management in R tutorial</a> <a href="http://r4ds.had.co.nz/transform.html">data transforms</a>. In essence, you need to have a common field in both datasets to merge upon. Often this is a geographic id for the county and state represented by <code>GEOID</code>. It could also be the unique FIPS Code given by the US Census. Below I am using state and county <code>GEOID</code>. In this example, we are converting one data frame&#39;s common fields to numeric so that they match the variable type of the other dataframe:</p>
<pre><code class="language-r">religion$STATEFP &lt;- religion$STATE
religion$COUNTYFP &lt;-religion$COUNTY
County_Aggregate_Data$STATEFP &lt;- as.numeric(as.character(County_Aggregate_Data$STATEFP))
County_Aggregate_Data$COUNTYFP &lt;- as.numeric(as.character(County_Aggregate_Data$COUNTYFP))
</code></pre>
<p>We then merge the data with the <code>SpatialDataFrame</code>, merging where state and counties ids match. This method is similar to the merge method used on the earlier merge, but we are now merging on multiple fields. To handle this situation, we are using a different format:</p>
<pre><code class="language-r">County_Aggregate_Data&lt;- merge(County_Aggregate_Data,religion,by=c(&quot;STATEFP&quot;,&quot;COUNTYFP&quot;))
</code></pre>
<p>This will bring in all additional fields into our <code>SpatialDataFrame</code>.</p>
<p>Now we have a large <code>SpatialDataFrame</code> called <code>County_Aggregate_Data</code> which has our geocoded count data, our external count data and our census data by county. It is now time to begin to look at the data distribution and assess if everything appears correct and is in a format that will allow for some visualization and data analysis. We have some inherent complexity to our data because it is considered &quot;count data.&quot; As such, we should be cognizant that our data is not measuring individuals directly but rather relationships between counties. We are attempting to discover if counties with certain traits lead to higher membership in our datasets. These realities can help us gather some assumptions on the individuals in these regions.</p>
<h2 id="visualizing">Visualizing</h2>
<p>Because we are analyzing geospatial data, it is often best to begin with geographic visuals. There are many options here, but I find it easiest to start with the qtm function from the TMAP library which creates <a href="https://en.wikipedia.org/wiki/Choropleth_map">choropleth</a> maps simply. We could also use [GGPlot2]<a href="http://web.archive.org/web/20190922234254/http://strimas.com/r/tidy-sf/">(http://strimas.com/r/tidy-sf/</a>) which which should be installed using the development version.</p>
<p>Now, we are going to prepare the map and look at some census data. First on our list should be membership numbers relative to population (relative membership distribution). One of the most commonly used and clearest ways to display this information is by number of members per 10,000 people. We will then do the math to create a relative population variable(number of members per 10,000 people). We do this because we have to ensure we are taking into account the variability of populations within the census regions that we are analyzing otherwise we will get misleading visualization in densely populated counties that represent general population trends rather than variable relationships. If we did not take this step, we would undoubtedly see a map that highlights urban areas rather than areas where membership is strongest.</p>
<p>To begin looking at this data, we need to find the variable in our <code>SpatialDataframe</code> that represents population. In the downloaded census data folders, there is a codebook that will reveal what fields represent what data. After looking through the codebook, I discovered AV0AA1990 is the total Census population as of 1990. Below, I take this variable and transform it into a variable that adjusts for population fluctuations(number of members per 10,000 people):</p>
<pre><code class="language-r">County_Aggregate_Data$RelativeTotal= ((County_Aggregate_Data$AV0AA1990/10000)/County_Aggregate_Data$CountMembers )
</code></pre>
<p>Now we will create the map. TMAP allows for the quick creation of thematic maps or more specifically choropleths. We can also vary text size based on another census variable. Here I am using the count of people living in rural areas (A57AA1980), making the text larger in more rural counties. Now I can start to assess visually if counties with higher distributions of membership also tend to be more rural as has been described. As the data shows, the membership is not clearly biased towards rural counties exclusively, giving us our first insight:</p>
<pre><code class="language-r">qtm(shp = County_Aggregate_Data, fill = &quot;RelativeTotal&quot;,text=&quot;NHGISNAM&quot;,text.size=&quot;A57AA1980&quot;)
</code></pre>
<p><img src="/images/geospatial-data-analysis/CH1.png" alt="CH1.png" title="Cholopleth of Normalized Data"></p>
<p>Feel free to experiment with the choropleth. In particular, try switching out the text.size variable to see if you can discover patterns that might appear to be linked to membership. Can you detect any trends between choropleth colors and text size? The income variable would be another test that could be run to see if counties with larger representation are wealthier. These visualizations, of course, are also be useful as a means to present information.</p>
<p>You can also look and the unadjusted distribution which shows the raw distribution of members(without adjusting for local population distribution) as I did below[^9]:</p>
<pre><code class="language-r">qtm(shp = County_Aggregate_Data, fill = &quot;CountMembers&quot;,text=&quot;NHGISNAM&quot;,text.size=&quot;A57AA1980&quot;)
</code></pre>
<h2 id="visualizing-data-relationships">Visualizing Data Relationships</h2>
<p>While choropleths and their many variations are an extremely helpful way to visualize the geospatial data, there are other methods that help visualize the data. One helpful method is the scatterplot which provides a visual means to show relationships between two variables. In particular, it is useful to assess if there are correlations between our event data and other characteristics as defined by the census data. For example, do we see a correlation between counties with low average income and membership. If so, that might indicate something about the nature of the movement or organization. We could look at a multitude of factors along these lines and our census data and codebook has many. While <a href="http://www.nature.com/nmeth/journal/v12/n10/full/nmeth.3587.html">correlations do not alone prove causality</a>, they provide basic insight. When doing these comparisons, we have to again ensure we are taking into account the variability of populations within the census regions we are analyzing otherwise we will get misleading correlation in densely populated counties. To do this we need to convert any population number into numbers per 10,000 people.</p>
<p>If, for example, we wanted to use <code>B18AA1990</code> which is the persons-white variable we would convert it to relative number:</p>
<pre><code class="language-r">WhitePer10K &lt;- ((County_Aggregate_Data$B18AA1990/County_Aggregate_Data$TOTPOP)*10000)
</code></pre>
<p>Other total data should take regional size into account as well. For example, if we wanted to look at churches of a particular denomination, we would need to convert that as well because larger counties would inherently be more likely to have churches of any particular denomination, presenting misleading correlations. To look at <code>AOG.C</code> which is Assemblies of God churches we would:</p>
<pre><code class="language-r">Assemblies_Of_God_ChurchesPer10K &lt;- ((County_Aggregate_Data$AOG.C/County_Aggregate_Data$CHTOTAL)*10000)
</code></pre>
<p>We could then plot this variable with the membership variable to inspect for correlations.</p>
<pre><code class="language-r">plot(Assemblies_Of_God_ChurchesPer10K,County_Aggregate_Data$BD5AA1990)
</code></pre>
<p>This previous command will result in a notable but small correlation, which makes sense since the para-church organization was affiliated with the Assemblies of God denomination. Most often, we are going to be comparing data points to our historical data, but we can also inspect for other relationships in the general census data that can provide basic information about the investigative areas. For example, here is scatterplot of race and per capita income in the Carolinas:</p>
<pre><code class="language-r">plot(WhitePer10K,County_Aggregate_Data$BD5AA1990)
</code></pre>
<p>Below we see the results of the above code. We see what is described as a strong positive correlation, which is typical in the United States as there are strong correlations between race and income. As the percentage of white people increases, the per-capita income rises accordingly. The dots on plot represent the graphed points of these two values. We can measure that statistically, but we can also see it visually.</p>
<p><img src="/images/geospatial-data-analysis/Plot.png" alt="Plot.png" title="Scatterplot of White people to per-capita income"></p>
<p>We can see this more precisely by adding a line of best fit to the plot which represents an estimated values based on the data presented. I also added red lines representing the distance from this line known as residuals. In essence, this showing us that we see a correlation between these two variables and it can be modeled with some accuracy.</p>
<pre><code class="language-r">x &lt;- WhitePer10K
y &lt;- County_Aggregate_Data$BD5AA1990
mod1 &lt;- lm(x ~ y)
plot(x ~ y,xlab=&quot;Per capita income in previous year&quot;,ylab=&quot;White People Per 10k&quot;)
summary(mod1)
abline(mod1)
res &lt;- signif(residuals(mod1), 5)
pre &lt;- predict(mod1) # plot distances between points and the regression line
segments(y, x, y, pre, col=&quot;red&quot;)
</code></pre>
<p>Here we see it:</p>
<p><img src="/images/geospatial-data-analysis/Fit.png" alt="Fit.png" title="Scatter Plot with Residuals"></p>
<p>Below, let&#39;s set up a variable to try to take a look at some of the variables to look for possible correlations. Below we are going to create a variable that measures the distribution of denominational churches in a county, which will allow us measure if our membership is correlated with a particular denomination:</p>
<pre><code class="language-r">Assemblies_Of_God_Churches_Per10K &lt;- ((County_Aggregate_Data$AOG.C/County_Aggregate_Data$CHTOTAL)*10000)
MembersPer10K &lt;- as.integer(((County_Aggregate_Data$CountMembers/County_Aggregate_Data$TOTPOP)*100000))
</code></pre>
<p>Now we will create a plot which show a small but significant correlation which makes sense since our organization is affiliated with this denomination. You can measure this statistically as well by using the <a href="https://www.r-bloggers.com/r-tutorial-series-simple-linear-regression/">lm function</a> which we will not cover:</p>
<pre><code class="language-r">plot(MembersPer10K,Assemblies_Of_God_Churches_Per10K)
</code></pre>
<p>We did a regular plot of the data but it is better to account for the fact that this is count data. Correlations and scatterplots are great ways to assess relationships, but they can be problematic with count data as it is often not linear or normally distributed and scatter plots work best when both of these <a href="https://www.statisticssolutions.com/assumptions-of-linear-regression/">conditions are true</a>. And historical data is often counts of people or occurrences. Because of this, I recommend taking a look at the distribution of the count data to asses relationships. For that I am going to use a <a href="https://www.r-bloggers.com/how-to-make-a-histogram-with-basic-r/">histogram</a> which is commonly used to represent distributions of data:</p>
<pre><code class="language-r">hist(County_Aggregate_Data$CountMembers,breaks = 15)
</code></pre>
<p><img src="/images/geospatial-data-analysis/Bar.png" alt="NCSC.png" title="Distribution Plot with Histogram"></p>
<p>OK, there are a significant number of low values which is typical of this type of information and some counties that are much higher than others.[^4]</p>
<p>A somewhat simple way to handle this is to perform a logarithmic transformation on a variable of the scatter plot to inspect for possible non-linear relationships. We add 1 to the values[^5] because log(0) is undefined. You could use .5 as some people do as well. Below we will analyze if there is a relationship between membership numbers and the count of churches in the counties observed using a log transformation. This can sometimes bring out correlations in count data that may have not been obvious using a non-adjusted scatterplot:</p>
<pre><code class="language-r">plot(MembersPer10K, log(Assemblies_Of_God_Churches_Per10K+1))
</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>Through this process, we have gathered and transformed geospatial data into a useable form. We have also created some visuals from this data, analyzing trends in the membership list of our organization. This tutorial should provide you with a basic template on how to take historical data and begin using geospatial analysis to analyze phenomenons such as the one we covered. In our case, the results illustrated that membership was not highly correlated with people who live in rural counties, suggesting that early characterizations of this movement as rural may not be entirely true, while we can see a slight relationship between the Assemblies of God and membership. This is just the beginning  of the possible means of inquiry. If we were to continue investigating, we could now start creating choropleths and scatter plots with other variables, looking for trends.  As you get more advanced, you can utilize some more advanced methods that can improve analysis as well.</p>
<h2 id="other-models-and-visualizations">Other Models and Visualizations</h2>
<p>There are many other models and visualizations available that can bring insight but they also add some complexity which demand further statistical understandings. For example, You can also create more complex scatterplots that can provide further insights. <a href="https://plot.ly/r/">Plot.ly</a> offers interactive scatter plots that can be customized and shared.[^8]. While statistical modeling usually focuses on a particular model&#39;s predictive insight, well-fit models also provide insight into the data they represent. In particular, the Poisson regression is frequently used to create <a href="http://www.theanalysisfactor.com/regression-models-for-count-data/">models of count data</a> which is how population data is often represented. <a href="https://rstudio-pubs-static.s3.amazonaws.com/44975_0342ec49f925426fa16ebcdc28210118.html">Geographically Weighted Regressions</a> also have particular advantages with this type of data. But assessing fit has some complexity. <a href="hhttps://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/">Decision trees</a> could also be useful for historical data because they give an understandable graphical representation of the the leading factors that caused inclusion in a group or list. Principal component analysis, <a href="/en/lessons/correspondence-analysis-in-R">correspondence analysis</a> and other clustering methods can also be helpful, especially when there is limited knowledge or insight into the event being analyzed yet there is an abundance of data associated with the event. I recommend background reading or discussions with a data scientist or statistician when exploring some of these modeling options as understanding the configuration and parameters of the individual models is essential to ensuring the results are trustworthy and significant.</p>
<p>[^1]: For an overview of R as it relates to the humanities with a chapter geospatial data also see Arnold Taylor and Lauren Tilton, Humanities Data in R (Cham: Springer, 2015). They also have a geospatial chapter that uses the sp library.</p>
<p>[^1a]: For a broader discussion on the role of geographic information and GIS in the humanities see Placing History: How Maps, Spatial Data, and GIS Are Changing Historical Scholarship (Esri Press, 2008) and Harris, Trevor M., John Corrigan, and David J. Bodenhamer, The Spatial Humanities: GIS and the Future of Humanities Scholarship (Bloomington: Indiana University Press, 2010).</p>
<p>[^2]: For a discussion on the benefits and drawbacks on this methodology and its assumptions see, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3732658/">Spatializing health research</a>. Some states like Kentucky have a larger number of counties (120) which often encompass entire cities which often leads to more homogeneity within those regions. In contrast, a state like Massachusetts has only 14 counties which can lead to more variability with the county geographies leading to more questionable results in some cases.</p>
<p>[^3]: This is often leveraged in the field of public health. See for example, <a href="https://www.cdc.gov/pcd/issues/2015/14_0404.htm">Spatial Analysis and Correlates of County-Level Diabetes Prevalence</a>. Other fields such as criminal justice also rely on similar analytics although criminal justice tends to look at smaller census areas within regions. See, for example, <code>https://www.ncjrs.gov/pdffiles1/nij/grants/204432.pdf</code></p>
<p>[^4]: Count data typically has large numbers of zero values which can add some complexity that will not be covered here. There are more complex ways to minimize this using more complex regression models. See, for example <a href="https://stats.idre.ucla.edu/stata/seminars/regression-models-with-count-data/">Regression Models with Count Data</a>. For general description of what normal distributions, which work well without modification look like see normal <a href="http://www.statisticshowto.com/probability-and-statistics/normal-distributions/">distributions</a></p>
<p>[^5]: There are different strategies to dealing with this type of data. See for example, <a href="http://www.sciencedirect.com/science/article/pii/S0031405608000073">The Excess-zero Problem in Soil Animal Count Data</a> or <a href="http://www.biostathandbook.com/transformation.html">Data Transformations</a>.</p>
<p>[^6]: For details on ggmap and and integration with Google Maps or other maps services see the <a href="http://stat405.had.co.nz/ggmap.pdf">ggmap overview</a>. For another broader discussions on google map making that utilizes a few of the libraries in this tutorial see <a href="https://rpubs.com/nickbearman/r-google-map-making">R and Google Map Making</a>. For a discussion of the sf library and it relationship to sp see <a href="https://cran.r-project.org/web/packages/sf/vignettes/sf1.html">Simple Features for R</a>. While sp has been the library spatial analysis library of choice, it is being superseded by sf.</p>
<p>[^7]: We are setting Coordinate Reference System(CRS) to EPSG 4326 which is the most common mapping system used int the U.S. It is used by Google  which is the origins of our data. EPSG 3857 is also used by google. For more on CRS see <a href="https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-to-coordinate-reference-systems/">Coordinate Reference Systems &amp; Spatial Projections</a>. Also see <a href="http://web.archive.org/web/20200225021219/https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf">coordinate systems reference in R</a>.</p>
<p>[^8]: These plots are a bit more complex and requires an extra library, but they have some advantages. They work well with complex datasets because they have the ability to model more than two relationships by altering the color or size of the data points(we did this earlier on the choropleths by altering font size). Moreover, they are interactive which allows you to explore extra information about data points after the plot is created without wrecking the visual makeup of the plot. Here is an example that looks at the relationship between income and membership but also adds urban status to the visual using color. I am also adjusting point size based on population so I can take a look at more populated areas alongside the other data:</p>
<pre><code class="language-r">library(plotly)

var = County_Aggregate_Data$A57AA1990
bins = unique(quantile(var, seq(0,1,length.out=8)))
interv = findInterval(var, bins)
County_Aggregate_Data$People_Urban &lt;-interv

p &lt;- plot_ly(County_Aggregate_Data, type = &quot;scatter&quot;, mode = &quot;markers&quot;) %&gt;%
    add_trace(x = ~(AV0AA1990/10000)/CountMembers,
              y = ~BD5AA1990,
              size = ~AV0AA1990,
              color = ~People_Urban,
              text = ~paste(&quot;AVG Incom: &quot;,BD5AA1990 ,
                            &#39;$&lt;br&gt;County:&#39;, COUNTY.y,
                            &#39;$&lt;br&gt;State:&#39;, STATENAM,
                            &#39;$&lt;br&gt;Members:&#39;, CountMembers),
              hoverinfo = &quot;text&quot;) %&gt;%
    layout(title = &#39;Members and Income, Size=Population&#39;,
           xaxis = list(title = &#39;Members per 10k population&#39;),
           yaxis = list(title = &#39;Income&#39;),
           hoverlabel = list(font = list(size = 16)))

p
</code></pre>
<p><img src="/images/geospatial-data-analysis/Ply1.png" alt="Ply1.png" title="Multi-deminsional scatterplot with Plot.ly"></p>
<p>[^9]: The variable <code>A57AA1980</code> should be converted to a relative population variable so it is accounting for how rural a county is rather than how many people live in the county. This wlll be covered later but it should also take place here as well. It could be converted to a percentage via: <code>County_Aggregate_Data$Percent_Rural = (cntyNCG$A57AA1980/cntyNCG$AV0AA1990)</code>.</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="geospatial-data-analysis/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Using Geospatial Data to Inform Historical Research in R\\n\",\"collection\":\"lessons\",\"layout\":\"lesson\",\"date\":\"2018-08-20T00:00:00.000Z\",\"authors\":[\"Eric Weinberg\"],\"reviewers\":[\"Lauren Tilton\",\"Adam Crymble\",\"Ryan Deschamps\"],\"editors\":[\"Jessica Parr\"],\"difficulty\":2,\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F102\",\"activity\":\"analyzing\",\"topics\":[\"mapping\"],\"abstract\":\"In this lesson, you will use R-language to analyze and map geospatial data.\",\"avatar_alt\":\"An aerial view of city blocks\",\"doi\":\"10.46430\u002Fphen0075\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"introduction\\\"\u003EIntroduction\u003C\u002Fh2\u003E\\n\u003Cp\u003EOne primary focus of historical scholarship is explaining the complex relationships that influence change in the past. More often then not, patterns of change are understood and dependent on geographic understandings—or variations over space. The Great Migration, early development of the European economy, and the development of the American West are just a few topics that are both historical and geographic in nature. Combining knowledge of both space and time can provide a fuller understanding of events and transformations. This is even more evident when we are able to discern characteristics about the people\u002Fpopulations who are part of historical transformations\u002Fevents.\u003C\u002Fp\u003E\\n\u003Cp\u003EOne way this can be done is through an array of maps and visualizations that provide snapshots of geographic realities. Historians have long used maps spread over time to explain historical transformations such as economic developments and migration patterns. With the onset of digital technologies, more complex maps have developed, and scholars have integrated more complex information into maps alongside new geographic visualizations. [^1a]\u003C\u002Fp\u003E\\n\u003Cp\u003EThe availability of historical Geographic Information Systems has expanded this further by providing scholars opportunities to analyze more data points using digital and statistical tools such as R. With these new tools, scholars can study a multitude of population characteristics that can be visualized and analyzed across variable regions and time periods. Essentially, this has allowed researchers to configure geographic data around their areas of inquiry, accelerating the growth of “spatial history.” With this data, scholars can inquire not only about how historical change relates to place, but also how it relates to a multitude of population characteristics, testing theories and questions in rapid fashion. Was there a possible link between railroad development and change in gender makeup of surrounding populations? Were there other relationships? What does this say about the development of these regions? We are limited only by the available data and the questions we choose to ask. These techniques offer scholars the opportunity to test assumptions and build understandings that may not be readily understood in traditional research or mapping techniques.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis tutorial will introduce scholars to some of these techniques for processing geospatial data, focusing on how geospatial data can be analyzed statistically as well as visualized. We will be comparing spatial regions and leveraging the variability of populations across defined spatial regions such as counties. This variability can provide insights into a broad range of social movements. Moreover, it can be used to assess significant variations in regions using some basic statistical models. Using these methods, we assume that there is a level of continuity or homogeneity within the defined regions. [^2] We can leverage these understandings to assess historical events and movements. For example, if a large proportion of members of a particular group come from a group of counties, the characteristics of these counties can provide insight into the nature of that movement; [^3] or if a set of events happen in particular counties, we can ask if there is something particular about these counties that could be linked to the event. In some cases, analysis can also reveal hidden realities about social movements or events based on their geographic nature. For example, if we step through characteristics of an area near a spatial event, we could discover an unknown characteristic we never realized correlated with the particular event, leading to new research possibilities. You can hopefully discover trends that may be surprising or some that we find are not as strong as has been assumed.\u003C\u002Fp\u003E\\n\u003Cp\u003ESpecifically, this tutorial is going to use a membership list--with addresses--from a para-religious organization in America (PTL Ministries) and downloadable geographic data to assess population characteristics that could provide insights into an organization that is often characterized as more rural and less wealthy, alongside a host of other characteristics. The tutorial will then visualize and analyze this data to assess possible insights. This process will provide the basic tools and understandings that will allow scholars to assess other events and organizations that have geographic data. From this, you should be able to discover or challenge understandings of historical events using geospatial analysis.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"pre-requisites\\\"\u003EPre-requisites\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe work for this lesson will be done in R and R Studio, an open source statistical package used by data scientists, statisticians and other researchers. We are using R, because it is a widely-used open source tool that will allow us to both visualize and analyze our data using a multitude of methods that can be expanded upon quite easily. Some background knowledge of the software and statistics will be helpful. For introductions to R, I recommend the \u003Ca href=\\\"\u002Flessons\u002Fr-basics-with-tabular-data\\\"\u003Er-basics\u003C\u002Fa\u003E tutorial  and the more comprehensive \u003Ca href=\\\"https:\u002F\u002Fdh-r.lincolnmullen.com\\\"\u003EComputational Historical Thinking\u003C\u002Fa\u003E as starting points. There are many other services such as this \u003Ca href=\\\"https:\u002F\u002Fwww.coursera.org\u002Flearn\u002Fr-programming\\\"\u003EMOOC\u003C\u002Fa\u003E and \u003Ca href=\\\"https:\u002F\u002Fwww.datacamp.com\u002F\\\"\u003EDataCamp\u003C\u002Fa\u003E that can introduce beginners to R&#39;s broader functionality. \u003Ca href=\\\"http:\u002F\u002Fwww.ats.ucla.edu\u002Fstat\u002Fr\u002Fdefault.htm\\\"\u003EUCLA\u003C\u002Fa\u003E also has a nice introduction.[^1] While this tutorial will attempt to step through the entire process in R, basic knowledge of R is needed. The tutorial also assumes users will have some knowledge about the event you are observing which you will use later as a means to test and contest assumptions.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"lesson-goals\\\"\u003ELesson Goals\u003C\u002Fh2\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EProvide basic knowledge on how to use geographic data to analyze historical movements, especially movements where we have datasets or lists that are geographic in nature\u003C\u002Fli\u003E\\n\u003Cli\u003EDemonstrate how to merge geographic points or membership lists with geospatial data for further analysis\u003C\u002Fli\u003E\\n\u003Cli\u003EDemonstrate how to visualize this data geographically for analysis using choropleths\u003C\u002Fli\u003E\\n\u003Cli\u003EHighlight some statistical visualizations and models that can provide further insights\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"getting-started\\\"\u003EGetting Started\u003C\u002Fh2\u003E\\n\u003Cp\u003EThis tutorial will assume that you have \u003Ca href=\\\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=Ohnk9hcxf9M\\\"\u003Eset up R and R studio\u003C\u002Fa\u003E or \u003Ca href=\\\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=ICGkG7Gg6j0\\\"\u003ER studio for Mac\u003C\u002Fa\u003E. Once setup, you should open up the program. I recommend creating a new project and setting up a working directory where you will store all of your information.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe first steps in R will be to load the necessary libraries that will allow R to perform the geographic functions used for this type of analysis:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Einstall.packages(&quot;sf&quot;) # helps us work with spatial data\\n## for os specific requirments and directions see https:\u002F\u002Fgithub.com\u002Fr-spatial\u002Fsf\\ninstall.packages(&quot;tmap&quot;) #helps create simple choropleths\\ninstall.packages(&quot;plotly&quot;) #helps create scatterplots\\nlibrary(sf)\\nlibrary(tmap)\\nlibrary(plotly)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"the-data\\\"\u003EThe Data\u003C\u002Fh2\u003E\\n\u003Cp\u003EWe are using two sources of data in this tutorial: our list of address from the organization, and the downloaded geospatial data that contains the demographic and geographic data that will aid our analysis. We are going to begin with geospatial data. This data is formatted as a \u003Ca href=\\\"https:\u002F\u002Fwww.esri.com\u002Flibrary\u002Fwhitepapers\u002Fpdfs\u002Fshapefile.pdf\\\"\u003Eshapefile\u003C\u002Fa\u003E. Shapefiles are data files that represent geographic regions and can also contain characteristics about that region. The U.S. census contains a bevy of information in shapefile format.\u003C\u002Fp\u003E\\n\u003Cp\u003EBut, in order to get this information from historic censuses we are going to use data from the \u003Ca href=\\\"https:\u002F\u002Fwww.nhgis.org\\\"\u003ENational Historical Geographic Information System (NHGIS)\u003C\u002Fa\u003E which is managed by the Minnesota Population Center at the University of Minnesota. NHGIS is a warehouse of historical census data covering the span of U.S. history. In order to use their services, you must first register and create an account. Once completed you can use their \u003Ca href=\\\"https:\u002F\u002Fdata2.nhgis.org\u002Fmain\\\"\u003Edatafinder\u003C\u002Fa\u003E to select the geographic level, time period, and the data that interests you. This \u003Ca href=\\\"#\\\"\u003Elesson\u003C\u002Fa\u003E provides detailed instructions on how to use their services to extract county-level census information, but the current lesson will provide the downloaded data.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you are looking nationally prior to 1990, the county-level data is often your best bet as more precise geographic levels had not been standardized. For some regions and cities, however, there are more precise levels and in some cases smaller than zip codes. For this tutorial, we will use county level data from a time-appropriate decennial census. In general, it is best to use the smallest geographic region possible, but for historical research that often ends up being at the county level. In general, larger population centers have more detailed historical data, but rural areas were not completely covered until the 1990 census. For a more detailed description of the census regions and an interactive map see \u003Ca href=\\\"https:\u002F\u002Fwww.nhgis.org\u002Fuser-resources\u002Fdata-availability#table-data\\\"\u003ENHGIS&#39;s discussion\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"reading-the-data\\\"\u003EReading the Data\u003C\u002Fh2\u003E\\n\u003Cp\u003EWe start by loading in the selected data. The data for this tutorial can be \u003Ca href=\\\"\u002Fassets\u002Fgeospatial-data-analysis\u002Fdata.zip\\\"\u003Edowloaded here\u003C\u002Fa\u003E. Once downloaded place all the files in a folder labeled data inside your working directory in R. We are going to create a variable and read in our data from our variable directory to it. Once run, the \u003Ccode\u003ECounty_Aggregate_Data\u003C\u002Fcode\u003E variable will contain the data and geographic information that we will analyze:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003ECounty_Aggregate_Data &lt;- st_read(&quot;.\u002Fdata\u002FCounty1990ussm\u002F&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWe should now have a data object loaded with attached data:\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cimg src=\\\"\u002Fimages\u002Fgeospatial-data-analysis\u002FDataLoaded.png\\\" alt=\\\"DataLoaded.png\\\" title=\\\"Data Loaded in R\\\"\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you are only interested in looking at particular states, I recommend filtering the results to speed up processing and data analysis. To accomplish this, use the following commands:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003ECounty_Aggregate_Data &lt;- County_Aggregate_Data[which(County_Aggregate_Data$STATENAM %in% c(&quot;North Carolina&quot;,&quot;South Carolina&quot;)),]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EFollowing this command, I usually take a look at the distribution of the data using the summary command to ensure I am looking at the newly filtered data:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Esummary(County_Aggregate_Data)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis will return a bunch of summary data but most importantly it is showing that I have data only for the states I am filtering on:\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cimg src=\\\"\u002Fimages\u002Fgeospatial-data-analysis\u002FData2.png\\\" alt=\\\"Data2.png\\\" title=\\\"Data Loaded in R Two\\\"\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EOptionally, you can also plot the results to view a map of the data that you have downloaded. This could take some time, especially if you are not filtering the data. As above, this helps confirm that you are looking at the right geographic areas as only the filtered areas should be drawn. Below we will use R&#39;s basic graphing function to do this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Eplot(County_Aggregate_Data$geometry,axes=TRUE)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E\u003Cimg src=\\\"\u002Fimages\u002Fgeospatial-data-analysis\u002FNCSC.png\\\" alt=\\\"NCSC.png\\\" title=\\\"FIRST DATA PLOT\\\"\u003E\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"merging-census-data\\\"\u003EMerging Census Data\u003C\u002Fh2\u003E\\n\u003Cp\u003ECurrently, our \u003Ccode\u003ECounty_Aggregate_Data\u003C\u002Fcode\u003E variable has the necessary geographic boundaries for our analysis (as the above plot highlighted), but not the demographic information that will allow us to assess characteristics of our membership list. Although the demographic data came along with the geographic data, it needs to be merged into our \u003Ccode\u003ECounty_Aggregate_Data\u003C\u002Fcode\u003E variable which is a SpatialDataFrame. The next step is to begin merging \u003Ccode\u003ECounty_Aggregate_Data\u003C\u002Fcode\u003E with NHGIS table data in the downloaded data directory.\u003C\u002Fp\u003E\\n\u003Cp\u003ELet&#39;s read in the NHGIS data and merge it on the common field. The \u003Ccode\u003Eby.x\u003C\u002Fcode\u003E and \u003Ccode\u003Eby.y\u003C\u002Fcode\u003E parameters indicate what fields the data is being joined on:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003ECensus_Data &lt;- read.csv(&quot;.\u002Fdata\u002FCounty1990_Data\u002Fnhgis0027_ts_nominal_county.csv&quot;, as.is=TRUE)\\nCounty_Aggregate_Data &lt;- merge(County_Aggregate_Data,Census_Data,by.x=&quot;GISJOIN&quot;, by.y=&quot;GISJOIN&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe number of variables in \u003Ccode\u003ECounty_Aggregate_Data\u003C\u002Fcode\u003E should now increase as all of table data is brought into this one object. We now have one large \u003Ccode\u003ESpatialDataFrame\u003C\u002Fcode\u003E that has all of the geographic and statistical data we downloaded. We could stop and analyze this data as it undoubtedly contains many insights but it is only the raw census data and not yet connected to the historical event or data we are analyzing.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"merging-external-data\\\"\u003EMerging External Data\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe next step is to merge our list with our \u003Ccode\u003ESpatialDataFrame\u003C\u002Fcode\u003E so we can perform our analysis. While we are using a membership list, it can be any list that is geographic in nature. For example, you may have a list of events that happened during a particular time period; or a list of places an individual chooses to visit. This type of data will come in two basic formats. The first is information such as locations, address, or incident locations--which will be converted to geographic coordinates. The second will be a table that lists the same information alongside the county (or geographic region) where it occurred. We can handle either.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"geocoding\\\"\u003EGeocoding\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn the first case we have raw addresses of the members of our organization which will necessitate some additional steps. The address will need be transformed into geographical points in a process called \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGeocoding\\\"\u003Egeocoding\u003C\u002Fa\u003E. This will create geographic points--from addresses--that can be linked to spatial regions in our downloaded census data so that we can analyze it to help us discover trends related to geographic location of these addresses. R can do some of this work but if you have a large number of addresses, you will need to use an external service because the free services R uses (such as google) will cap how many address you can geocode in a day. One popular outside service is hosted by \u003Ca href=\\\"http:\u002F\u002Fgeoservices.tamu.edu\u002FServices\u002FGeocode\u002F\\\"\u003ETexas A&amp;M Geocoding Services\u003C\u002Fa\u003E and can handle large batches at a reasonable price. In the end, our address will be transformed into a list of latitudes and longitudes. This is the data R needs.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you have less than 2,500 addresses this can be handled in R using Google&#39;s geocoder. In R, you must first gather the address from whatever dataset you have, and then transform it. In our example, the data has already been geocoded, but below is an example of the commands used when processing a list of address and turning them into a list of geographic coordinates:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003EAddresses &lt;- data$Address\\nMember_Coordinates &lt;- geocode(Addresses)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIn our example, we already have a list of geographic coordinates. But we still need to merge it with our \u003Ccode\u003ESpatialDataFrame\u003C\u002Fcode\u003E (\u003Ccode\u003ECounty_Aggregate_Data\u003C\u002Fcode\u003E) so we can analyze it in relation to the census and geographic data we have downloaded. First, we either get the externally geocoded data or the newly geocoded data. Since our data has been geocoded, we will use the first command below to pull in that data.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Egeocoded_addresses &lt;- read.csv(&quot;.\u002Fdata\u002FGeocodedAddresses.csv&quot;, as.is=TRUE)\\n#or\\ngeocoded_addresses &lt;- Member_Coordinates  ##if you have just geocoded these address\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWe now need to remove the records with empty data that represent addresses that could not be geocoded:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003E#Now remove empty data or rows that failed to geocode\\ngeocoded_addresses &lt;- geocoded_addresses[!is.na(geocoded_addresses$Latitude) &amp; !is.na(geocoded_addresses$Longitude),]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThen we convert the data into a \u003Ccode\u003ESpatialDataFrame\u003C\u002Fcode\u003E so that can be merged[^7] with \u003Ccode\u003ECounty_Aggregate_Data\u003C\u002Fcode\u003E which contains previously imported data. We can see the process below:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003E#Now create the dataframe with geographic information for the merge\\npoints &lt;- st_as_sf(geocoded_addresses, coords= c(&quot;Longitude&quot;,&quot;Latitude&quot;),crs = 4326, agr = &quot;constant&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EBefore we do the actual merge, we should ensure both objects are using the same coordinate systems otherwise the points and counties will not match up throwing everything off. To do that we transform our census data to our current system.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003ECounty_Aggregate_Data &lt;- st_transform(County_Aggregate_Data, st_crs(points))\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThen I like to glimpse at the distribution of the point data within the census. We do this for a couple of reasons: first to verify that the merge will function correctly; secondly, to begin to look at the data distribution. We should see a list of numbers where each list represents the points that intersected with a particular county. Many of the insights we gain will come from this distribution. If counties with particular characteristics show a higher distribution, that can provide insights into our membership.  We will be looking at this more in depth as we proceed, but we are beginning to see some information here:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Est_intersects(County_Aggregate_Data,points) # show which counties each point falls into\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWe can also place the data points on top of our map for a quick visual of our data again using plot and \u003Ca href=\\\"https:\u002F\u002Fwww.statmethods.net\u002Fadvgraphs\u002Fparameters.html\\\"\u003Esome parameters\u003C\u002Fa\u003E for better visualization:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Eplot(County_Aggregate_Data$geometry,axes=TRUE)\\nplot(points[which(points$State %in% c(&quot;NC&quot;,&quot;SC&quot;)),]$geometry,col = &quot;green&quot;, pch=20,cex=.5, axes=TRUE,add=TRUE)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENow we do the merge. This merge is a bit different than the earlier merge because we are going to create a new field that represents the number of &#39;hits&#39; within a county. Essentially, the \u003Ccode\u003ECountMembers\u003C\u002Fcode\u003E variable will now represent the number of members in a particular county which will, like the distribution data above, allow us to begin to get insights from the data. We are using the above st_intersects alongside R&#39;s \u003Ccode\u003Esapply\u003C\u002Fcode\u003E to compute this value. In essence, we are transforming our lists into count data so we can visualize and analyze the data:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003ECounty_Aggregate_Data$CountMembers &lt;- sapply(st_intersects(County_Aggregate_Data,points), function(z) if (length(z)==0) NA_integer_ else length(z))\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENow we have a large dataframe called \u003Ccode\u003ECounty_Aggregate_Data\u003C\u002Fcode\u003E which has our count data and our census data by county. CountMembers now contains the count of members for their respective counties. But we may also want to merge data that is not a geographic point but rather a count of events\u002Fmembers and associated counties. Essentially, this data that is already tallied for the geographic regions we are interested in. This data should come from roughly the same timeframe as the spatial data for accuracy. In our example, we have a list of churches by denomination, which will hopefully give us additional insight into our data as we can assess if counties with a high number of churches of particular denomination also tend to be high in membership to our organization. To do this, merge we need to load the list:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Ereligion &lt;- read.csv(&quot;.\u002Fdata\u002FReligion\u002FChurches.csv&quot;, as.is=TRUE)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EDepending on the state of the data you may need to do some data transformations in order to merge it back with the DataFrame. For complex transformations, see tutorials in R on working with data such as \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fdata_wrangling_and_management_in_R\\\"\u003EData Wrangling and Management in R tutorial\u003C\u002Fa\u003E \u003Ca href=\\\"http:\u002F\u002Fr4ds.had.co.nz\u002Ftransform.html\\\"\u003Edata transforms\u003C\u002Fa\u003E. In essence, you need to have a common field in both datasets to merge upon. Often this is a geographic id for the county and state represented by \u003Ccode\u003EGEOID\u003C\u002Fcode\u003E. It could also be the unique FIPS Code given by the US Census. Below I am using state and county \u003Ccode\u003EGEOID\u003C\u002Fcode\u003E. In this example, we are converting one data frame&#39;s common fields to numeric so that they match the variable type of the other dataframe:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Ereligion$STATEFP &lt;- religion$STATE\\nreligion$COUNTYFP &lt;-religion$COUNTY\\nCounty_Aggregate_Data$STATEFP &lt;- as.numeric(as.character(County_Aggregate_Data$STATEFP))\\nCounty_Aggregate_Data$COUNTYFP &lt;- as.numeric(as.character(County_Aggregate_Data$COUNTYFP))\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWe then merge the data with the \u003Ccode\u003ESpatialDataFrame\u003C\u002Fcode\u003E, merging where state and counties ids match. This method is similar to the merge method used on the earlier merge, but we are now merging on multiple fields. To handle this situation, we are using a different format:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003ECounty_Aggregate_Data&lt;- merge(County_Aggregate_Data,religion,by=c(&quot;STATEFP&quot;,&quot;COUNTYFP&quot;))\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis will bring in all additional fields into our \u003Ccode\u003ESpatialDataFrame\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ENow we have a large \u003Ccode\u003ESpatialDataFrame\u003C\u002Fcode\u003E called \u003Ccode\u003ECounty_Aggregate_Data\u003C\u002Fcode\u003E which has our geocoded count data, our external count data and our census data by county. It is now time to begin to look at the data distribution and assess if everything appears correct and is in a format that will allow for some visualization and data analysis. We have some inherent complexity to our data because it is considered &quot;count data.&quot; As such, we should be cognizant that our data is not measuring individuals directly but rather relationships between counties. We are attempting to discover if counties with certain traits lead to higher membership in our datasets. These realities can help us gather some assumptions on the individuals in these regions.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"visualizing\\\"\u003EVisualizing\u003C\u002Fh2\u003E\\n\u003Cp\u003EBecause we are analyzing geospatial data, it is often best to begin with geographic visuals. There are many options here, but I find it easiest to start with the qtm function from the TMAP library which creates \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FChoropleth_map\\\"\u003Echoropleth\u003C\u002Fa\u003E maps simply. We could also use [GGPlot2]\u003Ca href=\\\"http:\u002F\u002Fweb.archive.org\u002Fweb\u002F20190922234254\u002Fhttp:\u002F\u002Fstrimas.com\u002Fr\u002Ftidy-sf\u002F\\\"\u003E(http:\u002F\u002Fstrimas.com\u002Fr\u002Ftidy-sf\u002F\u003C\u002Fa\u003E) which which should be installed using the development version.\u003C\u002Fp\u003E\\n\u003Cp\u003ENow, we are going to prepare the map and look at some census data. First on our list should be membership numbers relative to population (relative membership distribution). One of the most commonly used and clearest ways to display this information is by number of members per 10,000 people. We will then do the math to create a relative population variable(number of members per 10,000 people). We do this because we have to ensure we are taking into account the variability of populations within the census regions that we are analyzing otherwise we will get misleading visualization in densely populated counties that represent general population trends rather than variable relationships. If we did not take this step, we would undoubtedly see a map that highlights urban areas rather than areas where membership is strongest.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo begin looking at this data, we need to find the variable in our \u003Ccode\u003ESpatialDataframe\u003C\u002Fcode\u003E that represents population. In the downloaded census data folders, there is a codebook that will reveal what fields represent what data. After looking through the codebook, I discovered AV0AA1990 is the total Census population as of 1990. Below, I take this variable and transform it into a variable that adjusts for population fluctuations(number of members per 10,000 people):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003ECounty_Aggregate_Data$RelativeTotal= ((County_Aggregate_Data$AV0AA1990\u002F10000)\u002FCounty_Aggregate_Data$CountMembers )\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENow we will create the map. TMAP allows for the quick creation of thematic maps or more specifically choropleths. We can also vary text size based on another census variable. Here I am using the count of people living in rural areas (A57AA1980), making the text larger in more rural counties. Now I can start to assess visually if counties with higher distributions of membership also tend to be more rural as has been described. As the data shows, the membership is not clearly biased towards rural counties exclusively, giving us our first insight:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Eqtm(shp = County_Aggregate_Data, fill = &quot;RelativeTotal&quot;,text=&quot;NHGISNAM&quot;,text.size=&quot;A57AA1980&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E\u003Cimg src=\\\"\u002Fimages\u002Fgeospatial-data-analysis\u002FCH1.png\\\" alt=\\\"CH1.png\\\" title=\\\"Cholopleth of Normalized Data\\\"\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EFeel free to experiment with the choropleth. In particular, try switching out the text.size variable to see if you can discover patterns that might appear to be linked to membership. Can you detect any trends between choropleth colors and text size? The income variable would be another test that could be run to see if counties with larger representation are wealthier. These visualizations, of course, are also be useful as a means to present information.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou can also look and the unadjusted distribution which shows the raw distribution of members(without adjusting for local population distribution) as I did below[^9]:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Eqtm(shp = County_Aggregate_Data, fill = &quot;CountMembers&quot;,text=&quot;NHGISNAM&quot;,text.size=&quot;A57AA1980&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"visualizing-data-relationships\\\"\u003EVisualizing Data Relationships\u003C\u002Fh2\u003E\\n\u003Cp\u003EWhile choropleths and their many variations are an extremely helpful way to visualize the geospatial data, there are other methods that help visualize the data. One helpful method is the scatterplot which provides a visual means to show relationships between two variables. In particular, it is useful to assess if there are correlations between our event data and other characteristics as defined by the census data. For example, do we see a correlation between counties with low average income and membership. If so, that might indicate something about the nature of the movement or organization. We could look at a multitude of factors along these lines and our census data and codebook has many. While \u003Ca href=\\\"http:\u002F\u002Fwww.nature.com\u002Fnmeth\u002Fjournal\u002Fv12\u002Fn10\u002Ffull\u002Fnmeth.3587.html\\\"\u003Ecorrelations do not alone prove causality\u003C\u002Fa\u003E, they provide basic insight. When doing these comparisons, we have to again ensure we are taking into account the variability of populations within the census regions we are analyzing otherwise we will get misleading correlation in densely populated counties. To do this we need to convert any population number into numbers per 10,000 people.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf, for example, we wanted to use \u003Ccode\u003EB18AA1990\u003C\u002Fcode\u003E which is the persons-white variable we would convert it to relative number:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003EWhitePer10K &lt;- ((County_Aggregate_Data$B18AA1990\u002FCounty_Aggregate_Data$TOTPOP)*10000)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EOther total data should take regional size into account as well. For example, if we wanted to look at churches of a particular denomination, we would need to convert that as well because larger counties would inherently be more likely to have churches of any particular denomination, presenting misleading correlations. To look at \u003Ccode\u003EAOG.C\u003C\u002Fcode\u003E which is Assemblies of God churches we would:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003EAssemblies_Of_God_ChurchesPer10K &lt;- ((County_Aggregate_Data$AOG.C\u002FCounty_Aggregate_Data$CHTOTAL)*10000)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWe could then plot this variable with the membership variable to inspect for correlations.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Eplot(Assemblies_Of_God_ChurchesPer10K,County_Aggregate_Data$BD5AA1990)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis previous command will result in a notable but small correlation, which makes sense since the para-church organization was affiliated with the Assemblies of God denomination. Most often, we are going to be comparing data points to our historical data, but we can also inspect for other relationships in the general census data that can provide basic information about the investigative areas. For example, here is scatterplot of race and per capita income in the Carolinas:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Eplot(WhitePer10K,County_Aggregate_Data$BD5AA1990)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EBelow we see the results of the above code. We see what is described as a strong positive correlation, which is typical in the United States as there are strong correlations between race and income. As the percentage of white people increases, the per-capita income rises accordingly. The dots on plot represent the graphed points of these two values. We can measure that statistically, but we can also see it visually.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cimg src=\\\"\u002Fimages\u002Fgeospatial-data-analysis\u002FPlot.png\\\" alt=\\\"Plot.png\\\" title=\\\"Scatterplot of White people to per-capita income\\\"\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EWe can see this more precisely by adding a line of best fit to the plot which represents an estimated values based on the data presented. I also added red lines representing the distance from this line known as residuals. In essence, this showing us that we see a correlation between these two variables and it can be modeled with some accuracy.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Ex &lt;- WhitePer10K\\ny &lt;- County_Aggregate_Data$BD5AA1990\\nmod1 &lt;- lm(x ~ y)\\nplot(x ~ y,xlab=&quot;Per capita income in previous year&quot;,ylab=&quot;White People Per 10k&quot;)\\nsummary(mod1)\\nabline(mod1)\\nres &lt;- signif(residuals(mod1), 5)\\npre &lt;- predict(mod1) # plot distances between points and the regression line\\nsegments(y, x, y, pre, col=&quot;red&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EHere we see it:\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cimg src=\\\"\u002Fimages\u002Fgeospatial-data-analysis\u002FFit.png\\\" alt=\\\"Fit.png\\\" title=\\\"Scatter Plot with Residuals\\\"\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EBelow, let&#39;s set up a variable to try to take a look at some of the variables to look for possible correlations. Below we are going to create a variable that measures the distribution of denominational churches in a county, which will allow us measure if our membership is correlated with a particular denomination:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003EAssemblies_Of_God_Churches_Per10K &lt;- ((County_Aggregate_Data$AOG.C\u002FCounty_Aggregate_Data$CHTOTAL)*10000)\\nMembersPer10K &lt;- as.integer(((County_Aggregate_Data$CountMembers\u002FCounty_Aggregate_Data$TOTPOP)*100000))\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENow we will create a plot which show a small but significant correlation which makes sense since our organization is affiliated with this denomination. You can measure this statistically as well by using the \u003Ca href=\\\"https:\u002F\u002Fwww.r-bloggers.com\u002Fr-tutorial-series-simple-linear-regression\u002F\\\"\u003Elm function\u003C\u002Fa\u003E which we will not cover:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Eplot(MembersPer10K,Assemblies_Of_God_Churches_Per10K)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWe did a regular plot of the data but it is better to account for the fact that this is count data. Correlations and scatterplots are great ways to assess relationships, but they can be problematic with count data as it is often not linear or normally distributed and scatter plots work best when both of these \u003Ca href=\\\"https:\u002F\u002Fwww.statisticssolutions.com\u002Fassumptions-of-linear-regression\u002F\\\"\u003Econditions are true\u003C\u002Fa\u003E. And historical data is often counts of people or occurrences. Because of this, I recommend taking a look at the distribution of the count data to asses relationships. For that I am going to use a \u003Ca href=\\\"https:\u002F\u002Fwww.r-bloggers.com\u002Fhow-to-make-a-histogram-with-basic-r\u002F\\\"\u003Ehistogram\u003C\u002Fa\u003E which is commonly used to represent distributions of data:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Ehist(County_Aggregate_Data$CountMembers,breaks = 15)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E\u003Cimg src=\\\"\u002Fimages\u002Fgeospatial-data-analysis\u002FBar.png\\\" alt=\\\"NCSC.png\\\" title=\\\"Distribution Plot with Histogram\\\"\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003EOK, there are a significant number of low values which is typical of this type of information and some counties that are much higher than others.[^4]\u003C\u002Fp\u003E\\n\u003Cp\u003EA somewhat simple way to handle this is to perform a logarithmic transformation on a variable of the scatter plot to inspect for possible non-linear relationships. We add 1 to the values[^5] because log(0) is undefined. You could use .5 as some people do as well. Below we will analyze if there is a relationship between membership numbers and the count of churches in the counties observed using a log transformation. This can sometimes bring out correlations in count data that may have not been obvious using a non-adjusted scatterplot:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Eplot(MembersPer10K, log(Assemblies_Of_God_Churches_Per10K+1))\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"conclusion\\\"\u003EConclusion\u003C\u002Fh2\u003E\\n\u003Cp\u003EThrough this process, we have gathered and transformed geospatial data into a useable form. We have also created some visuals from this data, analyzing trends in the membership list of our organization. This tutorial should provide you with a basic template on how to take historical data and begin using geospatial analysis to analyze phenomenons such as the one we covered. In our case, the results illustrated that membership was not highly correlated with people who live in rural counties, suggesting that early characterizations of this movement as rural may not be entirely true, while we can see a slight relationship between the Assemblies of God and membership. This is just the beginning  of the possible means of inquiry. If we were to continue investigating, we could now start creating choropleths and scatter plots with other variables, looking for trends.  As you get more advanced, you can utilize some more advanced methods that can improve analysis as well.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"other-models-and-visualizations\\\"\u003EOther Models and Visualizations\u003C\u002Fh2\u003E\\n\u003Cp\u003EThere are many other models and visualizations available that can bring insight but they also add some complexity which demand further statistical understandings. For example, You can also create more complex scatterplots that can provide further insights. \u003Ca href=\\\"https:\u002F\u002Fplot.ly\u002Fr\u002F\\\"\u003EPlot.ly\u003C\u002Fa\u003E offers interactive scatter plots that can be customized and shared.[^8]. While statistical modeling usually focuses on a particular model&#39;s predictive insight, well-fit models also provide insight into the data they represent. In particular, the Poisson regression is frequently used to create \u003Ca href=\\\"http:\u002F\u002Fwww.theanalysisfactor.com\u002Fregression-models-for-count-data\u002F\\\"\u003Emodels of count data\u003C\u002Fa\u003E which is how population data is often represented. \u003Ca href=\\\"https:\u002F\u002Frstudio-pubs-static.s3.amazonaws.com\u002F44975_0342ec49f925426fa16ebcdc28210118.html\\\"\u003EGeographically Weighted Regressions\u003C\u002Fa\u003E also have particular advantages with this type of data. But assessing fit has some complexity. \u003Ca href=\\\"hhttps:\u002F\u002Fwww.analyticsvidhya.com\u002Fblog\u002F2016\u002F04\u002Fcomplete-tutorial-tree-based-modeling-scratch-in-python\u002F\\\"\u003EDecision trees\u003C\u002Fa\u003E could also be useful for historical data because they give an understandable graphical representation of the the leading factors that caused inclusion in a group or list. Principal component analysis, \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fcorrespondence-analysis-in-R\\\"\u003Ecorrespondence analysis\u003C\u002Fa\u003E and other clustering methods can also be helpful, especially when there is limited knowledge or insight into the event being analyzed yet there is an abundance of data associated with the event. I recommend background reading or discussions with a data scientist or statistician when exploring some of these modeling options as understanding the configuration and parameters of the individual models is essential to ensuring the results are trustworthy and significant.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^1]: For an overview of R as it relates to the humanities with a chapter geospatial data also see Arnold Taylor and Lauren Tilton, Humanities Data in R (Cham: Springer, 2015). They also have a geospatial chapter that uses the sp library.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^1a]: For a broader discussion on the role of geographic information and GIS in the humanities see Placing History: How Maps, Spatial Data, and GIS Are Changing Historical Scholarship (Esri Press, 2008) and Harris, Trevor M., John Corrigan, and David J. Bodenhamer, The Spatial Humanities: GIS and the Future of Humanities Scholarship (Bloomington: Indiana University Press, 2010).\u003C\u002Fp\u003E\\n\u003Cp\u003E[^2]: For a discussion on the benefits and drawbacks on this methodology and its assumptions see, \u003Ca href=\\\"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpmc\u002Farticles\u002FPMC3732658\u002F\\\"\u003ESpatializing health research\u003C\u002Fa\u003E. Some states like Kentucky have a larger number of counties (120) which often encompass entire cities which often leads to more homogeneity within those regions. In contrast, a state like Massachusetts has only 14 counties which can lead to more variability with the county geographies leading to more questionable results in some cases.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^3]: This is often leveraged in the field of public health. See for example, \u003Ca href=\\\"https:\u002F\u002Fwww.cdc.gov\u002Fpcd\u002Fissues\u002F2015\u002F14_0404.htm\\\"\u003ESpatial Analysis and Correlates of County-Level Diabetes Prevalence\u003C\u002Fa\u003E. Other fields such as criminal justice also rely on similar analytics although criminal justice tends to look at smaller census areas within regions. See, for example, \u003Ccode\u003Ehttps:\u002F\u002Fwww.ncjrs.gov\u002Fpdffiles1\u002Fnij\u002Fgrants\u002F204432.pdf\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^4]: Count data typically has large numbers of zero values which can add some complexity that will not be covered here. There are more complex ways to minimize this using more complex regression models. See, for example \u003Ca href=\\\"https:\u002F\u002Fstats.idre.ucla.edu\u002Fstata\u002Fseminars\u002Fregression-models-with-count-data\u002F\\\"\u003ERegression Models with Count Data\u003C\u002Fa\u003E. For general description of what normal distributions, which work well without modification look like see normal \u003Ca href=\\\"http:\u002F\u002Fwww.statisticshowto.com\u002Fprobability-and-statistics\u002Fnormal-distributions\u002F\\\"\u003Edistributions\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^5]: There are different strategies to dealing with this type of data. See for example, \u003Ca href=\\\"http:\u002F\u002Fwww.sciencedirect.com\u002Fscience\u002Farticle\u002Fpii\u002FS0031405608000073\\\"\u003EThe Excess-zero Problem in Soil Animal Count Data\u003C\u002Fa\u003E or \u003Ca href=\\\"http:\u002F\u002Fwww.biostathandbook.com\u002Ftransformation.html\\\"\u003EData Transformations\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^6]: For details on ggmap and and integration with Google Maps or other maps services see the \u003Ca href=\\\"http:\u002F\u002Fstat405.had.co.nz\u002Fggmap.pdf\\\"\u003Eggmap overview\u003C\u002Fa\u003E. For another broader discussions on google map making that utilizes a few of the libraries in this tutorial see \u003Ca href=\\\"https:\u002F\u002Frpubs.com\u002Fnickbearman\u002Fr-google-map-making\\\"\u003ER and Google Map Making\u003C\u002Fa\u003E. For a discussion of the sf library and it relationship to sp see \u003Ca href=\\\"https:\u002F\u002Fcran.r-project.org\u002Fweb\u002Fpackages\u002Fsf\u002Fvignettes\u002Fsf1.html\\\"\u003ESimple Features for R\u003C\u002Fa\u003E. While sp has been the library spatial analysis library of choice, it is being superseded by sf.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^7]: We are setting Coordinate Reference System(CRS) to EPSG 4326 which is the most common mapping system used int the U.S. It is used by Google  which is the origins of our data. EPSG 3857 is also used by google. For more on CRS see \u003Ca href=\\\"https:\u002F\u002Fwww.earthdatascience.org\u002Fcourses\u002Fearth-analytics\u002Fspatial-data-r\u002Fintro-to-coordinate-reference-systems\u002F\\\"\u003ECoordinate Reference Systems &amp; Spatial Projections\u003C\u002Fa\u003E. Also see \u003Ca href=\\\"http:\u002F\u002Fweb.archive.org\u002Fweb\u002F20200225021219\u002Fhttps:\u002F\u002Fwww.nceas.ucsb.edu\u002F~frazier\u002FRSpatialGuides\u002FOverviewCoordinateReferenceSystems.pdf\\\"\u003Ecoordinate systems reference in R\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^8]: These plots are a bit more complex and requires an extra library, but they have some advantages. They work well with complex datasets because they have the ability to model more than two relationships by altering the color or size of the data points(we did this earlier on the choropleths by altering font size). Moreover, they are interactive which allows you to explore extra information about data points after the plot is created without wrecking the visual makeup of the plot. Here is an example that looks at the relationship between income and membership but also adds urban status to the visual using color. I am also adjusting point size based on population so I can take a look at more populated areas alongside the other data:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-r\\\"\u003Elibrary(plotly)\\n\\nvar = County_Aggregate_Data$A57AA1990\\nbins = unique(quantile(var, seq(0,1,length.out=8)))\\ninterv = findInterval(var, bins)\\nCounty_Aggregate_Data$People_Urban &lt;-interv\\n\\np &lt;- plot_ly(County_Aggregate_Data, type = &quot;scatter&quot;, mode = &quot;markers&quot;) %&gt;%\\n    add_trace(x = ~(AV0AA1990\u002F10000)\u002FCountMembers,\\n              y = ~BD5AA1990,\\n              size = ~AV0AA1990,\\n              color = ~People_Urban,\\n              text = ~paste(&quot;AVG Incom: &quot;,BD5AA1990 ,\\n                            &#39;$&lt;br&gt;County:&#39;, COUNTY.y,\\n                            &#39;$&lt;br&gt;State:&#39;, STATENAM,\\n                            &#39;$&lt;br&gt;Members:&#39;, CountMembers),\\n              hoverinfo = &quot;text&quot;) %&gt;%\\n    layout(title = &#39;Members and Income, Size=Population&#39;,\\n           xaxis = list(title = &#39;Members per 10k population&#39;),\\n           yaxis = list(title = &#39;Income&#39;),\\n           hoverlabel = list(font = list(size = 16)))\\n\\np\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E\u003Cimg src=\\\"\u002Fimages\u002Fgeospatial-data-analysis\u002FPly1.png\\\" alt=\\\"Ply1.png\\\" title=\\\"Multi-deminsional scatterplot with Plot.ly\\\"\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^9]: The variable \u003Ccode\u003EA57AA1980\u003C\u002Fcode\u003E should be converted to a relative population variable so it is accounting for how rural a county is rather than how many people live in the county. This wlll be covered later but it should also take place here as well. It could be converted to a percentage via: \u003Ccode\u003ECounty_Aggregate_Data$Percent_Rural = (cntyNCG$A57AA1980\u002FcntyNCG$AV0AA1990)\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
