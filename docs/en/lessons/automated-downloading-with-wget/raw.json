{"metadata":{"title":"Automated Downloading with Wget","layout":"lesson","date":"2012-06-27T00:00:00.000Z","authors":["Ian Milligan"],"reviewers":["Aurélien Berra"],"editors":["Adam Crymble"],"difficulty":1,"exclude_from_check":["review-ticket"],"activity":"acquiring","topics":["web-scraping"],"abstract":"Wget is a useful program, run through your computer's command line, for retrieving online material.","next":"applied-archival-downloading-with-wget","redirect_from":"/lessons/automated-downloading-with-wget","avatar_alt":"Diagram of an elevator system in a mineshaft","doi":"10.46430/phen0001"},"html_body":"<p>{% include toc.html %}</p>\n<h2 id=\"editors-note\">Editor&#39;s Note</h2>\n<p>This lesson requires you to use the command line. If you have no\nprevious experience using the command line you may find it helpful to\nwork through the <em>Programming Historian’s</em> <a href=\"/lessons/intro-to-bash\">Introduction to the Bash Programming Language</a>.</p>\n<h2 id=\"lesson-goals\">Lesson Goals</h2>\n<p>This is a lesson designed for intermediate users, although beginner\nusers should be able to follow along.</p>\n<p>Wget is a useful program, run through your computer&#39;s command line, for\nretrieving online material.</p>\n<p>{% include figure.html filename=&quot;Terminal-on-mac2.png&quot; caption=&quot;The Mac Command Line, Terminal&quot; %}</p>\n<p>It can be useful in the following situations:</p>\n<ul>\n<li>Retrieving or mirroring (creating an exact copy of) an entire\nwebsite. This website might contain historical documents, or it may\nsimply be your own personal website that you want to back up. One\ncommand can download the entire site onto your computer.</li>\n<li>Downloading specific files in a website&#39;s hierarchy (all websites\nwithin a certain part of a website, such as every page that is\ncontained within the <code>/papers/</code> directory of a website).</li>\n</ul>\n<p>In this lesson, we will work through three quick examples of how you\nmight use wget in your own work. At the end of the lesson, you will be\nable to quickly download large amounts of information from the Internet\nin an automated fashion. If you find a repository of online historical\ninformation, instead of right-clicking on every file and saving it to\nbuild your dataset, you will have the skills to craft a single command\nto do so.</p>\n<p>First, a caution is in order. You need to be careful about how you use\nwget. If you consult the manual when in doubt, and work through the\nlessons here, you should be okay. You should always build a delay into\nyour commands so that you do not overload the servers, and should also\nalways put a limit on the speed to which you download. This is all part\nof being a good Internet citizen, and can be seen as analogous to\nsipping from a firehose rather than turning it on all at once (it&#39;s not\ngood for you, or the water company).</p>\n<p>Be as specific as possible when formulating your download. One joke\nsuggests that you can accidentally download the entire Internet with\nwget. While that&#39;s a bit of an exaggeration, it isn&#39;t too far off!</p>\n<p>Let&#39;s begin.</p>\n<h2 id=\"step-one-installation\">Step One: Installation</h2>\n<h3 id=\"linux-instructions\">Linux Instructions</h3>\n<p>If you are using a Linux system, you should already have wget installed.\nTo check if you have it, open up your command line. Type <code>&#39;wget&#39;</code> and\npress enter. If you have wget installed the system will respond with:</p>\n<pre><code>-&gt; Missing URL.\n</code></pre>\n<p>If you do not have wget installed, it will respond with</p>\n<pre><code>-&gt; command not found.\n</code></pre>\n<p>If you are on OS X or Windows, you will need to download the program. If\non Linux, you receive the error message indicating that you do not have\nwget installed, follow the OS X instructions below.</p>\n<h3 id=\"os-x-instructions\">OS X Instructions</h3>\n<h4 id=\"os-x-option-one-the-preferred-method\">OS X Option One: The Preferred Method</h4>\n<p>On OS X, there are two ways to get wget and install it. The easiest is\nto install a package manager and use it to automatically install wget.\nThere is a second method, discussed below, that involves compiling it.</p>\n<p>Both, however, require that you install Apple&#39;s &#39;Command Line Tools&#39; to\nuse properly. This requires downloading XCode. If you have the &#39;App\nStore&#39;, you should be able to just <a href=\"https://itunes.apple.com/us/app/xcode/id497799835?mt=12\">download XCode via this link</a>.  If\nnot, the following instructions will work.</p>\n<p>To download this, go to the <a href=\"https://developer.apple.com/xcode/\">Apple Developer website</a>, register as a\ndeveloper, and then in the <a href=\"https://developer.apple.com/xcode/\">downloads for Apple developers</a> section you will need to find the correct version. If\nyou are on the most recent version, Lion as of July 2012, you can use\nthe main link. If not, you will need to click on the link: &quot;Looking for\nadditional developer tools? <a href=\"https://developer.apple.com/downloads/\">View Downloads</a>.&quot;</p>\n<p>After logging in with your free developer credentials, you will see a\nlong list. Type xcode in the search bar and find a version that is\ncompatible with your operating system version. This may take some\nclicking around to find the right version for you. For example, Xcode\n3.2 is the version for OS X 10.6 Snow Leopard, 3.0 is the version for OS\nX 10.5 Leopard, etc.</p>\n<p>It is a big download, and will take some time. Once you have the file,\ninstall it.</p>\n<p>You will need to install the &#39;<strong>Command Line Tools</strong>&#39; kit in XCode. Open\nup the &#39;Preferences&#39; tab, click on &#39;Downloads,&#39; and then click &#39;Install&#39;\nnext to Command Line Tools. We are now ready to install a package\nmanager.</p>\n<p>The easiest package manager to install is <em>Homebrew</em>. Go to\n<a href=\"https://brew.sh\">https://brew.sh</a> and review the instructions. There are\nmany important commands, like wget, that are not included by default in\nOS X. This program facilitates the downloading and installation of all\nrequired files.</p>\n<p>To install <em>Homebrew</em>, open up your terminal window and type the\nfollowing:</p>\n<pre><code class=\"language-bash\">/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;\n</code></pre>\n<p>This uses the ruby programming language, built into OS X, to install\nHomebrew. To see if the installation worked, type the following into\nyour terminal window:</p>\n<pre><code class=\"language-bash\">brew\n</code></pre>\n<p>A list of documentation options should appear if it has been installed.\nWe have one more command to run to make sure everything is working,\nwhich is:</p>\n<pre><code class=\"language-bash\">brew doctor\n</code></pre>\n<p>With <em>Homebrew</em> installed, we now have to install wget. This is now an\neasy step.</p>\n<pre><code class=\"language-bash\">brew install wget\n</code></pre>\n<p>It will proceed to download the most recent version of wget, which is\nwget 1.14. After the script stops running, and you are back to your main\nwindow, enter the following command into the terminal:</p>\n<pre><code class=\"language-bash\">wget\n</code></pre>\n<p>If you have installed it, you will see:</p>\n<pre><code>-&gt; Missing URL.\n</code></pre>\n<p>If not, you will see:</p>\n<pre><code>-&gt; command not found.\n</code></pre>\n<p>At this point, you should have installed wget successfully. We are now\nready to keep going!</p>\n<h4 id=\"os-x-option-two\">OS X Option Two</h4>\n<p>If for some reason you do not want to install a package manager, you are\nable to simply download wget alone. This will be applicable if you are\nusing a different packet manager (such as Mac Ports) or if you want to\nkeep your infrastructure to a minimum. Follow the same instructions\nagain to install xcode and the Command Line Tools set.</p>\n<p>Then you can subsequently download an uncompiled version of wget from\nthe <a href=\"http://www.gnu.org/software/wget/\">GNU website</a> (I chose to download the file &#39;wget-1.13.tar.gz&#39;,\nwhich you can find by following the link to either the <a href=\"http://ftp.gnu.org/gnu/wget/\">HTTP</a> or\n<a href=\"ftp://ftp.gnu.org/gnu/wget/\">FTP</a> download pages), unzip it (by double-clicking on it) into your\nhome directory (on a Mac, this will be your <code>/user/</code> directory – for\nexample, my user name is ianmilligan and it appears next to a house icon\nin my Finder), and then open up Terminal. For this tutorial, we have\ndownloaded <code>wget-1.13</code>.</p>\n<p>First, we will need to navigate to the directory that the wget files are\nin. At the terminal, type:</p>\n<pre><code class=\"language-bash\">cd wget-1.13\n</code></pre>\n<p>Note that if you have downloaded a different version of wget, the\nfollowing steps will work but you may have to replace the above version\nnumber (i.e. <code>1.13</code>) with your own.</p>\n<p>We now need to generate the instructions, or makefile, for the file.\nThis is sort of a blueprint for what the final file is going to look\nlike. Accordingly, type:</p>\n<pre><code class=\"language-bash\">./configure –with-ssl=openssl\n</code></pre>\n<p>Now that we have the blueprints, let\\&#39;s tell our computer to follow\nthem. Type:</p>\n<pre><code class=\"language-bash\">make\n</code></pre>\n<p>Then, you need to make the final file. By pre-pending the command sudo,\nyou are running the command with highest security privileges. This lets\nyou actually install the file into your system.</p>\n<pre><code class=\"language-bash\">sudo make install\n</code></pre>\n<p>At this point, you will be prompted for your computer&#39;s password. Type\nit.</p>\n<p>You should now have wget installed.</p>\n<h3 id=\"windows-instructions\">Windows Instructions</h3>\n<p>The easiest way is to download a working version. To do so, visit\n<a href=\"https://eternallybored.org/misc/wget/\">this website</a> and, download <code>wget.exe</code> (as of writing it is version 1.17.1, and you should download the 32-bit binary). The file is the second link in the 32-bit binary column, entitled just <code>wget.exe</code>.</p>\n<p>If you place <code>wget.exe</code> in\nyour <code>C:\\Windows</code> directory, you can then use wget from anywhere on your\ncomputer. This will make your life easier as you will not have to worry\nabout always running wget from only one place on your system. If it is\nin this directory, Windows will know that the command can be used\nanywhere in your terminal window.</p>\n<h2 id=\"step-two-learning-about-the-structure-of-wget--downloading-a-specific-set-of-files\">Step Two: Learning about the Structure of Wget – Downloading a Specific Set of Files</h2>\n<p>At this point, users of all three platforms should be on the same page.\nWe use wget through our operating system&#39;s command line interface\n(introduced previously as <code>Terminal</code> for Mac and Linux users, where you\nhave been playing around with some Python commands). You need to use\nyour command line, instead of the Komodo Edit client you may have used\nin other lessons.</p>\n<p>The comprehensive documentation for wget can be found on the <a href=\"http://www.gnu.org/software/wget/manual/wget.html\">GNU wget\nmanual</a> page.</p>\n<p>Let&#39;s take an example dataset. Say you wanted to download all of the\npapers hosted on the website ActiveHistory.ca. They are all located at:\n<a href=\"http://activehistory.ca/papers/\">http://activehistory.ca/papers/</a>; in the sense that they are all\ncontained within the <code>/papers/</code> directory: for example, the 9th paper\npublished on the website\nis <a href=\"http://activehistory.ca/papers/historypaper-9/\">http://activehistory.ca/papers/historypaper-9/</a>. Think of this\nstructure in the same way as directories on your own computer: if you\nhave a folder labeled <code>/History/</code>, it likely contains several files\nwithin it. The same structure holds true for websites, and we are using\nthis logic to tell our computer what files we want to download.</p>\n<p>If you wanted to download them all manually, you would either need to\nwrite a custom program, or right-click every single paper to do so. If\nthe files are organized in a way that fits your research needs, wget is\nthe quickest approach.</p>\n<p>To make sure wget is working, try the following.</p>\n<p>In your working directory, make a new directory. Let&#39;s call it\n<code>wget-activehistory</code>. You can make this using your Finder/Windows, or if\nyou are at a Terminal window at that path, you can type:</p>\n<pre><code class=\"language-bash\">mkdir wget-activehistory\n</code></pre>\n<p>Either way, you now have a directory that we will be working in. Now\nopen up your command line interface and navigate to\nthe <code>wget-activehistory</code> directory. As a reminder, you can type:</p>\n<pre><code class=\"language-bash\">cd [directory]\n</code></pre>\n<p>to navigate to a given directory. If you&#39;ve made this directory in your\nhome directory, you should be able to type <code>cd wget-activehistory</code> to\nmove to your new directory.</p>\n<p>Enter the following command:</p>\n<pre><code class=\"language-bash\">wget http://activehistory.ca/papers/\n</code></pre>\n<p>After some initial messages, you should see the following (figures,\ndates and some details will be different, however):</p>\n<pre><code>Saving to: `index.html.1&#39;\n\n[] 37,668 --.-K/s in 0.1s\n\n2012-05-15 15:50:26 (374 KB/s) - `index.html.1&#39; saved [37668]\n</code></pre>\n<p>What you have done is downloaded just the first page of\n<a href=\"http://activehistory.ca/papers/\">http://activehistory.ca/papers/</a>, the index page for the papers to your\nnew directory. If you open it, you&#39;ll see the main text on the home page\nof ActiveHistory.ca. So at a glance, we have already quickly downloaded\nsomething.</p>\n<p>What we want to do now, however, is to download every paper. So we need\nto add a few commands to wget.</p>\n<p>Wget operates on the following general basis:</p>\n<pre><code class=\"language-bash\">wget [options] [URL]\n</code></pre>\n<p>We have just learned about the [URL] component in the previous example,\nas it tells the program where to go. Options, however, give the program\na bit more information about what exactly we want to do. The program\nknows that an option is an option by the presence of a dash before the\nvariable. This lets it know the difference between the URL and the\noptions. So let&#39;s learn a few commands now:</p>\n<pre><code>-r\n</code></pre>\n<p>Recursive retrieval is the most important part of wget. What this means\nis that the program begins following links from the website and\ndownloading them too. So for example, the\n<a href=\"http://activehistory.ca/papers/\">http://activehistory.ca/papers/</a> has a link to\n<a href=\"http://activehistory.ca/papers/historypaper-9/\">http://activehistory.ca/papers/historypaper-9/</a>, so it will download\nthat too if we use recursive retrieval. However, it will also follow any\nother links: if there was a link to <a href=\"http://uwo.ca\">http://uwo.ca</a> somewhere on that\npage, it would follow that and download it as well. By default, -r sends\nwget to a depth of five sites after the first one. This is following\nlinks, to a limit of five clicks after the first website. At this point,\nit will be quite indiscriminate. So we need more commands:</p>\n<pre><code class=\"language-bash\">--no-parent\n</code></pre>\n<p>(The double-dash indicates the full-text of a command. All commands also\nhave a short version, this could be initiated using -np).</p>\n<p>This is an important one. What this means is that wget should follow\nlinks, but not beyond the last parent directory. In our case, that means\nthat it won&#39;t go anywhere that is not part of the\n<a href=\"http://activehistory.ca/papers/\">http://activehistory.ca/papers/</a> hierarchy. If it was a long path such as\n<a href=\"http://niche-canada.org/projects/events/new-events/not-yet-happened-events/\">http://niche-canada.org/projects/events/new-events/not-yet-happened-events/</a>,\nit would only find files in the <code>/not-yet-happened-events/</code> folder. It\nis a critical command for delineating your search.</p>\n<p>Here is a graphical representation:</p>\n<p>{% include figure.html filename=&quot;active-history-chart_edited-1.jpg&quot; caption=&quot;A graphical representation of how &#39;no-parent&#39; works with wget&quot; %}</p>\n<p>Finally, if you do want to go outside of a hierarchy, it is best to be\nspecific about how far you want to go. The default is to follow each\nlink and carry on to a limit of five pages away from the first page you\nprovide. However, perhaps you just want to follow one link and stop\nthere? In that case, you could input <code>-l 2</code>, which takes us to a depth\nof two web-pages. Note this is a lower-case &#39;L&#39;, not a number 1.</p>\n<pre><code class=\"language-bash\">-l 2\n</code></pre>\n<p>If these commands help direct wget, we also need to add a few more to be\nnice to servers and to stop any automated countermeasures from thinking\nthe server is under attack! To that end, we have two additional\nessential commands:</p>\n<pre><code class=\"language-bash\">-w 10\n</code></pre>\n<p>It is not polite to ask for too much at once from a web server. There\nare other people waiting for information, too, and it is thus important\nto share the load. The command -<code>w 10</code>, then, adds a ten second wait in\nbetween server requests. You can shorten this, as ten seconds is quite\nlong. In my own searches, I often use a 2 second wait. On rare\noccasions, you may come across a site that blocks automated downloading\naltogether. The website&#39;s terms of service, which you should consult,\nmay not mention a policy on automated downloading, but steps to prohibit\nit may be built into their website&#39;s architecture nonetheless. In such\nrare cases, you can use the command <code>––random-wait</code> which will vary the\nwait by 0.5 and 1.5 times the value you provide here.</p>\n<p>Another critical comment is to limit the bandwidth you will be using in\nthe download:</p>\n<pre><code class=\"language-bash\">--limit-rate=20k\n</code></pre>\n<p>This is another important, polite command. You don&#39;t want to use up too\nmuch of the servers&#39; bandwidth. So this command will limit the maximum\ndownload speed to 20kb/s. Opinion varies on what a good limit rate is,\nbut you are probably good up to about 200kb/s for small files – however,\nnot to tax the server, let us keep it at 20k. This will also keep us at\n<code>ActiveHistory.ca</code> happy!</p>\n<h3 id=\"step-three-mirror-an-entire-website\">Step Three: Mirror an Entire Website</h3>\n<p>Ok, with all of this, let&#39;s finally download all of the ActiveHistory.ca\npapers. Note that the trailing slash on the URL is critical – if you\nomit it, wget will think that papers is a file rather than a directory.\nDirectories end in slashes. Files do not. The command will then download\nthe entire ActiveHistory.ca page. The order of the options does not\nmatter.</p>\n<pre><code class=\"language-bash\">wget -r --no-parent -w 2 --limit-rate=20k http://activehistory.ca/papers/\n</code></pre>\n<p>It will be slower than before, but your terminal will begin downloading\nall of the ActiveHistory.ca papers. When it is done, you should have a\ndirectory labeled <code>ActiveHistory.ca</code> that contains the <code>/papers/</code>\nsub-directory – perfectly mirrored on your system. This directory will\nappear in the location that you ran the command from in your command\nline, so likely is in your <code>USER</code> directory. Links will be replaced with\ninternal links to the other pages you&#39;ve downloaded, so you can actually\nhave a fully working ActiveHistory.ca site on your computer. This lets\nyou start to play with it without worrying about your internet speed.</p>\n<p>To see if the download was a success, you will also have a log in your\ncommand screen. Take a look over it to make sure that all files were\ndownloaded successfully. If it did not download, it will let you know\nthat it failed.</p>\n<p>If you want to mirror an entire website, there is a built-in command to\nwget.</p>\n<pre><code>-m\n</code></pre>\n<p>This command means &#39;mirror,&#39; and is especially useful for backing up an\nentire website. It introduces the following set of commands:\ntime-stamping, which looks at the date of the site and doesn&#39;t replace\nit if you already have that version on your system (useful for repeated\ndownloads), as well as infinite recursion (it will go as many layers\ninto the site as necessary). The command for mirroring ActiveHistory.ca\nwould be:</p>\n<pre><code class=\"language-bash\">wget -m -w 2 --limit-rate=20k http://activehistory.ca\n</code></pre>\n<h2 id=\"a-flexible-tool-for-downloading-internet-sources\">A Flexible Tool for Downloading Internet Sources</h2>\n<p>As you become increasingly comfortable with the command line, you&#39;ll\nfind wget a helpful addition to your digital toolkit. If there is an\nentire set of archival documents that you want to download for text\nmining, if they&#39;re arranged in a directory and are all together (which\nis not as common as one might think), a quick wget command will be\nquicker than scraping the links with Python. Similarly, you can then\nbegin downloading things directly from your command line: programs,\nfiles, backups, etc.</p>\n<h3 id=\"further-reading\">Further Reading</h3>\n<p>I&#39;ve only given a snapshot of some of wget&#39;s functionalities. For more,\nplease visit the <a href=\"http://www.gnu.org/software/wget/manual/wget.html\">wget manual</a>.</p>\n"}