<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-a80c730b.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-a80c730b.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/analyzing-documents-with-tfidf"),
					params: {lang:"en",lessons:"lessons",slug:"analyzing-documents-with-tfidf"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Analyzing Documents with TF-IDF</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h1 id="overview">Overview</h1>
<p>This lesson focuses on a core natural language processing and information retrieval method called Term Frequency - Inverse Document Frequency (<strong>tf-idf</strong>). You may have heard about <strong>tf-idf</strong> in the context of topic modeling, machine learning, or or other approaches to text analysis. <strong>Tf-idf</strong> comes up a lot in published work because it&#39;s both a <a href="https://en.wikipedia.org/wiki/Text_corpus">corpus</a> exploration method and a pre-processing step for many other text-mining measures and models.</p>
<p>Looking closely at tf-idf will leave you with an immediately applicable text analysis method. This lesson will also introduce you to some of the questions and concepts of computationally oriented text analysis. Namely, this lesson addresses how you can isolate a document’s most important words from the kinds of words that tend to be highly frequent across a set of documents in that language. In addition to tf-idf, there are a number of computational methods for determining which words or phrases characterize a set of documents, and I highly recommend Ted Underwood&#39;s 2011 blog post as a supplement.[^1]</p>
<h1 id="preparation">Preparation</h1>
<h2 id="suggested-prior-skills">Suggested Prior Skills</h2>
<ul>
<li>Prior familiarity with Python or a similar programming language. Code for this lesson is written in Python 3.6, but you can run <strong>tf-idf</strong> in several different versions of Python, using one of several packages, or in various other programming languages. The precise level of code literacy or familiarity recommended is hard to estimate, but you will want to be comfortable with basic types and operations. To get the most out of this lesson, it is recommended that you work your way through something like <a href="https://www.codecademy.com/learn/learn-python">Codeacademy&#39;s &quot;Introduction to Python&quot; course</a>, or that you complete some of the <a href="/en/lessons/introduction-and-installation">introductory Python lessons on the <em>Programming Historian</em></a>.</li>
<li>In lieu of the above recommendation, you should <a href="https://www.learnpython.org/">review Python&#39;s basic types</a> (string, integer, float, list, tuple, dictionary), working with variables, writing loops in Python, and working with object classes/instances.</li>
<li>Experience with Excel or an equivalent spreadsheet application if you wish to examine the linked spreadsheet files. You can also use the pandas library in python to view the CSVs.</li>
</ul>
<h2 id="before-you-begin">Before You Begin</h2>
<ul>
<li>Install the Python 3 version of Anaconda. Installing Anaconda is covered in <a href="/en/lessons/text-mining-with-extracted-features">Text Mining in Python through the HTRC Feature Reader</a>. This will install Python 3.6 (or higher), the <a href="https://scikit-learn.org/stable/install.html">Scikit-Learn library</a> (which we will use for <strong>tf-idf</strong>), and the dependencies needed to run a <a href="https://jupyter.org/">Jupyter Notebook</a>.</li>
<li>It is possible to install all these dependencies without Anaconda (or with a lightweight alternative like <a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda</a>). For more information, see the section below titled <a href="#alternatives-to-anaconda">&quot;Alternatives to Anaconda&quot;</a></li>
</ul>
<h2 id="lesson-dataset">Lesson Dataset</h2>
<p><strong>Tf-idf</strong>, like many computational operations, is best understood by example. To this end, I&#39;ve prepared a dataset of 366 <em>New York Times</em> historic <a href="https://en.wikipedia.org/wiki/Obituary">obituaries</a> scraped from <a href="https://archive.nytimes.com/www.nytimes.com/learning/general/onthisday/">https://archive.nytimes.com/www.nytimes.com/learning/general/onthisday/</a>. On each day of the year, <em>The New York Times</em> featured an obituary of someone born on that day.</p>
<p>Lesson files, including, this dataset, can be downloaded from <a href="/assets/tf-idf/lesson-files.zip">lesson-files.zip</a>. The dataset is small enough that you should be able to open and read some if not all of the files. The original data is also available in the &#39;obituaries&#39; folder, containing the &#39;.html&#39; files downloaded from the 2011 &quot;On This Day&quot; website and a folder of &#39;.txt&#39; files that represent the body of each obituary. These text files were generated using a <a href="https://docs.python.org/3/library/intro.html">Python library</a> called <a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>, which is covered in another <em>Programming Historian</em> lesson (see <a href="/en/lessons/intro-to-beautiful-soup">Intro to BeautifulSoup</a>).</p>
<p>This obituary corpus is an historical object in its own right. It represents, on some level, how the questions of inclusion and representation might affect both the decision to publish an obituary, and the decision to highlight a particular obituary many years later. The significance of such decisions has been further highlighted in recent months by <em>The New York Times</em> itself. In March 2018, the newspaper began publishing obituaries for &quot;overlooked women&quot;.[^2] In the words of Amisha Padnani and Jessica Bennett, &quot;who gets remembered — and how — inherently involves judgment. To look back at the obituary archives can, therefore, be a stark lesson in how society valued various achievements and achievers.&quot; Viewed through this lens, the dataset provided here stands not as a representative sample of historic obituaries but, rather, a snapshot of who <em>The New York Times</em> in 2010-2011 considered worth highlighting. You&#39;ll notice that many of the historic figures are well known, which suggests a self-conscious effort to look back at the history of <em>The New York Times</em> and select obituaries based on some criteria.[^3]</p>
<h2 id="tf-idf-definition-and-background">Tf-idf Definition and Background</h2>
<p>Often inaccurately attributed to others, the procedure called Term Frequency - Inverse Document Frequency was introduced in a 1972 paper by Karen Spärck Jones under the name &quot;term specificity.&quot;[^4] Fittingly, Spärck Jones was the subject of an &quot;Overlooked No More&quot; obituary in January 2019.[^5]</p>
<p>With <strong>tf-idf</strong>, instead of representing a term in a document by its raw frequency (number of occurrences) or its relative frequency (term count divided by document length), each term is weighted by dividing the term frequency by the number of documents in the corpus containing the word. The overall effect of this weighting scheme is to avoid a common problem when conducting text analysis: the most frequently used words in a document are often the most frequently used words in all of the documents. In contrast, terms with the highest <strong>tf-idf</strong> scores are the terms in a document that are <em>distinctively</em> frequent in a document, when that document is compared other documents.</p>
<p>If this explanation doesn&#39;t quite resonate, a brief analogy might help. Imagine that you are on vacation for a weekend in a new city, called Idf City. You&#39;re trying to choose a restaurant for dinner, and you&#39;d like to balance two competing goals: first, you want to have a very good meal, and second, you want to choose a style of cuisine that&#39;s distinctively good in Idf City. That is, you don&#39;t want to have something you can get just anywhere. You can look up online reviews of restaurants all day, and that&#39;s just fine for your first goal, but what you need in order to satisfy the second goal is some way to tell the difference between good and distinctively good (or perhaps even uniquely good).</p>
<p>It&#39;s relatively easy, I think, to see that restaurant food could be:</p>
<ol>
<li>both good and distinctive,</li>
<li>good but not distinctive,</li>
<li>distinctive but not good, or</li>
<li>neither good nor distinctive.</li>
</ol>
<p>Term frequencies could have the same structure. A term might be:</p>
<ol>
<li>Frequently used in a language like English, and especially frequent or infrequent in one document</li>
<li>Frequently used in a language like English, but used to a typical degree in one document</li>
<li>Infrequently used in a language like English, but distinctly frequent or infrequent in one document</li>
<li>Infrequently used in a language like English, and used at to a typical degree in one document</li>
</ol>
<p>To understand how words can be frequent but not distinctive, or distinctive but not frequent, let&#39;s look at a text-based example. The following is a list of the top ten most frequent terms (and term counts) from one of the obituaries in our <em>New York Times</em> corpus.</p>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Term</th>
<th>Count</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>the</td>
<td>21</td>
</tr>
<tr>
<td>2</td>
<td>of</td>
<td>16</td>
</tr>
<tr>
<td>3</td>
<td>her</td>
<td>15</td>
</tr>
<tr>
<td>4</td>
<td>in</td>
<td>14</td>
</tr>
<tr>
<td>5</td>
<td>and</td>
<td>13</td>
</tr>
<tr>
<td>6</td>
<td>she</td>
<td>10</td>
</tr>
<tr>
<td>7</td>
<td>at</td>
<td>8</td>
</tr>
<tr>
<td>8</td>
<td>cochrane</td>
<td>4</td>
</tr>
<tr>
<td>9</td>
<td>was</td>
<td>4</td>
</tr>
<tr>
<td>10</td>
<td>to</td>
<td>4</td>
</tr>
</tbody></table>
<p>After looking at this list, imagine trying to discern information about the obituary that this table represents. We might infer from the presence of <em>her</em> and <em>cochrane</em> in the list that a woman named Cochrane is being discussed but, at the same time, this could easily be about a person from Cochrane, Wisconsin or someone associated with the <a href="https://en.wikipedia.org/wiki/Cochrane_(organisation)">Cochrane Collaboration</a>, a non-profit, non-governmental organization. The problem with this list is that most of top terms would be top terms in any obituary and, indeed, any sufficiently large chunk of writing in most languages. This is because most languages are heavily dependent on function words like <em>the,</em> <em>as,</em> <em>of,</em> <em>to,</em> and <em>from</em> that serve primarily grammatical or structural purposes, and appear regardless of the text&#39;s subject matter. A list of an obituary&#39;s most frequent terms tell us little about the obituary or the person being memorialized.  Now let&#39;s use <strong>tf-idf</strong> term weighting to compare the same obituary from the first example to the rest of our corpus of <em>New York Times</em> obituaries. The top ten term scores look like this:</p>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Term</th>
<th>Count</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>cochrane</td>
<td>24.85</td>
</tr>
<tr>
<td>2</td>
<td>her</td>
<td>22.74</td>
</tr>
<tr>
<td>3</td>
<td>she</td>
<td>16.22</td>
</tr>
<tr>
<td>4</td>
<td>seaman</td>
<td>14.88</td>
</tr>
<tr>
<td>5</td>
<td>bly</td>
<td>12.42</td>
</tr>
<tr>
<td>6</td>
<td>nellie</td>
<td>9.92</td>
</tr>
<tr>
<td>7</td>
<td>mark</td>
<td>8.64</td>
</tr>
<tr>
<td>8</td>
<td>ironclad</td>
<td>6.21</td>
</tr>
<tr>
<td>9</td>
<td>plume</td>
<td>6.21</td>
</tr>
<tr>
<td>10</td>
<td>vexations</td>
<td>6.21</td>
</tr>
</tbody></table>
<p>In this version of the list, <em>she</em> and <em>her</em> have both moved up. <em>cochrane</em> remains, but now we have at least two new name-like words: <em>nellie</em> and <em>bly.</em> <a href="https://en.wikipedia.org/wiki/Nellie_Bly">Nellie Bly</a> was a turn-of-the-century journalist best known today for her investigative journalism, perhaps most remarkably when she had herself committed to the New York City Lunatic Asylum for ten days in order to write an expose on the mistreatment of mental health patients. She was born Elizabeth Cochrane Seaman, and Bly was her pen name or <em>nom-de-plume</em>. With only a few details about Bly, we can account for seven of the top ten <strong>tf-idf</strong> terms: <em>cochrane,</em> <em>her,</em> <em>she,</em> <em>seaman,</em> <em>bly,</em> <em>nellie,</em> and <em>plume.</em> To understand <em>mark</em>, <em>ironclad</em>, and <em>vexations</em>, we can return to the original obituary and discover that Bly died at St. Mark&#39;s Hospital. <a href="https://en.wikipedia.org/wiki/Robert_Seaman">Her husband</a> was president of the Ironclad Manufacturing Company. Finally, &quot;a series of forgeries by her employees, disputes of various sorts, bankruptcy and a mass of vexations and costly litigations swallowed up Nellie Bly&#39;s fortune.&quot;[^6] Many of the terms on this list are mentioned as few as one, two, or three times; they are not frequent by any measure. Their presence in this one document, however, are all distinctive compared with the rest of the corpus.</p>
<h1 id="procedure">Procedure</h1>
<h2 id="how-the-algorithm-works">How the Algorithm Works</h2>
<p><strong>Tf-idf</strong> can be implemented in many flavors, some more complex than others. Before I begin discussing these complexities, however, I would like to trace the algorithmic operations of one particular version. To this end, we will go back to the Nellie Bly obituary and convert the top ten term counts into <strong>tf-idf</strong> scores using the same steps that were used to create the above <strong>tf-idf</strong> example. These steps parallel <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">Scikit-Learn&#39;s <strong>tf-idf</strong></a> implementation. Addition, multiplication, and division are the primary mathematical operations necessary to follow along. At one point, we must calculate the <a href="https://en.wikipedia.org/wiki/Natural_logarithm">natural logarithm</a> of a variable, but this can be done with most online calculators and calculator mobile apps. Below is a table with the raw term counts for the first thirty words, in alphabetical order, from Bly&#39;s obituary, but this version has a second column that represents the number of documents in which each term can be found. Document frequency (<strong>df</strong>) is a count of how many documents from the corpus each word appears in. (Document frequency for a particular word can be represented as <strong>df<sub>i</sub></strong>.)</p>
<table>
<thead>
<tr>
<th>Index</th>
<th>Term</th>
<th>Count</th>
<th>Df</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>afternoon</td>
<td>1</td>
<td>66</td>
</tr>
<tr>
<td>2</td>
<td>against</td>
<td>1</td>
<td>189</td>
</tr>
<tr>
<td>3</td>
<td>age</td>
<td>1</td>
<td>224</td>
</tr>
<tr>
<td>4</td>
<td>ago</td>
<td>1</td>
<td>161</td>
</tr>
<tr>
<td>5</td>
<td>air</td>
<td>1</td>
<td>80</td>
</tr>
<tr>
<td>6</td>
<td>all</td>
<td>1</td>
<td>310</td>
</tr>
<tr>
<td>7</td>
<td>american</td>
<td>1</td>
<td>277</td>
</tr>
<tr>
<td>8</td>
<td>an</td>
<td>1</td>
<td>352</td>
</tr>
<tr>
<td>9</td>
<td>and</td>
<td>13</td>
<td>364</td>
</tr>
<tr>
<td>10</td>
<td>around</td>
<td>2</td>
<td>149</td>
</tr>
<tr>
<td>11</td>
<td>as</td>
<td>2</td>
<td>357</td>
</tr>
<tr>
<td>12</td>
<td>ascension</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>13</td>
<td>asylum</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>14</td>
<td>at</td>
<td>8</td>
<td>362</td>
</tr>
<tr>
<td>15</td>
<td>avenue</td>
<td>2</td>
<td>68</td>
</tr>
<tr>
<td>16</td>
<td>balloon</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>17</td>
<td>bankruptcy</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>18</td>
<td>barrel</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>19</td>
<td>baxter</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>20</td>
<td>be</td>
<td>1</td>
<td>332</td>
</tr>
<tr>
<td>21</td>
<td>beat</td>
<td>1</td>
<td>33</td>
</tr>
<tr>
<td>22</td>
<td>began</td>
<td>1</td>
<td>241</td>
</tr>
<tr>
<td>23</td>
<td>bell</td>
<td>1</td>
<td>24</td>
</tr>
<tr>
<td>24</td>
<td>bly</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>25</td>
<td>body</td>
<td>1</td>
<td>112</td>
</tr>
<tr>
<td>26</td>
<td>born</td>
<td>1</td>
<td>342</td>
</tr>
<tr>
<td>27</td>
<td>but</td>
<td>1</td>
<td>343</td>
</tr>
<tr>
<td>28</td>
<td>by</td>
<td>3</td>
<td>349</td>
</tr>
<tr>
<td>29</td>
<td>career</td>
<td>1</td>
<td>223</td>
</tr>
<tr>
<td>30</td>
<td>character</td>
<td>1</td>
<td>89</td>
</tr>
</tbody></table>
<p>To calculate inverse document frequency for each term, the most direct formula would be <strong>N/df<sub>i</sub></strong>, where <strong>N</strong> represents the total number of documents in the corpus. However, many implementations normalize the results with additional operations. In TF-IDF, normalization is generally used in two ways: first, to prevent bias in term frequency from terms in shorter or longer documents; second, to calculate each term&#39;s idf value (inverse document frequency). For example, Scikit-Learn&#39;s implementation represents <strong>N</strong> as <strong>N+1</strong>, calculates the natural logarithm of <strong>(N+1)/df<sub>i</sub></strong>, and then adds 1 to the final result. This lesson will return to the topic of normalization in the section below titled <a href="#scikit-learn-settings">&quot;Scikit-Learn Settings&quot;</a>.</p>
<p>To express Scikit-Learn&#39;s <strong>idf</strong> transformation[^7], we can state the following equation:</p>
<p>$$ idf_i = ln[, ({N}+1) /, {df_i}] + 1 $$</p>
<p>Once <strong>idf<sub>i</sub></strong> is calculated, <strong>tf-idf<sub>i</sub></strong> is <strong>tf<sub>i</sub></strong> multiplied by <strong>idf<sub>i</sub></strong>.</p>
<p>$$ tf{\text -}idf_i = tf_i , \times , idf_i $$</p>
<p>Mathematical equations like these can be a bit bewildering if you&#39;re not used to them. Once you&#39;ve had some experience with them, they can provide a more lucid description of an algorithm&#39;s operations than any well written paragraph. (For more on this subject, Ben Schmidt&#39;s &quot;Do Digital Humanists Need to Understand Algorithms?&quot; is a good place to start.[^8]) To make the <strong>idf</strong> and <strong>tf-idf</strong> equations more concrete, I&#39;ve added two new columns to the terms frequency table from before. The first new column represents the derived <strong>idf</strong> score, and the second new column multiplies the Count and Idf columns to derive the final <strong>tf-idf</strong> score. Notice that that <strong>idf</strong> score is higher if the term appears in fewer documents, but that the range of visible <strong>idf</strong> scores is between 1 and 6. Different <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">normalization</a> schemes would produce different scales.</p>
<p>Note also that the <strong>tf-idf</strong> column, according to this version of the algorithm, cannot be lower than the count. This effect is also the result of our normalization method; adding 1 to the final <strong>idf</strong> value ensures that we will never multiply our Count columns by a number smaller than one, which preserves the original distribution of the data.</p>
<table>
<thead>
<tr>
<th>Index</th>
<th>Term</th>
<th>Count</th>
<th>Df</th>
<th>Idf</th>
<th>Tf-idf</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>afternoon</td>
<td>1</td>
<td>66</td>
<td>2.70066923</td>
<td>2.70066923</td>
</tr>
<tr>
<td>2</td>
<td>against</td>
<td>1</td>
<td>189</td>
<td>1.65833778</td>
<td>1.65833778</td>
</tr>
<tr>
<td>3</td>
<td>age</td>
<td>1</td>
<td>224</td>
<td>1.48926145</td>
<td>1.48926145</td>
</tr>
<tr>
<td>4</td>
<td>ago</td>
<td>1</td>
<td>161</td>
<td>1.81776551</td>
<td>1.81776551</td>
</tr>
<tr>
<td>5</td>
<td>air</td>
<td>1</td>
<td>80</td>
<td>2.51091269</td>
<td>2.51091269</td>
</tr>
<tr>
<td>6</td>
<td>all</td>
<td>1</td>
<td>310</td>
<td>1.16556894</td>
<td>1.16556894</td>
</tr>
<tr>
<td>7</td>
<td>american</td>
<td>1</td>
<td>277</td>
<td>1.27774073</td>
<td>1.27774073</td>
</tr>
<tr>
<td>8</td>
<td>an</td>
<td>1</td>
<td>352</td>
<td>1.03889379</td>
<td>1.03889379</td>
</tr>
<tr>
<td>9</td>
<td>and</td>
<td>13</td>
<td>364</td>
<td>1.00546449</td>
<td>13.07103843</td>
</tr>
<tr>
<td>10</td>
<td>around</td>
<td>2</td>
<td>149</td>
<td>1.89472655</td>
<td>3.78945311</td>
</tr>
<tr>
<td>11</td>
<td>as</td>
<td>2</td>
<td>357</td>
<td>1.02482886</td>
<td>2.04965772</td>
</tr>
<tr>
<td>12</td>
<td>ascension</td>
<td>1</td>
<td>6</td>
<td>4.95945170</td>
<td>4.95945170</td>
</tr>
<tr>
<td>13</td>
<td>asylum</td>
<td>1</td>
<td>2</td>
<td>5.80674956</td>
<td>5.80674956</td>
</tr>
<tr>
<td>14</td>
<td>at</td>
<td>8</td>
<td>362</td>
<td>1.01095901</td>
<td>8.08767211</td>
</tr>
<tr>
<td>15</td>
<td>avenue</td>
<td>2</td>
<td>68</td>
<td>2.67125534</td>
<td>5.34251069</td>
</tr>
<tr>
<td>16</td>
<td>balloon</td>
<td>1</td>
<td>2</td>
<td>5.80674956</td>
<td>5.80674956</td>
</tr>
<tr>
<td>17</td>
<td>bankruptcy</td>
<td>1</td>
<td>8</td>
<td>4.70813727</td>
<td>4.70813727</td>
</tr>
<tr>
<td>18</td>
<td>barrel</td>
<td>1</td>
<td>7</td>
<td>4.82592031</td>
<td>4.82592031</td>
</tr>
<tr>
<td>19</td>
<td>baxter</td>
<td>1</td>
<td>4</td>
<td>5.29592394</td>
<td>5.29592394</td>
</tr>
<tr>
<td>20</td>
<td>be</td>
<td>1</td>
<td>332</td>
<td>1.09721936</td>
<td>1.09721936</td>
</tr>
<tr>
<td>21</td>
<td>beat</td>
<td>1</td>
<td>33</td>
<td>3.37900132</td>
<td>3.37900132</td>
</tr>
<tr>
<td>22</td>
<td>began</td>
<td>1</td>
<td>241</td>
<td>1.41642412</td>
<td>1.41642412</td>
</tr>
<tr>
<td>23</td>
<td>bell</td>
<td>1</td>
<td>24</td>
<td>3.68648602</td>
<td>3.68648602</td>
</tr>
<tr>
<td>24</td>
<td>bly</td>
<td>2</td>
<td>1</td>
<td>6.21221467</td>
<td>12.42442933</td>
</tr>
<tr>
<td>25</td>
<td>body</td>
<td>1</td>
<td>112</td>
<td>2.17797403</td>
<td>2.17797403</td>
</tr>
<tr>
<td>26</td>
<td>born</td>
<td>1</td>
<td>342</td>
<td>1.06763140</td>
<td>1.06763140</td>
</tr>
<tr>
<td>27</td>
<td>but</td>
<td>1</td>
<td>343</td>
<td>1.06472019</td>
<td>1.06472019</td>
</tr>
<tr>
<td>28</td>
<td>by</td>
<td>3</td>
<td>349</td>
<td>1.04742869</td>
<td>3.14228608</td>
</tr>
<tr>
<td>29</td>
<td>career</td>
<td>1</td>
<td>223</td>
<td>1.49371580</td>
<td>1.49371580</td>
</tr>
<tr>
<td>30</td>
<td>character</td>
<td>1</td>
<td>89</td>
<td>2.40555218</td>
<td>2.40555218</td>
</tr>
</tbody></table>
<p>These tables collectively represent one particular version of the <strong>tf-idf</strong> transformation. Of course, <strong>tf-idf</strong> is generally calculated for all terms in all of the documents in your corpus so that you can see which terms in each document have the highest <strong>tf-idf</strong> scores. To get a better sense of the what your output might look like after executing such an operation, download and open the full Excel file for Bly&#39;s obituary by downloading <a href="/assets/tf-idf/lesson-files.zip">the lesson files</a>, extracting the &#39;.zip&#39; archive, and opening &#39;bly_tfidf_all.xlsx&#39;.</p>
<h2 id="how-to-run-it-in-python-3">How to Run it in Python 3</h2>
<p>In this section of the lesson, I will walk through the steps I followed to calculate <strong>tf-idf</strong> scores for all terms in all documents in the lesson&#39;s obituary corpus. If you would like to follow along, you can download the lesson files, extract the &#39;.zip&#39; archive, and run the Jupyter Notebook inside of the &#39;lesson&#39; folder. You can also create a new Jupyter Notebook in that location copy/paste blocks of code from this tutorial as you go. If you are using Anaconda, visit the <a href="https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/execute.html">Jupyter Notebook Documentation Page</a> for more information on changing the Jupyter Notebook startup location. As with any programming language, there&#39;s more than one way to do each of the steps I discuss below.</p>
<p>My first block of code is designed to retrieve all the filenames for &#39;.txt&#39; files in the &#39;txt&#39; folder. The following lines of code import the <code>Path</code> class from the <code>pathlib</code> library and use the <code>Path().rglob()</code> method to generate a list of all the files in the &#39;txt&#39; folder that end with &#39;.txt&#39;. <code>pathlib</code> will also join the <code>file.parent</code> folder location with each file name to provide full file paths for each file (on MacOS or Windows).</p>
<p>Using this method, I append each text file name to the list called <code>all_txt_files</code>. Finally, I return the length of <code>all_txt_files</code> to verify that I&#39;ve found 366 file names. This loop-and-append approach is very common in Python.</p>
<pre><code class="language-python">from pathlib import Path

all_txt_files =[]
for file in Path(&quot;txt&quot;).rglob(&quot;*.txt&quot;):
     all_txt_files.append(file.parent / file.name)
# counts the length of the list
n_files = len(all_txt_files)
print(n_files)
</code></pre>
<p>A quick note on variable names. The two most common variable naming patterns prioritize convenience and semantic meaning respectively. For convenience, one might name a variable <strong>x</strong> so it&#39;s easier and faster to type when referencing it. Semantic variables, meanwhile, attempt to describe function or purpose. By naming my list of all text files <code>all_txt_files</code> and the variable representing the number of files <code>n_files</code>, I&#39;m prioritizing semantic meaning. Meanwhile, I&#39;m using abbreviations like <code>txt</code> for text and <code>n</code> for number to save on typing, or using <code>all_txt_files</code> instead of <code>all_txt_file_names</code> because brevity is still a goal. Underscore and capitalization norms are specified in PEP-8, Python&#39;s official style guide, with which you should try to be generally familiar.[^9]</p>
<p>For various resons, we want our files to count up by day and month since there&#39;s on file for every day and month of a year. We can use the <code>sort()</code> method to put the files in ascending numerical order and print the first file to make sure it&#39;s &#39;txt/0101.txt&#39;.</p>
<pre><code class="language-python">all_txt_files.sort()
all_txt_files[0]
</code></pre>
<p>Next, we can use our list of file names to load each file and convert them to a format that Python can read and understand as text. In this block of code, I do another loop-and-append operation. This time, I loop my list of file names and open each file. I then use Python&#39;s <code>read()</code> method to convert each text file to a string (<code>str</code>), which is how Python knows to think of the data as text. I append each string, one by one, to a new list called <code>all_docs</code>. Crucially, the string objects in this list have the same order as the file names in the <code>all_txt_files</code> list.</p>
<pre><code class="language-python">all_docs = []
for txt_file in all_txt_files:
    with open(txt_file) as f:
        txt_file_as_string = f.read()
    all_docs.append(txt_file_as_string)
</code></pre>
<p>This is all the setup work we require. Text processing steps like <a href="https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization">tokenization</a> and removing punctuation will happen automatically when we use Scikit-Learn&#39;s <code>TfidfVectorizer</code> to convert documents from a list of strings to <strong>tf-idf</strong> scores. One could also supply a list of stopwords here (commonly used words that you want to ignore). To tokenize and remove stopwords in languages other than English, you may need to preprocess the text with another Python library or supply a custom tokenizer and stopword list when Scikit-Learn&#39;s <code>TfidfVectorizer</code>. The following block of code imports <code>TfidfVectorizer</code> from the Scikit-Learn library, which comes pre-installed with Anaconda. <code>TfidfVectorizer</code> is a class (written using object-oriented programming), so I instantiate it with specific parameters as a variable named <code>vectorizer</code>. (I’ll say more about these settings in the section titled <a href="#scikit-learn-settings">&quot;Scikit-Learn Settings&quot;</a>.) I then run the object&#39;s <code>fit_transform()</code> method on my list of strings (a variable called <code>all_docs</code>). The stored variable <code>X</code> is output of the <code>fit_transform()</code> method.</p>
<pre><code class="language-python">#import the TfidfVectorizer from Scikit-Learn.
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None)
transformed_documents = vectorizer.fit_transform(all_docs)
</code></pre>
<p>The <code>fit_transform()</code> method above converts the list of strings to something called a <a href="https://en.wikipedia.org/wiki/Sparse_matrix">sparse matrix</a>. In this case, the matrix represents <strong>tf-idf</strong> values for all texts. Sparse matrices save on memory by leaving out all zero values, but we want access to those, so the next block uses the <code>toarray()</code> method to convert the sparse matrices to a <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html">numpy array</a>. We can print the length of the array to ensure that it&#39;s the same length as our list of documents.</p>
<pre><code class="language-python">transformed_documents_as_array = transformed_documents.toarray()
# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list
len(transformed_documents_as_array)
</code></pre>
<p>A numpy array is list-like but not exactly a list, and I could fill an entire tutorial discussing the differences, but there&#39;s only one aspect of numpy arrays we need to know right now: it converts the data stored in <code>transformed_documents</code> to a format where every <strong>tf-idf</strong> score for every term in every document is represented. Sparse matrices, in contrast, exclude zero-value term scores.</p>
<p>We want every term represented so that each document has the same number of values, one for each word in the corpus. Each item in <code>transformed_documents_as_array</code> is an array of its own representing one document from our corpus. As a result of all this, we essentially have a grid where each row is a document, and each column is a term. Imagine one table from a spreadsheet representing each document, like the tables above, but without column or row labels.</p>
<p>To merge the values with their labels, we need two pieces of information: the order of the documents, and the order in which term scores are listed. The order of these documents is easy because it&#39;s the same order as the variable <code>all_docs list</code>. The full term list is stored in our <code>vectorizer</code> variable, and it&#39;s in the same order that each item in <code>transformed_documents_as_array</code> stores values. We can use the <code>the TFIDFVectorizer</code> class&#39;s <code>get_feature_names()</code> method to get that list, and each row of data (one document&#39;s <strong>tf-idf</strong> scores) can be rejoined with the term list. (For more details on pandas dataframes, see the lesson <a href="/en/lessons/visualizing-with-bokeh">&quot;Visualizing Data with Bokeh and Pandas&quot;</a>.)</p>
<pre><code class="language-python">import pandas as pd

# make the output folder if it doesn&#39;t already exist
Path(&quot;./tf_idf_output&quot;).mkdir(parents=True, exist_ok=True)

# construct a list of output file paths using the previous list of text files the relative path for tf_idf_output
output_filenames = [str(txt_file).replace(&quot;.txt&quot;, &quot;.csv&quot;).replace(&quot;txt/&quot;, &quot;tf_idf_output/&quot;) for txt_file in all_txt_files]

# loop each item in transformed_documents_as_array, using enumerate to keep track of the current position
for counter, doc in enumerate(transformed_documents_as_array):
    # construct a dataframe
    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))
    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=[&#39;term&#39;, &#39;score&#39;]).sort_values(by=&#39;score&#39;, ascending=False).reset_index(drop=True)

    # output to a csv using the enumerated value for the filename
    one_doc_as_df.to_csv(output_filenames[counter])
</code></pre>
<p>The above block of code has three parts:</p>
<ol>
<li><p>After importing the pandas library, it checks for a folder called &#39;tf_idf_output&#39; and creates it if it doesn&#39;t exist.</p>
</li>
<li><p>It takes the list of &#39;.txt&#39; files from my earlier block of code and use it to construct a counterpart &#39;.csv&#39; file path for each &#39;.txt&#39; file. The output_filenames variable will, for example, convert &#39;txt/0101.txt&#39; (the path of the first &#39;.txt&#39; file) to &#39;tf_idf_output/0101.csv&#39;, and on and on for each file.</p>
</li>
<li><p>Using a loop, it merges each vector of <strong>tf-idf</strong> scores with the feature names from vectorizer, converts each merged term/score pairs to a pandas dataframe, and saves each dataframe to its corresponding &#39;.csv&#39; file.</p>
</li>
</ol>
<h2 id="interpreting-word-lists-best-practices-and-cautionary-notes">Interpreting Word Lists: Best Practices and Cautionary Notes</h2>
<p>After you run the code excerpts above, you will end up with a folder called &#39;tf_idf_output&#39; with 366 &#39;.csv&#39; files in it. Each file corresponds to an obituary in the &#39;txt&#39; folder, and each contains a list of terms with <strong>tf-idf</strong> scores for that document. As we saw with Nellie Bly&#39;s obituary, these term lists can be very suggestive; however, it&#39;s important to understand that over-interpreting your results can actually distort your understanding of an underlying text.</p>
<p>In general, it&#39;s best to begin with the ideas that these term lists will be helpful for generating hypotheses or research questions. <strong>Tf-idf</strong> results but will not necessarily produce definitive claims. For example, I have assembled a quick list of obituaries for late 19th- and early 20th-century figures who all worked for newspapers and magazines and had some connection to social reform. My list includes Nellie Bly, <a href="https://en.wikipedia.org/wiki/Willa_Cather">Willa Cather</a>, <a href="https://en.wikipedia.org/wiki/W._E._B._Du_Bois">W.E.B. Du Bois</a>, <a href="https://en.wikipedia.org/wiki/Upton_Sinclair">Upton Sinclair</a>, <a href="https://en.wikipedia.org/wiki/Ida_Tarbell">Ida Tarbell</a>, but there may be other figures in the corpus who fit the same criteria.[^10]</p>
<p>I originally expected to see many shared terms, but I was surprised. Each list is dominated by individualized words (proper names, geographic places, companies, etc.) but I could screen these out using my <strong>tf-idf</strong> settings, or just ignore them. Simultaneously, I can look for words overtly indicating each figure&#39;s ties to the profession of authorship. (The section of this tutorial titled <a href="#scikit-learn-settings">Scikit-Learn Settings</a> says more about how you can treat a named entity or a phrase as a single token.) The following table shows the top 20 <strong>tf-idf</strong> terms by rank for each obituary:</p>
<p>| Tf-idf Rank | Nellie Bly | Willa Cather | W.E.B. Du Bois | Upton Sinclair | Ida Tarbell |
| 1 | cochrane | cather | dubois | sinclair | tarbell |
| 2 | her | her | dr | socialist | she |
| 3 | she | she | negro | upton | her |
| 4 | seaman | nebraska | ghana | <strong>books</strong> | lincoln |
| 5 | bly | miss | peace | lanny | miss |
| 6 | nellie | forrester | <strong>encyclopedia</strong> | social | oil |
| 7 | mark | sibert | communist | budd | abraham |
| 8 | ironclad | twilights | barrington | jungle | mcclure |
| 9 | <strong>plume</strong> | willa | fisk | brass | easton |
| 10 | vexations | antonia | atlanta | california | <strong>volumes</strong> |
| 11 | phileas | mcclure | folk | <strong>writer</strong> | minerva |
| 12 | 597 | <strong>novels</strong> | booker | vanzetti | standard |
| 13 | elizabeth | pioneers | successively | macfadden | business |
| 14 | <strong>nom</strong> | cloud | souls | sacco | titusville |
| 15 | balloon | <strong>book</strong> | council | <strong>wrote</strong> | <strong>articles</strong> |
| 16 | forgeries | calif | party | meat | bridgeport |
| 17 | mcalpin | <strong>novel</strong> | disagreed | <strong>pamphlets</strong> | expose |
| 18 | asylum | southwest | harvard | my | trusts |
| 19 | fogg | <strong>verse</strong> | <strong>arts</strong> | industry | mme
| 20 | verne | <strong>wrote</strong> | soviet | <strong>novel</strong> | <strong>magazine</strong> |</p>
<p>I&#39;ve used boldface to indicate terms that seem overtly related to authorship or writing. The list includes <em>articles</em>, <em>arts</em>, <em>book</em>, <em>book</em>, <em>books</em>, <em>encyclopedia</em>, <em>magazine</em>, <em>nom</em>, <em>novel</em>, <em>novels</em>, <em>pamphlets</em>, <em>plume</em>, <em>verse</em>, <em>volumes</em>, <em>writer</em>, and <em>wrote</em>, but it could be extended to include references to specific magazine or book titles. Setting aside momentarily such complexities, it is striking to me that Cather and Sinclair&#39;s lists have so many words for books and writing, whereas Bly, Du Bois and Tarbell&#39;s do not.</p>
<p>I could easily jump to conclusions. Cather&#39;s identity seems to be tied most to her gender, her sense of place, and her fiction and verse. Sinclair more so with his politics and his writings about meat, industry, and specifically the well known, controversial trial and execution of Nicola Sacco and Bartolomeo Vanzetti. Bly is tied to her pen name, her husband, and her writing about asylums. Du Bois is linked to race and his academic career. Tarbell is described by what she wrote about: namely business, the trusts, Standard Oil, and Abraham Lincoln. Going further, I could argue that gender seems more distinctive for women than it is for men; race is only a top term for the one African American in my set.</p>
<p>Each of these observations forms the basis for a deeper question, but these details aren&#39;t enough to make generalizations. Foremost, I need to consider whether my <strong>tf-idf</strong> settings are producing effects that would disappear under other conditions; robust results should be stable enough to appear with various settings. (Some of these settings are covered in the <a href="#scikit-learn-settings">&quot;Scikit-Learn Settings&quot;</a> section.) Next, I should read at least some of the underlying obituaries to make sure I&#39;m not getting false signals from any terms. If I read Du Bois&#39;s obituary, for example, I may discover that mentions of his work &quot;The Encyclopedia of the Negro,&quot; contribute at least partially to the overall score of the word <em>negro</em>.</p>
<p>Likewise, I can discover that Bly&#39;s obituary does include words like <em>journalism</em>, <em>journalistic</em>, <em>newspapers</em>, and <em>writing</em>, but the obituary is very short, meaning most words mentioned in it occur only once or twice, which means that words with very high <strong>idf</strong> scores are even more likely to top her list. I really want <strong>tf</strong> and <strong>idf</strong> to be balanced, so I could rule out words that appear in only a few documents, or I could ignore results for obituaries below a certain word count.</p>
<p>Finally, I can design tests to measure directly questions like: were obituaries of African Americans are more likely to mention race? I think the prediction that they did is a good hypothesis, but I should still subject my generalizations to scrutiny before I form conclusions.</p>
<h2 id="some-ways-tf-idf-can-be-used-in-computational-history">Some Ways Tf-idf Can Be Used in Computational History</h2>
<p>As I have described, <strong>tf-idf</strong> has its origins in information retrieval, and the idea of weighting term frequencies against norms in a larger corpus continues to be used to power various aspects of everyday web applications, especially text-based search engines. However, in a cultural analytics or computational history context, <strong>tf-idf</strong> is suited for a particular set of tasks. These uses tend to fall into one of three groups.</p>
<h3 id="1-as-an-exploratory-tool-or-visualization-technique">1. As an Exploratory Tool or Visualization Technique</h3>
<p>As I&#39;ve already demonstrated, terms lists with <strong>tf-idf</strong> scores for each document in a corpus can be a strong interpretive aid in themselves, they can help generate hypotheses or research questions. Word lists can also be the building bocks for more sophisticated browsing and visualization strategies. <a href="http://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs">&quot;A full-text visualization of the Iraq War Logs&quot;</a>, by Jonathan Stray and Julian Burgess, is a good example of this use case.[^11] Using <strong>tf-idf</strong>-transformed features, Stray and Burgess build a network visualization that positions Iraq War logs in relation to their most distinctive keywords. This way of visualizing textual information led Stray to develop <a href="https://www.overviewdocs.com">the Overview Project</a>, which provides a dashboard for users to visualize and search thousands of documents at a time. We could use this kind of approach to graph our obituaries corpus and see if there are keyword communities.</p>
<h3 id="2-textual-similarity-and-feature-sets">2. Textual Similarity and Feature Sets</h3>
<p>Since <strong>tf-idf</strong> will often produce lower scores for high frequency function words and increased scores for terms related to the topical signal of a text, it is well suited for tasks involving textual similarity. A search index will often perform <strong>tf-idf</strong> on a corpus and return ranked results to user searches by looking for documents with the highest cosine similarity to the user&#39;s search string. The same logic can be used to ask a question like &quot;Which obituary in our corpus is most similar to Nellie Bly&#39;s obituary?&quot;</p>
<p>Similarly, we could use <strong>tf-idf</strong> to discover the top terms related to a document or a group of documents. For example, I could gather together a selection of obituaries about journalists (Bly included) and combine them into one document before running <strong>tf-idf</strong>. The output for that document would now work as a heuristic for terms that are distinctive in my journalism obituaries in the corpus when compared with other obituaries in the corpus. I could use such a term list for a range of other computational tasks.</p>
<h3 id="3-as-a-pre-processing-step">3. As a Pre-processing Step</h3>
<p>The above paragraphs gesture at why <strong>tf-idf</strong> pre-processing is so often used with machine learning. <strong>Tf-idf</strong>-transformed features tend to have more predictive value than raw term frequencies, especially when classifying a supervised machine learning model, in part because it tends to increase the weight of topic words and reduce the weight of high frequency function words. One notable exception to this generalization is authorship attribution, where high frequency function words are highly predictive. As I will show in the <a href="#scikit-learn-settings">&quot;Scikit-Learn Settings&quot;</a> section, <strong>tf-idf</strong> can also be used to cull machine learning feature lists and, often, building a model with fewer features is desirable.</p>
<h2 id="potential-variations-of-tf-idf">Potential Variations of Tf-idf</h2>
<h3 id="scikit-learn-settings">Scikit-Learn Settings</h3>
<p>The Scikit-Learn <code>TfidfVectorizer</code> has several internal settings that can be changed to affect the output. In general, these settings all have pros and cons; there&#39;s no singular, correct way to preset them and produce output. Instead, it&#39;s best to understand exactly what each setting does so that you can describe and defend the choices you&#39;ve made. The full list of parameters is described in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">Scikit-Learn&#39;s documentation</a>, but here are some of the most important settings:</p>
<h4 id="1-stopwords">1. stopwords</h4>
<p>In my code, I used <code>python stopwords=None</code> but <code>python stopwords=&#39;english&#39;</code> is available. This setting will filter out words using a <a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/_stop_words.py">preselected list</a> of high frequency function words such as &#39;the&#39;, &#39;to&#39;, and &#39;of&#39;. Depending on your settings, many of these terms will have low <strong>tf-idf</strong> scores regardless because they tend to be found in all documents. For a discussion of some publicly available stop word lists (including Scikit-Learn&#39;s), see <a href="https://aclweb.org/anthology/W18-2502">&quot;Stop Word Lists in Free Open-source Software Packages&quot;</a>.</p>
<h4 id="2-min_df-max_df">2. min_df, max_df</h4>
<p>These settings control the minimum number of documents a term must be found in to be included and the maximum number of documents a term can be found in in order to be included. Either can be expressed as a decimal between 0 and 1 indicating the percent threshold, or as a whole number that represents a raw count. Setting max_df below .9 will typically remove most or all stopwords.</p>
<h4 id="3-max_features">3. max_features</h4>
<p>This parameter can be used to winnow out terms by frequency before running tf-idf. It can be especially useful in a machine learning context when you do not wish to exceed a maximum recommended number of term features.</p>
<h4 id="4-norm-smooth_idf-and-sublinear_tf">4. norm, smooth_idf, and sublinear_tf</h4>
<p>Each of these will affect the range of numerical scores that the <strong>tf-idf</strong> algorithm outputs. norm supports l1 and l2 normalization, which you can read about on <a href="https://machinelearningmastery.com/vector-norms-machine-learning/">machinelearningmastery.com</a>. Smooth-idf adds one to each document frequency score, &quot;as if an extra document was seen containing every term in the collection exactly once.&quot; Sublinear_tf applies another scaling transformation, replacing tf with log(tf). For more on <strong>tf-idf</strong> smoothing and normalization, see Manning, Raghavan, and Schütze.[^12]</p>
<h3 id="beyond-term-features">Beyond Term Features</h3>
<p>Since the basic idea of <strong>tf-idf</strong> is to weight term counts against the number of documents in which terms appear, the same logic can be used on other text-based features. For example, it is relatively straightforward to combine <strong>tf-idf</strong> with <a href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html">stemming or lemmatization</a>. Stemming and lemmatization are two common ways to group together different word forms/inflections; for example, the stem of both <em>happy</em> and <em>happiness</em> is <em>happi</em>, and the lemma of both is <em>happy</em>. After stemming or lemmatization, stem or lemma counts can be substituted for term counts, and the <strong>(s/l)f-idf</strong> transformation can be applied. Each stem or lemma will have a higher <strong>df</strong> score than each of the words it groups together, so lemmas or stems with many word variants will tend to have lower <strong>tf-idf</strong> scores.</p>
<p>Similarly, the <strong>tf-idf</strong> transformation can be applied to n-grams. A Fivethirtyeight.com post from March 2016 called <a href="https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/">&quot;These Are The Phrases Each GOP Candidate Repeats Most&quot;</a> uses such an approach to perform the inverse-document frequency calculation on phrases rather than words.[^13]</p>
<h2 id="tf-idf-and-common-alternatives">Tf-idf and Common Alternatives</h2>
<p><strong>Tf-idf</strong> can be compared with several other methods of isolating and/or ranking important term features in a document or collection of documents. This section provides a brief mention of four related but distinct measures that target similar but not identical aspects of textual information.</p>
<h3 id="1-keyness">1. Keyness</h3>
<p>Keyness is a catchall term for a constellation of statistical measures that attempt to indicate the numerical significance of a term to a document or set of documents, in direct comparison with a larger set of documents or corpus. Depending on how we set up our <strong>tf-idf</strong> transformation, it may isolate many of a document&#39;s most important features, but <strong>tf-idf</strong> is not as precise as the most commonly used measures of keyness. Rather than changing a document&#39;s term frequency scores, keyness testing produces a numerical indicator of how statistically typical or atypical the term&#39;s usage in a text is. With a <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-square test</a>, for example, we can evaluate the relationship of a term frequency to an established norm, and derive a <a href="https://en.wikipedia.org/wiki/P-value">P-value</a> indicating the probability of encountering the observed difference in a random sample. For more information on keyness, see Bondi and Scott.[^14]</p>
<h3 id="2-topic-models">2. Topic Models</h3>
<p>Topic modeling and <strong>tf-idf</strong> are radically different techniques, but I find that newcomers to digital humanities often want to run topic modeling on a corpus as a first step and, in at least some of those cases, running <strong>tf-idf</strong> instead of generating topic models would be preferable.[^15] <strong>Tf-idf</strong> is especially appropriate if you are looking for a way to get a bird&#39;s eye view of your corpus early in the exploratory phase of your research because the algorithm is transparent and the results are reproducible. As Ben Schmidt suggests, scholars using topic modeling need to know that &quot;topics may not be as coherent as they assume.&quot;[^16] This is one reason <strong>tf-idf</strong> is integrated into <a href="https://www.overviewdocs.com">the Overview Project</a>. Topic models can also help scholars explore their corpora, and they have several advantages over other techniques, namely that they suggest broad categories or communities of texts, but this a general advantage of unsupervised clustering methods. Topic models are especially appealing because documents are assigned scores for how well they fit each topic, and because topics are represented as lists of co-occurring terms, which provides a strong sense of how terms relate to groupings. However, the probabilistic model behind topic models is sophisticated, and it&#39;s easy to warp your results if you don&#39;t understand what you&#39;re doing. The math behind <strong>tf-idf</strong> is lucid enough to depict in a spreadsheet.</p>
<h3 id="3-automatic-text-summarization">3. Automatic Text Summarization</h3>
<p>Text summarization is yet another way to explore a corpus. Rada Mihalcea and Paul Tarau, for example, have published on TextRank, &quot;a graph-based ranking model for text processing&quot; with promising applications for keyword and sentence extraction.[^17] As with topic modeling, TextRank and <strong>tf-idf</strong> are altogether dissimilar in their approach to information retrieval, yet the goal of both algorithms has a great deal of overlap. It may be appropriate for your research, especially if your goal is to get a relatively quick a sense of your documents&#39; contents before designing a larger research project.</p>
<h1 id="references-and-further-reading">References and Further Reading</h1>
<ul>
<li><p>Beckman, Milo. &quot;These Are The Phrases Each GOP Candidate Repeats Most,&quot; <em>FiveThirtyEight</em>, March 10, 2016. <a href="https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/">https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/</a></p>
</li>
<li><p>Bennett, Jessica, and Amisha Padnani. &quot;Overlooked,&quot; March 8, 2018. <a href="https://www.nytimes.com/interactive/2018/obituaries/overlooked.html">https://www.nytimes.com/interactive/2018/obituaries/overlooked.html</a></p>
</li>
<li><p>Blei, David M., Andrew Y. Ng, and Michael I. Jordan, &quot;Latent Dirichlet Allocation&quot; <em>Journal of Machine Learning Research</em> 3 (January 2003): 993-1022.</p>
</li>
<li><p>Bondi, Marina, and Mike Scott, eds. <em>Keyness in Texts</em>. Philadelphia: John Benjamins, 2010.</p>
</li>
<li><p>Bowles, Nellie. &quot;Overlooked No More: Karen Sparck Jones, Who Established the Basis for Search Engines&quot; <em>The New York Times</em>, January 2, 2019. <a href="https://www.nytimes.com/2019/01/02/obituaries/karen-sparck-jones-overlooked.html">https://www.nytimes.com/2019/01/02/obituaries/karen-sparck-jones-overlooked.html</a></p>
</li>
<li><p>Documentation for TfidfVectorizer. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>
</li>
<li><p>Grimmer, Justin and King, Gary, Quantitative Discovery from Qualitative Information: A General-Purpose Document Clustering Methodology (2009). APSA 2009 Toronto Meeting Paper. Available at SSRN: <a href="https://ssrn.com/abstract=1450070">https://ssrn.com/abstract=1450070</a></p>
</li>
<li><p>&quot;Ida M. Tarbell, 86, Dies in Bridgeport&quot; <em>The New York Times</em>, January 7, 1944, 17. <a href="https://www.nytimes.com">https://www.nytimes.com</a></p>
</li>
<li><p>Manning, C.D., P. Raghavan, and H. Schütze, <em>Introduction to Information Retrieval</em>. Cambridge: Cambridge University Press, 2008.</p>
</li>
<li><p>Mihalcea, Rada, and Paul Tarau. &quot;Textrank: Bringing order into text.&quot; In Proceedings of the 2004 conference on empirical methods in natural language processing. 2004.</p>
</li>
<li><p>&quot;Nellie Bly, Journalist, Dies of Pneumonia&quot; <em>The New York Times</em>, January 28, 1922, 11. <a href="https://www.nytimes.com">https://www.nytimes.com</a></p>
</li>
<li><p>Salton, G. and M.J. McGill, <em>Introduction to Modern Information Retrieval</em>. New York: McGraw-Hill, 1983.</p>
</li>
<li><p>Schmidt, Ben. &quot;Do Digital Humanists Need to Understand Algorithms?&quot; <em>Debates in the Digital Humanities 2016</em>. Online edition. Minneapois: University of Minnesota Press. <a href="http://dhdebates.gc.cuny.edu/debates/text/99">http://dhdebates.gc.cuny.edu/debates/text/99</a></p>
</li>
<li><p>--. &quot;Words Alone: Dismantling Topic Models in the Humanities,&quot; <em>Journal of Digital Humanities</em>. Vol. 2, No. 1 (2012): n.p. <a href="http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/">http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/</a></p>
</li>
<li><p>Spärck Jones, Karen. &quot;A Statistical Interpretation of Term Specificity and Its Application in Retrieval.&quot; Journal of Documentation 28, no. 1 (1972): 11–21.</p>
</li>
<li><p>Stray, Jonathan, and Julian Burgess. &quot;A Full-text Visualization of the Iraq War Logs,&quot; December 10, 2010 (Update April 2012). <a href="http://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs">http://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs</a></p>
</li>
<li><p>Underwood, Ted. &quot;Identifying diction that characterizes an author or genre: why Dunning&#39;s may not be the best method,&quot; <em>The Stone and the Shell</em>, November 9, 2011. <a href="https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/">https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/</a></p>
</li>
<li><p>--. &quot;The Historical Significance of Textual Distances&quot;, Preprint of LaTeCH-CLfL Workshop, COLING, Santa Fe, 2018. <a href="https://arxiv.org/abs/1807.00181">https://arxiv.org/abs/1807.00181</a></p>
</li>
<li><p>van Rossum,  Guido, Barry Warsaw, and Nick Coghlan. &quot;PEP 8 -- Style Guide for Python Code.&quot; July 5, 2001. Updated July 2013. <a href="https://www.python.org/dev/peps/pep-0008/">https://www.python.org/dev/peps/pep-0008/</a></p>
</li>
<li><p>Whitman, Alden. &quot;Upton Sinclair, Author, Dead; Crusader for Social Justice, 90&quot; <em>The New York Times</em>, November 26, 1968, 1, 34. <a href="https://www.nytimes.com">https://www.nytimes.com</a></p>
</li>
<li><p>&quot;W. E. B. DuBois Dies in Ghana; Negro Leader and Author, 95&quot; <em>The New York Times</em>, August 28, 1963, 27. <a href="https://www.nytimes.com">https://www.nytimes.com</a></p>
</li>
<li><p>&quot;Willa Cather Dies; Noted Novelist, 70&quot; <em>The New York Times</em>, April 25, 1947, 21. <a href="https://www.nytimes.com">https://www.nytimes.com</a></p>
</li>
</ul>
<h2 id="alternatives-to-anaconda">Alternatives to Anaconda</h2>
<p>If you are not using Anaconda, you will need to cover the following dependencies:</p>
<ol>
<li>Install Python 2 or 3 (preferably Python 3.6 or later)</li>
<li>Recommended: install and run a virtual environment</li>
<li>Install the Scikit-Learn library and its dependencies (see <a href="http://scikit-learn.org/stable/install.html">http://scikit-learn.org/stable/install.html</a>).</li>
<li>Install Jupyter Notebook and its dependencies</li>
</ol>
<h1 id="endnotes">Endnotes</h1>
<p>[^1]: Underwood, Ted. &quot;Identifying diction that characterizes an author or genre: why Dunning&#39;s may not be the best method,&quot; <em>The Stone and the Shell</em>, November 9, 2011. <a href="https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/">https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/</a></p>
<p>[^2]: Bennett, Jessica, and Amisha Padnani. &quot;Overlooked,&quot; March 8, 2018. <a href="https://www.nytimes.com/interactive/2018/obituaries/overlooked.html">https://www.nytimes.com/interactive/2018/obituaries/overlooked.html</a></p>
<p>[^3]: This dataset is from a version of <em>The New York Times</em> &quot;On This Day&quot; website that hasn&#39;t been updated since January 31, 2011, and it has been replaced by a newer, sleeker blog located at <a href="https://learning.blogs.nytimes.com/on-this-day/">https://learning.blogs.nytimes.com/on-this-day/</a>. What&#39;s left on the older &quot;On This Day&quot; Website is a static .html file for each day of the year (0101.html, 0102.html, etc.), including a static page for February 29th (0229.html). Content appears to have been overwritten whenever it was last updated, so there are no archives of content by year. Presumably, the &quot;On This Day&quot; entries for January 1 - January 31 were last updated on their corresponding days in 2011. Meanwhile, February 1 - December 31 were probably last updated on their corresponding days in 2010. The page representing February 29 was probably last updated on February 29, 2008.</p>
<p>[^4]: Spärck Jones, Karen. &quot;A Statistical Interpretation of Term Specificity and Its Application in Retrieval.&quot; <em>Journal of Documentation</em> vol. 28, no. 1 (1972): 16.</p>
<p>[^5]: Bowles, Nellie. &quot;Overlooked No More: Karen Spärck Jones, Who Established the Basis for Search Engines&quot; <em>The New York Times</em>, January 2, 2019. <a href="https://www.nytimes.com/2019/01/02/obituaries/karen-sparck-jones-overlooked.html">https://www.nytimes.com/2019/01/02/obituaries/karen-sparck-jones-overlooked.html</a></p>
<p>[^6]: &quot;Nellie Bly, Journalist, Dies of Pneumonia&quot; <em>The New York Times</em>, January 28, 1922, 11. <a href="https://www.nytimes.com">https://www.nytimes.com</a></p>
<p>[^7]: Documentation for TfidfVectorizer. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>
<p>[^8]: Schmidt, Ben. &quot;Do Digital Humanists Need to Understand Algorithms?&quot; <em>Debates in the Digital Humanities 2016</em>. Online edition. (Minneapois: University of Minnesota Press): n.p. <a href="http://dhdebates.gc.cuny.edu/debates/text/99">http://dhdebates.gc.cuny.edu/debates/text/99</a></p>
<p>[^9]: van Rossum,  Guido, Barry Warsaw, and Nick Coghlan. &quot;PEP 8 -- Style Guide for Python Code.&quot; July 5, 2001. Updated July 2013. <a href="https://www.python.org/dev/peps/pep-0008/">https://www.python.org/dev/peps/pep-0008/</a></p>
<p>[^10]: &quot;Ida M. Tarbell, 86, Dies in Bridgeport&quot; <em>The New York Times</em>, January 7, 1944, 17. <a href="https://www.nytimes.com">https://www.nytimes.com</a>; &quot;Nellie Bly, Journalist, Dies of Pneumonia&quot; <em>The New York Times</em>, January 28, 1922, 11. <a href="https://www.nytimes.com">https://www.nytimes.com</a>; &quot;W. E. B. DuBois Dies in Ghana; Negro Leader and Author, 95&quot; <em>The New York Times</em>, August 28, 1963, 27. <a href="https://www.nytimes.com">https://www.nytimes.com</a>; Whitman, Alden. &quot;Upton Sinclair, Author, Dead; Crusader for Social Justice, 90&quot; <em>The New York Times</em>, November 26, 1968, 1, 34. <a href="https://www.nytimes.com">https://www.nytimes.com</a>; &quot;Willa Cather Dies; Noted Novelist, 70&quot; <em>The New York Times</em>, April 25, 1947, 21. <a href="https://www.nytimes.com">https://www.nytimes.com</a></p>
<p>[^11]: Stray, Jonathan, and Julian Burgess. &quot;A Full-text Visualization of the Iraq War Logs,&quot; December 10, 2010 (Update April 2012). <a href="http://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs">http://jonathanstray.com/a-full-text-visualization-of-the-iraq-war-logs</a></p>
<p>[^12]: Manning, C.D., P. Raghavan, and H. Schütze, <em>Introduction to Information Retrieval</em>. (Cambridge: Cambridge University Press, 2008): 118-120.</p>
<p>[^13]: Beckman, Milo. &quot;These Are The Phrases Each GOP Candidate Repeats Most,&quot; <em>FiveThirtyEight</em>, March 10, 2016. <a href="https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/">https://fivethirtyeight.com/features/these-are-the-phrases-each-gop-candidate-repeats-most/</a></p>
<p>[^14]: Bondi, Marina, and Mike Scott, eds. <em>Keyness in Texts</em>. (Philadelphia: John Benjamins, 2010).</p>
<p>[^15]: <strong>Tf-idf</strong> is not typically a recommended pre-processing step when generating topic models. See <a href="https://datascience.stackexchange.com/questions/21950/why-we-should-not-feed-lda-with-tfidf">https://datascience.stackexchange.com/questions/21950/why-we-should-not-feed-lda-with-tfidf</a></p>
<p>[^16]: Schmidt, Ben. &quot;Words Alone: Dismantling Topic Models in the Humanities,&quot; <em>Journal of Digital Humanities</em>. Vol. 2, No. 1 (2012): n.p. <a href="http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/">http://journalofdigitalhumanities.org/2-1/words-alone-by-benjamin-m-schmidt/</a></p>
<p>[^17]: Mihalcea, Rada, and Paul Tarau. &quot;Textrank: Bringing order into text.&quot; In <em>Proceedings of the 2004 conference on empirical methods in natural language processing</em>. 2004.</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="analyzing-documents-with-tfidf/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Analyzing Documents with TF-IDF\",\"collection\":\"lessons\",\"layout\":\"lesson\",\"slug\":\"analyzing-documents-with-tfidf\",\"date\":\"2019-05-13T00:00:00.000Z\",\"authors\":[\"Matthew J. Lavin\"],\"reviewers\":[\"Quinn Dombrowski\",\"Catherine Nygren\"],\"editors\":[\"Zoe LeBlanc\"],\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F206\",\"difficulty\":2,\"activity\":\"analyzing\",\"topics\":[\"distant-reading\"],\"abstract\":\"This lesson focuses on a foundational natural language processing and information retrieval method called Term Frequency - Inverse Document Frequency (tf-idf). This lesson explores the foundations of tf-idf, and will also introduce you to some of the questions and concepts of computationally oriented text analysis.\",\"mathjax\":true,\"avatar_alt\":\"An old mechanical typewriter\",\"doi\":\"10.46430\u002Fphen0082\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"overview\\\"\u003EOverview\u003C\u002Fh1\u003E\\n\u003Cp\u003EThis lesson focuses on a core natural language processing and information retrieval method called Term Frequency - Inverse Document Frequency (\u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E). You may have heard about \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E in the context of topic modeling, machine learning, or or other approaches to text analysis. \u003Cstrong\u003ETf-idf\u003C\u002Fstrong\u003E comes up a lot in published work because it&#39;s both a \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FText_corpus\\\"\u003Ecorpus\u003C\u002Fa\u003E exploration method and a pre-processing step for many other text-mining measures and models.\u003C\u002Fp\u003E\\n\u003Cp\u003ELooking closely at tf-idf will leave you with an immediately applicable text analysis method. This lesson will also introduce you to some of the questions and concepts of computationally oriented text analysis. Namely, this lesson addresses how you can isolate a document’s most important words from the kinds of words that tend to be highly frequent across a set of documents in that language. In addition to tf-idf, there are a number of computational methods for determining which words or phrases characterize a set of documents, and I highly recommend Ted Underwood&#39;s 2011 blog post as a supplement.[^1]\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"preparation\\\"\u003EPreparation\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"suggested-prior-skills\\\"\u003ESuggested Prior Skills\u003C\u002Fh2\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EPrior familiarity with Python or a similar programming language. Code for this lesson is written in Python 3.6, but you can run \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E in several different versions of Python, using one of several packages, or in various other programming languages. The precise level of code literacy or familiarity recommended is hard to estimate, but you will want to be comfortable with basic types and operations. To get the most out of this lesson, it is recommended that you work your way through something like \u003Ca href=\\\"https:\u002F\u002Fwww.codecademy.com\u002Flearn\u002Flearn-python\\\"\u003ECodeacademy&#39;s &quot;Introduction to Python&quot; course\u003C\u002Fa\u003E, or that you complete some of the \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fintroduction-and-installation\\\"\u003Eintroductory Python lessons on the \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E\u003C\u002Fa\u003E.\u003C\u002Fli\u003E\\n\u003Cli\u003EIn lieu of the above recommendation, you should \u003Ca href=\\\"https:\u002F\u002Fwww.learnpython.org\u002F\\\"\u003Ereview Python&#39;s basic types\u003C\u002Fa\u003E (string, integer, float, list, tuple, dictionary), working with variables, writing loops in Python, and working with object classes\u002Finstances.\u003C\u002Fli\u003E\\n\u003Cli\u003EExperience with Excel or an equivalent spreadsheet application if you wish to examine the linked spreadsheet files. You can also use the pandas library in python to view the CSVs.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"before-you-begin\\\"\u003EBefore You Begin\u003C\u002Fh2\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EInstall the Python 3 version of Anaconda. Installing Anaconda is covered in \u003Ca href=\\\"\u002Fen\u002Flessons\u002Ftext-mining-with-extracted-features\\\"\u003EText Mining in Python through the HTRC Feature Reader\u003C\u002Fa\u003E. This will install Python 3.6 (or higher), the \u003Ca href=\\\"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Finstall.html\\\"\u003EScikit-Learn library\u003C\u002Fa\u003E (which we will use for \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E), and the dependencies needed to run a \u003Ca href=\\\"https:\u002F\u002Fjupyter.org\u002F\\\"\u003EJupyter Notebook\u003C\u002Fa\u003E.\u003C\u002Fli\u003E\\n\u003Cli\u003EIt is possible to install all these dependencies without Anaconda (or with a lightweight alternative like \u003Ca href=\\\"https:\u002F\u002Fdocs.conda.io\u002Fen\u002Flatest\u002Fminiconda.html\\\"\u003EMiniconda\u003C\u002Fa\u003E). For more information, see the section below titled \u003Ca href=\\\"#alternatives-to-anaconda\\\"\u003E&quot;Alternatives to Anaconda&quot;\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"lesson-dataset\\\"\u003ELesson Dataset\u003C\u002Fh2\u003E\\n\u003Cp\u003E\u003Cstrong\u003ETf-idf\u003C\u002Fstrong\u003E, like many computational operations, is best understood by example. To this end, I&#39;ve prepared a dataset of 366 \u003Cem\u003ENew York Times\u003C\u002Fem\u003E historic \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FObituary\\\"\u003Eobituaries\u003C\u002Fa\u003E scraped from \u003Ca href=\\\"https:\u002F\u002Farchive.nytimes.com\u002Fwww.nytimes.com\u002Flearning\u002Fgeneral\u002Fonthisday\u002F\\\"\u003Ehttps:\u002F\u002Farchive.nytimes.com\u002Fwww.nytimes.com\u002Flearning\u002Fgeneral\u002Fonthisday\u002F\u003C\u002Fa\u003E. On each day of the year, \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E featured an obituary of someone born on that day.\u003C\u002Fp\u003E\\n\u003Cp\u003ELesson files, including, this dataset, can be downloaded from \u003Ca href=\\\"\u002Fassets\u002Ftf-idf\u002Flesson-files.zip\\\"\u003Elesson-files.zip\u003C\u002Fa\u003E. The dataset is small enough that you should be able to open and read some if not all of the files. The original data is also available in the &#39;obituaries&#39; folder, containing the &#39;.html&#39; files downloaded from the 2011 &quot;On This Day&quot; website and a folder of &#39;.txt&#39; files that represent the body of each obituary. These text files were generated using a \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F3\u002Flibrary\u002Fintro.html\\\"\u003EPython library\u003C\u002Fa\u003E called \u003Ca href=\\\"https:\u002F\u002Fwww.crummy.com\u002Fsoftware\u002FBeautifulSoup\u002F\\\"\u003EBeautifulSoup\u003C\u002Fa\u003E, which is covered in another \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E lesson (see \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fintro-to-beautiful-soup\\\"\u003EIntro to BeautifulSoup\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\\n\u003Cp\u003EThis obituary corpus is an historical object in its own right. It represents, on some level, how the questions of inclusion and representation might affect both the decision to publish an obituary, and the decision to highlight a particular obituary many years later. The significance of such decisions has been further highlighted in recent months by \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E itself. In March 2018, the newspaper began publishing obituaries for &quot;overlooked women&quot;.[^2] In the words of Amisha Padnani and Jessica Bennett, &quot;who gets remembered — and how — inherently involves judgment. To look back at the obituary archives can, therefore, be a stark lesson in how society valued various achievements and achievers.&quot; Viewed through this lens, the dataset provided here stands not as a representative sample of historic obituaries but, rather, a snapshot of who \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E in 2010-2011 considered worth highlighting. You&#39;ll notice that many of the historic figures are well known, which suggests a self-conscious effort to look back at the history of \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E and select obituaries based on some criteria.[^3]\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"tf-idf-definition-and-background\\\"\u003ETf-idf Definition and Background\u003C\u002Fh2\u003E\\n\u003Cp\u003EOften inaccurately attributed to others, the procedure called Term Frequency - Inverse Document Frequency was introduced in a 1972 paper by Karen Spärck Jones under the name &quot;term specificity.&quot;[^4] Fittingly, Spärck Jones was the subject of an &quot;Overlooked No More&quot; obituary in January 2019.[^5]\u003C\u002Fp\u003E\\n\u003Cp\u003EWith \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E, instead of representing a term in a document by its raw frequency (number of occurrences) or its relative frequency (term count divided by document length), each term is weighted by dividing the term frequency by the number of documents in the corpus containing the word. The overall effect of this weighting scheme is to avoid a common problem when conducting text analysis: the most frequently used words in a document are often the most frequently used words in all of the documents. In contrast, terms with the highest \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores are the terms in a document that are \u003Cem\u003Edistinctively\u003C\u002Fem\u003E frequent in a document, when that document is compared other documents.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf this explanation doesn&#39;t quite resonate, a brief analogy might help. Imagine that you are on vacation for a weekend in a new city, called Idf City. You&#39;re trying to choose a restaurant for dinner, and you&#39;d like to balance two competing goals: first, you want to have a very good meal, and second, you want to choose a style of cuisine that&#39;s distinctively good in Idf City. That is, you don&#39;t want to have something you can get just anywhere. You can look up online reviews of restaurants all day, and that&#39;s just fine for your first goal, but what you need in order to satisfy the second goal is some way to tell the difference between good and distinctively good (or perhaps even uniquely good).\u003C\u002Fp\u003E\\n\u003Cp\u003EIt&#39;s relatively easy, I think, to see that restaurant food could be:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003Eboth good and distinctive,\u003C\u002Fli\u003E\\n\u003Cli\u003Egood but not distinctive,\u003C\u002Fli\u003E\\n\u003Cli\u003Edistinctive but not good, or\u003C\u002Fli\u003E\\n\u003Cli\u003Eneither good nor distinctive.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003ETerm frequencies could have the same structure. A term might be:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EFrequently used in a language like English, and especially frequent or infrequent in one document\u003C\u002Fli\u003E\\n\u003Cli\u003EFrequently used in a language like English, but used to a typical degree in one document\u003C\u002Fli\u003E\\n\u003Cli\u003EInfrequently used in a language like English, but distinctly frequent or infrequent in one document\u003C\u002Fli\u003E\\n\u003Cli\u003EInfrequently used in a language like English, and used at to a typical degree in one document\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003ETo understand how words can be frequent but not distinctive, or distinctive but not frequent, let&#39;s look at a text-based example. The following is a list of the top ten most frequent terms (and term counts) from one of the obituaries in our \u003Cem\u003ENew York Times\u003C\u002Fem\u003E corpus.\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003ERank\u003C\u002Fth\u003E\\n\u003Cth\u003ETerm\u003C\u002Fth\u003E\\n\u003Cth\u003ECount\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ethe\u003C\u002Ftd\u003E\\n\u003Ctd\u003E21\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eof\u003C\u002Ftd\u003E\\n\u003Ctd\u003E16\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eher\u003C\u002Ftd\u003E\\n\u003Ctd\u003E15\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ein\u003C\u002Ftd\u003E\\n\u003Ctd\u003E14\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eand\u003C\u002Ftd\u003E\\n\u003Ctd\u003E13\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eshe\u003C\u002Ftd\u003E\\n\u003Ctd\u003E10\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eat\u003C\u002Ftd\u003E\\n\u003Ctd\u003E8\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ecochrane\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E9\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ewas\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eto\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EAfter looking at this list, imagine trying to discern information about the obituary that this table represents. We might infer from the presence of \u003Cem\u003Eher\u003C\u002Fem\u003E and \u003Cem\u003Ecochrane\u003C\u002Fem\u003E in the list that a woman named Cochrane is being discussed but, at the same time, this could easily be about a person from Cochrane, Wisconsin or someone associated with the \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCochrane_(organisation)\\\"\u003ECochrane Collaboration\u003C\u002Fa\u003E, a non-profit, non-governmental organization. The problem with this list is that most of top terms would be top terms in any obituary and, indeed, any sufficiently large chunk of writing in most languages. This is because most languages are heavily dependent on function words like \u003Cem\u003Ethe,\u003C\u002Fem\u003E \u003Cem\u003Eas,\u003C\u002Fem\u003E \u003Cem\u003Eof,\u003C\u002Fem\u003E \u003Cem\u003Eto,\u003C\u002Fem\u003E and \u003Cem\u003Efrom\u003C\u002Fem\u003E that serve primarily grammatical or structural purposes, and appear regardless of the text&#39;s subject matter. A list of an obituary&#39;s most frequent terms tell us little about the obituary or the person being memorialized.  Now let&#39;s use \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E term weighting to compare the same obituary from the first example to the rest of our corpus of \u003Cem\u003ENew York Times\u003C\u002Fem\u003E obituaries. The top ten term scores look like this:\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003ERank\u003C\u002Fth\u003E\\n\u003Cth\u003ETerm\u003C\u002Fth\u003E\\n\u003Cth\u003ECount\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ecochrane\u003C\u002Ftd\u003E\\n\u003Ctd\u003E24.85\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eher\u003C\u002Ftd\u003E\\n\u003Ctd\u003E22.74\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eshe\u003C\u002Ftd\u003E\\n\u003Ctd\u003E16.22\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eseaman\u003C\u002Ftd\u003E\\n\u003Ctd\u003E14.88\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebly\u003C\u002Ftd\u003E\\n\u003Ctd\u003E12.42\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd\u003Enellie\u003C\u002Ftd\u003E\\n\u003Ctd\u003E9.92\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd\u003Emark\u003C\u002Ftd\u003E\\n\u003Ctd\u003E8.64\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eironclad\u003C\u002Ftd\u003E\\n\u003Ctd\u003E6.21\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E9\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eplume\u003C\u002Ftd\u003E\\n\u003Ctd\u003E6.21\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd\u003Evexations\u003C\u002Ftd\u003E\\n\u003Ctd\u003E6.21\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EIn this version of the list, \u003Cem\u003Eshe\u003C\u002Fem\u003E and \u003Cem\u003Eher\u003C\u002Fem\u003E have both moved up. \u003Cem\u003Ecochrane\u003C\u002Fem\u003E remains, but now we have at least two new name-like words: \u003Cem\u003Enellie\u003C\u002Fem\u003E and \u003Cem\u003Ebly.\u003C\u002Fem\u003E \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FNellie_Bly\\\"\u003ENellie Bly\u003C\u002Fa\u003E was a turn-of-the-century journalist best known today for her investigative journalism, perhaps most remarkably when she had herself committed to the New York City Lunatic Asylum for ten days in order to write an expose on the mistreatment of mental health patients. She was born Elizabeth Cochrane Seaman, and Bly was her pen name or \u003Cem\u003Enom-de-plume\u003C\u002Fem\u003E. With only a few details about Bly, we can account for seven of the top ten \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E terms: \u003Cem\u003Ecochrane,\u003C\u002Fem\u003E \u003Cem\u003Eher,\u003C\u002Fem\u003E \u003Cem\u003Eshe,\u003C\u002Fem\u003E \u003Cem\u003Eseaman,\u003C\u002Fem\u003E \u003Cem\u003Ebly,\u003C\u002Fem\u003E \u003Cem\u003Enellie,\u003C\u002Fem\u003E and \u003Cem\u003Eplume.\u003C\u002Fem\u003E To understand \u003Cem\u003Emark\u003C\u002Fem\u003E, \u003Cem\u003Eironclad\u003C\u002Fem\u003E, and \u003Cem\u003Evexations\u003C\u002Fem\u003E, we can return to the original obituary and discover that Bly died at St. Mark&#39;s Hospital. \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FRobert_Seaman\\\"\u003EHer husband\u003C\u002Fa\u003E was president of the Ironclad Manufacturing Company. Finally, &quot;a series of forgeries by her employees, disputes of various sorts, bankruptcy and a mass of vexations and costly litigations swallowed up Nellie Bly&#39;s fortune.&quot;[^6] Many of the terms on this list are mentioned as few as one, two, or three times; they are not frequent by any measure. Their presence in this one document, however, are all distinctive compared with the rest of the corpus.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"procedure\\\"\u003EProcedure\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"how-the-algorithm-works\\\"\u003EHow the Algorithm Works\u003C\u002Fh2\u003E\\n\u003Cp\u003E\u003Cstrong\u003ETf-idf\u003C\u002Fstrong\u003E can be implemented in many flavors, some more complex than others. Before I begin discussing these complexities, however, I would like to trace the algorithmic operations of one particular version. To this end, we will go back to the Nellie Bly obituary and convert the top ten term counts into \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores using the same steps that were used to create the above \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E example. These steps parallel \u003Ca href=\\\"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.feature_extraction.text.TfidfVectorizer.html\\\"\u003EScikit-Learn&#39;s \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E\u003C\u002Fa\u003E implementation. Addition, multiplication, and division are the primary mathematical operations necessary to follow along. At one point, we must calculate the \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FNatural_logarithm\\\"\u003Enatural logarithm\u003C\u002Fa\u003E of a variable, but this can be done with most online calculators and calculator mobile apps. Below is a table with the raw term counts for the first thirty words, in alphabetical order, from Bly&#39;s obituary, but this version has a second column that represents the number of documents in which each term can be found. Document frequency (\u003Cstrong\u003Edf\u003C\u002Fstrong\u003E) is a count of how many documents from the corpus each word appears in. (Document frequency for a particular word can be represented as \u003Cstrong\u003Edf\u003Csub\u003Ei\u003C\u002Fsub\u003E\u003C\u002Fstrong\u003E.)\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003EIndex\u003C\u002Fth\u003E\\n\u003Cth\u003ETerm\u003C\u002Fth\u003E\\n\u003Cth\u003ECount\u003C\u002Fth\u003E\\n\u003Cth\u003EDf\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eafternoon\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E66\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eagainst\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E189\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eage\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E224\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eago\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E161\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eair\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E80\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eall\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E310\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eamerican\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E277\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ean\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E352\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E9\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eand\u003C\u002Ftd\u003E\\n\u003Ctd\u003E13\u003C\u002Ftd\u003E\\n\u003Ctd\u003E364\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd\u003Earound\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E149\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E11\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eas\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E357\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E12\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eascension\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E6\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E13\u003C\u002Ftd\u003E\\n\u003Ctd\u003Easylum\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E14\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eat\u003C\u002Ftd\u003E\\n\u003Ctd\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd\u003E362\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E15\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eavenue\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E68\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E16\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eballoon\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E17\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebankruptcy\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E8\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E18\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebarrel\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E7\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E19\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebaxter\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E20\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebe\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E332\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E21\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebeat\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E33\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E22\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebegan\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E241\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E23\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebell\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E24\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E24\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebly\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E25\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebody\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E112\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E26\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eborn\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E342\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E27\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebut\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E343\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E28\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eby\u003C\u002Ftd\u003E\\n\u003Ctd\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd\u003E349\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E29\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ecareer\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E223\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E30\u003C\u002Ftd\u003E\\n\u003Ctd\u003Echaracter\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E89\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003ETo calculate inverse document frequency for each term, the most direct formula would be \u003Cstrong\u003EN\u002Fdf\u003Csub\u003Ei\u003C\u002Fsub\u003E\u003C\u002Fstrong\u003E, where \u003Cstrong\u003EN\u003C\u002Fstrong\u003E represents the total number of documents in the corpus. However, many implementations normalize the results with additional operations. In TF-IDF, normalization is generally used in two ways: first, to prevent bias in term frequency from terms in shorter or longer documents; second, to calculate each term&#39;s idf value (inverse document frequency). For example, Scikit-Learn&#39;s implementation represents \u003Cstrong\u003EN\u003C\u002Fstrong\u003E as \u003Cstrong\u003EN+1\u003C\u002Fstrong\u003E, calculates the natural logarithm of \u003Cstrong\u003E(N+1)\u002Fdf\u003Csub\u003Ei\u003C\u002Fsub\u003E\u003C\u002Fstrong\u003E, and then adds 1 to the final result. This lesson will return to the topic of normalization in the section below titled \u003Ca href=\\\"#scikit-learn-settings\\\"\u003E&quot;Scikit-Learn Settings&quot;\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo express Scikit-Learn&#39;s \u003Cstrong\u003Eidf\u003C\u002Fstrong\u003E transformation[^7], we can state the following equation:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$ idf_i = ln[, ({N}+1) \u002F, {df_i}] + 1 $$\u003C\u002Fp\u003E\\n\u003Cp\u003EOnce \u003Cstrong\u003Eidf\u003Csub\u003Ei\u003C\u002Fsub\u003E\u003C\u002Fstrong\u003E is calculated, \u003Cstrong\u003Etf-idf\u003Csub\u003Ei\u003C\u002Fsub\u003E\u003C\u002Fstrong\u003E is \u003Cstrong\u003Etf\u003Csub\u003Ei\u003C\u002Fsub\u003E\u003C\u002Fstrong\u003E multiplied by \u003Cstrong\u003Eidf\u003Csub\u003Ei\u003C\u002Fsub\u003E\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E$$ tf{\\\\text -}idf_i = tf_i , \\\\times , idf_i $$\u003C\u002Fp\u003E\\n\u003Cp\u003EMathematical equations like these can be a bit bewildering if you&#39;re not used to them. Once you&#39;ve had some experience with them, they can provide a more lucid description of an algorithm&#39;s operations than any well written paragraph. (For more on this subject, Ben Schmidt&#39;s &quot;Do Digital Humanists Need to Understand Algorithms?&quot; is a good place to start.[^8]) To make the \u003Cstrong\u003Eidf\u003C\u002Fstrong\u003E and \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E equations more concrete, I&#39;ve added two new columns to the terms frequency table from before. The first new column represents the derived \u003Cstrong\u003Eidf\u003C\u002Fstrong\u003E score, and the second new column multiplies the Count and Idf columns to derive the final \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E score. Notice that that \u003Cstrong\u003Eidf\u003C\u002Fstrong\u003E score is higher if the term appears in fewer documents, but that the range of visible \u003Cstrong\u003Eidf\u003C\u002Fstrong\u003E scores is between 1 and 6. Different \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FNormalization_(statistics)\\\"\u003Enormalization\u003C\u002Fa\u003E schemes would produce different scales.\u003C\u002Fp\u003E\\n\u003Cp\u003ENote also that the \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E column, according to this version of the algorithm, cannot be lower than the count. This effect is also the result of our normalization method; adding 1 to the final \u003Cstrong\u003Eidf\u003C\u002Fstrong\u003E value ensures that we will never multiply our Count columns by a number smaller than one, which preserves the original distribution of the data.\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003EIndex\u003C\u002Fth\u003E\\n\u003Cth\u003ETerm\u003C\u002Fth\u003E\\n\u003Cth\u003ECount\u003C\u002Fth\u003E\\n\u003Cth\u003EDf\u003C\u002Fth\u003E\\n\u003Cth\u003EIdf\u003C\u002Fth\u003E\\n\u003Cth\u003ETf-idf\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eafternoon\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E66\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.70066923\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.70066923\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eagainst\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E189\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.65833778\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.65833778\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eage\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E224\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.48926145\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.48926145\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eago\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E161\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.81776551\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.81776551\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eair\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E80\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.51091269\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.51091269\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eall\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E310\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.16556894\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.16556894\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eamerican\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E277\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.27774073\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.27774073\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ean\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E352\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.03889379\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.03889379\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E9\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eand\u003C\u002Ftd\u003E\\n\u003Ctd\u003E13\u003C\u002Ftd\u003E\\n\u003Ctd\u003E364\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.00546449\u003C\u002Ftd\u003E\\n\u003Ctd\u003E13.07103843\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd\u003Earound\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E149\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.89472655\u003C\u002Ftd\u003E\\n\u003Ctd\u003E3.78945311\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E11\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eas\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E357\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.02482886\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.04965772\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E12\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eascension\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4.95945170\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4.95945170\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E13\u003C\u002Ftd\u003E\\n\u003Ctd\u003Easylum\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E5.80674956\u003C\u002Ftd\u003E\\n\u003Ctd\u003E5.80674956\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E14\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eat\u003C\u002Ftd\u003E\\n\u003Ctd\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd\u003E362\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.01095901\u003C\u002Ftd\u003E\\n\u003Ctd\u003E8.08767211\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E15\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eavenue\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E68\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.67125534\u003C\u002Ftd\u003E\\n\u003Ctd\u003E5.34251069\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E16\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eballoon\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E5.80674956\u003C\u002Ftd\u003E\\n\u003Ctd\u003E5.80674956\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E17\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebankruptcy\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4.70813727\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4.70813727\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E18\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebarrel\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4.82592031\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4.82592031\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E19\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebaxter\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd\u003E5.29592394\u003C\u002Ftd\u003E\\n\u003Ctd\u003E5.29592394\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E20\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebe\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E332\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.09721936\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.09721936\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E21\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebeat\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E33\u003C\u002Ftd\u003E\\n\u003Ctd\u003E3.37900132\u003C\u002Ftd\u003E\\n\u003Ctd\u003E3.37900132\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E22\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebegan\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E241\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.41642412\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.41642412\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E23\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebell\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E24\u003C\u002Ftd\u003E\\n\u003Ctd\u003E3.68648602\u003C\u002Ftd\u003E\\n\u003Ctd\u003E3.68648602\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E24\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebly\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E6.21221467\u003C\u002Ftd\u003E\\n\u003Ctd\u003E12.42442933\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E25\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebody\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E112\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.17797403\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.17797403\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E26\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eborn\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E342\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.06763140\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.06763140\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E27\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ebut\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E343\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.06472019\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.06472019\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E28\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eby\u003C\u002Ftd\u003E\\n\u003Ctd\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd\u003E349\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.04742869\u003C\u002Ftd\u003E\\n\u003Ctd\u003E3.14228608\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E29\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ecareer\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E223\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.49371580\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.49371580\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E30\u003C\u002Ftd\u003E\\n\u003Ctd\u003Echaracter\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E89\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.40555218\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2.40555218\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EThese tables collectively represent one particular version of the \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E transformation. Of course, \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E is generally calculated for all terms in all of the documents in your corpus so that you can see which terms in each document have the highest \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores. To get a better sense of the what your output might look like after executing such an operation, download and open the full Excel file for Bly&#39;s obituary by downloading \u003Ca href=\\\"\u002Fassets\u002Ftf-idf\u002Flesson-files.zip\\\"\u003Ethe lesson files\u003C\u002Fa\u003E, extracting the &#39;.zip&#39; archive, and opening &#39;bly_tfidf_all.xlsx&#39;.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"how-to-run-it-in-python-3\\\"\u003EHow to Run it in Python 3\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn this section of the lesson, I will walk through the steps I followed to calculate \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores for all terms in all documents in the lesson&#39;s obituary corpus. If you would like to follow along, you can download the lesson files, extract the &#39;.zip&#39; archive, and run the Jupyter Notebook inside of the &#39;lesson&#39; folder. You can also create a new Jupyter Notebook in that location copy\u002Fpaste blocks of code from this tutorial as you go. If you are using Anaconda, visit the \u003Ca href=\\\"https:\u002F\u002Fjupyter-notebook-beginner-guide.readthedocs.io\u002Fen\u002Flatest\u002Fexecute.html\\\"\u003EJupyter Notebook Documentation Page\u003C\u002Fa\u003E for more information on changing the Jupyter Notebook startup location. As with any programming language, there&#39;s more than one way to do each of the steps I discuss below.\u003C\u002Fp\u003E\\n\u003Cp\u003EMy first block of code is designed to retrieve all the filenames for &#39;.txt&#39; files in the &#39;txt&#39; folder. The following lines of code import the \u003Ccode\u003EPath\u003C\u002Fcode\u003E class from the \u003Ccode\u003Epathlib\u003C\u002Fcode\u003E library and use the \u003Ccode\u003EPath().rglob()\u003C\u002Fcode\u003E method to generate a list of all the files in the &#39;txt&#39; folder that end with &#39;.txt&#39;. \u003Ccode\u003Epathlib\u003C\u002Fcode\u003E will also join the \u003Ccode\u003Efile.parent\u003C\u002Fcode\u003E folder location with each file name to provide full file paths for each file (on MacOS or Windows).\u003C\u002Fp\u003E\\n\u003Cp\u003EUsing this method, I append each text file name to the list called \u003Ccode\u003Eall_txt_files\u003C\u002Fcode\u003E. Finally, I return the length of \u003Ccode\u003Eall_txt_files\u003C\u002Fcode\u003E to verify that I&#39;ve found 366 file names. This loop-and-append approach is very common in Python.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efrom pathlib import Path\\n\\nall_txt_files =[]\\nfor file in Path(&quot;txt&quot;).rglob(&quot;*.txt&quot;):\\n     all_txt_files.append(file.parent \u002F file.name)\\n# counts the length of the list\\nn_files = len(all_txt_files)\\nprint(n_files)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EA quick note on variable names. The two most common variable naming patterns prioritize convenience and semantic meaning respectively. For convenience, one might name a variable \u003Cstrong\u003Ex\u003C\u002Fstrong\u003E so it&#39;s easier and faster to type when referencing it. Semantic variables, meanwhile, attempt to describe function or purpose. By naming my list of all text files \u003Ccode\u003Eall_txt_files\u003C\u002Fcode\u003E and the variable representing the number of files \u003Ccode\u003En_files\u003C\u002Fcode\u003E, I&#39;m prioritizing semantic meaning. Meanwhile, I&#39;m using abbreviations like \u003Ccode\u003Etxt\u003C\u002Fcode\u003E for text and \u003Ccode\u003En\u003C\u002Fcode\u003E for number to save on typing, or using \u003Ccode\u003Eall_txt_files\u003C\u002Fcode\u003E instead of \u003Ccode\u003Eall_txt_file_names\u003C\u002Fcode\u003E because brevity is still a goal. Underscore and capitalization norms are specified in PEP-8, Python&#39;s official style guide, with which you should try to be generally familiar.[^9]\u003C\u002Fp\u003E\\n\u003Cp\u003EFor various resons, we want our files to count up by day and month since there&#39;s on file for every day and month of a year. We can use the \u003Ccode\u003Esort()\u003C\u002Fcode\u003E method to put the files in ascending numerical order and print the first file to make sure it&#39;s &#39;txt\u002F0101.txt&#39;.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eall_txt_files.sort()\\nall_txt_files[0]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENext, we can use our list of file names to load each file and convert them to a format that Python can read and understand as text. In this block of code, I do another loop-and-append operation. This time, I loop my list of file names and open each file. I then use Python&#39;s \u003Ccode\u003Eread()\u003C\u002Fcode\u003E method to convert each text file to a string (\u003Ccode\u003Estr\u003C\u002Fcode\u003E), which is how Python knows to think of the data as text. I append each string, one by one, to a new list called \u003Ccode\u003Eall_docs\u003C\u002Fcode\u003E. Crucially, the string objects in this list have the same order as the file names in the \u003Ccode\u003Eall_txt_files\u003C\u002Fcode\u003E list.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eall_docs = []\\nfor txt_file in all_txt_files:\\n    with open(txt_file) as f:\\n        txt_file_as_string = f.read()\\n    all_docs.append(txt_file_as_string)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis is all the setup work we require. Text processing steps like \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLexical_analysis#Tokenization\\\"\u003Etokenization\u003C\u002Fa\u003E and removing punctuation will happen automatically when we use Scikit-Learn&#39;s \u003Ccode\u003ETfidfVectorizer\u003C\u002Fcode\u003E to convert documents from a list of strings to \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores. One could also supply a list of stopwords here (commonly used words that you want to ignore). To tokenize and remove stopwords in languages other than English, you may need to preprocess the text with another Python library or supply a custom tokenizer and stopword list when Scikit-Learn&#39;s \u003Ccode\u003ETfidfVectorizer\u003C\u002Fcode\u003E. The following block of code imports \u003Ccode\u003ETfidfVectorizer\u003C\u002Fcode\u003E from the Scikit-Learn library, which comes pre-installed with Anaconda. \u003Ccode\u003ETfidfVectorizer\u003C\u002Fcode\u003E is a class (written using object-oriented programming), so I instantiate it with specific parameters as a variable named \u003Ccode\u003Evectorizer\u003C\u002Fcode\u003E. (I’ll say more about these settings in the section titled \u003Ca href=\\\"#scikit-learn-settings\\\"\u003E&quot;Scikit-Learn Settings&quot;\u003C\u002Fa\u003E.) I then run the object&#39;s \u003Ccode\u003Efit_transform()\u003C\u002Fcode\u003E method on my list of strings (a variable called \u003Ccode\u003Eall_docs\u003C\u002Fcode\u003E). The stored variable \u003Ccode\u003EX\u003C\u002Fcode\u003E is output of the \u003Ccode\u003Efit_transform()\u003C\u002Fcode\u003E method.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E#import the TfidfVectorizer from Scikit-Learn.\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\nvectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None)\\ntransformed_documents = vectorizer.fit_transform(all_docs)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe \u003Ccode\u003Efit_transform()\u003C\u002Fcode\u003E method above converts the list of strings to something called a \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSparse_matrix\\\"\u003Esparse matrix\u003C\u002Fa\u003E. In this case, the matrix represents \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E values for all texts. Sparse matrices save on memory by leaving out all zero values, but we want access to those, so the next block uses the \u003Ccode\u003Etoarray()\u003C\u002Fcode\u003E method to convert the sparse matrices to a \u003Ca href=\\\"https:\u002F\u002Fdocs.scipy.org\u002Fdoc\u002Fnumpy\u002Freference\u002Fgenerated\u002Fnumpy.array.html\\\"\u003Enumpy array\u003C\u002Fa\u003E. We can print the length of the array to ensure that it&#39;s the same length as our list of documents.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etransformed_documents_as_array = transformed_documents.toarray()\\n# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\\nlen(transformed_documents_as_array)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EA numpy array is list-like but not exactly a list, and I could fill an entire tutorial discussing the differences, but there&#39;s only one aspect of numpy arrays we need to know right now: it converts the data stored in \u003Ccode\u003Etransformed_documents\u003C\u002Fcode\u003E to a format where every \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E score for every term in every document is represented. Sparse matrices, in contrast, exclude zero-value term scores.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe want every term represented so that each document has the same number of values, one for each word in the corpus. Each item in \u003Ccode\u003Etransformed_documents_as_array\u003C\u002Fcode\u003E is an array of its own representing one document from our corpus. As a result of all this, we essentially have a grid where each row is a document, and each column is a term. Imagine one table from a spreadsheet representing each document, like the tables above, but without column or row labels.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo merge the values with their labels, we need two pieces of information: the order of the documents, and the order in which term scores are listed. The order of these documents is easy because it&#39;s the same order as the variable \u003Ccode\u003Eall_docs list\u003C\u002Fcode\u003E. The full term list is stored in our \u003Ccode\u003Evectorizer\u003C\u002Fcode\u003E variable, and it&#39;s in the same order that each item in \u003Ccode\u003Etransformed_documents_as_array\u003C\u002Fcode\u003E stores values. We can use the \u003Ccode\u003Ethe TFIDFVectorizer\u003C\u002Fcode\u003E class&#39;s \u003Ccode\u003Eget_feature_names()\u003C\u002Fcode\u003E method to get that list, and each row of data (one document&#39;s \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores) can be rejoined with the term list. (For more details on pandas dataframes, see the lesson \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fvisualizing-with-bokeh\\\"\u003E&quot;Visualizing Data with Bokeh and Pandas&quot;\u003C\u002Fa\u003E.)\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eimport pandas as pd\\n\\n# make the output folder if it doesn&#39;t already exist\\nPath(&quot;.\u002Ftf_idf_output&quot;).mkdir(parents=True, exist_ok=True)\\n\\n# construct a list of output file paths using the previous list of text files the relative path for tf_idf_output\\noutput_filenames = [str(txt_file).replace(&quot;.txt&quot;, &quot;.csv&quot;).replace(&quot;txt\u002F&quot;, &quot;tf_idf_output\u002F&quot;) for txt_file in all_txt_files]\\n\\n# loop each item in transformed_documents_as_array, using enumerate to keep track of the current position\\nfor counter, doc in enumerate(transformed_documents_as_array):\\n    # construct a dataframe\\n    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\\n    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=[&#39;term&#39;, &#39;score&#39;]).sort_values(by=&#39;score&#39;, ascending=False).reset_index(drop=True)\\n\\n    # output to a csv using the enumerated value for the filename\\n    one_doc_as_df.to_csv(output_filenames[counter])\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe above block of code has three parts:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Cp\u003EAfter importing the pandas library, it checks for a folder called &#39;tf_idf_output&#39; and creates it if it doesn&#39;t exist.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EIt takes the list of &#39;.txt&#39; files from my earlier block of code and use it to construct a counterpart &#39;.csv&#39; file path for each &#39;.txt&#39; file. The output_filenames variable will, for example, convert &#39;txt\u002F0101.txt&#39; (the path of the first &#39;.txt&#39; file) to &#39;tf_idf_output\u002F0101.csv&#39;, and on and on for each file.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EUsing a loop, it merges each vector of \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores with the feature names from vectorizer, converts each merged term\u002Fscore pairs to a pandas dataframe, and saves each dataframe to its corresponding &#39;.csv&#39; file.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch2 id=\\\"interpreting-word-lists-best-practices-and-cautionary-notes\\\"\u003EInterpreting Word Lists: Best Practices and Cautionary Notes\u003C\u002Fh2\u003E\\n\u003Cp\u003EAfter you run the code excerpts above, you will end up with a folder called &#39;tf_idf_output&#39; with 366 &#39;.csv&#39; files in it. Each file corresponds to an obituary in the &#39;txt&#39; folder, and each contains a list of terms with \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores for that document. As we saw with Nellie Bly&#39;s obituary, these term lists can be very suggestive; however, it&#39;s important to understand that over-interpreting your results can actually distort your understanding of an underlying text.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn general, it&#39;s best to begin with the ideas that these term lists will be helpful for generating hypotheses or research questions. \u003Cstrong\u003ETf-idf\u003C\u002Fstrong\u003E results but will not necessarily produce definitive claims. For example, I have assembled a quick list of obituaries for late 19th- and early 20th-century figures who all worked for newspapers and magazines and had some connection to social reform. My list includes Nellie Bly, \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FWilla_Cather\\\"\u003EWilla Cather\u003C\u002Fa\u003E, \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FW._E._B._Du_Bois\\\"\u003EW.E.B. Du Bois\u003C\u002Fa\u003E, \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FUpton_Sinclair\\\"\u003EUpton Sinclair\u003C\u002Fa\u003E, \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FIda_Tarbell\\\"\u003EIda Tarbell\u003C\u002Fa\u003E, but there may be other figures in the corpus who fit the same criteria.[^10]\u003C\u002Fp\u003E\\n\u003Cp\u003EI originally expected to see many shared terms, but I was surprised. Each list is dominated by individualized words (proper names, geographic places, companies, etc.) but I could screen these out using my \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E settings, or just ignore them. Simultaneously, I can look for words overtly indicating each figure&#39;s ties to the profession of authorship. (The section of this tutorial titled \u003Ca href=\\\"#scikit-learn-settings\\\"\u003EScikit-Learn Settings\u003C\u002Fa\u003E says more about how you can treat a named entity or a phrase as a single token.) The following table shows the top 20 \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E terms by rank for each obituary:\u003C\u002Fp\u003E\\n\u003Cp\u003E| Tf-idf Rank | Nellie Bly | Willa Cather | W.E.B. Du Bois | Upton Sinclair | Ida Tarbell |\\n| 1 | cochrane | cather | dubois | sinclair | tarbell |\\n| 2 | her | her | dr | socialist | she |\\n| 3 | she | she | negro | upton | her |\\n| 4 | seaman | nebraska | ghana | \u003Cstrong\u003Ebooks\u003C\u002Fstrong\u003E | lincoln |\\n| 5 | bly | miss | peace | lanny | miss |\\n| 6 | nellie | forrester | \u003Cstrong\u003Eencyclopedia\u003C\u002Fstrong\u003E | social | oil |\\n| 7 | mark | sibert | communist | budd | abraham |\\n| 8 | ironclad | twilights | barrington | jungle | mcclure |\\n| 9 | \u003Cstrong\u003Eplume\u003C\u002Fstrong\u003E | willa | fisk | brass | easton |\\n| 10 | vexations | antonia | atlanta | california | \u003Cstrong\u003Evolumes\u003C\u002Fstrong\u003E |\\n| 11 | phileas | mcclure | folk | \u003Cstrong\u003Ewriter\u003C\u002Fstrong\u003E | minerva |\\n| 12 | 597 | \u003Cstrong\u003Enovels\u003C\u002Fstrong\u003E | booker | vanzetti | standard |\\n| 13 | elizabeth | pioneers | successively | macfadden | business |\\n| 14 | \u003Cstrong\u003Enom\u003C\u002Fstrong\u003E | cloud | souls | sacco | titusville |\\n| 15 | balloon | \u003Cstrong\u003Ebook\u003C\u002Fstrong\u003E | council | \u003Cstrong\u003Ewrote\u003C\u002Fstrong\u003E | \u003Cstrong\u003Earticles\u003C\u002Fstrong\u003E |\\n| 16 | forgeries | calif | party | meat | bridgeport |\\n| 17 | mcalpin | \u003Cstrong\u003Enovel\u003C\u002Fstrong\u003E | disagreed | \u003Cstrong\u003Epamphlets\u003C\u002Fstrong\u003E | expose |\\n| 18 | asylum | southwest | harvard | my | trusts |\\n| 19 | fogg | \u003Cstrong\u003Everse\u003C\u002Fstrong\u003E | \u003Cstrong\u003Earts\u003C\u002Fstrong\u003E | industry | mme\\n| 20 | verne | \u003Cstrong\u003Ewrote\u003C\u002Fstrong\u003E | soviet | \u003Cstrong\u003Enovel\u003C\u002Fstrong\u003E | \u003Cstrong\u003Emagazine\u003C\u002Fstrong\u003E |\u003C\u002Fp\u003E\\n\u003Cp\u003EI&#39;ve used boldface to indicate terms that seem overtly related to authorship or writing. The list includes \u003Cem\u003Earticles\u003C\u002Fem\u003E, \u003Cem\u003Earts\u003C\u002Fem\u003E, \u003Cem\u003Ebook\u003C\u002Fem\u003E, \u003Cem\u003Ebook\u003C\u002Fem\u003E, \u003Cem\u003Ebooks\u003C\u002Fem\u003E, \u003Cem\u003Eencyclopedia\u003C\u002Fem\u003E, \u003Cem\u003Emagazine\u003C\u002Fem\u003E, \u003Cem\u003Enom\u003C\u002Fem\u003E, \u003Cem\u003Enovel\u003C\u002Fem\u003E, \u003Cem\u003Enovels\u003C\u002Fem\u003E, \u003Cem\u003Epamphlets\u003C\u002Fem\u003E, \u003Cem\u003Eplume\u003C\u002Fem\u003E, \u003Cem\u003Everse\u003C\u002Fem\u003E, \u003Cem\u003Evolumes\u003C\u002Fem\u003E, \u003Cem\u003Ewriter\u003C\u002Fem\u003E, and \u003Cem\u003Ewrote\u003C\u002Fem\u003E, but it could be extended to include references to specific magazine or book titles. Setting aside momentarily such complexities, it is striking to me that Cather and Sinclair&#39;s lists have so many words for books and writing, whereas Bly, Du Bois and Tarbell&#39;s do not.\u003C\u002Fp\u003E\\n\u003Cp\u003EI could easily jump to conclusions. Cather&#39;s identity seems to be tied most to her gender, her sense of place, and her fiction and verse. Sinclair more so with his politics and his writings about meat, industry, and specifically the well known, controversial trial and execution of Nicola Sacco and Bartolomeo Vanzetti. Bly is tied to her pen name, her husband, and her writing about asylums. Du Bois is linked to race and his academic career. Tarbell is described by what she wrote about: namely business, the trusts, Standard Oil, and Abraham Lincoln. Going further, I could argue that gender seems more distinctive for women than it is for men; race is only a top term for the one African American in my set.\u003C\u002Fp\u003E\\n\u003Cp\u003EEach of these observations forms the basis for a deeper question, but these details aren&#39;t enough to make generalizations. Foremost, I need to consider whether my \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E settings are producing effects that would disappear under other conditions; robust results should be stable enough to appear with various settings. (Some of these settings are covered in the \u003Ca href=\\\"#scikit-learn-settings\\\"\u003E&quot;Scikit-Learn Settings&quot;\u003C\u002Fa\u003E section.) Next, I should read at least some of the underlying obituaries to make sure I&#39;m not getting false signals from any terms. If I read Du Bois&#39;s obituary, for example, I may discover that mentions of his work &quot;The Encyclopedia of the Negro,&quot; contribute at least partially to the overall score of the word \u003Cem\u003Enegro\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ELikewise, I can discover that Bly&#39;s obituary does include words like \u003Cem\u003Ejournalism\u003C\u002Fem\u003E, \u003Cem\u003Ejournalistic\u003C\u002Fem\u003E, \u003Cem\u003Enewspapers\u003C\u002Fem\u003E, and \u003Cem\u003Ewriting\u003C\u002Fem\u003E, but the obituary is very short, meaning most words mentioned in it occur only once or twice, which means that words with very high \u003Cstrong\u003Eidf\u003C\u002Fstrong\u003E scores are even more likely to top her list. I really want \u003Cstrong\u003Etf\u003C\u002Fstrong\u003E and \u003Cstrong\u003Eidf\u003C\u002Fstrong\u003E to be balanced, so I could rule out words that appear in only a few documents, or I could ignore results for obituaries below a certain word count.\u003C\u002Fp\u003E\\n\u003Cp\u003EFinally, I can design tests to measure directly questions like: were obituaries of African Americans are more likely to mention race? I think the prediction that they did is a good hypothesis, but I should still subject my generalizations to scrutiny before I form conclusions.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"some-ways-tf-idf-can-be-used-in-computational-history\\\"\u003ESome Ways Tf-idf Can Be Used in Computational History\u003C\u002Fh2\u003E\\n\u003Cp\u003EAs I have described, \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E has its origins in information retrieval, and the idea of weighting term frequencies against norms in a larger corpus continues to be used to power various aspects of everyday web applications, especially text-based search engines. However, in a cultural analytics or computational history context, \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E is suited for a particular set of tasks. These uses tend to fall into one of three groups.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"1-as-an-exploratory-tool-or-visualization-technique\\\"\u003E1. As an Exploratory Tool or Visualization Technique\u003C\u002Fh3\u003E\\n\u003Cp\u003EAs I&#39;ve already demonstrated, terms lists with \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores for each document in a corpus can be a strong interpretive aid in themselves, they can help generate hypotheses or research questions. Word lists can also be the building bocks for more sophisticated browsing and visualization strategies. \u003Ca href=\\\"http:\u002F\u002Fjonathanstray.com\u002Fa-full-text-visualization-of-the-iraq-war-logs\\\"\u003E&quot;A full-text visualization of the Iraq War Logs&quot;\u003C\u002Fa\u003E, by Jonathan Stray and Julian Burgess, is a good example of this use case.[^11] Using \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E-transformed features, Stray and Burgess build a network visualization that positions Iraq War logs in relation to their most distinctive keywords. This way of visualizing textual information led Stray to develop \u003Ca href=\\\"https:\u002F\u002Fwww.overviewdocs.com\\\"\u003Ethe Overview Project\u003C\u002Fa\u003E, which provides a dashboard for users to visualize and search thousands of documents at a time. We could use this kind of approach to graph our obituaries corpus and see if there are keyword communities.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"2-textual-similarity-and-feature-sets\\\"\u003E2. Textual Similarity and Feature Sets\u003C\u002Fh3\u003E\\n\u003Cp\u003ESince \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E will often produce lower scores for high frequency function words and increased scores for terms related to the topical signal of a text, it is well suited for tasks involving textual similarity. A search index will often perform \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E on a corpus and return ranked results to user searches by looking for documents with the highest cosine similarity to the user&#39;s search string. The same logic can be used to ask a question like &quot;Which obituary in our corpus is most similar to Nellie Bly&#39;s obituary?&quot;\u003C\u002Fp\u003E\\n\u003Cp\u003ESimilarly, we could use \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E to discover the top terms related to a document or a group of documents. For example, I could gather together a selection of obituaries about journalists (Bly included) and combine them into one document before running \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E. The output for that document would now work as a heuristic for terms that are distinctive in my journalism obituaries in the corpus when compared with other obituaries in the corpus. I could use such a term list for a range of other computational tasks.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"3-as-a-pre-processing-step\\\"\u003E3. As a Pre-processing Step\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe above paragraphs gesture at why \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E pre-processing is so often used with machine learning. \u003Cstrong\u003ETf-idf\u003C\u002Fstrong\u003E-transformed features tend to have more predictive value than raw term frequencies, especially when classifying a supervised machine learning model, in part because it tends to increase the weight of topic words and reduce the weight of high frequency function words. One notable exception to this generalization is authorship attribution, where high frequency function words are highly predictive. As I will show in the \u003Ca href=\\\"#scikit-learn-settings\\\"\u003E&quot;Scikit-Learn Settings&quot;\u003C\u002Fa\u003E section, \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E can also be used to cull machine learning feature lists and, often, building a model with fewer features is desirable.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"potential-variations-of-tf-idf\\\"\u003EPotential Variations of Tf-idf\u003C\u002Fh2\u003E\\n\u003Ch3 id=\\\"scikit-learn-settings\\\"\u003EScikit-Learn Settings\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe Scikit-Learn \u003Ccode\u003ETfidfVectorizer\u003C\u002Fcode\u003E has several internal settings that can be changed to affect the output. In general, these settings all have pros and cons; there&#39;s no singular, correct way to preset them and produce output. Instead, it&#39;s best to understand exactly what each setting does so that you can describe and defend the choices you&#39;ve made. The full list of parameters is described in \u003Ca href=\\\"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.feature_extraction.text.TfidfVectorizer.html\\\"\u003EScikit-Learn&#39;s documentation\u003C\u002Fa\u003E, but here are some of the most important settings:\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"1-stopwords\\\"\u003E1. stopwords\u003C\u002Fh4\u003E\\n\u003Cp\u003EIn my code, I used \u003Ccode\u003Epython stopwords=None\u003C\u002Fcode\u003E but \u003Ccode\u003Epython stopwords=&#39;english&#39;\u003C\u002Fcode\u003E is available. This setting will filter out words using a \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fscikit-learn\u002Fscikit-learn\u002Fblob\u002Fmaster\u002Fsklearn\u002Ffeature_extraction\u002F_stop_words.py\\\"\u003Epreselected list\u003C\u002Fa\u003E of high frequency function words such as &#39;the&#39;, &#39;to&#39;, and &#39;of&#39;. Depending on your settings, many of these terms will have low \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores regardless because they tend to be found in all documents. For a discussion of some publicly available stop word lists (including Scikit-Learn&#39;s), see \u003Ca href=\\\"https:\u002F\u002Faclweb.org\u002Fanthology\u002FW18-2502\\\"\u003E&quot;Stop Word Lists in Free Open-source Software Packages&quot;\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"2-min_df-max_df\\\"\u003E2. min_df, max_df\u003C\u002Fh4\u003E\\n\u003Cp\u003EThese settings control the minimum number of documents a term must be found in to be included and the maximum number of documents a term can be found in in order to be included. Either can be expressed as a decimal between 0 and 1 indicating the percent threshold, or as a whole number that represents a raw count. Setting max_df below .9 will typically remove most or all stopwords.\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"3-max_features\\\"\u003E3. max_features\u003C\u002Fh4\u003E\\n\u003Cp\u003EThis parameter can be used to winnow out terms by frequency before running tf-idf. It can be especially useful in a machine learning context when you do not wish to exceed a maximum recommended number of term features.\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"4-norm-smooth_idf-and-sublinear_tf\\\"\u003E4. norm, smooth_idf, and sublinear_tf\u003C\u002Fh4\u003E\\n\u003Cp\u003EEach of these will affect the range of numerical scores that the \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E algorithm outputs. norm supports l1 and l2 normalization, which you can read about on \u003Ca href=\\\"https:\u002F\u002Fmachinelearningmastery.com\u002Fvector-norms-machine-learning\u002F\\\"\u003Emachinelearningmastery.com\u003C\u002Fa\u003E. Smooth-idf adds one to each document frequency score, &quot;as if an extra document was seen containing every term in the collection exactly once.&quot; Sublinear_tf applies another scaling transformation, replacing tf with log(tf). For more on \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E smoothing and normalization, see Manning, Raghavan, and Schütze.[^12]\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"beyond-term-features\\\"\u003EBeyond Term Features\u003C\u002Fh3\u003E\\n\u003Cp\u003ESince the basic idea of \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E is to weight term counts against the number of documents in which terms appear, the same logic can be used on other text-based features. For example, it is relatively straightforward to combine \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E with \u003Ca href=\\\"https:\u002F\u002Fnlp.stanford.edu\u002FIR-book\u002Fhtml\u002Fhtmledition\u002Fstemming-and-lemmatization-1.html\\\"\u003Estemming or lemmatization\u003C\u002Fa\u003E. Stemming and lemmatization are two common ways to group together different word forms\u002Finflections; for example, the stem of both \u003Cem\u003Ehappy\u003C\u002Fem\u003E and \u003Cem\u003Ehappiness\u003C\u002Fem\u003E is \u003Cem\u003Ehappi\u003C\u002Fem\u003E, and the lemma of both is \u003Cem\u003Ehappy\u003C\u002Fem\u003E. After stemming or lemmatization, stem or lemma counts can be substituted for term counts, and the \u003Cstrong\u003E(s\u002Fl)f-idf\u003C\u002Fstrong\u003E transformation can be applied. Each stem or lemma will have a higher \u003Cstrong\u003Edf\u003C\u002Fstrong\u003E score than each of the words it groups together, so lemmas or stems with many word variants will tend to have lower \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E scores.\u003C\u002Fp\u003E\\n\u003Cp\u003ESimilarly, the \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E transformation can be applied to n-grams. A Fivethirtyeight.com post from March 2016 called \u003Ca href=\\\"https:\u002F\u002Ffivethirtyeight.com\u002Ffeatures\u002Fthese-are-the-phrases-each-gop-candidate-repeats-most\u002F\\\"\u003E&quot;These Are The Phrases Each GOP Candidate Repeats Most&quot;\u003C\u002Fa\u003E uses such an approach to perform the inverse-document frequency calculation on phrases rather than words.[^13]\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"tf-idf-and-common-alternatives\\\"\u003ETf-idf and Common Alternatives\u003C\u002Fh2\u003E\\n\u003Cp\u003E\u003Cstrong\u003ETf-idf\u003C\u002Fstrong\u003E can be compared with several other methods of isolating and\u002For ranking important term features in a document or collection of documents. This section provides a brief mention of four related but distinct measures that target similar but not identical aspects of textual information.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"1-keyness\\\"\u003E1. Keyness\u003C\u002Fh3\u003E\\n\u003Cp\u003EKeyness is a catchall term for a constellation of statistical measures that attempt to indicate the numerical significance of a term to a document or set of documents, in direct comparison with a larger set of documents or corpus. Depending on how we set up our \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E transformation, it may isolate many of a document&#39;s most important features, but \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E is not as precise as the most commonly used measures of keyness. Rather than changing a document&#39;s term frequency scores, keyness testing produces a numerical indicator of how statistically typical or atypical the term&#39;s usage in a text is. With a \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FChi-squared_test\\\"\u003EChi-square test\u003C\u002Fa\u003E, for example, we can evaluate the relationship of a term frequency to an established norm, and derive a \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FP-value\\\"\u003EP-value\u003C\u002Fa\u003E indicating the probability of encountering the observed difference in a random sample. For more information on keyness, see Bondi and Scott.[^14]\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"2-topic-models\\\"\u003E2. Topic Models\u003C\u002Fh3\u003E\\n\u003Cp\u003ETopic modeling and \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E are radically different techniques, but I find that newcomers to digital humanities often want to run topic modeling on a corpus as a first step and, in at least some of those cases, running \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E instead of generating topic models would be preferable.[^15] \u003Cstrong\u003ETf-idf\u003C\u002Fstrong\u003E is especially appropriate if you are looking for a way to get a bird&#39;s eye view of your corpus early in the exploratory phase of your research because the algorithm is transparent and the results are reproducible. As Ben Schmidt suggests, scholars using topic modeling need to know that &quot;topics may not be as coherent as they assume.&quot;[^16] This is one reason \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E is integrated into \u003Ca href=\\\"https:\u002F\u002Fwww.overviewdocs.com\\\"\u003Ethe Overview Project\u003C\u002Fa\u003E. Topic models can also help scholars explore their corpora, and they have several advantages over other techniques, namely that they suggest broad categories or communities of texts, but this a general advantage of unsupervised clustering methods. Topic models are especially appealing because documents are assigned scores for how well they fit each topic, and because topics are represented as lists of co-occurring terms, which provides a strong sense of how terms relate to groupings. However, the probabilistic model behind topic models is sophisticated, and it&#39;s easy to warp your results if you don&#39;t understand what you&#39;re doing. The math behind \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E is lucid enough to depict in a spreadsheet.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"3-automatic-text-summarization\\\"\u003E3. Automatic Text Summarization\u003C\u002Fh3\u003E\\n\u003Cp\u003EText summarization is yet another way to explore a corpus. Rada Mihalcea and Paul Tarau, for example, have published on TextRank, &quot;a graph-based ranking model for text processing&quot; with promising applications for keyword and sentence extraction.[^17] As with topic modeling, TextRank and \u003Cstrong\u003Etf-idf\u003C\u002Fstrong\u003E are altogether dissimilar in their approach to information retrieval, yet the goal of both algorithms has a great deal of overlap. It may be appropriate for your research, especially if your goal is to get a relatively quick a sense of your documents&#39; contents before designing a larger research project.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"references-and-further-reading\\\"\u003EReferences and Further Reading\u003C\u002Fh1\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Cp\u003EBeckman, Milo. &quot;These Are The Phrases Each GOP Candidate Repeats Most,&quot; \u003Cem\u003EFiveThirtyEight\u003C\u002Fem\u003E, March 10, 2016. \u003Ca href=\\\"https:\u002F\u002Ffivethirtyeight.com\u002Ffeatures\u002Fthese-are-the-phrases-each-gop-candidate-repeats-most\u002F\\\"\u003Ehttps:\u002F\u002Ffivethirtyeight.com\u002Ffeatures\u002Fthese-are-the-phrases-each-gop-candidate-repeats-most\u002F\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EBennett, Jessica, and Amisha Padnani. &quot;Overlooked,&quot; March 8, 2018. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\u002Finteractive\u002F2018\u002Fobituaries\u002Foverlooked.html\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u002Finteractive\u002F2018\u002Fobituaries\u002Foverlooked.html\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EBlei, David M., Andrew Y. Ng, and Michael I. Jordan, &quot;Latent Dirichlet Allocation&quot; \u003Cem\u003EJournal of Machine Learning Research\u003C\u002Fem\u003E 3 (January 2003): 993-1022.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EBondi, Marina, and Mike Scott, eds. \u003Cem\u003EKeyness in Texts\u003C\u002Fem\u003E. Philadelphia: John Benjamins, 2010.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EBowles, Nellie. &quot;Overlooked No More: Karen Sparck Jones, Who Established the Basis for Search Engines&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, January 2, 2019. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\u002F2019\u002F01\u002F02\u002Fobituaries\u002Fkaren-sparck-jones-overlooked.html\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u002F2019\u002F01\u002F02\u002Fobituaries\u002Fkaren-sparck-jones-overlooked.html\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EDocumentation for TfidfVectorizer. \u003Ca href=\\\"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.feature_extraction.text.TfidfVectorizer.html\\\"\u003Ehttps:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.feature_extraction.text.TfidfVectorizer.html\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EGrimmer, Justin and King, Gary, Quantitative Discovery from Qualitative Information: A General-Purpose Document Clustering Methodology (2009). APSA 2009 Toronto Meeting Paper. Available at SSRN: \u003Ca href=\\\"https:\u002F\u002Fssrn.com\u002Fabstract=1450070\\\"\u003Ehttps:\u002F\u002Fssrn.com\u002Fabstract=1450070\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003E&quot;Ida M. Tarbell, 86, Dies in Bridgeport&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, January 7, 1944, 17. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EManning, C.D., P. Raghavan, and H. Schütze, \u003Cem\u003EIntroduction to Information Retrieval\u003C\u002Fem\u003E. Cambridge: Cambridge University Press, 2008.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EMihalcea, Rada, and Paul Tarau. &quot;Textrank: Bringing order into text.&quot; In Proceedings of the 2004 conference on empirical methods in natural language processing. 2004.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003E&quot;Nellie Bly, Journalist, Dies of Pneumonia&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, January 28, 1922, 11. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003ESalton, G. and M.J. McGill, \u003Cem\u003EIntroduction to Modern Information Retrieval\u003C\u002Fem\u003E. New York: McGraw-Hill, 1983.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003ESchmidt, Ben. &quot;Do Digital Humanists Need to Understand Algorithms?&quot; \u003Cem\u003EDebates in the Digital Humanities 2016\u003C\u002Fem\u003E. Online edition. Minneapois: University of Minnesota Press. \u003Ca href=\\\"http:\u002F\u002Fdhdebates.gc.cuny.edu\u002Fdebates\u002Ftext\u002F99\\\"\u003Ehttp:\u002F\u002Fdhdebates.gc.cuny.edu\u002Fdebates\u002Ftext\u002F99\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003E--. &quot;Words Alone: Dismantling Topic Models in the Humanities,&quot; \u003Cem\u003EJournal of Digital Humanities\u003C\u002Fem\u003E. Vol. 2, No. 1 (2012): n.p. \u003Ca href=\\\"http:\u002F\u002Fjournalofdigitalhumanities.org\u002F2-1\u002Fwords-alone-by-benjamin-m-schmidt\u002F\\\"\u003Ehttp:\u002F\u002Fjournalofdigitalhumanities.org\u002F2-1\u002Fwords-alone-by-benjamin-m-schmidt\u002F\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003ESpärck Jones, Karen. &quot;A Statistical Interpretation of Term Specificity and Its Application in Retrieval.&quot; Journal of Documentation 28, no. 1 (1972): 11–21.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EStray, Jonathan, and Julian Burgess. &quot;A Full-text Visualization of the Iraq War Logs,&quot; December 10, 2010 (Update April 2012). \u003Ca href=\\\"http:\u002F\u002Fjonathanstray.com\u002Fa-full-text-visualization-of-the-iraq-war-logs\\\"\u003Ehttp:\u002F\u002Fjonathanstray.com\u002Fa-full-text-visualization-of-the-iraq-war-logs\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EUnderwood, Ted. &quot;Identifying diction that characterizes an author or genre: why Dunning&#39;s may not be the best method,&quot; \u003Cem\u003EThe Stone and the Shell\u003C\u002Fem\u003E, November 9, 2011. \u003Ca href=\\\"https:\u002F\u002Ftedunderwood.com\u002F2011\u002F11\u002F09\u002Fidentifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method\u002F\\\"\u003Ehttps:\u002F\u002Ftedunderwood.com\u002F2011\u002F11\u002F09\u002Fidentifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method\u002F\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003E--. &quot;The Historical Significance of Textual Distances&quot;, Preprint of LaTeCH-CLfL Workshop, COLING, Santa Fe, 2018. \u003Ca href=\\\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1807.00181\\\"\u003Ehttps:\u002F\u002Farxiv.org\u002Fabs\u002F1807.00181\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003Evan Rossum,  Guido, Barry Warsaw, and Nick Coghlan. &quot;PEP 8 -- Style Guide for Python Code.&quot; July 5, 2001. Updated July 2013. \u003Ca href=\\\"https:\u002F\u002Fwww.python.org\u002Fdev\u002Fpeps\u002Fpep-0008\u002F\\\"\u003Ehttps:\u002F\u002Fwww.python.org\u002Fdev\u002Fpeps\u002Fpep-0008\u002F\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003EWhitman, Alden. &quot;Upton Sinclair, Author, Dead; Crusader for Social Justice, 90&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, November 26, 1968, 1, 34. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003E&quot;W. E. B. DuBois Dies in Ghana; Negro Leader and Author, 95&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, August 28, 1963, 27. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003E&quot;Willa Cather Dies; Noted Novelist, 70&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, April 25, 1947, 21. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"alternatives-to-anaconda\\\"\u003EAlternatives to Anaconda\u003C\u002Fh2\u003E\\n\u003Cp\u003EIf you are not using Anaconda, you will need to cover the following dependencies:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EInstall Python 2 or 3 (preferably Python 3.6 or later)\u003C\u002Fli\u003E\\n\u003Cli\u003ERecommended: install and run a virtual environment\u003C\u002Fli\u003E\\n\u003Cli\u003EInstall the Scikit-Learn library and its dependencies (see \u003Ca href=\\\"http:\u002F\u002Fscikit-learn.org\u002Fstable\u002Finstall.html\\\"\u003Ehttp:\u002F\u002Fscikit-learn.org\u002Fstable\u002Finstall.html\u003C\u002Fa\u003E).\u003C\u002Fli\u003E\\n\u003Cli\u003EInstall Jupyter Notebook and its dependencies\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch1 id=\\\"endnotes\\\"\u003EEndnotes\u003C\u002Fh1\u003E\\n\u003Cp\u003E[^1]: Underwood, Ted. &quot;Identifying diction that characterizes an author or genre: why Dunning&#39;s may not be the best method,&quot; \u003Cem\u003EThe Stone and the Shell\u003C\u002Fem\u003E, November 9, 2011. \u003Ca href=\\\"https:\u002F\u002Ftedunderwood.com\u002F2011\u002F11\u002F09\u002Fidentifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method\u002F\\\"\u003Ehttps:\u002F\u002Ftedunderwood.com\u002F2011\u002F11\u002F09\u002Fidentifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method\u002F\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^2]: Bennett, Jessica, and Amisha Padnani. &quot;Overlooked,&quot; March 8, 2018. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\u002Finteractive\u002F2018\u002Fobituaries\u002Foverlooked.html\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u002Finteractive\u002F2018\u002Fobituaries\u002Foverlooked.html\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^3]: This dataset is from a version of \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E &quot;On This Day&quot; website that hasn&#39;t been updated since January 31, 2011, and it has been replaced by a newer, sleeker blog located at \u003Ca href=\\\"https:\u002F\u002Flearning.blogs.nytimes.com\u002Fon-this-day\u002F\\\"\u003Ehttps:\u002F\u002Flearning.blogs.nytimes.com\u002Fon-this-day\u002F\u003C\u002Fa\u003E. What&#39;s left on the older &quot;On This Day&quot; Website is a static .html file for each day of the year (0101.html, 0102.html, etc.), including a static page for February 29th (0229.html). Content appears to have been overwritten whenever it was last updated, so there are no archives of content by year. Presumably, the &quot;On This Day&quot; entries for January 1 - January 31 were last updated on their corresponding days in 2011. Meanwhile, February 1 - December 31 were probably last updated on their corresponding days in 2010. The page representing February 29 was probably last updated on February 29, 2008.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^4]: Spärck Jones, Karen. &quot;A Statistical Interpretation of Term Specificity and Its Application in Retrieval.&quot; \u003Cem\u003EJournal of Documentation\u003C\u002Fem\u003E vol. 28, no. 1 (1972): 16.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^5]: Bowles, Nellie. &quot;Overlooked No More: Karen Spärck Jones, Who Established the Basis for Search Engines&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, January 2, 2019. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\u002F2019\u002F01\u002F02\u002Fobituaries\u002Fkaren-sparck-jones-overlooked.html\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u002F2019\u002F01\u002F02\u002Fobituaries\u002Fkaren-sparck-jones-overlooked.html\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^6]: &quot;Nellie Bly, Journalist, Dies of Pneumonia&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, January 28, 1922, 11. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^7]: Documentation for TfidfVectorizer. \u003Ca href=\\\"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.feature_extraction.text.TfidfVectorizer.html\\\"\u003Ehttps:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.feature_extraction.text.TfidfVectorizer.html\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^8]: Schmidt, Ben. &quot;Do Digital Humanists Need to Understand Algorithms?&quot; \u003Cem\u003EDebates in the Digital Humanities 2016\u003C\u002Fem\u003E. Online edition. (Minneapois: University of Minnesota Press): n.p. \u003Ca href=\\\"http:\u002F\u002Fdhdebates.gc.cuny.edu\u002Fdebates\u002Ftext\u002F99\\\"\u003Ehttp:\u002F\u002Fdhdebates.gc.cuny.edu\u002Fdebates\u002Ftext\u002F99\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^9]: van Rossum,  Guido, Barry Warsaw, and Nick Coghlan. &quot;PEP 8 -- Style Guide for Python Code.&quot; July 5, 2001. Updated July 2013. \u003Ca href=\\\"https:\u002F\u002Fwww.python.org\u002Fdev\u002Fpeps\u002Fpep-0008\u002F\\\"\u003Ehttps:\u002F\u002Fwww.python.org\u002Fdev\u002Fpeps\u002Fpep-0008\u002F\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^10]: &quot;Ida M. Tarbell, 86, Dies in Bridgeport&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, January 7, 1944, 17. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E; &quot;Nellie Bly, Journalist, Dies of Pneumonia&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, January 28, 1922, 11. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E; &quot;W. E. B. DuBois Dies in Ghana; Negro Leader and Author, 95&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, August 28, 1963, 27. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E; Whitman, Alden. &quot;Upton Sinclair, Author, Dead; Crusader for Social Justice, 90&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, November 26, 1968, 1, 34. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E; &quot;Willa Cather Dies; Noted Novelist, 70&quot; \u003Cem\u003EThe New York Times\u003C\u002Fem\u003E, April 25, 1947, 21. \u003Ca href=\\\"https:\u002F\u002Fwww.nytimes.com\\\"\u003Ehttps:\u002F\u002Fwww.nytimes.com\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^11]: Stray, Jonathan, and Julian Burgess. &quot;A Full-text Visualization of the Iraq War Logs,&quot; December 10, 2010 (Update April 2012). \u003Ca href=\\\"http:\u002F\u002Fjonathanstray.com\u002Fa-full-text-visualization-of-the-iraq-war-logs\\\"\u003Ehttp:\u002F\u002Fjonathanstray.com\u002Fa-full-text-visualization-of-the-iraq-war-logs\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^12]: Manning, C.D., P. Raghavan, and H. Schütze, \u003Cem\u003EIntroduction to Information Retrieval\u003C\u002Fem\u003E. (Cambridge: Cambridge University Press, 2008): 118-120.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^13]: Beckman, Milo. &quot;These Are The Phrases Each GOP Candidate Repeats Most,&quot; \u003Cem\u003EFiveThirtyEight\u003C\u002Fem\u003E, March 10, 2016. \u003Ca href=\\\"https:\u002F\u002Ffivethirtyeight.com\u002Ffeatures\u002Fthese-are-the-phrases-each-gop-candidate-repeats-most\u002F\\\"\u003Ehttps:\u002F\u002Ffivethirtyeight.com\u002Ffeatures\u002Fthese-are-the-phrases-each-gop-candidate-repeats-most\u002F\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^14]: Bondi, Marina, and Mike Scott, eds. \u003Cem\u003EKeyness in Texts\u003C\u002Fem\u003E. (Philadelphia: John Benjamins, 2010).\u003C\u002Fp\u003E\\n\u003Cp\u003E[^15]: \u003Cstrong\u003ETf-idf\u003C\u002Fstrong\u003E is not typically a recommended pre-processing step when generating topic models. See \u003Ca href=\\\"https:\u002F\u002Fdatascience.stackexchange.com\u002Fquestions\u002F21950\u002Fwhy-we-should-not-feed-lda-with-tfidf\\\"\u003Ehttps:\u002F\u002Fdatascience.stackexchange.com\u002Fquestions\u002F21950\u002Fwhy-we-should-not-feed-lda-with-tfidf\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^16]: Schmidt, Ben. &quot;Words Alone: Dismantling Topic Models in the Humanities,&quot; \u003Cem\u003EJournal of Digital Humanities\u003C\u002Fem\u003E. Vol. 2, No. 1 (2012): n.p. \u003Ca href=\\\"http:\u002F\u002Fjournalofdigitalhumanities.org\u002F2-1\u002Fwords-alone-by-benjamin-m-schmidt\u002F\\\"\u003Ehttp:\u002F\u002Fjournalofdigitalhumanities.org\u002F2-1\u002Fwords-alone-by-benjamin-m-schmidt\u002F\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E[^17]: Mihalcea, Rada, and Paul Tarau. &quot;Textrank: Bringing order into text.&quot; In \u003Cem\u003EProceedings of the 2004 conference on empirical methods in natural language processing\u003C\u002Fem\u003E. 2004.\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
