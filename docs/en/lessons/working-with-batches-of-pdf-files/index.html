<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-c09d08cd.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-c09d08cd.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/working-with-batches-of-pdf-files"),
					params: {lang:"en",lessons:"lessons",slug:"working-with-batches-of-pdf-files"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Working with batches of PDF files</h1>

<!-- HTML_TAG_START --><h1 id="a-table-of-contents">A Table of Contents</h1>
<p>{% include toc.html %}</p>
<h1 id="overview">Overview</h1>
<h2 id="motivation">Motivation</h2>
<p>Humanities scholars often work with text-based historical and contemporary sources. In most cases, the <a href="https://en.wikipedia.org/wiki/PDF">Portable Document Format (PDF)</a> is used as an exchange format. This includes digital reproductions of physical sources such as books and photographs as well as digitally created documents. The digitisation of these objects increases their accessibility and availability. Archives have begun to digitise entire collections and make them accessible via the Internet. Even more dramatic is the increase in the amount of data in digitally created sources such as those necessary for corporate and government reporting. As a result, humanities scholars are increasingly exploring larger collections by means of Distant Reading and other algorithmic tools. However, PDF documents are only suitable for digital processing to a limited extent and must first be converted into plain text files.</p>
<h2 id="scope">Scope</h2>
<p>If you meet one or more of the following criteria, this lesson will be instructive for you:</p>
<ul>
<li>You work with text-based sources and need to extract the content of the sources</li>
<li>Your files are in PDF file format or can be converted to this file format</li>
<li>You work with a large corpus and you do not want to touch each file individually (batch processing)</li>
<li>You want to examine your corpus by the means of <a href="/en/lessons/?topic=distant-reading">Distant Reading</a> and therefore need it to be in plain text format</li>
<li>You don’t have access to commercial software, such as Adobe Acrobat Professional or Abbyy FineReader</li>
</ul>
<h2 id="objectives">Objectives</h2>
<p>In more technical terms, in this lesson you will learn to:</p>
<ul>
<li>Recognize and extract texts in PDFs with <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">Optical Character Recognition (OCR)</a></li>
<li>Extract embedded texts from PDFs</li>
<li>Extract embedded images from PDFs</li>
<li>Combine images and PDFs into a single PDF file</li>
<li>Do all of the above at once (batch processing) with a large corpus.</li>
<li>Analyze a large corpus using <a href="https://en.wikipedia.org/wiki/Topic_model">Topic Modelling</a> to get a quick overview of the topics it contains</li>
</ul>
<div class="alert alert-info">
<a href="https://en.wikipedia.org/wiki/Tesseract_(software)">Tesseract OCR software</a> used in this lesson supports over 110 languages including non-western languages and writing systems.

</div>

<h1 id="prerequisites">Prerequisites</h1>
<h2 id="skills">Skills</h2>
<p>You should feel comfortable using the command line of your computer. Windows users should take a look at <a href="/en/lessons/intro-to-powershell">Introduction to the Windows Command Line with PowerShell</a>. MacOS and Linux users should take a look at <a href="/en/lessons/intro-to-bash">Introduction to the Bash Command Line</a>.</p>
<h2 id="software">Software</h2>
<h3 id="windows-10">Windows 10</h3>
<p>Some components of the unix-based open source software used in this lesson do not run on Windows systems natively. Fortunately, since the Windows 10 Fall Creators Update there is a workaround. Open <a href="https://docs.microsoft.com/en-us/powershell/scripting/getting-started/starting-windows-powershell">PowerShell</a> as administrator and run <code>Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux</code>. Install <a href="https://www.microsoft.com/store/apps/9N9TNGVNDL3Q">Ubuntu 18.04 LTS</a> from the Microsoft Store. To <a href="https://docs.microsoft.com/en-us/windows/wsl/">initialize</a> the Windows Subsystem for Linux (WSL) click on the Ubuntu tile in the Start Menu and create a user account.[^1]</p>
<div class="alert alert-warning">

<p>Follow these instructions carefully and do not lose your credentials. You will need them as soon as you run programs as administrator.</p>
</div>

<p>Once the WSL is up and running, navigate to your working directory (for example Downloads). Invoke <code>bash</code> through PowerShell and install all requirements via the built-in package manager <a href="https://wiki.debian.org/Aptitude">Aptitude</a>.</p>
<pre><code class="language-powershell">bash
</code></pre>
<pre><code class="language-bash">sudo apt install ocrmypdf tesseract-ocr-all poppler-utils imagemagick
</code></pre>
<h3 id="macos">MacOS</h3>
<p>Installing all the requirements without a package manager is cumbersome. Therefore install the <a href="https://itunes.apple.com/us/app/xcode/id497799835">Command Line Tools for Xcode</a> and <a href="https://brew.sh">Homebrew</a> first. It offers an easy way to install all the tools and software needed for this lesson.</p>
<pre><code class="language-bash">xcode-select --install
/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;
brew install ocrmypdf tesseract-lang poppler imagemagick
</code></pre>
<h3 id="linux">Linux</h3>
<p>On <a href="https://ubuntu.com/download/desktop">Ubuntu 18.04 LTS</a> and most Debian-based Linux distributions you can install all requirements via <code>aptitude</code>.</p>
<pre><code class="language-bash">apt install ocrmypdf tesseract-ocr-all poppler-utils imagemagick
</code></pre>
<p>Even though all tools used in this lesson are shipped with Ubuntu, an update is recommended.</p>
<h3 id="topic-modelling">Topic Modelling</h3>
<p>The Topic Modelling in the case study is performed with the <a href="https://dariah-de.github.io/TopicsExplorer/">DARIAH Topics Explorer</a>. It is a very easy to use tool with a graphical user interface. You can download the open source program for Windows, Mac and Linux <a href="https://github.com/DARIAH-DE/TopicsExplorer/releases">here</a>.</p>
<div class="alert alert-warning">

<p>If you are using a Mac and receive an error message that the file is from an “unidentified developer,” you can overwrite it by holding control while double-clicking it. If that doesn&#39;t work, go to Systems Preferences, click on Security &amp; Privacy, and then click Open Anyway.</p>
</div>

<h2 id="data">Data</h2>
<p>Throughout this lesson you will work with historical documents from the <a href="https://web.archive.org/web/20200606003222/https://ilostat.ilo.org/resources/methods/icls/icls-documents/">First International Conference of Labour Statisticians</a> from 1923. The data of all past conferences is provided by the <a href="https://www.ilo.org/global/about-the-ilo/history/lang--en/index.htm">International Labour Organization (ILO)</a> and is <a href="https://www.ilo.org/public/libdoc/ilo/ILO-SR/">publicly available</a>.</p>
<p>To make it easier for you to navigate through the file system and create folders, here are some basic commands of the Bash Command Line:</p>
<ul>
<li>Display the path of the current folder with <code>pwd</code></li>
<li>Display the contents of the current folder with <code>ls</code></li>
<li>Display only PDF files in the current folder with <code>ls *.pdf</code></li>
<li>Create a folder named proghist with <code>mkdir proghist</code></li>
<li>Change to this folder with <code>cd proghist</code></li>
<li>Open your current folder with a file browser <code>open .</code> (On Windows use <code>explorer.exe .</code>)</li>
<li>Change to the parent folder with <code>cd ..</code></li>
<li>Change to your users home directory with <code>cd</code></li>
<li>Paste the code snippets into <a href="https://explainshell.com/">explainshell.com</a> to see what the code actually does</li>
</ul>
<div class="alert alert-warning">

<p>Throughout the lesson I will assume that &#39;proghist&#39; is your working directory.</p>
</div>

<p>Save all files below to your working directory:</p>
<ul>
<li><a href="https://www.ilo.org/public/libdoc/ilo/ILO-SR/ILO-SR_N1_engl.pdf">Classification of industries</a><!--text extraction--></li>
<li><a href="https://www.ilo.org/public/libdoc/ilo/ILO-SR/ILO-SR_N2_engl.pdf">Statistics of wages and hours of labour</a><!--ocr--></li>
<li><a href="https://www.ilo.org/public/libdoc/ilo/ILO-SR/ILO-SR_N3_engl.pdf">Statistics of industrial accidents</a><!--text extraction--></li>
<li><a href="https://www.ilo.org/public/libdoc/ilo/ILO-SR/ILO-SR_N4_engl.pdf">Report of the Conference</a><!--text extraction--></li>
<li><a href="https://www.ilo.org/public/libdoc/ilo/P/09602/09602(1924-9-1)3-30.pdf">International labour review</a><!--text extraction--></li>
</ul>
<p>To illustrate image extraction and PDF merging you will include one more files to our corpus that is not directly related to the First International Conference of Labour Statisticians from 1923.</p>
<ul>
<li><a href="https://www.ilo.org/public/libdoc/ilo/1923/23B09_5_engl.pdf">Speeches made at the ceremony on 21 October 1923</a> <!--extract images, combine documents--></li>
</ul>
<p>For the Topic Modelling of the <a href="#use-topic-modelling-to-analyze-the-corpus">case study</a> you will download more files later in the lesson.</p>
<div class="alert alert-danger">

<p>Always make a backup copy of your data before using the commands in this course. Text recognition and combining PDFs can change the original files.</p>
</div>

<h1 id="assessing-your-pdfs">Assessing Your PDF(s)</h1>
<p>In order to make this lesson as realistic as possible, you will be guided by a concrete historical case study. The study draws on the extensive collection of the <a href="http://web.archive.org/web/20200606003222/https://ilostat.ilo.org/resources/methods/icls/icls-documents/">International Labour Organization (ILO)</a>, in particular the sources of the First International Conference of Labour Statisticians.</p>
<p>You are interested in what topics were discussed by the labour statisticians. For this purpose you will want to analyze all available documents of this conference using Topic Modelling. This assumes that all documents are available in plain text.</p>
<p>First you will get an overview of our corpus. Large databases can create a false impression of evidence. Therefore, the documents must be subjected to qualitative analysis. For this you will use scientific methods such as <a href="https://en.wikipedia.org/wiki/Source_criticism">source criticism</a>. All documents are written in English and are set in the same font. <code>ILO-SR_N1_engl.pdf</code>, <code>ILO-SR_N2_engl.pdf</code>, <code>ILO-SR_N3_engl.pdf</code> and <code>ILO-SR_N4_engl.pdf</code> are part of the same series. In addition, you note that the <code>ILO-SR_N2_engl.pdf</code> file does not contain any embedded text. You also note that <code>23B09_5_engl.pdf</code> contains images. One of these images contains text.[^2]</p>
<ol>
<li>You will recognize the text of <code>ILO-SR_N2_engl.pdf</code></li>
<li>You will extract the text from all PDF files</li>
<li>You will extract images from <code>23B09_5_engl.pdf</code></li>
<li>For illustrative purposes, you will combine different images and documents into a single PDF document. This can be helpful if the scanning process involves individual image files that are to be combined into a single document</li>
<li>You will analyze a lot of plain text files using Topic Modelling</li>
</ol>
<h2 id="text-recognition-in-pdf-files">Text Recognition in PDF Files</h2>
<p>For the text recognition, you will use <a href="https://ocrmypdf.readthedocs.io">OCRmyPDF</a>. This software is based on the state-of-the-art open source text recognition software <a href="https://github.com/tesseract-ocr/tesseract">Tesseract</a>, which is maintained and further developed by Google. The software automatically recognizes the page orientation, corrects skewed pages, cleans up image artifacts, and adds an OCR text layer to the PDF. Only the document language must be given as a parameter.</p>
<pre><code class="language-bash">ocrmypdf --language eng --deskew --clean &#39;ILO-SR_N2_engl.pdf&#39; &#39;ILO-SR_N2_engl.pdf&#39;
</code></pre>
<p>{% include figure.html filename=&quot;working-with-batches-of-pdf-files1.png&quot; caption=&quot;Figure 1: The status messages of the software indicate recognition errors in the OCR process.&quot; %}</p>
<p>The status messages of the software indicate recognition errors during the OCR process (see Figure 1). If certain errors occur systematically, it may be worthwhile to write a correction script. See <a href="/en/lessons/cleaning-ocrd-text-with-regular-expressions">Cleaning OCR’d text with Regular Expressions</a>.</p>
<div class="alert alert-info">

<p>OCRmyPDF has many useful parameters to optimize your results. See the <a href="https://ocrmypdf.readthedocs.io/en/latest/cookbook.html">documentation</a>. The output might look slightly different depending on the version used.</p>
</div>

<p>To process all PDF files in your working directory at once. OCRmyPDF automatically skips PDFs that already contain embedded text.</p>
<pre><code class="language-bash">find . -name &#39;*.pdf&#39; -exec ocrmypdf --language eng --deskew --clean &#39;{}&#39; &#39;{}&#39; \;
</code></pre>
<h2 id="extract-embedded-text-from-pdfs">Extract Embedded Text from PDFs</h2>
<p>To extract the embedded texts from the PDF files use <a href="https://en.wikipedia.org/wiki/Poppler_(software)">Poppler</a>). It is a very powerful command line tool for processing PDF files that is used by many other programs.</p>
<pre><code class="language-bash">pdftotext &#39;ILO-SR_N1_engl.pdf&#39; &#39;ILO-SR_N1_engl.txt&#39;
</code></pre>
<p>To process all PDF files in your working directory at once. The status message <code>Syntax Warning: Invalid Font Weight</code> means that the file contains formatting that does not meet the standard specifications of PDF. You can safely ignore this message.</p>
<pre><code class="language-bash">find . -name &#39;*.pdf&#39; -exec pdftotext &#39;{}&#39; &#39;{}.txt&#39; \;
</code></pre>
<p>Once you have extracted all the embedded text from the PDFs, you can easily browse the text files. You can use the Windows Explorer, MacOS Finder, or a command line program like <code>grep</code>. You can display all the mentions of the term “statistics”.</p>
<pre><code class="language-bash">grep &#39;statistic&#39; . -R
</code></pre>
<p><code>grep</code> is also able to process complicated search queries (so-called <a href="https://manpages.ubuntu.com/manpages/bionic/en/man1/grep.1.html#regular%20expressions">regular expressions</a>). For example, you can also search for all files containing either “labour statistics” or “wage statistics”.</p>
<pre><code class="language-bash">grep -E &#39;labour statistics|wage statistics&#39; . -R
</code></pre>
<p>Regular expressions also include numbers. This is particularly interesting for historians. This command displays all years in the twentieth century.</p>
<pre><code class="language-bash">grep -E &#39;19[0-9][0-9]&#39; . -R
</code></pre>
<p>Once you have successfully extracted all text from the PDF files, they can be further analyzed using methods of <a href="/en/lessons/?topic=distant-reading">Distant Reading</a> such as <a href="/en/lessons/topic-modeling-and-mallet">Topic Modelling</a>. You will apply such methods to the case study later in this lesson.</p>
<h2 id="extract-embedded-images-from-pdfs">Extract Embedded Images from PDFs</h2>
<p>PDF is a container file format and can contain multiple embedded images per page. You can also use Poppler to extract those images. The program allows us to select a target format for the extracted images. It is recommended to you use a lossless image format like PNG when working with the images.</p>
<pre><code class="language-bash">pdfimages -png &#39;23B09_5_engl.pdf&#39; &#39;23B09_5_engl&#39;
</code></pre>
<div class="alert alert-info">

<p>For digitally created documents, Poppler extracts all embedded image files. This often includes image files that are outside the visible area or overlaid by other objects.</p>
</div>

<p>To process all PDF files in your working directory at once.</p>
<pre><code class="language-bash">find . -name &#39;*.pdf&#39; -exec pdfimages -png &#39;{}&#39; &#39;{}&#39; \;
</code></pre>
<p>Poppler can only extract illustrations if they are available as individual images in the PDF file. If you want to extract illustrations from a scanned page take a look at this lesson: <a href="/en/lessons/extracting-illustrated-pages">Extracting Illustrated Pages from Digital Libraries with Python</a>.</p>
<h2 id="combine-images-and-pdfs-into-a-single-pdf">Combine Images and PDFs into a Single PDF</h2>
<p>Although OCRmyPDF can process image files directly, there are cases where you first want to combine the images into a PDF document. Because most image formats do not support multiple pages, each page of a document has to be saved as a single file. With the widespread command line image editing software <a href="https://imagemagick.org/">ImageMagick</a> you can achieve this very easily.</p>
<pre><code class="language-bash">convert &#39;23B09_5_engl-002.png&#39; &#39;23B09_5_engl-004.png&#39; &#39;23B09_5_engl-006.png&#39; &#39;23B09_5_engl-007.png&#39; &#39;some-images-combined.pdf&#39;
</code></pre>
<p>To combine all images into a PDF file at once use the wildcard operator <code>*.png</code>. This step could take a few minutes.</p>
<pre><code class="language-bash">convert &#39;*.png&#39; &#39;all-images-combined.pdf&#39;
</code></pre>
<p>If you want to combine different PDF files, you can rely on Poppler. Poppler does this job much faster than ImageMagick and preserves attributes of the original documents.</p>
<pre><code class="language-bash">pdfunite &#39;ILO-SR_N1_engl.pdf&#39; &#39;ILO-SR_N2_engl.pdf&#39; &#39;ILO-SR_N3_engl.pdf&#39; &#39;ILO-SR_N4_engl.pdf&#39; &#39;some-pdfs-combined.pdf&#39;
</code></pre>
<h2 id="use-topic-modelling-to-analyze-the-corpus">Use Topic Modelling to Analyze the Corpus</h2>
<p>Now that you have performed all the steps of the PDF processing on some examples, you can return to the historical question of the case study. Which topics were discussed by the labour statisticians at the international conferences of the ILO? In order to answer this question using Topic Modelling, the following steps are necessary:</p>
<ol>
<li>Download the corpus</li>
<li>Prepare and clean up the corpus</li>
<li>Create the Topic Model</li>
<li>Evaluate the Topic Model</li>
</ol>
<div class="alert alert-info">
  Both the download and the processing of the corpus are time consuming and resource intensive. At <a href="https://zenodo.org/record/3582818/files/20191218-ilo-dataset.zip?download=1">doi.org/10.5281/zenodo.3582736</a> you can download the collection as a ZIP file and go directly to step
  3.
</div>

<h3 id="download-the-corpus">Download the Corpus</h3>
<p>To avoid confusion create a new folder with <code>mkdir</code> and open it with <code>cd</code>.</p>
<pre><code class="language-bash">mkdir case_study
cd case_study
</code></pre>
<p>You can download the corpus from the <a href="https://www.ilo.org/public/libdoc/ilo/ILO-SR/">ILO website</a>. All English documents contain ‘engl’ in the title. It’s over a gigabyte of data. Depending on your internet speed this may take a while.</p>
<p>To automate this step you can use the following command line commands. This will download all English documents (340 files) at once.</p>
<pre><code class="language-bash">curl https://www.ilo.org/public/libdoc/ilo/ILO-SR/ |
grep -o &#39;ILO[^&quot;]*engl[^&quot;&gt;&lt;\/]*&#39; |
uniq |
sed &#39;s,ILO,https://www.ilo.org/public/libdoc/ilo/ILO-SR/ILO,g&#39; &gt; list_of_files.txt
xargs -n 1 curl -O &lt; list_of_files.txt
rm list_of_files.txt
</code></pre>
<h3 id="prepare-and-clean-up-the-corpus">Prepare and Clean Up the Corpus</h3>
<p>Now you can batch process all downloaded PDF files. First, perform text recognition on all files that don’t have embedded text. Then extract all embedded text from the files. Depending on the performance of your computer, this step may take several hours.</p>
<pre><code class="language-bash">find . -name &#39;*.pdf&#39; -exec ocrmypdf --language eng --deskew --clean &#39;{}&#39; &#39;{}&#39; \; &amp;&amp;
find . -name &#39;*.pdf&#39; -exec pdftotext &#39;{}&#39; &#39;{}.txt&#39; \;
</code></pre>
<h3 id="create-the-topic-model">Create the Topic Model</h3>
<p>In order to create a Topic Model with the DARIAH Topics Explorer, you don’t need to have any deeper mathematical knowledge about the Latent Dirichlet Allocation (LDA) that is used.[^3] Nevertheless, it is worth clarifying some implicit assumptions of the model before you begin:</p>
<ul>
<li>A corpus consists of documents. Each document consists of words. Words are carriers of meaning. The order (sentences, sections, etc.) of the words is very important to understand its content. But it is not factored in for the purposes of this analysis. Only the frequency of words in a document or corpus (or more precisely the co-occurrence of words) is measured</li>
<li>You determine how many topics are present in the corpus.</li>
<li>Each word has a specific probability of belonging to a topic. The algorithm finds the corresponding probabilities of the individual words</li>
<li>Words that occur very frequently do little to discriminate between the individual topics. They are often function words such as &#39;and&#39;, &#39;but&#39; and so forth. Therefore, they should not be included in the analysis</li>
<li>Topic modeling using LDA is non-deterministic. This means that a different result can be obtained for each run. Fortunately, the result usually converges towards a stable state. Run it several times and compare the results. You will quickly see if the topics remain stable</li>
</ul>
<p>Now open the <a href="https://dariah-de.github.io/TopicsExplorer/">DARIAH Topics Explorer</a> and follow the steps given in the software. Then:</p>
<ol>
<li>Select all 340 text files for the analysis.</li>
<li>Remove the 150 most common words. Alternatively, you can also load the file with the English stop words contained in the <a href="https://github.com/DARIAH-DE/TopicsExplorer/tree/master/data">example Corpus</a> of the DARIAH Topics Explorer.</li>
<li>Choose 30 for the number of topics and 200 for the number of iterations. You should play with the number of topics and choose a value between 10 and 100. With the number of iterations you increase the accuracy to the price of the calculation duration.</li>
<li>Click on &#39;Train Model&#39;. Depending on the speed of your computer, this process may take several minutes.</li>
</ol>
<h3 id="evaluate-the-topic-model">Evaluate the Topic Model</h3>
<p>The <a href="https://dariah-de.github.io/TopicsExplorer/">DARIAH Topics Explorer</a> has a graphical user interface that makes it very easy to explore and evaluate the Topic Model and its thirty topics. In this run, the second topic looks like this (see Figure 2).</p>
<p>{% include figure.html filename=&quot;working-with-batches-of-pdf-files2.png&quot; caption=&quot;Figure 2: DARIAH Topics Explorer showing related words, related documents and similar topics of a single topic.&quot; %}</p>
<p>This topic deals with various social insurance schemes. Both old-age provision and unemployment benefits are included. The words are sorted in descending order of relevance and give a good overview of the topic. You can also see which documents have the highest correspondence with this topic. As you can see when you look at related topics, this topic is close to accident insurance and legislation.</p>
<p>To further process or visualize the results with a spreadsheet program, click on the &#39;Export Data&#39; button. The paper &#39;Parliament’s Debates about Infrastructure&#39; by Jo Guldi illustrates how Topic Modelling can be put to use for historical research.[^4]</p>
<h1 id="concluding-remarks">Concluding Remarks</h1>
<p>Over the past decades, PDF has become the de facto standard for archiving and exchanging digital text documents.[^5] However, this is not only the case for projects that focus primarily on digitized historical sources. For most digitally generated content, such as websites and interactive documents, as yet no generally accepted archiving formats have been established. Therefore, PDF is often used in these cases as well. Sometimes contemporary source documents present us with the same challenges as inferior scans of historical documents.</p>
<p>The Mueller Report is the official report documenting the findings and conclusions of the investigation by former Special Advisor Robert Mueller into Russian efforts to interfere in the 2016 presidential election in the United States, the allegation of conspiracy or coordination between Donald Trump&#39;s presidential campaign and Russia, and the allegation of obstruction of justice. As a technical analysis by Duff Johnson shows, the Mueller Report was digitally created, printed, scanned at least once, and sent for text recognition in an inferior version. Text and metadata were lost, which would have made working with the document much easier. It is unclear whether this deterioration in quality is intentional or due to complicated administrative procedures. In any case, it makes researchers&#39; lives more difficult.[^6]</p>
<h2 id="alternatives">Alternatives</h2>
<p>This lesson focused on tools that are easy to use and are available as open source software free of charge. There are a lot of open source and commercial <a href="https://en.wikipedia.org/wiki/List_of_PDF_software">alternatives</a> to process PDF files.[^7] <a href="/en/lessons/topic-modeling-and-mallet">Getting Started with Topic Modeling and MALLET</a> covers one of many alternatives for Topic Modelling.</p>
<p>[^1]: If you run into troubles <a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">activating</a> the WSL check out the <a href="https://docs.microsoft.com/en-us/windows/wsl/troubleshooting">troubleshooting</a>, <a href="https://aka.ms/wsldocs">documentation</a>, or the <a href="https://aka.ms/learnwsl">learning</a> resources.</p>
<p>[^2]: In the case of a larger corpus, it is advisable to carry out random sampling instead of a detailed analysis. If no text is embedded in certain files, text recognition can be run over the entire corpus. Text recognition identifies embedded text when it is missing, and attempts to recognize it.</p>
<p>[^3]: If you still want to learn more, see Ganegedara, Thushan. “Intuitive Guide to Latent Dirichlet Allocation.” <em>Medium</em>, August 23, 2018. <a href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158">https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158</a>.</p>
<p>[^4]: Guldi, Jo. “Parliament’s Debates about Infrastructure: An Exercise in Using Dynamic Topic Models to Synthesize Historical Change.” <em>Technology and Culture</em> 60, no. 1 (2019): 1–33. <a href="https://doi.org/10.1353/tech.2019.0000">https://doi.org/10.1353/tech.2019.0000</a>.</p>
<p>[^5]: The fourth chapter describes in detail the history of the PDF file format and the associated socio-technological upheaval, see Gitelman, Lisa. <em>Paper Knowledge: Toward a Media History of Documents.</em> Durham: Duke University Press, 2014.</p>
<p>[^6]: Johnson, Duff. “A Technical and Cultural Assessment of the Mueller Report PDF.” <em>PDF Association</em> (blog), April 19, 2019. <a href="https://www.pdfa.org/a-technical-and-cultural-assessment-of-the-mueller-report-pdf/">https://www.pdfa.org/a-technical-and-cultural-assessment-of-the-mueller-report-pdf/</a>.</p>
<p>[^7]: Especially worth mentioning are the German wiki pages of the Ubuntu community about <a href="https://wiki.ubuntuusers.de/PDF/">PDF</a> and <a href="https://wiki.ubuntuusers.de/Texterkennung/">OCR</a>. These pages contain references to free software for working with PDF files and improving text recognition. Unfortunately, there are no translations into other languages for these pages, so a translation service should be used.</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="working-with-batches-of-pdf-files/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Working with batches of PDF files\",\"collection\":\"lessons\",\"layout\":\"lesson\",\"slug\":\"working-with-batches-of-pdf-files\",\"date\":\"2020-01-30T00:00:00.000Z\",\"authors\":[\"Moritz Mähr\"],\"reviewers\":[\"Catherine DeRose\",\"Jack Pay\"],\"editors\":[\"Anna-Maria Sichani\"],\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F258\",\"difficulty\":2,\"activity\":\"transforming\",\"topics\":[\"data-manipulation\",\"data-management\"],\"abstract\":\"Learn how to perform OCR and text extraction with free command line tools like Tesseract and Poppler and how to get an overview of large numbers of PDF documents using topic modeling.\",\"avatar_alt\":\"working-with-batches-of-pdf-files\",\"doi\":\"10.46430\u002Fphen0088\"},\"html_body\":\"\u003Ch1 id=\\\"a-table-of-contents\\\"\u003EA Table of Contents\u003C\u002Fh1\u003E\\n\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"overview\\\"\u003EOverview\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"motivation\\\"\u003EMotivation\u003C\u002Fh2\u003E\\n\u003Cp\u003EHumanities scholars often work with text-based historical and contemporary sources. In most cases, the \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPDF\\\"\u003EPortable Document Format (PDF)\u003C\u002Fa\u003E is used as an exchange format. This includes digital reproductions of physical sources such as books and photographs as well as digitally created documents. The digitisation of these objects increases their accessibility and availability. Archives have begun to digitise entire collections and make them accessible via the Internet. Even more dramatic is the increase in the amount of data in digitally created sources such as those necessary for corporate and government reporting. As a result, humanities scholars are increasingly exploring larger collections by means of Distant Reading and other algorithmic tools. However, PDF documents are only suitable for digital processing to a limited extent and must first be converted into plain text files.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"scope\\\"\u003EScope\u003C\u002Fh2\u003E\\n\u003Cp\u003EIf you meet one or more of the following criteria, this lesson will be instructive for you:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EYou work with text-based sources and need to extract the content of the sources\u003C\u002Fli\u003E\\n\u003Cli\u003EYour files are in PDF file format or can be converted to this file format\u003C\u002Fli\u003E\\n\u003Cli\u003EYou work with a large corpus and you do not want to touch each file individually (batch processing)\u003C\u002Fli\u003E\\n\u003Cli\u003EYou want to examine your corpus by the means of \u003Ca href=\\\"\u002Fen\u002Flessons\u002F?topic=distant-reading\\\"\u003EDistant Reading\u003C\u002Fa\u003E and therefore need it to be in plain text format\u003C\u002Fli\u003E\\n\u003Cli\u003EYou don’t have access to commercial software, such as Adobe Acrobat Professional or Abbyy FineReader\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"objectives\\\"\u003EObjectives\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn more technical terms, in this lesson you will learn to:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ERecognize and extract texts in PDFs with \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FOptical_character_recognition\\\"\u003EOptical Character Recognition (OCR)\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EExtract embedded texts from PDFs\u003C\u002Fli\u003E\\n\u003Cli\u003EExtract embedded images from PDFs\u003C\u002Fli\u003E\\n\u003Cli\u003ECombine images and PDFs into a single PDF file\u003C\u002Fli\u003E\\n\u003Cli\u003EDo all of the above at once (batch processing) with a large corpus.\u003C\u002Fli\u003E\\n\u003Cli\u003EAnalyze a large corpus using \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FTopic_model\\\"\u003ETopic Modelling\u003C\u002Fa\u003E to get a quick overview of the topics it contains\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cdiv class=\\\"alert alert-info\\\"\u003E\\n\u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FTesseract_(software)\\\"\u003ETesseract OCR software\u003C\u002Fa\u003E used in this lesson supports over 110 languages including non-western languages and writing systems.\\n\\n\u003C\u002Fdiv\u003E\\n\\n\u003Ch1 id=\\\"prerequisites\\\"\u003EPrerequisites\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"skills\\\"\u003ESkills\u003C\u002Fh2\u003E\\n\u003Cp\u003EYou should feel comfortable using the command line of your computer. Windows users should take a look at \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fintro-to-powershell\\\"\u003EIntroduction to the Windows Command Line with PowerShell\u003C\u002Fa\u003E. MacOS and Linux users should take a look at \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fintro-to-bash\\\"\u003EIntroduction to the Bash Command Line\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"software\\\"\u003ESoftware\u003C\u002Fh2\u003E\\n\u003Ch3 id=\\\"windows-10\\\"\u003EWindows 10\u003C\u002Fh3\u003E\\n\u003Cp\u003ESome components of the unix-based open source software used in this lesson do not run on Windows systems natively. Fortunately, since the Windows 10 Fall Creators Update there is a workaround. Open \u003Ca href=\\\"https:\u002F\u002Fdocs.microsoft.com\u002Fen-us\u002Fpowershell\u002Fscripting\u002Fgetting-started\u002Fstarting-windows-powershell\\\"\u003EPowerShell\u003C\u002Fa\u003E as administrator and run \u003Ccode\u003EEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\u003C\u002Fcode\u003E. Install \u003Ca href=\\\"https:\u002F\u002Fwww.microsoft.com\u002Fstore\u002Fapps\u002F9N9TNGVNDL3Q\\\"\u003EUbuntu 18.04 LTS\u003C\u002Fa\u003E from the Microsoft Store. To \u003Ca href=\\\"https:\u002F\u002Fdocs.microsoft.com\u002Fen-us\u002Fwindows\u002Fwsl\u002F\\\"\u003Einitialize\u003C\u002Fa\u003E the Windows Subsystem for Linux (WSL) click on the Ubuntu tile in the Start Menu and create a user account.[^1]\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\n\\n\u003Cp\u003EFollow these instructions carefully and do not lose your credentials. You will need them as soon as you run programs as administrator.\u003C\u002Fp\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\u003Cp\u003EOnce the WSL is up and running, navigate to your working directory (for example Downloads). Invoke \u003Ccode\u003Ebash\u003C\u002Fcode\u003E through PowerShell and install all requirements via the built-in package manager \u003Ca href=\\\"https:\u002F\u002Fwiki.debian.org\u002FAptitude\\\"\u003EAptitude\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-powershell\\\"\u003Ebash\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Esudo apt install ocrmypdf tesseract-ocr-all poppler-utils imagemagick\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch3 id=\\\"macos\\\"\u003EMacOS\u003C\u002Fh3\u003E\\n\u003Cp\u003EInstalling all the requirements without a package manager is cumbersome. Therefore install the \u003Ca href=\\\"https:\u002F\u002Fitunes.apple.com\u002Fus\u002Fapp\u002Fxcode\u002Fid497799835\\\"\u003ECommand Line Tools for Xcode\u003C\u002Fa\u003E and \u003Ca href=\\\"https:\u002F\u002Fbrew.sh\\\"\u003EHomebrew\u003C\u002Fa\u003E first. It offers an easy way to install all the tools and software needed for this lesson.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Excode-select --install\\n\u002Fusr\u002Fbin\u002Fruby -e &quot;$(curl -fsSL https:\u002F\u002Fraw.githubusercontent.com\u002FHomebrew\u002Finstall\u002Fmaster\u002Finstall)&quot;\\nbrew install ocrmypdf tesseract-lang poppler imagemagick\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch3 id=\\\"linux\\\"\u003ELinux\u003C\u002Fh3\u003E\\n\u003Cp\u003EOn \u003Ca href=\\\"https:\u002F\u002Fubuntu.com\u002Fdownload\u002Fdesktop\\\"\u003EUbuntu 18.04 LTS\u003C\u002Fa\u003E and most Debian-based Linux distributions you can install all requirements via \u003Ccode\u003Eaptitude\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Eapt install ocrmypdf tesseract-ocr-all poppler-utils imagemagick\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EEven though all tools used in this lesson are shipped with Ubuntu, an update is recommended.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"topic-modelling\\\"\u003ETopic Modelling\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe Topic Modelling in the case study is performed with the \u003Ca href=\\\"https:\u002F\u002Fdariah-de.github.io\u002FTopicsExplorer\u002F\\\"\u003EDARIAH Topics Explorer\u003C\u002Fa\u003E. It is a very easy to use tool with a graphical user interface. You can download the open source program for Windows, Mac and Linux \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FDARIAH-DE\u002FTopicsExplorer\u002Freleases\\\"\u003Ehere\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\n\\n\u003Cp\u003EIf you are using a Mac and receive an error message that the file is from an “unidentified developer,” you can overwrite it by holding control while double-clicking it. If that doesn&#39;t work, go to Systems Preferences, click on Security &amp; Privacy, and then click Open Anyway.\u003C\u002Fp\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\u003Ch2 id=\\\"data\\\"\u003EData\u003C\u002Fh2\u003E\\n\u003Cp\u003EThroughout this lesson you will work with historical documents from the \u003Ca href=\\\"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20200606003222\u002Fhttps:\u002F\u002Filostat.ilo.org\u002Fresources\u002Fmethods\u002Ficls\u002Ficls-documents\u002F\\\"\u003EFirst International Conference of Labour Statisticians\u003C\u002Fa\u003E from 1923. The data of all past conferences is provided by the \u003Ca href=\\\"https:\u002F\u002Fwww.ilo.org\u002Fglobal\u002Fabout-the-ilo\u002Fhistory\u002Flang--en\u002Findex.htm\\\"\u003EInternational Labour Organization (ILO)\u003C\u002Fa\u003E and is \u003Ca href=\\\"https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002FILO-SR\u002F\\\"\u003Epublicly available\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo make it easier for you to navigate through the file system and create folders, here are some basic commands of the Bash Command Line:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EDisplay the path of the current folder with \u003Ccode\u003Epwd\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EDisplay the contents of the current folder with \u003Ccode\u003Els\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EDisplay only PDF files in the current folder with \u003Ccode\u003Els *.pdf\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate a folder named proghist with \u003Ccode\u003Emkdir proghist\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EChange to this folder with \u003Ccode\u003Ecd proghist\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EOpen your current folder with a file browser \u003Ccode\u003Eopen .\u003C\u002Fcode\u003E (On Windows use \u003Ccode\u003Eexplorer.exe .\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003EChange to the parent folder with \u003Ccode\u003Ecd ..\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EChange to your users home directory with \u003Ccode\u003Ecd\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EPaste the code snippets into \u003Ca href=\\\"https:\u002F\u002Fexplainshell.com\u002F\\\"\u003Eexplainshell.com\u003C\u002Fa\u003E to see what the code actually does\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\n\\n\u003Cp\u003EThroughout the lesson I will assume that &#39;proghist&#39; is your working directory.\u003C\u002Fp\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\u003Cp\u003ESave all files below to your working directory:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002FILO-SR\u002FILO-SR_N1_engl.pdf\\\"\u003EClassification of industries\u003C\u002Fa\u003E\u003C!--text extraction--\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002FILO-SR\u002FILO-SR_N2_engl.pdf\\\"\u003EStatistics of wages and hours of labour\u003C\u002Fa\u003E\u003C!--ocr--\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002FILO-SR\u002FILO-SR_N3_engl.pdf\\\"\u003EStatistics of industrial accidents\u003C\u002Fa\u003E\u003C!--text extraction--\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002FILO-SR\u002FILO-SR_N4_engl.pdf\\\"\u003EReport of the Conference\u003C\u002Fa\u003E\u003C!--text extraction--\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002FP\u002F09602\u002F09602(1924-9-1)3-30.pdf\\\"\u003EInternational labour review\u003C\u002Fa\u003E\u003C!--text extraction--\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003ETo illustrate image extraction and PDF merging you will include one more files to our corpus that is not directly related to the First International Conference of Labour Statisticians from 1923.\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002F1923\u002F23B09_5_engl.pdf\\\"\u003ESpeeches made at the ceremony on 21 October 1923\u003C\u002Fa\u003E \u003C!--extract images, combine documents--\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EFor the Topic Modelling of the \u003Ca href=\\\"#use-topic-modelling-to-analyze-the-corpus\\\"\u003Ecase study\u003C\u002Fa\u003E you will download more files later in the lesson.\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-danger\\\"\u003E\\n\\n\u003Cp\u003EAlways make a backup copy of your data before using the commands in this course. Text recognition and combining PDFs can change the original files.\u003C\u002Fp\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\u003Ch1 id=\\\"assessing-your-pdfs\\\"\u003EAssessing Your PDF(s)\u003C\u002Fh1\u003E\\n\u003Cp\u003EIn order to make this lesson as realistic as possible, you will be guided by a concrete historical case study. The study draws on the extensive collection of the \u003Ca href=\\\"http:\u002F\u002Fweb.archive.org\u002Fweb\u002F20200606003222\u002Fhttps:\u002F\u002Filostat.ilo.org\u002Fresources\u002Fmethods\u002Ficls\u002Ficls-documents\u002F\\\"\u003EInternational Labour Organization (ILO)\u003C\u002Fa\u003E, in particular the sources of the First International Conference of Labour Statisticians.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou are interested in what topics were discussed by the labour statisticians. For this purpose you will want to analyze all available documents of this conference using Topic Modelling. This assumes that all documents are available in plain text.\u003C\u002Fp\u003E\\n\u003Cp\u003EFirst you will get an overview of our corpus. Large databases can create a false impression of evidence. Therefore, the documents must be subjected to qualitative analysis. For this you will use scientific methods such as \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSource_criticism\\\"\u003Esource criticism\u003C\u002Fa\u003E. All documents are written in English and are set in the same font. \u003Ccode\u003EILO-SR_N1_engl.pdf\u003C\u002Fcode\u003E, \u003Ccode\u003EILO-SR_N2_engl.pdf\u003C\u002Fcode\u003E, \u003Ccode\u003EILO-SR_N3_engl.pdf\u003C\u002Fcode\u003E and \u003Ccode\u003EILO-SR_N4_engl.pdf\u003C\u002Fcode\u003E are part of the same series. In addition, you note that the \u003Ccode\u003EILO-SR_N2_engl.pdf\u003C\u002Fcode\u003E file does not contain any embedded text. You also note that \u003Ccode\u003E23B09_5_engl.pdf\u003C\u002Fcode\u003E contains images. One of these images contains text.[^2]\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EYou will recognize the text of \u003Ccode\u003EILO-SR_N2_engl.pdf\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EYou will extract the text from all PDF files\u003C\u002Fli\u003E\\n\u003Cli\u003EYou will extract images from \u003Ccode\u003E23B09_5_engl.pdf\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EFor illustrative purposes, you will combine different images and documents into a single PDF document. This can be helpful if the scanning process involves individual image files that are to be combined into a single document\u003C\u002Fli\u003E\\n\u003Cli\u003EYou will analyze a lot of plain text files using Topic Modelling\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch2 id=\\\"text-recognition-in-pdf-files\\\"\u003EText Recognition in PDF Files\u003C\u002Fh2\u003E\\n\u003Cp\u003EFor the text recognition, you will use \u003Ca href=\\\"https:\u002F\u002Focrmypdf.readthedocs.io\\\"\u003EOCRmyPDF\u003C\u002Fa\u003E. This software is based on the state-of-the-art open source text recognition software \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Ftesseract-ocr\u002Ftesseract\\\"\u003ETesseract\u003C\u002Fa\u003E, which is maintained and further developed by Google. The software automatically recognizes the page orientation, corrects skewed pages, cleans up image artifacts, and adds an OCR text layer to the PDF. Only the document language must be given as a parameter.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Eocrmypdf --language eng --deskew --clean &#39;ILO-SR_N2_engl.pdf&#39; &#39;ILO-SR_N2_engl.pdf&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;working-with-batches-of-pdf-files1.png&quot; caption=&quot;Figure 1: The status messages of the software indicate recognition errors in the OCR process.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe status messages of the software indicate recognition errors during the OCR process (see Figure 1). If certain errors occur systematically, it may be worthwhile to write a correction script. See \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fcleaning-ocrd-text-with-regular-expressions\\\"\u003ECleaning OCR’d text with Regular Expressions\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-info\\\"\u003E\\n\\n\u003Cp\u003EOCRmyPDF has many useful parameters to optimize your results. See the \u003Ca href=\\\"https:\u002F\u002Focrmypdf.readthedocs.io\u002Fen\u002Flatest\u002Fcookbook.html\\\"\u003Edocumentation\u003C\u002Fa\u003E. The output might look slightly different depending on the version used.\u003C\u002Fp\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\u003Cp\u003ETo process all PDF files in your working directory at once. OCRmyPDF automatically skips PDFs that already contain embedded text.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Efind . -name &#39;*.pdf&#39; -exec ocrmypdf --language eng --deskew --clean &#39;{}&#39; &#39;{}&#39; \\\\;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"extract-embedded-text-from-pdfs\\\"\u003EExtract Embedded Text from PDFs\u003C\u002Fh2\u003E\\n\u003Cp\u003ETo extract the embedded texts from the PDF files use \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPoppler_(software)\\\"\u003EPoppler\u003C\u002Fa\u003E). It is a very powerful command line tool for processing PDF files that is used by many other programs.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Epdftotext &#39;ILO-SR_N1_engl.pdf&#39; &#39;ILO-SR_N1_engl.txt&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ETo process all PDF files in your working directory at once. The status message \u003Ccode\u003ESyntax Warning: Invalid Font Weight\u003C\u002Fcode\u003E means that the file contains formatting that does not meet the standard specifications of PDF. You can safely ignore this message.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Efind . -name &#39;*.pdf&#39; -exec pdftotext &#39;{}&#39; &#39;{}.txt&#39; \\\\;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EOnce you have extracted all the embedded text from the PDFs, you can easily browse the text files. You can use the Windows Explorer, MacOS Finder, or a command line program like \u003Ccode\u003Egrep\u003C\u002Fcode\u003E. You can display all the mentions of the term “statistics”.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Egrep &#39;statistic&#39; . -R\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E\u003Ccode\u003Egrep\u003C\u002Fcode\u003E is also able to process complicated search queries (so-called \u003Ca href=\\\"https:\u002F\u002Fmanpages.ubuntu.com\u002Fmanpages\u002Fbionic\u002Fen\u002Fman1\u002Fgrep.1.html#regular%20expressions\\\"\u003Eregular expressions\u003C\u002Fa\u003E). For example, you can also search for all files containing either “labour statistics” or “wage statistics”.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Egrep -E &#39;labour statistics|wage statistics&#39; . -R\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ERegular expressions also include numbers. This is particularly interesting for historians. This command displays all years in the twentieth century.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Egrep -E &#39;19[0-9][0-9]&#39; . -R\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EOnce you have successfully extracted all text from the PDF files, they can be further analyzed using methods of \u003Ca href=\\\"\u002Fen\u002Flessons\u002F?topic=distant-reading\\\"\u003EDistant Reading\u003C\u002Fa\u003E such as \u003Ca href=\\\"\u002Fen\u002Flessons\u002Ftopic-modeling-and-mallet\\\"\u003ETopic Modelling\u003C\u002Fa\u003E. You will apply such methods to the case study later in this lesson.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"extract-embedded-images-from-pdfs\\\"\u003EExtract Embedded Images from PDFs\u003C\u002Fh2\u003E\\n\u003Cp\u003EPDF is a container file format and can contain multiple embedded images per page. You can also use Poppler to extract those images. The program allows us to select a target format for the extracted images. It is recommended to you use a lossless image format like PNG when working with the images.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Epdfimages -png &#39;23B09_5_engl.pdf&#39; &#39;23B09_5_engl&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv class=\\\"alert alert-info\\\"\u003E\\n\\n\u003Cp\u003EFor digitally created documents, Poppler extracts all embedded image files. This often includes image files that are outside the visible area or overlaid by other objects.\u003C\u002Fp\u003E\\n\u003C\u002Fdiv\u003E\\n\\n\u003Cp\u003ETo process all PDF files in your working directory at once.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Efind . -name &#39;*.pdf&#39; -exec pdfimages -png &#39;{}&#39; &#39;{}&#39; \\\\;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EPoppler can only extract illustrations if they are available as individual images in the PDF file. If you want to extract illustrations from a scanned page take a look at this lesson: \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fextracting-illustrated-pages\\\"\u003EExtracting Illustrated Pages from Digital Libraries with Python\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"combine-images-and-pdfs-into-a-single-pdf\\\"\u003ECombine Images and PDFs into a Single PDF\u003C\u002Fh2\u003E\\n\u003Cp\u003EAlthough OCRmyPDF can process image files directly, there are cases where you first want to combine the images into a PDF document. Because most image formats do not support multiple pages, each page of a document has to be saved as a single file. With the widespread command line image editing software \u003Ca href=\\\"https:\u002F\u002Fimagemagick.org\u002F\\\"\u003EImageMagick\u003C\u002Fa\u003E you can achieve this very easily.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Econvert &#39;23B09_5_engl-002.png&#39; &#39;23B09_5_engl-004.png&#39; &#39;23B09_5_engl-006.png&#39; &#39;23B09_5_engl-007.png&#39; &#39;some-images-combined.pdf&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ETo combine all images into a PDF file at once use the wildcard operator \u003Ccode\u003E*.png\u003C\u002Fcode\u003E. This step could take a few minutes.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Econvert &#39;*.png&#39; &#39;all-images-combined.pdf&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf you want to combine different PDF files, you can rely on Poppler. Poppler does this job much faster than ImageMagick and preserves attributes of the original documents.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Epdfunite &#39;ILO-SR_N1_engl.pdf&#39; &#39;ILO-SR_N2_engl.pdf&#39; &#39;ILO-SR_N3_engl.pdf&#39; &#39;ILO-SR_N4_engl.pdf&#39; &#39;some-pdfs-combined.pdf&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"use-topic-modelling-to-analyze-the-corpus\\\"\u003EUse Topic Modelling to Analyze the Corpus\u003C\u002Fh2\u003E\\n\u003Cp\u003ENow that you have performed all the steps of the PDF processing on some examples, you can return to the historical question of the case study. Which topics were discussed by the labour statisticians at the international conferences of the ILO? In order to answer this question using Topic Modelling, the following steps are necessary:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EDownload the corpus\u003C\u002Fli\u003E\\n\u003Cli\u003EPrepare and clean up the corpus\u003C\u002Fli\u003E\\n\u003Cli\u003ECreate the Topic Model\u003C\u002Fli\u003E\\n\u003Cli\u003EEvaluate the Topic Model\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cdiv class=\\\"alert alert-info\\\"\u003E\\n  Both the download and the processing of the corpus are time consuming and resource intensive. At \u003Ca href=\\\"https:\u002F\u002Fzenodo.org\u002Frecord\u002F3582818\u002Ffiles\u002F20191218-ilo-dataset.zip?download=1\\\"\u003Edoi.org\u002F10.5281\u002Fzenodo.3582736\u003C\u002Fa\u003E you can download the collection as a ZIP file and go directly to step\\n  3.\\n\u003C\u002Fdiv\u003E\\n\\n\u003Ch3 id=\\\"download-the-corpus\\\"\u003EDownload the Corpus\u003C\u002Fh3\u003E\\n\u003Cp\u003ETo avoid confusion create a new folder with \u003Ccode\u003Emkdir\u003C\u002Fcode\u003E and open it with \u003Ccode\u003Ecd\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Emkdir case_study\\ncd case_study\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EYou can download the corpus from the \u003Ca href=\\\"https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002FILO-SR\u002F\\\"\u003EILO website\u003C\u002Fa\u003E. All English documents contain ‘engl’ in the title. It’s over a gigabyte of data. Depending on your internet speed this may take a while.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo automate this step you can use the following command line commands. This will download all English documents (340 files) at once.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Ecurl https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002FILO-SR\u002F |\\ngrep -o &#39;ILO[^&quot;]*engl[^&quot;&gt;&lt;\\\\\u002F]*&#39; |\\nuniq |\\nsed &#39;s,ILO,https:\u002F\u002Fwww.ilo.org\u002Fpublic\u002Flibdoc\u002Filo\u002FILO-SR\u002FILO,g&#39; &gt; list_of_files.txt\\nxargs -n 1 curl -O &lt; list_of_files.txt\\nrm list_of_files.txt\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch3 id=\\\"prepare-and-clean-up-the-corpus\\\"\u003EPrepare and Clean Up the Corpus\u003C\u002Fh3\u003E\\n\u003Cp\u003ENow you can batch process all downloaded PDF files. First, perform text recognition on all files that don’t have embedded text. Then extract all embedded text from the files. Depending on the performance of your computer, this step may take several hours.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Efind . -name &#39;*.pdf&#39; -exec ocrmypdf --language eng --deskew --clean &#39;{}&#39; &#39;{}&#39; \\\\; &amp;&amp;\\nfind . -name &#39;*.pdf&#39; -exec pdftotext &#39;{}&#39; &#39;{}.txt&#39; \\\\;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch3 id=\\\"create-the-topic-model\\\"\u003ECreate the Topic Model\u003C\u002Fh3\u003E\\n\u003Cp\u003EIn order to create a Topic Model with the DARIAH Topics Explorer, you don’t need to have any deeper mathematical knowledge about the Latent Dirichlet Allocation (LDA) that is used.[^3] Nevertheless, it is worth clarifying some implicit assumptions of the model before you begin:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EA corpus consists of documents. Each document consists of words. Words are carriers of meaning. The order (sentences, sections, etc.) of the words is very important to understand its content. But it is not factored in for the purposes of this analysis. Only the frequency of words in a document or corpus (or more precisely the co-occurrence of words) is measured\u003C\u002Fli\u003E\\n\u003Cli\u003EYou determine how many topics are present in the corpus.\u003C\u002Fli\u003E\\n\u003Cli\u003EEach word has a specific probability of belonging to a topic. The algorithm finds the corresponding probabilities of the individual words\u003C\u002Fli\u003E\\n\u003Cli\u003EWords that occur very frequently do little to discriminate between the individual topics. They are often function words such as &#39;and&#39;, &#39;but&#39; and so forth. Therefore, they should not be included in the analysis\u003C\u002Fli\u003E\\n\u003Cli\u003ETopic modeling using LDA is non-deterministic. This means that a different result can be obtained for each run. Fortunately, the result usually converges towards a stable state. Run it several times and compare the results. You will quickly see if the topics remain stable\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003ENow open the \u003Ca href=\\\"https:\u002F\u002Fdariah-de.github.io\u002FTopicsExplorer\u002F\\\"\u003EDARIAH Topics Explorer\u003C\u002Fa\u003E and follow the steps given in the software. Then:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003ESelect all 340 text files for the analysis.\u003C\u002Fli\u003E\\n\u003Cli\u003ERemove the 150 most common words. Alternatively, you can also load the file with the English stop words contained in the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FDARIAH-DE\u002FTopicsExplorer\u002Ftree\u002Fmaster\u002Fdata\\\"\u003Eexample Corpus\u003C\u002Fa\u003E of the DARIAH Topics Explorer.\u003C\u002Fli\u003E\\n\u003Cli\u003EChoose 30 for the number of topics and 200 for the number of iterations. You should play with the number of topics and choose a value between 10 and 100. With the number of iterations you increase the accuracy to the price of the calculation duration.\u003C\u002Fli\u003E\\n\u003Cli\u003EClick on &#39;Train Model&#39;. Depending on the speed of your computer, this process may take several minutes.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch3 id=\\\"evaluate-the-topic-model\\\"\u003EEvaluate the Topic Model\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe \u003Ca href=\\\"https:\u002F\u002Fdariah-de.github.io\u002FTopicsExplorer\u002F\\\"\u003EDARIAH Topics Explorer\u003C\u002Fa\u003E has a graphical user interface that makes it very easy to explore and evaluate the Topic Model and its thirty topics. In this run, the second topic looks like this (see Figure 2).\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;working-with-batches-of-pdf-files2.png&quot; caption=&quot;Figure 2: DARIAH Topics Explorer showing related words, related documents and similar topics of a single topic.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThis topic deals with various social insurance schemes. Both old-age provision and unemployment benefits are included. The words are sorted in descending order of relevance and give a good overview of the topic. You can also see which documents have the highest correspondence with this topic. As you can see when you look at related topics, this topic is close to accident insurance and legislation.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo further process or visualize the results with a spreadsheet program, click on the &#39;Export Data&#39; button. The paper &#39;Parliament’s Debates about Infrastructure&#39; by Jo Guldi illustrates how Topic Modelling can be put to use for historical research.[^4]\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"concluding-remarks\\\"\u003EConcluding Remarks\u003C\u002Fh1\u003E\\n\u003Cp\u003EOver the past decades, PDF has become the de facto standard for archiving and exchanging digital text documents.[^5] However, this is not only the case for projects that focus primarily on digitized historical sources. For most digitally generated content, such as websites and interactive documents, as yet no generally accepted archiving formats have been established. Therefore, PDF is often used in these cases as well. Sometimes contemporary source documents present us with the same challenges as inferior scans of historical documents.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe Mueller Report is the official report documenting the findings and conclusions of the investigation by former Special Advisor Robert Mueller into Russian efforts to interfere in the 2016 presidential election in the United States, the allegation of conspiracy or coordination between Donald Trump&#39;s presidential campaign and Russia, and the allegation of obstruction of justice. As a technical analysis by Duff Johnson shows, the Mueller Report was digitally created, printed, scanned at least once, and sent for text recognition in an inferior version. Text and metadata were lost, which would have made working with the document much easier. It is unclear whether this deterioration in quality is intentional or due to complicated administrative procedures. In any case, it makes researchers&#39; lives more difficult.[^6]\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"alternatives\\\"\u003EAlternatives\u003C\u002Fh2\u003E\\n\u003Cp\u003EThis lesson focused on tools that are easy to use and are available as open source software free of charge. There are a lot of open source and commercial \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FList_of_PDF_software\\\"\u003Ealternatives\u003C\u002Fa\u003E to process PDF files.[^7] \u003Ca href=\\\"\u002Fen\u002Flessons\u002Ftopic-modeling-and-mallet\\\"\u003EGetting Started with Topic Modeling and MALLET\u003C\u002Fa\u003E covers one of many alternatives for Topic Modelling.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^1]: If you run into troubles \u003Ca href=\\\"https:\u002F\u002Fdocs.microsoft.com\u002Fen-us\u002Fwindows\u002Fwsl\u002Finstall-win10\\\"\u003Eactivating\u003C\u002Fa\u003E the WSL check out the \u003Ca href=\\\"https:\u002F\u002Fdocs.microsoft.com\u002Fen-us\u002Fwindows\u002Fwsl\u002Ftroubleshooting\\\"\u003Etroubleshooting\u003C\u002Fa\u003E, \u003Ca href=\\\"https:\u002F\u002Faka.ms\u002Fwsldocs\\\"\u003Edocumentation\u003C\u002Fa\u003E, or the \u003Ca href=\\\"https:\u002F\u002Faka.ms\u002Flearnwsl\\\"\u003Elearning\u003C\u002Fa\u003E resources.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^2]: In the case of a larger corpus, it is advisable to carry out random sampling instead of a detailed analysis. If no text is embedded in certain files, text recognition can be run over the entire corpus. Text recognition identifies embedded text when it is missing, and attempts to recognize it.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^3]: If you still want to learn more, see Ganegedara, Thushan. “Intuitive Guide to Latent Dirichlet Allocation.” \u003Cem\u003EMedium\u003C\u002Fem\u003E, August 23, 2018. \u003Ca href=\\\"https:\u002F\u002Ftowardsdatascience.com\u002Flight-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\\\"\u003Ehttps:\u002F\u002Ftowardsdatascience.com\u002Flight-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^4]: Guldi, Jo. “Parliament’s Debates about Infrastructure: An Exercise in Using Dynamic Topic Models to Synthesize Historical Change.” \u003Cem\u003ETechnology and Culture\u003C\u002Fem\u003E 60, no. 1 (2019): 1–33. \u003Ca href=\\\"https:\u002F\u002Fdoi.org\u002F10.1353\u002Ftech.2019.0000\\\"\u003Ehttps:\u002F\u002Fdoi.org\u002F10.1353\u002Ftech.2019.0000\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^5]: The fourth chapter describes in detail the history of the PDF file format and the associated socio-technological upheaval, see Gitelman, Lisa. \u003Cem\u003EPaper Knowledge: Toward a Media History of Documents.\u003C\u002Fem\u003E Durham: Duke University Press, 2014.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^6]: Johnson, Duff. “A Technical and Cultural Assessment of the Mueller Report PDF.” \u003Cem\u003EPDF Association\u003C\u002Fem\u003E (blog), April 19, 2019. \u003Ca href=\\\"https:\u002F\u002Fwww.pdfa.org\u002Fa-technical-and-cultural-assessment-of-the-mueller-report-pdf\u002F\\\"\u003Ehttps:\u002F\u002Fwww.pdfa.org\u002Fa-technical-and-cultural-assessment-of-the-mueller-report-pdf\u002F\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^7]: Especially worth mentioning are the German wiki pages of the Ubuntu community about \u003Ca href=\\\"https:\u002F\u002Fwiki.ubuntuusers.de\u002FPDF\u002F\\\"\u003EPDF\u003C\u002Fa\u003E and \u003Ca href=\\\"https:\u002F\u002Fwiki.ubuntuusers.de\u002FTexterkennung\u002F\\\"\u003EOCR\u003C\u002Fa\u003E. These pages contain references to free software for working with PDF files and improving text recognition. Unfortunately, there are no translations into other languages for these pages, so a translation service should be used.\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
