<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-a80c730b.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-a80c730b.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/fetch-and-parse-data-with-openrefine"),
					params: {lang:"en",lessons:"lessons",slug:"fetch-and-parse-data-with-openrefine"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Fetching and Parsing Data from the Web with OpenRefine</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h1 id="lesson-goals">Lesson Goals</h1>
<p>OpenRefine is a powerful tool for exploring, cleaning, and transforming data.
An earlier Programming Historian lesson, <a href="/lessons/cleaning-data-with-openrefine">&quot;Cleaning Data with OpenRefine&quot;</a>, introduced the basic functionality of Refine to efficiently discover and correct inconsistency in a data set.
Building on those essential data wrangling skills, this lesson focuses on Refine&#39;s ability to fetch URLs and parse web content.
Examples introduce some of the advanced features to transform and enhance a data set including:</p>
<ul>
<li>fetch URLs using Refine</li>
<li>construct URL queries to retrieve information from a simple web API</li>
<li>parse HTML and JSON responses to extract relevant data</li>
<li>use array functions to manipulate string values</li>
<li>use Jython to extend Refine&#39;s functionality</li>
</ul>
<p>It will be helpful to have basic familiarity with <a href="/lessons/cleaning-data-with-openrefine">OpenRefine</a>, <a href="/lessons/viewing-html-files">HTML</a>, and programming concepts such as variables and loops to complete this lesson.</p>
<h2 id="why-use-openrefine">Why Use OpenRefine?</h2>
<p>The ability to create data sets from unstructured documents available on the web opens possibilities for research using digitized primary materials, web archives, texts, and contemporary media streams.
Programming Historian lessons introduce a number of methods to gather and interact with this content, from <a href="/lessons/applied-archival-downloading-with-wget">wget</a> to <a href="/lessons/intro-to-beautiful-soup">Python</a>.
When working with text documents, Refine is particularly suited for this task, allowing users to fetch urls and directly process the results in an iterative, exploratory manner.</p>
<p>David Huynh, the creator of Freebase Gridworks (2009) which became GoogleRefine (2010) and then OpenRefine (2012+), describes Refine as:</p>
<ul>
<li>more powerful than a spreadsheet</li>
<li>more interactive and visual than scripting</li>
<li>more provisional / exploratory / experimental / playful than a database [^huynh]</li>
</ul>
<p>Refine is a unique tool that combines the power of databases and scripting languages into an interactive and user friendly visual interface.
Because of this flexibility it has been embraced by <a href="https://www.propublica.org/nerds/item/using-google-refine-for-data-cleaning">journalists</a>, <a href="http://web.archive.org/web/20180129051941/http://data-lessons.github.io/library-openrefine/">librarians</a>, <a href="http://www.datacarpentry.org/OpenRefine-ecology-lesson/">scientists</a>, and others needing to wrangle data from diverse sources and formats into structured information.</p>
<p>{% include figure.html filename=&quot;openrefine.png&quot; caption=&quot;OpenRefine terminal and GUI&quot; %}</p>
<blockquote>
<p>OpenRefine is a <a href="https://www.gnu.org/philosophy/free-sw.en.html">free</a> and <a href="https://github.com/OpenRefine/OpenRefine">open source</a> Java application.
The user interface is rendered by your web browser, but Refine is not a web application. No information is sent online and no internet connection is necessary.
Full documentation is available on the <a href="https://github.com/OpenRefine/OpenRefine/wiki/">official wiki</a>.
For installation and staring Refine check this <a href="https://uidaholib.github.io/clean-your-data/3-start.html">workshop page</a>.</p>
<p>Note: this lesson was written using openrefine-2.7. Although almost all functionality is interchangeable between versions, I suggest using the newest version.</p>
</blockquote>
<h2 id="lesson-outline">Lesson Outline</h2>
<p>This lesson presents three examples demonstrating workflows to harvest and process data from the web:</p>
<ol>
<li><em><a href="#example-1-fetching-and-parsing-html">Example 1: Fetching and Parsing HTML</a></em> transforms an ebook into a structured data set by parsing the HTML and using string array functions.</li>
<li><em><a href="#example-2-url-queries-and-parsing-json">Example 2: URL Queries and Parsing JSON</a></em> interacts with a simple web API to construct a full text data set of historic newspaper front pages.</li>
<li><em><a href="#example-3-advanced-apis">Example 3: Advanced APIs</a></em> demonstrates using Jython to implement a POST request to a natural language processing web service.</li>
</ol>
<h1 id="example-1-fetching-and-parsing-html">Example 1: Fetching and Parsing HTML</h1>
<p>This example downloads a single web page and parses it into a structured table using Refine&#39;s built in functions.
A similar workflow can be applied to a list of URLs, often generated by parsing another web page, creating a flexible web harvesting tool.</p>
<p>The raw data for this example is an HTML copy of Shakespeare&#39;s <a href="http://www.gutenberg.org/ebooks/1105">Sonnets</a> from <a href="http://www.gutenberg.org/">Project Gutenberg</a>.
Processing a book of poems into structured data enables new ways of reading text, allowing us to sort, manipulate, and connect with other information.</p>
<div class="alert alert-warning">
Please note that Project Gutenberg provides <a href="http://www.gutenberg.org/wiki/Gutenberg:Feeds">feeds</a> to bulk download catalog data.
Their public website should not be used for web scraping purposes.
A copy of the HTML ebook is hosted on GitHub for this example to avoid redirects built in to the Gutenberg site.
</div>

<h2 id="start-sonnets-project">Start &quot;Sonnets&quot; Project</h2>
<p>Start OpenRefine and select <em>Create Project</em>.
Refine can import data from a wide variety of formats and sources, from a local Excel file to web accessible RDF.
One often over looked method is the <em>Clipboard</em>, which allows entering data via copy &amp; paste.
Under &quot;Get Data From&quot;, click <em>Clipboard</em>, and paste this URL into the text box:</p>
<pre><code>https://programminghistorian.org/assets/fetch-and-parse-data-with-openrefine/pg1105.html
</code></pre>
<p>{% include figure.html filename=&quot;refine-clipboard1.png&quot; caption=&quot;Start project with clipboard&quot; %}</p>
<p>After clicking <em>Next</em>, Refine should automatically identify the content as a line-based text file and the default parsing options should be correct.
Add the project name &quot;Sonnets&quot; at the top right and click <em>Create project</em>.
This will result in a project with one column and one row.</p>
<h2 id="fetch-html">Fetch HTML</h2>
<p>Refine&#39;s built-in function to retrieve a list of URLs is done by creating a new column.
Click on the menu arrow of <em>Column 1</em> &gt; <em>Edit column</em> &gt; <em>Add column by fetching urls</em>.</p>
<p>{% include figure.html caption=&quot;Edit column &gt; Add column by fetching URL&quot; filename=&quot;refine-fetch1.png&quot; %}</p>
<p>Name the new column &quot;fetch&quot;.
The <em>Throttle delay</em> option sets a pause time between requests to avoid being blocked by a server.
The default is conservative.</p>
<p>{% include figure.html caption=&quot;Add column by fetch dialog box&quot; filename=&quot;refine-fetch1.2.png&quot; %}</p>
<p>After clicking &quot;OK&quot;, Refine will start requesting the URLs from the base column as if you were opening the pages in your browser, and will store each response in the cells of the new column.
In this case, there is one URL in <em>Column 1</em> resulting in one cell in the <em>fetch</em> column containing the full HTML source for the Sonnets web page.</p>
<p>{% include figure.html caption=&quot;Fetch results&quot; filename=&quot;refine-fetch1.3.png&quot; %}</p>
<h2 id="parse-html">Parse HTML</h2>
<p>Much of the web page is not sonnet text and must be removed to create a clean data set.
First, it is necessary to identify a pattern that can isolate the desired content.
Items will often be nested in a unique container or given a meaningful class or id.</p>
<p>To make examining the HTML easier, click on the URL in <em>Column 1</em> to open the link in a new tab, then right click on the page to &quot;View Page Source&quot;.
In this case the sonnets page does not have distinctive semantic markup, but each poem is contained inside a single <code>&lt;p&gt;</code> element.
Thus, if all the paragraphs are selected, the sonnets can be extracted from the group.</p>
<p>{% include figure.html caption=&quot;Each sonnet is a &lt;p&gt; with lines separated by &lt;br /&gt;&quot; filename=&quot;refine-sonnet-markup.png&quot; %}</p>
<p>On the <em>fetch</em> column, click on the menu arrow &gt; <em>edit column</em> &gt; <em>Add column based on this column</em>.
Give the new column the name &quot;parse&quot;, then click in the <em>Expression</em> text box.</p>
<p>{% include figure.html caption=&quot;Edit column &gt; Add column based on this column&quot; filename=&quot;refine-expression-box.png&quot; %}</p>
<p>Data in Refine can be transformed using the <a href="https://github.com/OpenRefine/OpenRefine/wiki/General-Refine-Expression-Language">General Refine Expression Language</a> (GREL).
The <em>Expression</em> box accepts GREL functions that will be applied to each cell in the existing column to create values for the new one.
The <em>Preview</em> window below the <em>Expression</em> box displays the current value on the left and the value for the new column on the right.</p>
<p>The default expression is <code>value</code>, the <a href="https://github.com/OpenRefine/OpenRefine/wiki/Variables">GREL variable</a> representing the current contents of a cell.
This means that each cell is simply copied to the new column, which is reflected in the <em>Preview</em>.
GREL variables and functions are strung together in sequence using a period, called dot notation.
This allows complex operations to be constructed by passing the results of each function to the next.</p>
<p>GREL&#39;s <code>parseHtml()</code> function can read HTML content, allowing elements to be accessed using the <code>select()</code> function and the <a href="https://jsoup.org/cookbook/extracting-data/selector-syntax">jsoup selector syntax</a>.
Starting with <code>value</code>, add the functions <code>parseHtml()</code> and <code>select(&quot;p&quot;)</code> in the <em>Expression</em> box using dot notation, resulting in:</p>
<pre><code>value.parseHtml().select(&quot;p&quot;)
</code></pre>
<p>Do not click <em>OK</em> at this point, simply look at the <em>Preview</em> to see the result of the expression.</p>
<p>{% include figure.html caption=&quot;Edit the GREL expression, parseHtml function&quot; filename=&quot;refine-parse-html.png&quot; %}</p>
<p>Notice that the output on the right no longer starts with the HTML root elements (<code>&lt;!DOCTYPE html</code> etc.) seen on the left.
Instead, it starts with a square bracket <code>[</code>, displaying an <a href="https://en.wikipedia.org/wiki/Array_data_type">array</a> of all the <code>p</code> elements found in the page.
Refine represents an array as a comma separated list enclosed in square brackets, for example <code>[ &quot;one&quot;, &quot;two&quot;, &quot;three&quot; ]</code>.</p>
<p>Refine is visual and iterative; it is common to gradually build up an expression while checking the preview to see the result.
In addition to helping debug your GREL, this provides an opportunity learn more about the data set before adding more functions.
Try the following GREL statements in the <em>Expression</em> box without clicking <em>OK</em>.
Watch the preview window to understand how they function:</p>
<ul>
<li>Adding an index number to the expression selects one element from the array, for example <code>value.parseHtml().select(&quot;p&quot;)[0]</code>. The beginning of the sonnets file contains many paragraphs of license information that are unnecessary for the data set. Skipping ahead through the index numbers, the first sonnet is found at <code>value.parseHtml().select(&quot;p&quot;)[37]</code>.</li>
<li>GREL also supports using negative index numbers, thus <code>value.parseHtml().select(&quot;p&quot;)[-1]</code> will return the last item in the array. Working backwards, the last sonnet is at index <code>[-3]</code>.</li>
<li>Using these index numbers, it is possible to slice the array, extracting only the range of <code>p</code> that contain sonnets. Add the <code>slice()</code> function to the expression to preview the sub-set: <code>value.parseHtml().select(&quot;p&quot;).slice(37,-2)</code>.</li>
</ul>
<p>Clicking <em>OK</em> with the expression above will result in a blank column, a common cause of confusion when working with arrays.
Refine will not store an array object as a cell value.
It is necessary to use <code>toString()</code> or <code>join()</code> to convert the array into a string variable.
The <code>join()</code> function concatenates an array with the specified separator.
For example, the expression <code>[ &quot;one&quot;, &quot;two&quot;, &quot;three&quot; ].join(&quot;;&quot;)</code> will result in the string &quot;one;two;three&quot;.
Thus, the final expression to create the <em>parse</em> column is:</p>
<pre><code>value.parseHtml().select(&quot;p&quot;).slice(37,-2).join(&quot;|&quot;)
</code></pre>
<p>Click <em>OK</em> to create the new column using the expression.</p>
<div class="alert alert-warning">
Test out a transformation to see what happens--it is very easy to undo!
The full history of operations is recorded in the "Undo / Redo" tab.
</div>

<h2 id="split-cells">Split Cells</h2>
<p>The <em>parse</em> column now contains all the sonnets separated by &quot;|&quot;, but the project still contains only one row.
Individual rows for each sonnet can be created by splitting the cell.
Click the menu arrow on the <em>parse</em> column &gt; <em>Edit cells</em> &gt; <em>Split multi-valued cells</em>.
Enter the separator <code>|</code> that was used to <code>join</code> in the last step.</p>
<p>{% include figure.html caption=&quot;Edit cells &gt; Split multivalued cells&quot; filename=&quot;refine-split-multivalued.png&quot; %}</p>
<p>After this operation, the top of the project table should now read 154 rows.
Below the number is an option toggle &quot;Show as: <em>rows</em> <em>records</em>&quot;.
Clicking on <em>records</em> will group the rows based on the original table, in this case it will read 1.
Keeping track of these numbers is an important &quot;sanity check&quot; when transforming data in Refine.
The 154 rows make sense because the ebook contained 154 sonnets, while 1 record represents the original table with only one row.
An unexpected number would indicate a problem with the transformation.</p>
<p>{% include figure.html caption=&quot;Project rows&quot; filename=&quot;refine-rows.png&quot; %}</p>
<p>Each cell in the <em>parse</em> column now contains one sonnet surround by a <code>&lt;p&gt;</code> tag.
The tags can be cleaned up by parsing the HTML again.
Click on the <em>parse</em> column and select <em>Edit cells</em> &gt; <em>Transform</em>.
This will bring up a dialog box similar to creating a new column.
Transform will overwrite the cells of the current column rather than creating a new one.</p>
<p>In the expression box, type <code>value.parseHtml()</code>.
The preview will show a complete HTML tree starting with the <code>&lt;html&gt;</code> element.
It is important to note that <code>parseHtml()</code> will automatically fill in missing tags, allowing it to parse these cell values despite not being valid HTML documents.
Select the <code>p</code> tag, add an index number, and use the function <code>innerHtml()</code> to extract the sonnet text:</p>
<pre><code>value.parseHtml().select(&quot;p&quot;)[0].innerHtml()
</code></pre>
<p>Click <em>OK</em> to transform all 154 cells in the column.</p>
<p>{% include figure.html caption=&quot;Edit cells &gt; Transform&quot; filename=&quot;refine-innerhtml.png&quot; %}</p>
<div class="alert alert-warning">
In the expression above <code>select</code> returns an array of <code>p</code> elements even though there is only one in each cell.
Attempting to pass an array to <code>innerHtml()</code> will raise an error.
Thus, an index number is necessary to select the first (and only) item in the array to pass the correct object type to <code>innerHtml()</code>.
<br>Keep data object types in mind when debugging GREL expressions!
</div>

<h2 id="unescape">Unescape</h2>
<p>Notice that each cell has dozens of <code>&amp;nbsp;</code>, an HTML entity used to represent &quot;no-break space&quot; since browsers ignore extra white space in the source.
These entities are common when harvesting web pages and can be quickly replaced with the corresponding plain text characters using the <code>unescape()</code> function.
On the <em>parse</em> column, select <em>Edit cells</em> &gt; <em>Transform</em> and type the following in the expression box:</p>
<pre><code>value.unescape(&#39;html&#39;)
</code></pre>
<p>The entities will be replaced with normal whitespace.</p>
<h2 id="extract-information-with-array-functions">Extract Information with Array Functions</h2>
<p><a href="https://github.com/OpenRefine/OpenRefine/wiki/GREL-Array-Functions">GREL array functions</a> provide a powerful way to manipulate text data and can be used to finish processing the sonnets.
Any string value can be turned into an array using the <code>split()</code> function by providing the character or expression that separates the items (basically the opposite of <code>join()</code>).</p>
<p>In the sonnets each line ends with <code>&lt;br /&gt;</code>, providing a convenient separator for splitting.
The expression <code>value.split(&quot;&lt;br /&gt;&quot;)</code> will create an array of the lines of each sonnet.
Index numbers and slices can then be used to populate new columns.
Keep in mind that Refine will not output an array directly to a cell.
Be sure to select one element from the array using an index number or convert it back to a string with <code>join()</code>.</p>
<p>Furthermore, the sonnet text contains a huge amount of unnecessary white space that was used to layout the poems in the ebook.
This can be cut from each line using the <code>trim()</code> function.
Trim automatically removes all leading and trailing white space in a cell, an essential for data cleaning.</p>
<p>Using these concepts, a single line can be extracted and trimmed to create clean columns representing the sonnet number and first line.
Create two new columns from the <em>parse</em> column using these names and expressions:</p>
<ul>
<li>&quot;number&quot;, <code>value.split(&quot;&lt;br /&gt;&quot;)[0].trim()</code></li>
<li>&quot;first&quot;, <code>value.split(&quot;&lt;br /&gt;&quot;)[1].trim()</code></li>
</ul>
<p>{% include figure.html caption=&quot;GREL split and trim&quot; filename=&quot;refine-add-num-column.png&quot; %}</p>
<p>The next column to create is the full sonnet text which contains multiple lines.
However, <code>trim()</code> will only clean the beginning and end of the cell, leaving unnecessary whitespace in the body of the sonnet.
To trim each line individually use the <a href="https://github.com/OpenRefine/OpenRefine/wiki/GREL-Controls">GREL control</a> <code>forEach()</code>, a handy loop that iterates over an array.</p>
<p>From the <em>parse</em> column, create a new column named &quot;text&quot;, and click in the <em>Expression</em> box.
A <code>forEach()</code> statement asks for an array, a variable name, and an expression applied to the variable.
Following the form <code>forEach(array, variable, expression)</code>, construct the loop using these parameters:</p>
<ul>
<li>array: <code>value.split(&quot;&lt;br /&gt;&quot;)</code>, creates an array from the lines of the sonnet in each cell.</li>
<li>variable: <code>line</code>, each item in the array is then represented as the variable (it could be anything, <code>v</code> is often used).</li>
<li>expression: <code>line.trim()</code>, each item is then evaluated separately with the specified expression. In this case, <code>trim()</code> cleans the white space from each sonnet line in the array.</li>
</ul>
<p>At this point, the statement should look like <code>forEach(value.split(&quot;&lt;br /&gt;&quot;), line, line.trim())</code> in the <em>Expression</em> box.
Notice that the <em>Preview</em> now shows an array where the first element is the sonnet number.
Since the results of the <code>forEach()</code> are returned as a new array, additional array functions can be applied, such as slice and join.
Add <code>slice(1)</code> to remove the sonnet number, and <code>join(&quot;\n&quot;)</code> to concatenate the lines in to a string value (<code>\n</code> is the symbol for new line in plain text).
Thus, the final expression to extract and clean the full sonnet text is:</p>
<pre><code>forEach(value.split(&quot;&lt;br /&gt;&quot;), line, line.trim()).slice(1).join(&quot;\n&quot;)
</code></pre>
<p>{% include figure.html caption=&quot;GREL forEach expression&quot; filename=&quot;refine-foreach.png&quot; %}</p>
<p>Click &quot;OK&quot; to create the column.
Following the same technique, add another new column from <em>parse</em> named &quot;last&quot; to represent the final couplet lines using:</p>
<pre><code>forEach(value.split(&quot;&lt;br /&gt;&quot;), line, line.trim()).slice(-3).join(&quot;\n&quot;)
</code></pre>
<p>Finally, numeric columns can be added using the <code>length()</code> function.
Create new columns from <em>text</em> with the names and expressions below:</p>
<ul>
<li>&quot;characters&quot;, <code>value.length()</code></li>
<li>&quot;lines&quot;, <code>value.split(/\n/).length()</code></li>
</ul>
<h2 id="cleanup-and-export">Cleanup and Export</h2>
<p>In this example, we used a number of operations to create new columns with clean data.
This is a typical Refine workflow, allowing each transformation to be easily checked against the existing data.
At this point the unnecessary columns can be removed.
Click on the <em>All</em> column &gt; <em>Edit columns</em> &gt; <em>Re-order / remove columns</em>.</p>
<p>{% include figure.html caption=&quot;All &gt; Edit columns&quot; filename=&quot;refine-reorder.png&quot; %}</p>
<p>Drag unwanted column names to the right side of the dialog box, in this case <em>Column 1</em>, <em>fetch</em>, and <em>parse</em>.
Drag the remaining columns into the desired order on the left side.
Click <em>Ok</em> to remove and reorder the data set.</p>
<p>{% include figure.html caption=&quot;Re-order / Remove columns&quot; filename=&quot;refine-reorder2.png&quot; %}</p>
<p>Use filters and facets to explore and subset the collection of sonnets.
Then click the export button to generate a version of the new sonnet table for use outside of Refine.
Only the currently selected subset will be exported.</p>
<p>{% include figure.html caption=&quot;Export CSV&quot; filename=&quot;refine-export.png&quot; %}</p>
<h1 id="example-2-url-queries-and-parsing-json">Example 2: URL Queries and Parsing JSON</h1>
<p>Many cultural institutions provide web APIs allowing users to access information about their collections via simple <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol">HTTP</a> requests.
These sources enable new queries and aggregations of text that were previously impossible, cutting across boundaries of repositories and collections to support large scale analysis of both content and metadata.
This example will harvest data from the <a href="https://chroniclingamerica.loc.gov/">Chronicling America</a> project to assemble a small set of newspaper front pages with full text.
Following a common web scraping workflow, Refine is used to construct the query URL, fetch the information, and parse the JSON response.</p>
<div class="alert alert-warning">
Chronicling America is fully open, thus no key or account is needed to access the API and there are no limits on the use.
Other aggregators are often proprietary and restricted.
Please review the specific terms of use before web scraping or using the information in research.
</div>

<h2 id="start-chronicling-america-project">Start &quot;Chronicling America&quot; Project</h2>
<p>To get started after completing <em>Example 1</em>, click the <em>Open</em> button in the upper right.
A new tab will open with the Refine start project view.
The tab with the Sonnets project can be left open without impacting performance.
Create a project from <em>Clipboard</em> by pasting this CSV into the text box:</p>
<pre><code>state,year
Idaho,1865
Montana,1865
Oregon,1865
Washington,1865
</code></pre>
<p>After clicking <em>Next</em>, Refine should automatically identify the content as a CSV with the correct parsing options.
Add the <em>Project name</em> &quot;ChronAm&quot; at the top right and click <em>Create project</em>.</p>
<p>{% include figure.html caption=&quot;Create project&quot; filename=&quot;refine-start-project.png&quot; %}</p>
<h2 id="construct-a-query">Construct a Query</h2>
<p>Chronicling America provides <a href="https://chroniclingamerica.loc.gov/about/api/">documentation</a> for their API and URL patterns.
In addition to formal documentation, information about alternative formats and search API are sometimes given in the <code>&lt;head&gt;</code> element of a web page.
Check for <code>&lt;link rel=&quot;alternate&quot;</code>, <code>&lt;link rel=&quot;search&quot;</code>, or <code>&lt;!--</code> comments which provide hints on how to interact with the site.
These clues provide a recipe book for interacting with the server using public links.</p>
<p>The basic components of the ChromAm API are:</p>
<ul>
<li>the base URL, <code>https://chroniclingamerica.loc.gov/</code></li>
<li>the search service location for individual newspaper pages, <code>search/pages/results</code></li>
<li>a query string, starting with <code>?</code> and made up of value pairs (<code>fieldname=value</code>) separated by <code>&amp;</code>. Much like using the <a href="https://chroniclingamerica.loc.gov/#tab=tab_advanced_search">advanced search form</a>, the value pairs of the query string set the <a href="https://chroniclingamerica.loc.gov/search/pages/opensearch.xml">search options</a>.</li>
</ul>
<p>Using a GREL expression, these components can be combined with the values in the &quot;ChronAm&quot; project to construct a search query URL.
The contents of the data table can be accessed using <a href="https://github.com/OpenRefine/OpenRefine/wiki/Variables">GREL variables</a>.
As introduced in <em>Example 1</em>, the value of each cell in the current column is represented by <code>value</code>.
Values in the same row can be retrieved using the <code>cells</code> variable plus the column name.
There are two ways to write a <code>cells</code> statement: bracket notation <code>cells[&#39;column name&#39;].value</code> which allows column names that include a space, or dot notation <code>cells.column_name.value</code> which allows only single word column names.</p>
<p>In GREL, strings are concatenated using the plus sign.
For example, the expression <code>&quot;one&quot; + &quot;two&quot;</code> would result in &quot;onetwo&quot;.</p>
<p>To create the set of search queries, from the <em>state</em> column, add a column named &quot;url&quot; with this expression:</p>
<pre><code>&quot;https://chroniclingamerica.loc.gov/search/pages/results/?state=&quot; + value.escape(&#39;url&#39;) + &quot;&amp;date1=&quot; + cells[&#39;year&#39;].value.escape(&#39;url&#39;) + &quot;&amp;date2=&quot; + cells[&#39;year&#39;].value.escape(&#39;url&#39;) + &quot;&amp;dateFilterType=yearRange&amp;sequence=1&amp;sort=date&amp;rows=5&amp;format=json&quot;
</code></pre>
<p>{% include figure.html caption=&quot;Create query URL&quot; filename=&quot;refine-chronam-url.png&quot; %}</p>
<p>The expression concatenates the constants (base URL, search service, and query field names) together with the values in each row.
The <code>escape()</code> function is added to the cell variables to ensure the string will be safe in a URL (the opposite of the <code>unescape()</code> function introduced in <em>Example 1</em>).</p>
<p>Look at the value pairs after the <code>?</code> to understand the parameters of the search.
Explicitly, the first query URL will ask for newspapers:</p>
<ul>
<li>from Idaho (<code>state=Idaho</code>)</li>
<li>from the year 1865, (<code>date1=1865&amp;date2=1865&amp;dateFilterType=yearRange</code>)</li>
<li>only the front pages (<code>sequence=1</code>)</li>
<li>sorting by date (<code>sort=date</code>)</li>
<li>returning a maximum of five (<code>rows=5</code>)</li>
<li>in JSON (<code>format=json</code>)</li>
</ul>
<h2 id="fetch-urls">Fetch URLs</h2>
<p>The <em>url</em> column is a list of web queries that could be accessed with a browser.
To test, click one of the links.
The url will open in a new tab, returning a JSON response.</p>
<p>Fetch the URLs using <em>url</em> column by selecting <em>Edit column</em> &gt; <em>Add column by fetching urls</em>.
Name the new column &quot;fetch&quot; and click <em>OK</em>.
In a few seconds, the operation should complete and the <em>fetch</em> column will be filled with <a href="http://www.json.org/">JSON</a> data.</p>
<h2 id="parse-json-to-get-items">Parse JSON to Get Items</h2>
<p>The first name/value pairs of the query response look like <code>&quot;totalItems&quot;: 52, &quot;endIndex&quot;: 5</code>.
This indicates that the search resulted in 52 total items, but the response contains only five (since it was limited by the <code>rows=5</code> option).
The JSON key <code>items</code> contains an array of the individual newspapers returned by the search.
To construct a orderly data set, it is necessary to parse the JSON and split each newspaper into its own row.</p>
<p>GREL&#39;s <code>parseJson()</code> function allows us to select a key name to retrieve the corresponding values.
Add a new column based on <em>fetch</em> with the name &quot;items&quot; and enter this expression:</p>
<pre><code>value.parseJson()[&#39;items&#39;].join(&quot;|||&quot;)
</code></pre>
<p>{% include figure.html caption=&quot;parse json items&quot; filename=&quot;refine-parse-items.png&quot; %}</p>
<p>Selecting <code>[&#39;items&#39;]</code> exposes the array of newspaper records nested inside the JSON response.
The <code>join()</code> function concatenates the array with the given separator resulting in a string value.
Since the newspaper records contain an OCR text field, the strange separator &quot;|||&quot; is necessary to ensure that it is unique and can be used to split the values.</p>
<h2 id="split-multivalued-cells">Split Multivalued Cells</h2>
<p>With the individual newspapers isolated, separate rows can be created by splitting the cells.
On the <em>items</em> column, select <em>Edit cells</em> &gt;  <em>Split multivalued cells</em>, and enter the join used in the last step, <code>|||</code>.
After the operation, the top of the project table should read 20 rows.
Clicking on Show as <em>records</em> should read 4, representing the original CSV rows.</p>
<p>Notice that the new rows are empty in all columns except <em>items</em>.
To ensure the state is available with each newspaper issue, the empty values can be filled using the <code>Fill down</code> function.
Click on the <em>state</em> column &gt; <em>Edit cells</em> &gt; <em>Fill down</em>.</p>
<p>{% include figure.html caption=&quot;fill down&quot; filename=&quot;refine-fill-down.png&quot; %}</p>
<p>This is a good point to clean up the unnecessary columns.
Click on the <em>All</em> column &gt; <em>Edit columns</em> &gt; <em>Re-order / remove columns</em>.
Drag all columns except <em>state</em> and <em>items</em> to the right side, then click <em>OK</em> to remove them.</p>
<p>{% include figure.html caption=&quot;Re-order / remove columns&quot; filename=&quot;refine-chronam-reorder.png&quot; %}</p>
<p>Sanity check: with the original columns removed, both <em>records</em> and <em>rows</em> will read 20.
This makes sense, as the project started with four states and fetched five records for each.</p>
<h2 id="parse-json-values">Parse JSON Values</h2>
<p>To complete the data set, it is necessary to parse each newspaper&#39;s JSON record into individual columns.
This is a common task, as many web APIs return information in JSON format.
Again, GREL&#39;s <code>parseJson()</code> function makes this easy.
Create a new column from <em>items</em> for each newspaper metadata element by parsing the JSON and selecting the key:</p>
<ul>
<li>&quot;date&quot;, <code>value.parseJson()[&#39;date&#39;]</code></li>
<li>&quot;title&quot;, <code>value.parseJson()[&#39;title&#39;]</code></li>
<li>&quot;city&quot;, <code>value.parseJson()[&#39;city&#39;].join(&quot;, &quot;)</code></li>
<li>&quot;lccn&quot;, <code>value.parseJson()[&#39;lccn&#39;]</code></li>
<li>&quot;text&quot;, <code>value.parseJson()[&#39;ocr_eng&#39;]</code></li>
</ul>
<p>After the desired information is extracted, the <em>items</em> column can be removed by selecting <em>Edit column</em> &gt; <em>Remove this column</em>.</p>
<p>{% include figure.html caption=&quot;Final ChronAm project columns&quot; filename=&quot;refine-chronam-final.png&quot; %}</p>
<p>Each column could be further refined using other GREL transformations.
For example, to convert <em>date</em> to a more readable format, use <a href="https://github.com/OpenRefine/OpenRefine/wiki/GREL-Date-Functions">GREL date functions</a>.
Transform the <em>date</em> column with the expression <code>value.toDate(&quot;yyyymmdd&quot;).toString(&quot;yyyy-MM-dd&quot;)</code>.</p>
<p>Another common workflow is to extend the data with further URL queries.
For example, a link to full information about each issue can be formed based on the <em>lccn</em>.
Create a new column based on <em>lccn</em> using the expression <code>&quot;https://chroniclingamerica.loc.gov/lccn/&quot; + value + &quot;/&quot; + cells[&#39;date&#39;].value + &quot;/ed-1.json&quot;</code>.
Fetching this URL returns a complete list of the issue&#39;s pages, which could in turn be harvested.</p>
<p>For now, the headlines of 1865 from the Northwest are ready to enjoy!</p>
<h1 id="example-3-advanced-apis">Example 3: Advanced APIs</h1>
<p><em><a href="#example-2-url-queries-and-parsing-json">Example 2</a></em> demonstrated Refine&#39;s fetch function with a simple web API, essentially utilizing URL patterns to request information from a server.
This workflow uses the HTTP GET protocol, meaning the query is encoded in the URL string, thus limited in length (2048 ASCII characters), complexity, and security.
Instead, many API services used to enhance text data, such as <a href="https://en.wikipedia.org/wiki/Geocoding">geocoding</a> or <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">named entity recognition</a>, use HTTP POST to transfer information to the server for processing.</p>
<p>GREL does not have a built in function to use this type of API.
However, the expression window language can be changed to <a href="http://www.jython.org/">Jython</a>, providing a more complete scripting environment where it is possible to implement a POST request.</p>
<blockquote>
<p><a href="http://www.jython.org/">Jython</a> is Python implemented for the Java VM and comes bundled with Refine.
This means <a href="https://docs.python.org/2.7/">Python 2</a> scripts using the Standard Library can be written or loaded into the expression window, and Refine will apply them to each cell in the transformation.
The <a href="https://github.com/OpenRefine/OpenRefine/wiki/Jython">official documentation</a> is sparse, but the built-in Jython can be extended with non-standard libraries using a <a href="https://github.com/OpenRefine/OpenRefine/wiki/Extending-Jython-with-pypi-modules">work around</a>.</p>
<p>Keep in mind that spending time writing complex scripts moves away from the strengths of Refine.
If it is necessary to develop a lengthy Jython routine, it will likely be more efficient to process the data directly in Python.
On the other hand, if you know a handy method to process data in Python 2, Jython is a easy way to apply it in a Refine project.</p>
</blockquote>
<h2 id="jython-in-the-expression-window">Jython in the Expression Window</h2>
<p>Return to the &quot;Sonnets&quot; project completed in <em><a href="#example-1-fetching-and-parsing-html">Example 1</a></em>.
If the tab was closed, click <em>Open</em> &gt; <em>Open Project</em> and find the Sonnets example (Refine saves everything for you!).</p>
<p>Add a new column based on the <em>first</em> column named &quot;sentiment&quot;.
We will use this window to test out a series of expressions, so leave it open until we get to the final iteration of the request.</p>
<p>On the right side of the <em>Expression</em> box is a drop down to change the expression language.
Select <em>Python / Jython</em> from the list.</p>
<p>{% include figure.html caption=&quot;Jython expression&quot; filename=&quot;refine-jython.png&quot; %}</p>
<p>Notice that the preview now shows <code>null</code> for the output.
A Jython expression in Refine must have a <code>return</code> statement to add the output to the new cells in the transformation.
Type <code>return value</code> into the <em>Expression</em> box.
The preview will update showing the current cells copied to the output.
The basic <a href="https://github.com/OpenRefine/OpenRefine/wiki/Variables">GREL variables</a> can be used in Jython by substituting brackets instead of periods.
For example, the GREL <code>cells.column-name.value</code> would be Jython <code>cells[&#39;column-name&#39;][&#39;value&#39;]</code>.</p>
<h2 id="jython-get-request">Jython GET Request</h2>
<p>To create a HTTP request in Jython, use the standard library <a href="https://docs.python.org/2/library/urllib2.html">urllib2</a>.
Refine&#39;s fetch function can be recreated with Jython to demonstrate the basics of the library.
In the expression box, type:</p>
<pre><code>import urllib2
get = urllib2.urlopen(&quot;http://www.jython.org/&quot;)
return get.read()
</code></pre>
<p>{% include figure.html caption=&quot;Jython GET request&quot; filename=&quot;refine-jython-expression.png&quot; %}</p>
<p>The preview should display the HTML source of the Jython home page, this is an HTTP GET request as in previous fetch examples.
Notice that similar to opening and reading a text file with Python, <code>urlopen()</code> returns a file-like object that must be <code>read()</code> into a string.
The URL could be replaced with cell variables to construct a query similar to the fetch used in <em>Example 2</em>.</p>
<h2 id="post-request">POST Request</h2>
<p>Urllib2 will automatically send a POST if data is added to the request object.
For example, <a href="http://text-processing.com/">Text Processing</a> provides natural language processing APIs based on <a href="http://www.nltk.org/">Python NLTK</a>.
The documentation for the <a href="http://text-processing.com/docs/sentiment.html">Sentiment Analysis service</a> provides a base URL and the name of the key used for the data to be analyzed.
No authentication is required and 1,000 calls per day are free for non-commercial use.[^use]</p>
<p>This type of API is often demonstrated using <a href="https://curl.haxx.se/">curl</a> on the commandline.
Text Processing gives the example <code>curl -d &quot;text=great&quot; http://text-processing.com/api/sentiment/</code> which can be recreated in Jython to test the service.
Building on the GET expression above, the POST data is added as the second parameter of <em>urlopen</em>, thus the request will be in the form <code>urllib2.urlopen(url, data)</code>.
Type this script into the expression window:</p>
<pre><code>import urllib2
url = &quot;http://text-processing.com/api/sentiment/&quot;
data = &quot;text=what is the sentiment of this sentence&quot;
post = urllib2.urlopen(url, data)
return post.read()
</code></pre>
<p>The preview should show a JSON response with sentiment probability values.
To retrieve sentiment analysis data for the first lines of the sonnets (remember we are still adding a column based on <em>first</em>!), put the basic Jython pattern together with the values of the cells:</p>
<pre><code>import urllib2
url = &quot;http://text-processing.com/api/sentiment/&quot;
data = &quot;text=&quot; + value
post = urllib2.urlopen(url, data)
return post.read()
</code></pre>
<p>{% include figure.html caption=&quot;jython request&quot; filename=&quot;refine-jython-request.png&quot; %}</p>
<p>Click <em>OK</em> and the Jython script will run for every row in the column.
The JSON response can then be parsed with GREL using the methods demonstrated in <em>Example 2</em> (for example, <code>value.parseJson()[&#39;label&#39;]</code>).</p>
<p>Given the small expression window and uniform data, the script above is pragmatically simplified and compressed.
When Refine is encountering problems, it is better to implement a more complete script with error handling.
If necessary, a throttle delay can be implemented by importing <code>time</code> and adding <code>time.sleep()</code> to the script.
For example, the POST request could be rewritten:</p>
<pre><code>import urllib2, urllib, time
time.sleep(15)
url = &quot;http://text-processing.com/api/sentiment/&quot;
data = urllib.urlencode({&quot;text&quot;: value.encode(&quot;utf-8&quot;)})
req = urllib2.Request(url,data)
try:
    post = urllib2.urlopen(req)
except urllib2.URLError as e:
    if hasattr(e, &quot;reason&quot;):
        return &quot;Failed: &quot;, e.reason
    elif hasattr(e, &quot;code&quot;):
        return &quot;Error code: &quot;, e.code
else:
    response = post.read()
    return response
</code></pre>
<blockquote>
<p>Some APIs require authentication tokens to be passed with the POST request as data or headers.
Headers can be added as the third parameter of <code>urllib2.Request()</code> similar to how data was added in the example above.
Check the Python <a href="https://docs.python.org/2/library/urllib2.html">urllib2 documentation</a> and <a href="https://docs.python.org/2/howto/urllib2.html">how-to</a> for advanced options.</p>
<p>When harvesting web content, character encoding issues commonly produce errors in Python.
Trimming whitespace, using GREL <code>escape()</code> / <code>unescape()</code>, or Jython <code>encode(&quot;utf-8&quot;)</code> will often fix the problem.</p>
</blockquote>
<h2 id="compare-sentiment">Compare Sentiment</h2>
<p>To practice constructing a POST request, read the documentation for <a href="http://sentiment.vivekn.com/docs/api/">Sentiment Tool</a>, another free API.
Find the service URL and data key necessary to modify the Jython pattern above.
Create a new column from <em>first</em> named <code>sentiment2</code> and test the script.</p>
<p>There are many possible ways to create the request, for example:</p>
<pre><code>import urllib2
url = &quot;http://sentiment.vivekn.com/api/text/&quot;
data = &quot;txt=&quot; + value
post = urllib2.urlopen(url, data)
return post.read()
</code></pre>
<p>The JSON response contains different metrics, but it will be obvious that the APIs disagree on many of the sentiment &quot;labels&quot; (for example, use <code>value.parseJson()[&#39;result&#39;][&#39;sentiment&#39;]</code> to extract a label comparable to the first API).
These are simple free APIs for demonstration purposes, but it is important to critically investigate services to more fully understand the potential of the metrics.</p>
<p>Both APIs use a <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">naive bayes classifier</a> to categorize text input.
These models must be trained on pre-labeled data and will be most accurate on similar content.
Text Processing is trained on twitter and movie reviews[^1], and Sentiment Tool on IMDb movie reviews[^2].
Thus both are optimized for small chunks of modern English language similar to a review, with a limited bag of words used to determine the sentiment probabilities.</p>
<p>Archaic words and phrases contribute significantly to the sonnets&#39; sentiment, yet are unlikely to be given any weight in these models since they are not present in the training data.
While comparing the metrics is fascinating, neither is likely to produce quality results for this data set.
Rather than an accurate sentiment, we might be surprised to find a quantifiable dissonance between the sonnet&#39;s English and our modern web usage.
However, a model optimized to Shakespeare&#39;s words could be developed using more appropriate training data.
To learn more about classifiers and how to implement one, see Vilja Hulden&#39;s PH lesson <a href="/lessons/naive-bayesian">&quot;Supervised Classification: The Naive Bayesian Returns to the Old Bailey&quot;</a> or Steven Bird, Ewan Klein, and Edward Loper&#39;s <a href="http://www.nltk.org/book/ch06.html">&quot;Learning to Classify Text&quot;</a> in the <a href="http://www.nltk.org/book/">NTLK Book</a>.</p>
<p>Accessing data and services on the web opens new possibilities and efficiencies for humanities research.
While powerful, these APIs are often not aimed at humanities scholarship and may not be appropriate or optimized for our inquiries.
The training data may be incomplete, biased, or secret.
We should always be asking questions about these aggregations and algorithms, thinking critically about the metrics they are capable of producing.
This is not a new technical skill, but an application of the historian&#39;s traditional expertise, not unlike interrogating physical primary materials to unravel bias and read between the lines.
Humanities scholars routinely synthesize and evaluate convoluted sources to tell important narratives, and must carry that skill into digital realm.
We can critically evaluate data sources, algorithms, and API services, as well as create new ones more suited to our questions and methods.</p>
<p>With its unique ability to interactively wrangle data from raw aggregation to analysis, Refine supports exploratory research and offers a wonderfully fluid and playful approach to tabular data.
OpenRefine is a flexible, pragmatic tool that simplifies routine tasks and, when combined with domain knowledge, extends research capabilities.</p>
<p>[^huynh]: David Huynh, &quot;Google Refine&quot;, Computer-Assisted Reporting Conference 2011, <a href="http://web.archive.org/web/20150528125345/http://davidhuynh.net/spaces/nicar2011/tutorial.pdf">http://web.archive.org/web/20150528125345/http://davidhuynh.net/spaces/nicar2011/tutorial.pdf</a>.
[^use]: As of July 2017, see <a href="http://text-processing.com/docs/index.html">API Documentation</a>.
[^1]: Jacob Perkins, &quot;Sentiment Analysis with Python NLTK Text Classification&quot;, <a href="http://text-processing.com/demo/sentiment/">http://text-processing.com/demo/sentiment/</a>.
[^2]: Vivek Narayanan, Ishan Arora, and Arjun Bhatia, &quot;Fast and accurate sentiment classification using an enhanced Naive Bayes model&quot;, 2013, <a href="https://arxiv.org/abs/1305.6143">arXiv:1305.6143</a>.</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="fetch-and-parse-data-with-openrefine/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Fetching and Parsing Data from the Web with OpenRefine\",\"layout\":\"lesson\",\"date\":\"2017-08-12T00:00:00.000Z\",\"authors\":[\"Evan Peter Williamson\"],\"reviewers\":[\"Peggy Griesinger\",\"Lisa Lowry\"],\"editors\":[\"Jeri Wieringa\"],\"difficulty\":2,\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F69\",\"activity\":\"acquiring\",\"topics\":[\"data-manipulation\",\"web-scraping\",\"api\"],\"abstract\":\"OpenRefine is a powerful tool for exploring, cleaning, and transforming data. In this lesson you will learn how to use Refine to fetch URLs and parse web content.\",\"redirect_from\":\"\u002Flessons\u002Ffetch-and-parse-data-with-openrefine\",\"avatar_alt\":\"Machine for water filtration\",\"doi\":\"10.46430\u002Fphen0065\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"lesson-goals\\\"\u003ELesson Goals\u003C\u002Fh1\u003E\\n\u003Cp\u003EOpenRefine is a powerful tool for exploring, cleaning, and transforming data.\\nAn earlier Programming Historian lesson, \u003Ca href=\\\"\u002Flessons\u002Fcleaning-data-with-openrefine\\\"\u003E&quot;Cleaning Data with OpenRefine&quot;\u003C\u002Fa\u003E, introduced the basic functionality of Refine to efficiently discover and correct inconsistency in a data set.\\nBuilding on those essential data wrangling skills, this lesson focuses on Refine&#39;s ability to fetch URLs and parse web content.\\nExamples introduce some of the advanced features to transform and enhance a data set including:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003Efetch URLs using Refine\u003C\u002Fli\u003E\\n\u003Cli\u003Econstruct URL queries to retrieve information from a simple web API\u003C\u002Fli\u003E\\n\u003Cli\u003Eparse HTML and JSON responses to extract relevant data\u003C\u002Fli\u003E\\n\u003Cli\u003Euse array functions to manipulate string values\u003C\u002Fli\u003E\\n\u003Cli\u003Euse Jython to extend Refine&#39;s functionality\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EIt will be helpful to have basic familiarity with \u003Ca href=\\\"\u002Flessons\u002Fcleaning-data-with-openrefine\\\"\u003EOpenRefine\u003C\u002Fa\u003E, \u003Ca href=\\\"\u002Flessons\u002Fviewing-html-files\\\"\u003EHTML\u003C\u002Fa\u003E, and programming concepts such as variables and loops to complete this lesson.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"why-use-openrefine\\\"\u003EWhy Use OpenRefine?\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe ability to create data sets from unstructured documents available on the web opens possibilities for research using digitized primary materials, web archives, texts, and contemporary media streams.\\nProgramming Historian lessons introduce a number of methods to gather and interact with this content, from \u003Ca href=\\\"\u002Flessons\u002Fapplied-archival-downloading-with-wget\\\"\u003Ewget\u003C\u002Fa\u003E to \u003Ca href=\\\"\u002Flessons\u002Fintro-to-beautiful-soup\\\"\u003EPython\u003C\u002Fa\u003E.\\nWhen working with text documents, Refine is particularly suited for this task, allowing users to fetch urls and directly process the results in an iterative, exploratory manner.\u003C\u002Fp\u003E\\n\u003Cp\u003EDavid Huynh, the creator of Freebase Gridworks (2009) which became GoogleRefine (2010) and then OpenRefine (2012+), describes Refine as:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003Emore powerful than a spreadsheet\u003C\u002Fli\u003E\\n\u003Cli\u003Emore interactive and visual than scripting\u003C\u002Fli\u003E\\n\u003Cli\u003Emore provisional \u002F exploratory \u002F experimental \u002F playful than a database [^huynh]\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003ERefine is a unique tool that combines the power of databases and scripting languages into an interactive and user friendly visual interface.\\nBecause of this flexibility it has been embraced by \u003Ca href=\\\"https:\u002F\u002Fwww.propublica.org\u002Fnerds\u002Fitem\u002Fusing-google-refine-for-data-cleaning\\\"\u003Ejournalists\u003C\u002Fa\u003E, \u003Ca href=\\\"http:\u002F\u002Fweb.archive.org\u002Fweb\u002F20180129051941\u002Fhttp:\u002F\u002Fdata-lessons.github.io\u002Flibrary-openrefine\u002F\\\"\u003Elibrarians\u003C\u002Fa\u003E, \u003Ca href=\\\"http:\u002F\u002Fwww.datacarpentry.org\u002FOpenRefine-ecology-lesson\u002F\\\"\u003Escientists\u003C\u002Fa\u003E, and others needing to wrangle data from diverse sources and formats into structured information.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;openrefine.png&quot; caption=&quot;OpenRefine terminal and GUI&quot; %}\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EOpenRefine is a \u003Ca href=\\\"https:\u002F\u002Fwww.gnu.org\u002Fphilosophy\u002Ffree-sw.en.html\\\"\u003Efree\u003C\u002Fa\u003E and \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\\\"\u003Eopen source\u003C\u002Fa\u003E Java application.\\nThe user interface is rendered by your web browser, but Refine is not a web application. No information is sent online and no internet connection is necessary.\\nFull documentation is available on the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002F\\\"\u003Eofficial wiki\u003C\u002Fa\u003E.\\nFor installation and staring Refine check this \u003Ca href=\\\"https:\u002F\u002Fuidaholib.github.io\u002Fclean-your-data\u002F3-start.html\\\"\u003Eworkshop page\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ENote: this lesson was written using openrefine-2.7. Although almost all functionality is interchangeable between versions, I suggest using the newest version.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Ch2 id=\\\"lesson-outline\\\"\u003ELesson Outline\u003C\u002Fh2\u003E\\n\u003Cp\u003EThis lesson presents three examples demonstrating workflows to harvest and process data from the web:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Cem\u003E\u003Ca href=\\\"#example-1-fetching-and-parsing-html\\\"\u003EExample 1: Fetching and Parsing HTML\u003C\u002Fa\u003E\u003C\u002Fem\u003E transforms an ebook into a structured data set by parsing the HTML and using string array functions.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cem\u003E\u003Ca href=\\\"#example-2-url-queries-and-parsing-json\\\"\u003EExample 2: URL Queries and Parsing JSON\u003C\u002Fa\u003E\u003C\u002Fem\u003E interacts with a simple web API to construct a full text data set of historic newspaper front pages.\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cem\u003E\u003Ca href=\\\"#example-3-advanced-apis\\\"\u003EExample 3: Advanced APIs\u003C\u002Fa\u003E\u003C\u002Fem\u003E demonstrates using Jython to implement a POST request to a natural language processing web service.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch1 id=\\\"example-1-fetching-and-parsing-html\\\"\u003EExample 1: Fetching and Parsing HTML\u003C\u002Fh1\u003E\\n\u003Cp\u003EThis example downloads a single web page and parses it into a structured table using Refine&#39;s built in functions.\\nA similar workflow can be applied to a list of URLs, often generated by parsing another web page, creating a flexible web harvesting tool.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe raw data for this example is an HTML copy of Shakespeare&#39;s \u003Ca href=\\\"http:\u002F\u002Fwww.gutenberg.org\u002Febooks\u002F1105\\\"\u003ESonnets\u003C\u002Fa\u003E from \u003Ca href=\\\"http:\u002F\u002Fwww.gutenberg.org\u002F\\\"\u003EProject Gutenberg\u003C\u002Fa\u003E.\\nProcessing a book of poems into structured data enables new ways of reading text, allowing us to sort, manipulate, and connect with other information.\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\nPlease note that Project Gutenberg provides \u003Ca href=\\\"http:\u002F\u002Fwww.gutenberg.org\u002Fwiki\u002FGutenberg:Feeds\\\"\u003Efeeds\u003C\u002Fa\u003E to bulk download catalog data.\\nTheir public website should not be used for web scraping purposes.\\nA copy of the HTML ebook is hosted on GitHub for this example to avoid redirects built in to the Gutenberg site.\\n\u003C\u002Fdiv\u003E\\n\\n\u003Ch2 id=\\\"start-sonnets-project\\\"\u003EStart &quot;Sonnets&quot; Project\u003C\u002Fh2\u003E\\n\u003Cp\u003EStart OpenRefine and select \u003Cem\u003ECreate Project\u003C\u002Fem\u003E.\\nRefine can import data from a wide variety of formats and sources, from a local Excel file to web accessible RDF.\\nOne often over looked method is the \u003Cem\u003EClipboard\u003C\u002Fem\u003E, which allows entering data via copy &amp; paste.\\nUnder &quot;Get Data From&quot;, click \u003Cem\u003EClipboard\u003C\u002Fem\u003E, and paste this URL into the text box:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Ehttps:\u002F\u002Fprogramminghistorian.org\u002Fassets\u002Ffetch-and-parse-data-with-openrefine\u002Fpg1105.html\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;refine-clipboard1.png&quot; caption=&quot;Start project with clipboard&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EAfter clicking \u003Cem\u003ENext\u003C\u002Fem\u003E, Refine should automatically identify the content as a line-based text file and the default parsing options should be correct.\\nAdd the project name &quot;Sonnets&quot; at the top right and click \u003Cem\u003ECreate project\u003C\u002Fem\u003E.\\nThis will result in a project with one column and one row.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"fetch-html\\\"\u003EFetch HTML\u003C\u002Fh2\u003E\\n\u003Cp\u003ERefine&#39;s built-in function to retrieve a list of URLs is done by creating a new column.\\nClick on the menu arrow of \u003Cem\u003EColumn 1\u003C\u002Fem\u003E &gt; \u003Cem\u003EEdit column\u003C\u002Fem\u003E &gt; \u003Cem\u003EAdd column by fetching urls\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Edit column &gt; Add column by fetching URL&quot; filename=&quot;refine-fetch1.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EName the new column &quot;fetch&quot;.\\nThe \u003Cem\u003EThrottle delay\u003C\u002Fem\u003E option sets a pause time between requests to avoid being blocked by a server.\\nThe default is conservative.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Add column by fetch dialog box&quot; filename=&quot;refine-fetch1.2.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EAfter clicking &quot;OK&quot;, Refine will start requesting the URLs from the base column as if you were opening the pages in your browser, and will store each response in the cells of the new column.\\nIn this case, there is one URL in \u003Cem\u003EColumn 1\u003C\u002Fem\u003E resulting in one cell in the \u003Cem\u003Efetch\u003C\u002Fem\u003E column containing the full HTML source for the Sonnets web page.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Fetch results&quot; filename=&quot;refine-fetch1.3.png&quot; %}\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"parse-html\\\"\u003EParse HTML\u003C\u002Fh2\u003E\\n\u003Cp\u003EMuch of the web page is not sonnet text and must be removed to create a clean data set.\\nFirst, it is necessary to identify a pattern that can isolate the desired content.\\nItems will often be nested in a unique container or given a meaningful class or id.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo make examining the HTML easier, click on the URL in \u003Cem\u003EColumn 1\u003C\u002Fem\u003E to open the link in a new tab, then right click on the page to &quot;View Page Source&quot;.\\nIn this case the sonnets page does not have distinctive semantic markup, but each poem is contained inside a single \u003Ccode\u003E&lt;p&gt;\u003C\u002Fcode\u003E element.\\nThus, if all the paragraphs are selected, the sonnets can be extracted from the group.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Each sonnet is a &lt;p&gt; with lines separated by &lt;br \u002F&gt;&quot; filename=&quot;refine-sonnet-markup.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EOn the \u003Cem\u003Efetch\u003C\u002Fem\u003E column, click on the menu arrow &gt; \u003Cem\u003Eedit column\u003C\u002Fem\u003E &gt; \u003Cem\u003EAdd column based on this column\u003C\u002Fem\u003E.\\nGive the new column the name &quot;parse&quot;, then click in the \u003Cem\u003EExpression\u003C\u002Fem\u003E text box.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Edit column &gt; Add column based on this column&quot; filename=&quot;refine-expression-box.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EData in Refine can be transformed using the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002FGeneral-Refine-Expression-Language\\\"\u003EGeneral Refine Expression Language\u003C\u002Fa\u003E (GREL).\\nThe \u003Cem\u003EExpression\u003C\u002Fem\u003E box accepts GREL functions that will be applied to each cell in the existing column to create values for the new one.\\nThe \u003Cem\u003EPreview\u003C\u002Fem\u003E window below the \u003Cem\u003EExpression\u003C\u002Fem\u003E box displays the current value on the left and the value for the new column on the right.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe default expression is \u003Ccode\u003Evalue\u003C\u002Fcode\u003E, the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002FVariables\\\"\u003EGREL variable\u003C\u002Fa\u003E representing the current contents of a cell.\\nThis means that each cell is simply copied to the new column, which is reflected in the \u003Cem\u003EPreview\u003C\u002Fem\u003E.\\nGREL variables and functions are strung together in sequence using a period, called dot notation.\\nThis allows complex operations to be constructed by passing the results of each function to the next.\u003C\u002Fp\u003E\\n\u003Cp\u003EGREL&#39;s \u003Ccode\u003EparseHtml()\u003C\u002Fcode\u003E function can read HTML content, allowing elements to be accessed using the \u003Ccode\u003Eselect()\u003C\u002Fcode\u003E function and the \u003Ca href=\\\"https:\u002F\u002Fjsoup.org\u002Fcookbook\u002Fextracting-data\u002Fselector-syntax\\\"\u003Ejsoup selector syntax\u003C\u002Fa\u003E.\\nStarting with \u003Ccode\u003Evalue\u003C\u002Fcode\u003E, add the functions \u003Ccode\u003EparseHtml()\u003C\u002Fcode\u003E and \u003Ccode\u003Eselect(&quot;p&quot;)\u003C\u002Fcode\u003E in the \u003Cem\u003EExpression\u003C\u002Fem\u003E box using dot notation, resulting in:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Evalue.parseHtml().select(&quot;p&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EDo not click \u003Cem\u003EOK\u003C\u002Fem\u003E at this point, simply look at the \u003Cem\u003EPreview\u003C\u002Fem\u003E to see the result of the expression.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Edit the GREL expression, parseHtml function&quot; filename=&quot;refine-parse-html.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ENotice that the output on the right no longer starts with the HTML root elements (\u003Ccode\u003E&lt;!DOCTYPE html\u003C\u002Fcode\u003E etc.) seen on the left.\\nInstead, it starts with a square bracket \u003Ccode\u003E[\u003C\u002Fcode\u003E, displaying an \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FArray_data_type\\\"\u003Earray\u003C\u002Fa\u003E of all the \u003Ccode\u003Ep\u003C\u002Fcode\u003E elements found in the page.\\nRefine represents an array as a comma separated list enclosed in square brackets, for example \u003Ccode\u003E[ &quot;one&quot;, &quot;two&quot;, &quot;three&quot; ]\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ERefine is visual and iterative; it is common to gradually build up an expression while checking the preview to see the result.\\nIn addition to helping debug your GREL, this provides an opportunity learn more about the data set before adding more functions.\\nTry the following GREL statements in the \u003Cem\u003EExpression\u003C\u002Fem\u003E box without clicking \u003Cem\u003EOK\u003C\u002Fem\u003E.\\nWatch the preview window to understand how they function:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EAdding an index number to the expression selects one element from the array, for example \u003Ccode\u003Evalue.parseHtml().select(&quot;p&quot;)[0]\u003C\u002Fcode\u003E. The beginning of the sonnets file contains many paragraphs of license information that are unnecessary for the data set. Skipping ahead through the index numbers, the first sonnet is found at \u003Ccode\u003Evalue.parseHtml().select(&quot;p&quot;)[37]\u003C\u002Fcode\u003E.\u003C\u002Fli\u003E\\n\u003Cli\u003EGREL also supports using negative index numbers, thus \u003Ccode\u003Evalue.parseHtml().select(&quot;p&quot;)[-1]\u003C\u002Fcode\u003E will return the last item in the array. Working backwards, the last sonnet is at index \u003Ccode\u003E[-3]\u003C\u002Fcode\u003E.\u003C\u002Fli\u003E\\n\u003Cli\u003EUsing these index numbers, it is possible to slice the array, extracting only the range of \u003Ccode\u003Ep\u003C\u002Fcode\u003E that contain sonnets. Add the \u003Ccode\u003Eslice()\u003C\u002Fcode\u003E function to the expression to preview the sub-set: \u003Ccode\u003Evalue.parseHtml().select(&quot;p&quot;).slice(37,-2)\u003C\u002Fcode\u003E.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EClicking \u003Cem\u003EOK\u003C\u002Fem\u003E with the expression above will result in a blank column, a common cause of confusion when working with arrays.\\nRefine will not store an array object as a cell value.\\nIt is necessary to use \u003Ccode\u003EtoString()\u003C\u002Fcode\u003E or \u003Ccode\u003Ejoin()\u003C\u002Fcode\u003E to convert the array into a string variable.\\nThe \u003Ccode\u003Ejoin()\u003C\u002Fcode\u003E function concatenates an array with the specified separator.\\nFor example, the expression \u003Ccode\u003E[ &quot;one&quot;, &quot;two&quot;, &quot;three&quot; ].join(&quot;;&quot;)\u003C\u002Fcode\u003E will result in the string &quot;one;two;three&quot;.\\nThus, the final expression to create the \u003Cem\u003Eparse\u003C\u002Fem\u003E column is:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Evalue.parseHtml().select(&quot;p&quot;).slice(37,-2).join(&quot;|&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EClick \u003Cem\u003EOK\u003C\u002Fem\u003E to create the new column using the expression.\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\nTest out a transformation to see what happens--it is very easy to undo!\\nThe full history of operations is recorded in the \\\"Undo \u002F Redo\\\" tab.\\n\u003C\u002Fdiv\u003E\\n\\n\u003Ch2 id=\\\"split-cells\\\"\u003ESplit Cells\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe \u003Cem\u003Eparse\u003C\u002Fem\u003E column now contains all the sonnets separated by &quot;|&quot;, but the project still contains only one row.\\nIndividual rows for each sonnet can be created by splitting the cell.\\nClick the menu arrow on the \u003Cem\u003Eparse\u003C\u002Fem\u003E column &gt; \u003Cem\u003EEdit cells\u003C\u002Fem\u003E &gt; \u003Cem\u003ESplit multi-valued cells\u003C\u002Fem\u003E.\\nEnter the separator \u003Ccode\u003E|\u003C\u002Fcode\u003E that was used to \u003Ccode\u003Ejoin\u003C\u002Fcode\u003E in the last step.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Edit cells &gt; Split multivalued cells&quot; filename=&quot;refine-split-multivalued.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EAfter this operation, the top of the project table should now read 154 rows.\\nBelow the number is an option toggle &quot;Show as: \u003Cem\u003Erows\u003C\u002Fem\u003E \u003Cem\u003Erecords\u003C\u002Fem\u003E&quot;.\\nClicking on \u003Cem\u003Erecords\u003C\u002Fem\u003E will group the rows based on the original table, in this case it will read 1.\\nKeeping track of these numbers is an important &quot;sanity check&quot; when transforming data in Refine.\\nThe 154 rows make sense because the ebook contained 154 sonnets, while 1 record represents the original table with only one row.\\nAn unexpected number would indicate a problem with the transformation.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Project rows&quot; filename=&quot;refine-rows.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EEach cell in the \u003Cem\u003Eparse\u003C\u002Fem\u003E column now contains one sonnet surround by a \u003Ccode\u003E&lt;p&gt;\u003C\u002Fcode\u003E tag.\\nThe tags can be cleaned up by parsing the HTML again.\\nClick on the \u003Cem\u003Eparse\u003C\u002Fem\u003E column and select \u003Cem\u003EEdit cells\u003C\u002Fem\u003E &gt; \u003Cem\u003ETransform\u003C\u002Fem\u003E.\\nThis will bring up a dialog box similar to creating a new column.\\nTransform will overwrite the cells of the current column rather than creating a new one.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the expression box, type \u003Ccode\u003Evalue.parseHtml()\u003C\u002Fcode\u003E.\\nThe preview will show a complete HTML tree starting with the \u003Ccode\u003E&lt;html&gt;\u003C\u002Fcode\u003E element.\\nIt is important to note that \u003Ccode\u003EparseHtml()\u003C\u002Fcode\u003E will automatically fill in missing tags, allowing it to parse these cell values despite not being valid HTML documents.\\nSelect the \u003Ccode\u003Ep\u003C\u002Fcode\u003E tag, add an index number, and use the function \u003Ccode\u003EinnerHtml()\u003C\u002Fcode\u003E to extract the sonnet text:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Evalue.parseHtml().select(&quot;p&quot;)[0].innerHtml()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EClick \u003Cem\u003EOK\u003C\u002Fem\u003E to transform all 154 cells in the column.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Edit cells &gt; Transform&quot; filename=&quot;refine-innerhtml.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\nIn the expression above \u003Ccode\u003Eselect\u003C\u002Fcode\u003E returns an array of \u003Ccode\u003Ep\u003C\u002Fcode\u003E elements even though there is only one in each cell.\\nAttempting to pass an array to \u003Ccode\u003EinnerHtml()\u003C\u002Fcode\u003E will raise an error.\\nThus, an index number is necessary to select the first (and only) item in the array to pass the correct object type to \u003Ccode\u003EinnerHtml()\u003C\u002Fcode\u003E.\\n\u003Cbr\u003EKeep data object types in mind when debugging GREL expressions!\\n\u003C\u002Fdiv\u003E\\n\\n\u003Ch2 id=\\\"unescape\\\"\u003EUnescape\u003C\u002Fh2\u003E\\n\u003Cp\u003ENotice that each cell has dozens of \u003Ccode\u003E&amp;nbsp;\u003C\u002Fcode\u003E, an HTML entity used to represent &quot;no-break space&quot; since browsers ignore extra white space in the source.\\nThese entities are common when harvesting web pages and can be quickly replaced with the corresponding plain text characters using the \u003Ccode\u003Eunescape()\u003C\u002Fcode\u003E function.\\nOn the \u003Cem\u003Eparse\u003C\u002Fem\u003E column, select \u003Cem\u003EEdit cells\u003C\u002Fem\u003E &gt; \u003Cem\u003ETransform\u003C\u002Fem\u003E and type the following in the expression box:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Evalue.unescape(&#39;html&#39;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe entities will be replaced with normal whitespace.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"extract-information-with-array-functions\\\"\u003EExtract Information with Array Functions\u003C\u002Fh2\u003E\\n\u003Cp\u003E\u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002FGREL-Array-Functions\\\"\u003EGREL array functions\u003C\u002Fa\u003E provide a powerful way to manipulate text data and can be used to finish processing the sonnets.\\nAny string value can be turned into an array using the \u003Ccode\u003Esplit()\u003C\u002Fcode\u003E function by providing the character or expression that separates the items (basically the opposite of \u003Ccode\u003Ejoin()\u003C\u002Fcode\u003E).\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the sonnets each line ends with \u003Ccode\u003E&lt;br \u002F&gt;\u003C\u002Fcode\u003E, providing a convenient separator for splitting.\\nThe expression \u003Ccode\u003Evalue.split(&quot;&lt;br \u002F&gt;&quot;)\u003C\u002Fcode\u003E will create an array of the lines of each sonnet.\\nIndex numbers and slices can then be used to populate new columns.\\nKeep in mind that Refine will not output an array directly to a cell.\\nBe sure to select one element from the array using an index number or convert it back to a string with \u003Ccode\u003Ejoin()\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EFurthermore, the sonnet text contains a huge amount of unnecessary white space that was used to layout the poems in the ebook.\\nThis can be cut from each line using the \u003Ccode\u003Etrim()\u003C\u002Fcode\u003E function.\\nTrim automatically removes all leading and trailing white space in a cell, an essential for data cleaning.\u003C\u002Fp\u003E\\n\u003Cp\u003EUsing these concepts, a single line can be extracted and trimmed to create clean columns representing the sonnet number and first line.\\nCreate two new columns from the \u003Cem\u003Eparse\u003C\u002Fem\u003E column using these names and expressions:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E&quot;number&quot;, \u003Ccode\u003Evalue.split(&quot;&lt;br \u002F&gt;&quot;)[0].trim()\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E&quot;first&quot;, \u003Ccode\u003Evalue.split(&quot;&lt;br \u002F&gt;&quot;)[1].trim()\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;GREL split and trim&quot; filename=&quot;refine-add-num-column.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe next column to create is the full sonnet text which contains multiple lines.\\nHowever, \u003Ccode\u003Etrim()\u003C\u002Fcode\u003E will only clean the beginning and end of the cell, leaving unnecessary whitespace in the body of the sonnet.\\nTo trim each line individually use the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002FGREL-Controls\\\"\u003EGREL control\u003C\u002Fa\u003E \u003Ccode\u003EforEach()\u003C\u002Fcode\u003E, a handy loop that iterates over an array.\u003C\u002Fp\u003E\\n\u003Cp\u003EFrom the \u003Cem\u003Eparse\u003C\u002Fem\u003E column, create a new column named &quot;text&quot;, and click in the \u003Cem\u003EExpression\u003C\u002Fem\u003E box.\\nA \u003Ccode\u003EforEach()\u003C\u002Fcode\u003E statement asks for an array, a variable name, and an expression applied to the variable.\\nFollowing the form \u003Ccode\u003EforEach(array, variable, expression)\u003C\u002Fcode\u003E, construct the loop using these parameters:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003Earray: \u003Ccode\u003Evalue.split(&quot;&lt;br \u002F&gt;&quot;)\u003C\u002Fcode\u003E, creates an array from the lines of the sonnet in each cell.\u003C\u002Fli\u003E\\n\u003Cli\u003Evariable: \u003Ccode\u003Eline\u003C\u002Fcode\u003E, each item in the array is then represented as the variable (it could be anything, \u003Ccode\u003Ev\u003C\u002Fcode\u003E is often used).\u003C\u002Fli\u003E\\n\u003Cli\u003Eexpression: \u003Ccode\u003Eline.trim()\u003C\u002Fcode\u003E, each item is then evaluated separately with the specified expression. In this case, \u003Ccode\u003Etrim()\u003C\u002Fcode\u003E cleans the white space from each sonnet line in the array.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EAt this point, the statement should look like \u003Ccode\u003EforEach(value.split(&quot;&lt;br \u002F&gt;&quot;), line, line.trim())\u003C\u002Fcode\u003E in the \u003Cem\u003EExpression\u003C\u002Fem\u003E box.\\nNotice that the \u003Cem\u003EPreview\u003C\u002Fem\u003E now shows an array where the first element is the sonnet number.\\nSince the results of the \u003Ccode\u003EforEach()\u003C\u002Fcode\u003E are returned as a new array, additional array functions can be applied, such as slice and join.\\nAdd \u003Ccode\u003Eslice(1)\u003C\u002Fcode\u003E to remove the sonnet number, and \u003Ccode\u003Ejoin(&quot;\\\\n&quot;)\u003C\u002Fcode\u003E to concatenate the lines in to a string value (\u003Ccode\u003E\\\\n\u003C\u002Fcode\u003E is the symbol for new line in plain text).\\nThus, the final expression to extract and clean the full sonnet text is:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003EforEach(value.split(&quot;&lt;br \u002F&gt;&quot;), line, line.trim()).slice(1).join(&quot;\\\\n&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;GREL forEach expression&quot; filename=&quot;refine-foreach.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EClick &quot;OK&quot; to create the column.\\nFollowing the same technique, add another new column from \u003Cem\u003Eparse\u003C\u002Fem\u003E named &quot;last&quot; to represent the final couplet lines using:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003EforEach(value.split(&quot;&lt;br \u002F&gt;&quot;), line, line.trim()).slice(-3).join(&quot;\\\\n&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EFinally, numeric columns can be added using the \u003Ccode\u003Elength()\u003C\u002Fcode\u003E function.\\nCreate new columns from \u003Cem\u003Etext\u003C\u002Fem\u003E with the names and expressions below:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E&quot;characters&quot;, \u003Ccode\u003Evalue.length()\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E&quot;lines&quot;, \u003Ccode\u003Evalue.split(\u002F\\\\n\u002F).length()\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"cleanup-and-export\\\"\u003ECleanup and Export\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn this example, we used a number of operations to create new columns with clean data.\\nThis is a typical Refine workflow, allowing each transformation to be easily checked against the existing data.\\nAt this point the unnecessary columns can be removed.\\nClick on the \u003Cem\u003EAll\u003C\u002Fem\u003E column &gt; \u003Cem\u003EEdit columns\u003C\u002Fem\u003E &gt; \u003Cem\u003ERe-order \u002F remove columns\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;All &gt; Edit columns&quot; filename=&quot;refine-reorder.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EDrag unwanted column names to the right side of the dialog box, in this case \u003Cem\u003EColumn 1\u003C\u002Fem\u003E, \u003Cem\u003Efetch\u003C\u002Fem\u003E, and \u003Cem\u003Eparse\u003C\u002Fem\u003E.\\nDrag the remaining columns into the desired order on the left side.\\nClick \u003Cem\u003EOk\u003C\u002Fem\u003E to remove and reorder the data set.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Re-order \u002F Remove columns&quot; filename=&quot;refine-reorder2.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EUse filters and facets to explore and subset the collection of sonnets.\\nThen click the export button to generate a version of the new sonnet table for use outside of Refine.\\nOnly the currently selected subset will be exported.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Export CSV&quot; filename=&quot;refine-export.png&quot; %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"example-2-url-queries-and-parsing-json\\\"\u003EExample 2: URL Queries and Parsing JSON\u003C\u002Fh1\u003E\\n\u003Cp\u003EMany cultural institutions provide web APIs allowing users to access information about their collections via simple \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FHypertext_Transfer_Protocol\\\"\u003EHTTP\u003C\u002Fa\u003E requests.\\nThese sources enable new queries and aggregations of text that were previously impossible, cutting across boundaries of repositories and collections to support large scale analysis of both content and metadata.\\nThis example will harvest data from the \u003Ca href=\\\"https:\u002F\u002Fchroniclingamerica.loc.gov\u002F\\\"\u003EChronicling America\u003C\u002Fa\u003E project to assemble a small set of newspaper front pages with full text.\\nFollowing a common web scraping workflow, Refine is used to construct the query URL, fetch the information, and parse the JSON response.\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\nChronicling America is fully open, thus no key or account is needed to access the API and there are no limits on the use.\\nOther aggregators are often proprietary and restricted.\\nPlease review the specific terms of use before web scraping or using the information in research.\\n\u003C\u002Fdiv\u003E\\n\\n\u003Ch2 id=\\\"start-chronicling-america-project\\\"\u003EStart &quot;Chronicling America&quot; Project\u003C\u002Fh2\u003E\\n\u003Cp\u003ETo get started after completing \u003Cem\u003EExample 1\u003C\u002Fem\u003E, click the \u003Cem\u003EOpen\u003C\u002Fem\u003E button in the upper right.\\nA new tab will open with the Refine start project view.\\nThe tab with the Sonnets project can be left open without impacting performance.\\nCreate a project from \u003Cem\u003EClipboard\u003C\u002Fem\u003E by pasting this CSV into the text box:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Estate,year\\nIdaho,1865\\nMontana,1865\\nOregon,1865\\nWashington,1865\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAfter clicking \u003Cem\u003ENext\u003C\u002Fem\u003E, Refine should automatically identify the content as a CSV with the correct parsing options.\\nAdd the \u003Cem\u003EProject name\u003C\u002Fem\u003E &quot;ChronAm&quot; at the top right and click \u003Cem\u003ECreate project\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Create project&quot; filename=&quot;refine-start-project.png&quot; %}\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"construct-a-query\\\"\u003EConstruct a Query\u003C\u002Fh2\u003E\\n\u003Cp\u003EChronicling America provides \u003Ca href=\\\"https:\u002F\u002Fchroniclingamerica.loc.gov\u002Fabout\u002Fapi\u002F\\\"\u003Edocumentation\u003C\u002Fa\u003E for their API and URL patterns.\\nIn addition to formal documentation, information about alternative formats and search API are sometimes given in the \u003Ccode\u003E&lt;head&gt;\u003C\u002Fcode\u003E element of a web page.\\nCheck for \u003Ccode\u003E&lt;link rel=&quot;alternate&quot;\u003C\u002Fcode\u003E, \u003Ccode\u003E&lt;link rel=&quot;search&quot;\u003C\u002Fcode\u003E, or \u003Ccode\u003E&lt;!--\u003C\u002Fcode\u003E comments which provide hints on how to interact with the site.\\nThese clues provide a recipe book for interacting with the server using public links.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe basic components of the ChromAm API are:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003Ethe base URL, \u003Ccode\u003Ehttps:\u002F\u002Fchroniclingamerica.loc.gov\u002F\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003Ethe search service location for individual newspaper pages, \u003Ccode\u003Esearch\u002Fpages\u002Fresults\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003Ea query string, starting with \u003Ccode\u003E?\u003C\u002Fcode\u003E and made up of value pairs (\u003Ccode\u003Efieldname=value\u003C\u002Fcode\u003E) separated by \u003Ccode\u003E&amp;\u003C\u002Fcode\u003E. Much like using the \u003Ca href=\\\"https:\u002F\u002Fchroniclingamerica.loc.gov\u002F#tab=tab_advanced_search\\\"\u003Eadvanced search form\u003C\u002Fa\u003E, the value pairs of the query string set the \u003Ca href=\\\"https:\u002F\u002Fchroniclingamerica.loc.gov\u002Fsearch\u002Fpages\u002Fopensearch.xml\\\"\u003Esearch options\u003C\u002Fa\u003E.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EUsing a GREL expression, these components can be combined with the values in the &quot;ChronAm&quot; project to construct a search query URL.\\nThe contents of the data table can be accessed using \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002FVariables\\\"\u003EGREL variables\u003C\u002Fa\u003E.\\nAs introduced in \u003Cem\u003EExample 1\u003C\u002Fem\u003E, the value of each cell in the current column is represented by \u003Ccode\u003Evalue\u003C\u002Fcode\u003E.\\nValues in the same row can be retrieved using the \u003Ccode\u003Ecells\u003C\u002Fcode\u003E variable plus the column name.\\nThere are two ways to write a \u003Ccode\u003Ecells\u003C\u002Fcode\u003E statement: bracket notation \u003Ccode\u003Ecells[&#39;column name&#39;].value\u003C\u002Fcode\u003E which allows column names that include a space, or dot notation \u003Ccode\u003Ecells.column_name.value\u003C\u002Fcode\u003E which allows only single word column names.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn GREL, strings are concatenated using the plus sign.\\nFor example, the expression \u003Ccode\u003E&quot;one&quot; + &quot;two&quot;\u003C\u002Fcode\u003E would result in &quot;onetwo&quot;.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo create the set of search queries, from the \u003Cem\u003Estate\u003C\u002Fem\u003E column, add a column named &quot;url&quot; with this expression:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E&quot;https:\u002F\u002Fchroniclingamerica.loc.gov\u002Fsearch\u002Fpages\u002Fresults\u002F?state=&quot; + value.escape(&#39;url&#39;) + &quot;&amp;date1=&quot; + cells[&#39;year&#39;].value.escape(&#39;url&#39;) + &quot;&amp;date2=&quot; + cells[&#39;year&#39;].value.escape(&#39;url&#39;) + &quot;&amp;dateFilterType=yearRange&amp;sequence=1&amp;sort=date&amp;rows=5&amp;format=json&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Create query URL&quot; filename=&quot;refine-chronam-url.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe expression concatenates the constants (base URL, search service, and query field names) together with the values in each row.\\nThe \u003Ccode\u003Eescape()\u003C\u002Fcode\u003E function is added to the cell variables to ensure the string will be safe in a URL (the opposite of the \u003Ccode\u003Eunescape()\u003C\u002Fcode\u003E function introduced in \u003Cem\u003EExample 1\u003C\u002Fem\u003E).\u003C\u002Fp\u003E\\n\u003Cp\u003ELook at the value pairs after the \u003Ccode\u003E?\u003C\u002Fcode\u003E to understand the parameters of the search.\\nExplicitly, the first query URL will ask for newspapers:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003Efrom Idaho (\u003Ccode\u003Estate=Idaho\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003Efrom the year 1865, (\u003Ccode\u003Edate1=1865&amp;date2=1865&amp;dateFilterType=yearRange\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003Eonly the front pages (\u003Ccode\u003Esequence=1\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003Esorting by date (\u003Ccode\u003Esort=date\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003Ereturning a maximum of five (\u003Ccode\u003Erows=5\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003Ein JSON (\u003Ccode\u003Eformat=json\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"fetch-urls\\\"\u003EFetch URLs\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe \u003Cem\u003Eurl\u003C\u002Fem\u003E column is a list of web queries that could be accessed with a browser.\\nTo test, click one of the links.\\nThe url will open in a new tab, returning a JSON response.\u003C\u002Fp\u003E\\n\u003Cp\u003EFetch the URLs using \u003Cem\u003Eurl\u003C\u002Fem\u003E column by selecting \u003Cem\u003EEdit column\u003C\u002Fem\u003E &gt; \u003Cem\u003EAdd column by fetching urls\u003C\u002Fem\u003E.\\nName the new column &quot;fetch&quot; and click \u003Cem\u003EOK\u003C\u002Fem\u003E.\\nIn a few seconds, the operation should complete and the \u003Cem\u003Efetch\u003C\u002Fem\u003E column will be filled with \u003Ca href=\\\"http:\u002F\u002Fwww.json.org\u002F\\\"\u003EJSON\u003C\u002Fa\u003E data.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"parse-json-to-get-items\\\"\u003EParse JSON to Get Items\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe first name\u002Fvalue pairs of the query response look like \u003Ccode\u003E&quot;totalItems&quot;: 52, &quot;endIndex&quot;: 5\u003C\u002Fcode\u003E.\\nThis indicates that the search resulted in 52 total items, but the response contains only five (since it was limited by the \u003Ccode\u003Erows=5\u003C\u002Fcode\u003E option).\\nThe JSON key \u003Ccode\u003Eitems\u003C\u002Fcode\u003E contains an array of the individual newspapers returned by the search.\\nTo construct a orderly data set, it is necessary to parse the JSON and split each newspaper into its own row.\u003C\u002Fp\u003E\\n\u003Cp\u003EGREL&#39;s \u003Ccode\u003EparseJson()\u003C\u002Fcode\u003E function allows us to select a key name to retrieve the corresponding values.\\nAdd a new column based on \u003Cem\u003Efetch\u003C\u002Fem\u003E with the name &quot;items&quot; and enter this expression:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Evalue.parseJson()[&#39;items&#39;].join(&quot;|||&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;parse json items&quot; filename=&quot;refine-parse-items.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ESelecting \u003Ccode\u003E[&#39;items&#39;]\u003C\u002Fcode\u003E exposes the array of newspaper records nested inside the JSON response.\\nThe \u003Ccode\u003Ejoin()\u003C\u002Fcode\u003E function concatenates the array with the given separator resulting in a string value.\\nSince the newspaper records contain an OCR text field, the strange separator &quot;|||&quot; is necessary to ensure that it is unique and can be used to split the values.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"split-multivalued-cells\\\"\u003ESplit Multivalued Cells\u003C\u002Fh2\u003E\\n\u003Cp\u003EWith the individual newspapers isolated, separate rows can be created by splitting the cells.\\nOn the \u003Cem\u003Eitems\u003C\u002Fem\u003E column, select \u003Cem\u003EEdit cells\u003C\u002Fem\u003E &gt;  \u003Cem\u003ESplit multivalued cells\u003C\u002Fem\u003E, and enter the join used in the last step, \u003Ccode\u003E|||\u003C\u002Fcode\u003E.\\nAfter the operation, the top of the project table should read 20 rows.\\nClicking on Show as \u003Cem\u003Erecords\u003C\u002Fem\u003E should read 4, representing the original CSV rows.\u003C\u002Fp\u003E\\n\u003Cp\u003ENotice that the new rows are empty in all columns except \u003Cem\u003Eitems\u003C\u002Fem\u003E.\\nTo ensure the state is available with each newspaper issue, the empty values can be filled using the \u003Ccode\u003EFill down\u003C\u002Fcode\u003E function.\\nClick on the \u003Cem\u003Estate\u003C\u002Fem\u003E column &gt; \u003Cem\u003EEdit cells\u003C\u002Fem\u003E &gt; \u003Cem\u003EFill down\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;fill down&quot; filename=&quot;refine-fill-down.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThis is a good point to clean up the unnecessary columns.\\nClick on the \u003Cem\u003EAll\u003C\u002Fem\u003E column &gt; \u003Cem\u003EEdit columns\u003C\u002Fem\u003E &gt; \u003Cem\u003ERe-order \u002F remove columns\u003C\u002Fem\u003E.\\nDrag all columns except \u003Cem\u003Estate\u003C\u002Fem\u003E and \u003Cem\u003Eitems\u003C\u002Fem\u003E to the right side, then click \u003Cem\u003EOK\u003C\u002Fem\u003E to remove them.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Re-order \u002F remove columns&quot; filename=&quot;refine-chronam-reorder.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ESanity check: with the original columns removed, both \u003Cem\u003Erecords\u003C\u002Fem\u003E and \u003Cem\u003Erows\u003C\u002Fem\u003E will read 20.\\nThis makes sense, as the project started with four states and fetched five records for each.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"parse-json-values\\\"\u003EParse JSON Values\u003C\u002Fh2\u003E\\n\u003Cp\u003ETo complete the data set, it is necessary to parse each newspaper&#39;s JSON record into individual columns.\\nThis is a common task, as many web APIs return information in JSON format.\\nAgain, GREL&#39;s \u003Ccode\u003EparseJson()\u003C\u002Fcode\u003E function makes this easy.\\nCreate a new column from \u003Cem\u003Eitems\u003C\u002Fem\u003E for each newspaper metadata element by parsing the JSON and selecting the key:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E&quot;date&quot;, \u003Ccode\u003Evalue.parseJson()[&#39;date&#39;]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E&quot;title&quot;, \u003Ccode\u003Evalue.parseJson()[&#39;title&#39;]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E&quot;city&quot;, \u003Ccode\u003Evalue.parseJson()[&#39;city&#39;].join(&quot;, &quot;)\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E&quot;lccn&quot;, \u003Ccode\u003Evalue.parseJson()[&#39;lccn&#39;]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E&quot;text&quot;, \u003Ccode\u003Evalue.parseJson()[&#39;ocr_eng&#39;]\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EAfter the desired information is extracted, the \u003Cem\u003Eitems\u003C\u002Fem\u003E column can be removed by selecting \u003Cem\u003EEdit column\u003C\u002Fem\u003E &gt; \u003Cem\u003ERemove this column\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Final ChronAm project columns&quot; filename=&quot;refine-chronam-final.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EEach column could be further refined using other GREL transformations.\\nFor example, to convert \u003Cem\u003Edate\u003C\u002Fem\u003E to a more readable format, use \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002FGREL-Date-Functions\\\"\u003EGREL date functions\u003C\u002Fa\u003E.\\nTransform the \u003Cem\u003Edate\u003C\u002Fem\u003E column with the expression \u003Ccode\u003Evalue.toDate(&quot;yyyymmdd&quot;).toString(&quot;yyyy-MM-dd&quot;)\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EAnother common workflow is to extend the data with further URL queries.\\nFor example, a link to full information about each issue can be formed based on the \u003Cem\u003Elccn\u003C\u002Fem\u003E.\\nCreate a new column based on \u003Cem\u003Elccn\u003C\u002Fem\u003E using the expression \u003Ccode\u003E&quot;https:\u002F\u002Fchroniclingamerica.loc.gov\u002Flccn\u002F&quot; + value + &quot;\u002F&quot; + cells[&#39;date&#39;].value + &quot;\u002Fed-1.json&quot;\u003C\u002Fcode\u003E.\\nFetching this URL returns a complete list of the issue&#39;s pages, which could in turn be harvested.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor now, the headlines of 1865 from the Northwest are ready to enjoy!\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"example-3-advanced-apis\\\"\u003EExample 3: Advanced APIs\u003C\u002Fh1\u003E\\n\u003Cp\u003E\u003Cem\u003E\u003Ca href=\\\"#example-2-url-queries-and-parsing-json\\\"\u003EExample 2\u003C\u002Fa\u003E\u003C\u002Fem\u003E demonstrated Refine&#39;s fetch function with a simple web API, essentially utilizing URL patterns to request information from a server.\\nThis workflow uses the HTTP GET protocol, meaning the query is encoded in the URL string, thus limited in length (2048 ASCII characters), complexity, and security.\\nInstead, many API services used to enhance text data, such as \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGeocoding\\\"\u003Egeocoding\u003C\u002Fa\u003E or \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FNamed-entity_recognition\\\"\u003Enamed entity recognition\u003C\u002Fa\u003E, use HTTP POST to transfer information to the server for processing.\u003C\u002Fp\u003E\\n\u003Cp\u003EGREL does not have a built in function to use this type of API.\\nHowever, the expression window language can be changed to \u003Ca href=\\\"http:\u002F\u002Fwww.jython.org\u002F\\\"\u003EJython\u003C\u002Fa\u003E, providing a more complete scripting environment where it is possible to implement a POST request.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003E\u003Ca href=\\\"http:\u002F\u002Fwww.jython.org\u002F\\\"\u003EJython\u003C\u002Fa\u003E is Python implemented for the Java VM and comes bundled with Refine.\\nThis means \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F2.7\u002F\\\"\u003EPython 2\u003C\u002Fa\u003E scripts using the Standard Library can be written or loaded into the expression window, and Refine will apply them to each cell in the transformation.\\nThe \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002FJython\\\"\u003Eofficial documentation\u003C\u002Fa\u003E is sparse, but the built-in Jython can be extended with non-standard libraries using a \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002FExtending-Jython-with-pypi-modules\\\"\u003Ework around\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EKeep in mind that spending time writing complex scripts moves away from the strengths of Refine.\\nIf it is necessary to develop a lengthy Jython routine, it will likely be more efficient to process the data directly in Python.\\nOn the other hand, if you know a handy method to process data in Python 2, Jython is a easy way to apply it in a Refine project.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Ch2 id=\\\"jython-in-the-expression-window\\\"\u003EJython in the Expression Window\u003C\u002Fh2\u003E\\n\u003Cp\u003EReturn to the &quot;Sonnets&quot; project completed in \u003Cem\u003E\u003Ca href=\\\"#example-1-fetching-and-parsing-html\\\"\u003EExample 1\u003C\u002Fa\u003E\u003C\u002Fem\u003E.\\nIf the tab was closed, click \u003Cem\u003EOpen\u003C\u002Fem\u003E &gt; \u003Cem\u003EOpen Project\u003C\u002Fem\u003E and find the Sonnets example (Refine saves everything for you!).\u003C\u002Fp\u003E\\n\u003Cp\u003EAdd a new column based on the \u003Cem\u003Efirst\u003C\u002Fem\u003E column named &quot;sentiment&quot;.\\nWe will use this window to test out a series of expressions, so leave it open until we get to the final iteration of the request.\u003C\u002Fp\u003E\\n\u003Cp\u003EOn the right side of the \u003Cem\u003EExpression\u003C\u002Fem\u003E box is a drop down to change the expression language.\\nSelect \u003Cem\u003EPython \u002F Jython\u003C\u002Fem\u003E from the list.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Jython expression&quot; filename=&quot;refine-jython.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ENotice that the preview now shows \u003Ccode\u003Enull\u003C\u002Fcode\u003E for the output.\\nA Jython expression in Refine must have a \u003Ccode\u003Ereturn\u003C\u002Fcode\u003E statement to add the output to the new cells in the transformation.\\nType \u003Ccode\u003Ereturn value\u003C\u002Fcode\u003E into the \u003Cem\u003EExpression\u003C\u002Fem\u003E box.\\nThe preview will update showing the current cells copied to the output.\\nThe basic \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002FOpenRefine\u002FOpenRefine\u002Fwiki\u002FVariables\\\"\u003EGREL variables\u003C\u002Fa\u003E can be used in Jython by substituting brackets instead of periods.\\nFor example, the GREL \u003Ccode\u003Ecells.column-name.value\u003C\u002Fcode\u003E would be Jython \u003Ccode\u003Ecells[&#39;column-name&#39;][&#39;value&#39;]\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"jython-get-request\\\"\u003EJython GET Request\u003C\u002Fh2\u003E\\n\u003Cp\u003ETo create a HTTP request in Jython, use the standard library \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F2\u002Flibrary\u002Furllib2.html\\\"\u003Eurllib2\u003C\u002Fa\u003E.\\nRefine&#39;s fetch function can be recreated with Jython to demonstrate the basics of the library.\\nIn the expression box, type:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Eimport urllib2\\nget = urllib2.urlopen(&quot;http:\u002F\u002Fwww.jython.org\u002F&quot;)\\nreturn get.read()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;Jython GET request&quot; filename=&quot;refine-jython-expression.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe preview should display the HTML source of the Jython home page, this is an HTTP GET request as in previous fetch examples.\\nNotice that similar to opening and reading a text file with Python, \u003Ccode\u003Eurlopen()\u003C\u002Fcode\u003E returns a file-like object that must be \u003Ccode\u003Eread()\u003C\u002Fcode\u003E into a string.\\nThe URL could be replaced with cell variables to construct a query similar to the fetch used in \u003Cem\u003EExample 2\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"post-request\\\"\u003EPOST Request\u003C\u002Fh2\u003E\\n\u003Cp\u003EUrllib2 will automatically send a POST if data is added to the request object.\\nFor example, \u003Ca href=\\\"http:\u002F\u002Ftext-processing.com\u002F\\\"\u003EText Processing\u003C\u002Fa\u003E provides natural language processing APIs based on \u003Ca href=\\\"http:\u002F\u002Fwww.nltk.org\u002F\\\"\u003EPython NLTK\u003C\u002Fa\u003E.\\nThe documentation for the \u003Ca href=\\\"http:\u002F\u002Ftext-processing.com\u002Fdocs\u002Fsentiment.html\\\"\u003ESentiment Analysis service\u003C\u002Fa\u003E provides a base URL and the name of the key used for the data to be analyzed.\\nNo authentication is required and 1,000 calls per day are free for non-commercial use.[^use]\u003C\u002Fp\u003E\\n\u003Cp\u003EThis type of API is often demonstrated using \u003Ca href=\\\"https:\u002F\u002Fcurl.haxx.se\u002F\\\"\u003Ecurl\u003C\u002Fa\u003E on the commandline.\\nText Processing gives the example \u003Ccode\u003Ecurl -d &quot;text=great&quot; http:\u002F\u002Ftext-processing.com\u002Fapi\u002Fsentiment\u002F\u003C\u002Fcode\u003E which can be recreated in Jython to test the service.\\nBuilding on the GET expression above, the POST data is added as the second parameter of \u003Cem\u003Eurlopen\u003C\u002Fem\u003E, thus the request will be in the form \u003Ccode\u003Eurllib2.urlopen(url, data)\u003C\u002Fcode\u003E.\\nType this script into the expression window:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Eimport urllib2\\nurl = &quot;http:\u002F\u002Ftext-processing.com\u002Fapi\u002Fsentiment\u002F&quot;\\ndata = &quot;text=what is the sentiment of this sentence&quot;\\npost = urllib2.urlopen(url, data)\\nreturn post.read()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe preview should show a JSON response with sentiment probability values.\\nTo retrieve sentiment analysis data for the first lines of the sonnets (remember we are still adding a column based on \u003Cem\u003Efirst\u003C\u002Fem\u003E!), put the basic Jython pattern together with the values of the cells:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Eimport urllib2\\nurl = &quot;http:\u002F\u002Ftext-processing.com\u002Fapi\u002Fsentiment\u002F&quot;\\ndata = &quot;text=&quot; + value\\npost = urllib2.urlopen(url, data)\\nreturn post.read()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E{% include figure.html caption=&quot;jython request&quot; filename=&quot;refine-jython-request.png&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EClick \u003Cem\u003EOK\u003C\u002Fem\u003E and the Jython script will run for every row in the column.\\nThe JSON response can then be parsed with GREL using the methods demonstrated in \u003Cem\u003EExample 2\u003C\u002Fem\u003E (for example, \u003Ccode\u003Evalue.parseJson()[&#39;label&#39;]\u003C\u002Fcode\u003E).\u003C\u002Fp\u003E\\n\u003Cp\u003EGiven the small expression window and uniform data, the script above is pragmatically simplified and compressed.\\nWhen Refine is encountering problems, it is better to implement a more complete script with error handling.\\nIf necessary, a throttle delay can be implemented by importing \u003Ccode\u003Etime\u003C\u002Fcode\u003E and adding \u003Ccode\u003Etime.sleep()\u003C\u002Fcode\u003E to the script.\\nFor example, the POST request could be rewritten:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Eimport urllib2, urllib, time\\ntime.sleep(15)\\nurl = &quot;http:\u002F\u002Ftext-processing.com\u002Fapi\u002Fsentiment\u002F&quot;\\ndata = urllib.urlencode({&quot;text&quot;: value.encode(&quot;utf-8&quot;)})\\nreq = urllib2.Request(url,data)\\ntry:\\n    post = urllib2.urlopen(req)\\nexcept urllib2.URLError as e:\\n    if hasattr(e, &quot;reason&quot;):\\n        return &quot;Failed: &quot;, e.reason\\n    elif hasattr(e, &quot;code&quot;):\\n        return &quot;Error code: &quot;, e.code\\nelse:\\n    response = post.read()\\n    return response\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ESome APIs require authentication tokens to be passed with the POST request as data or headers.\\nHeaders can be added as the third parameter of \u003Ccode\u003Eurllib2.Request()\u003C\u002Fcode\u003E similar to how data was added in the example above.\\nCheck the Python \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F2\u002Flibrary\u002Furllib2.html\\\"\u003Eurllib2 documentation\u003C\u002Fa\u003E and \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F2\u002Fhowto\u002Furllib2.html\\\"\u003Ehow-to\u003C\u002Fa\u003E for advanced options.\u003C\u002Fp\u003E\\n\u003Cp\u003EWhen harvesting web content, character encoding issues commonly produce errors in Python.\\nTrimming whitespace, using GREL \u003Ccode\u003Eescape()\u003C\u002Fcode\u003E \u002F \u003Ccode\u003Eunescape()\u003C\u002Fcode\u003E, or Jython \u003Ccode\u003Eencode(&quot;utf-8&quot;)\u003C\u002Fcode\u003E will often fix the problem.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Ch2 id=\\\"compare-sentiment\\\"\u003ECompare Sentiment\u003C\u002Fh2\u003E\\n\u003Cp\u003ETo practice constructing a POST request, read the documentation for \u003Ca href=\\\"http:\u002F\u002Fsentiment.vivekn.com\u002Fdocs\u002Fapi\u002F\\\"\u003ESentiment Tool\u003C\u002Fa\u003E, another free API.\\nFind the service URL and data key necessary to modify the Jython pattern above.\\nCreate a new column from \u003Cem\u003Efirst\u003C\u002Fem\u003E named \u003Ccode\u003Esentiment2\u003C\u002Fcode\u003E and test the script.\u003C\u002Fp\u003E\\n\u003Cp\u003EThere are many possible ways to create the request, for example:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Eimport urllib2\\nurl = &quot;http:\u002F\u002Fsentiment.vivekn.com\u002Fapi\u002Ftext\u002F&quot;\\ndata = &quot;txt=&quot; + value\\npost = urllib2.urlopen(url, data)\\nreturn post.read()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe JSON response contains different metrics, but it will be obvious that the APIs disagree on many of the sentiment &quot;labels&quot; (for example, use \u003Ccode\u003Evalue.parseJson()[&#39;result&#39;][&#39;sentiment&#39;]\u003C\u002Fcode\u003E to extract a label comparable to the first API).\\nThese are simple free APIs for demonstration purposes, but it is important to critically investigate services to more fully understand the potential of the metrics.\u003C\u002Fp\u003E\\n\u003Cp\u003EBoth APIs use a \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FNaive_Bayes_classifier\\\"\u003Enaive bayes classifier\u003C\u002Fa\u003E to categorize text input.\\nThese models must be trained on pre-labeled data and will be most accurate on similar content.\\nText Processing is trained on twitter and movie reviews[^1], and Sentiment Tool on IMDb movie reviews[^2].\\nThus both are optimized for small chunks of modern English language similar to a review, with a limited bag of words used to determine the sentiment probabilities.\u003C\u002Fp\u003E\\n\u003Cp\u003EArchaic words and phrases contribute significantly to the sonnets&#39; sentiment, yet are unlikely to be given any weight in these models since they are not present in the training data.\\nWhile comparing the metrics is fascinating, neither is likely to produce quality results for this data set.\\nRather than an accurate sentiment, we might be surprised to find a quantifiable dissonance between the sonnet&#39;s English and our modern web usage.\\nHowever, a model optimized to Shakespeare&#39;s words could be developed using more appropriate training data.\\nTo learn more about classifiers and how to implement one, see Vilja Hulden&#39;s PH lesson \u003Ca href=\\\"\u002Flessons\u002Fnaive-bayesian\\\"\u003E&quot;Supervised Classification: The Naive Bayesian Returns to the Old Bailey&quot;\u003C\u002Fa\u003E or Steven Bird, Ewan Klein, and Edward Loper&#39;s \u003Ca href=\\\"http:\u002F\u002Fwww.nltk.org\u002Fbook\u002Fch06.html\\\"\u003E&quot;Learning to Classify Text&quot;\u003C\u002Fa\u003E in the \u003Ca href=\\\"http:\u002F\u002Fwww.nltk.org\u002Fbook\u002F\\\"\u003ENTLK Book\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EAccessing data and services on the web opens new possibilities and efficiencies for humanities research.\\nWhile powerful, these APIs are often not aimed at humanities scholarship and may not be appropriate or optimized for our inquiries.\\nThe training data may be incomplete, biased, or secret.\\nWe should always be asking questions about these aggregations and algorithms, thinking critically about the metrics they are capable of producing.\\nThis is not a new technical skill, but an application of the historian&#39;s traditional expertise, not unlike interrogating physical primary materials to unravel bias and read between the lines.\\nHumanities scholars routinely synthesize and evaluate convoluted sources to tell important narratives, and must carry that skill into digital realm.\\nWe can critically evaluate data sources, algorithms, and API services, as well as create new ones more suited to our questions and methods.\u003C\u002Fp\u003E\\n\u003Cp\u003EWith its unique ability to interactively wrangle data from raw aggregation to analysis, Refine supports exploratory research and offers a wonderfully fluid and playful approach to tabular data.\\nOpenRefine is a flexible, pragmatic tool that simplifies routine tasks and, when combined with domain knowledge, extends research capabilities.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^huynh]: David Huynh, &quot;Google Refine&quot;, Computer-Assisted Reporting Conference 2011, \u003Ca href=\\\"http:\u002F\u002Fweb.archive.org\u002Fweb\u002F20150528125345\u002Fhttp:\u002F\u002Fdavidhuynh.net\u002Fspaces\u002Fnicar2011\u002Ftutorial.pdf\\\"\u003Ehttp:\u002F\u002Fweb.archive.org\u002Fweb\u002F20150528125345\u002Fhttp:\u002F\u002Fdavidhuynh.net\u002Fspaces\u002Fnicar2011\u002Ftutorial.pdf\u003C\u002Fa\u003E.\\n[^use]: As of July 2017, see \u003Ca href=\\\"http:\u002F\u002Ftext-processing.com\u002Fdocs\u002Findex.html\\\"\u003EAPI Documentation\u003C\u002Fa\u003E.\\n[^1]: Jacob Perkins, &quot;Sentiment Analysis with Python NLTK Text Classification&quot;, \u003Ca href=\\\"http:\u002F\u002Ftext-processing.com\u002Fdemo\u002Fsentiment\u002F\\\"\u003Ehttp:\u002F\u002Ftext-processing.com\u002Fdemo\u002Fsentiment\u002F\u003C\u002Fa\u003E.\\n[^2]: Vivek Narayanan, Ishan Arora, and Arjun Bhatia, &quot;Fast and accurate sentiment classification using an enhanced Naive Bayes model&quot;, 2013, \u003Ca href=\\\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1305.6143\\\"\u003EarXiv:1305.6143\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
