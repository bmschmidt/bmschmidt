{"metadata":{"title":"Normalizing Textual Data with Python","layout":"lesson","date":"2012-07-17T00:00:00.000Z","authors":["William J. Turkel","Adam Crymble"],"reviewers":["Jim Clifford","Francesca Benatti","Frederik Elwert"],"editors":["Miriam Posner"],"difficulty":2,"exclude_from_check":["review-ticket"],"activity":"transforming","topics":["python"],"abstract":"In this lesson, we will make the list we created in the 'From HTML to a List of Words' lesson easier to analyze by normalizing this data.","next":"counting-frequencies","previous":"from-html-to-list-of-words-2","series_total":"15 lessons","sequence":9,"python_warning":false,"redirect_from":"/lessons/normalizing-data","avatar_alt":"Tall woman dragging a short young man","doi":"10.46430/phen0014"},"html_body":"<p>{% include toc.html %}</p>\n<h2 id=\"lesson-goals\">Lesson Goals</h2>\n<p>The list that we created in the <a href=\"/lessons/from-html-to-list-of-words-2\">From HTML to a List of Words (2)</a>\nneeds some <em>normalizing</em> before it can be used further. We are going to do\nthis by applying additional string methods, as well as by using <em>regular</em>\n<em>expressions</em>. Once normalized, we will be able to more easily analyze our\ndata.</p>\n<h2 id=\"files-needed-for-this-lesson\">Files Needed For This Lesson</h2>\n<ul>\n<li><em>html-to-list-1.py</em></li>\n<li><em>obo.py</em></li>\n</ul>\n<p>If you do not have these files from the previous lesson, you can\ndownload a <a href=\"/assets/python-lessons3.zip\">zip</a></p>\n<h2 id=\"cleaning-up-the-list\">Cleaning up the List</h2>\n<p>In <a href=\"/lessons/from-html-to-list-of-words-2\">From HTML to a List of Words (2)</a>, we wrote a Python program\ncalled <em>html-to-list-1.py</em> which downloaded a <a href=\"http://www.oldbaileyonline.org/browse.jsp?id=t17800628-33&amp;div=t17800628-33\">web page</a>, stripped\nout the HTML formatting and metadata and returned a list of “words” like\nthe one shown below. Technically, these entities are called “<em>tokens</em>”\nrather than “words”. They include some things that are, strictly\nspeaking, not words at all (like the abbreviation &amp;c. for “etcetera”).\nThey also include some things that may be considered composites of more\nthan one word. The possessive “Akerman’s,” for example, is sometimes\nanalyzed by linguists as two words: “Akerman” plus a possessive marker.\nIs “o’clock” one word or two? And so on.</p>\n<p>Turn back to your program <em>html-to-list-1.py</em> and make sure that your\nresults look something like this:</p>\n<pre><code class=\"language-python\">[&#39;324.&#39;, &#39;\\xc2\\xa0&#39;, &#39;BENJAMIN&#39;, &#39;BOWSEY&#39;, &#39;(a&#39;, &#39;blackmoor&#39;, &#39;)&#39;, &#39;was&#39;,\n&#39;indicted&#39;, &#39;for&#39;, &#39;that&#39;, &#39;he&#39;, &#39;together&#39;, &#39;with&#39;, &#39;five&#39;, &#39;hundred&#39;,\n&#39;other&#39;, &#39;persons&#39;, &#39;and&#39;, &#39;more,&#39;, &#39;did,&#39;, &#39;unlawfully,&#39;, &#39;riotously,&#39;,\n&#39;and&#39;, &#39;tumultuously&#39;, &#39;assemble&#39;, &#39;on&#39;, &#39;the&#39;, &#39;6th&#39;, &#39;of&#39;, &#39;June&#39;, &#39;to&#39;,\n&#39;the&#39;, &#39;disturbance&#39;, &#39;of&#39;, &#39;the&#39;, &#39;public&#39;, &#39;peace&#39;, &#39;and&#39;, &#39;did&#39;, &#39;begin&#39;,\n&#39;to&#39;, &#39;demolish&#39;, &#39;and&#39;, &#39;pull&#39;, &#39;down&#39;, &#39;the&#39;, &#39;dwelling&#39;, &#39;house&#39;, &#39;of&#39;,\n&#39;\\xc2\\xa0&#39;, &#39;Richard&#39;, &#39;Akerman&#39;, &#39;,&#39;, &#39;against&#39;, &#39;the&#39;, &#39;form&#39;, &#39;of&#39;,\n&#39;the&#39;, &#39;statute,&#39;, &#39;&amp;amp;c.&#39;, &#39;\\xc2\\xa0&#39;, &#39;ROSE&#39;, &#39;JENNINGS&#39;, &#39;,&#39;, &#39;Esq.&#39;,\n&#39;sworn.&#39;, &#39;Had&#39;, &#39;you&#39;, &#39;any&#39;, &#39;occasion&#39;, &#39;to&#39;, &#39;be&#39;, &#39;in&#39;, &#39;this&#39;, &#39;part&#39;,\n&#39;of&#39;, &#39;the&#39;, &#39;town,&#39;, &#39;on&#39;, &#39;the&#39;, &#39;6th&#39;, &#39;of&#39;, &#39;June&#39;, &#39;in&#39;, &#39;the&#39;,\n&#39;evening?&#39;, &#39;-&#39;, &#39;I&#39;, &#39;dined&#39;, &#39;with&#39;, &#39;my&#39;, &#39;brother&#39;, &#39;who&#39;, &#39;lives&#39;,\n&#39;opposite&#39;, &#39;Mr.&#39;, &quot;Akerman&#39;s&quot;, &#39;house.&#39;, &#39;They&#39;, &#39;attacked&#39;, &#39;Mr.&#39;,\n&quot;Akerman&#39;s&quot;, &#39;house&#39;, &#39;precisely&#39;, &#39;at&#39;, &#39;seven&#39;, &quot;o&#39;clock;&quot;, &#39;they&#39;,\n&#39;were&#39;, &#39;preceded&#39;, &#39;by&#39;, &#39;a&#39;, &#39;man&#39;, &#39;better&#39;, &#39;dressed&#39;, &#39;than&#39;, &#39;the&#39;,\n&#39;rest,&#39;, &#39;who&#39;]\n</code></pre>\n<p>By itself, this ability to separate the document into words doesn’t buy\nus much because we already know how to read. We can use the text,\nhowever, to do things that aren’t usually possible without special\nsoftware. We’re going to start by computing the frequencies of tokens\nand other linguistic units, a classic measure of a text.</p>\n<p>It is clear that our list is going to need some cleaning up before we\ncan use it to count frequencies. In keeping with the practices\nestablished in <a href=\"/lessons/from-html-to-list-of-words-1\">From HTML to a List of Words (1)</a>, let’s try to\ndescribe our algorithm in plain English first. We want to know the\nfrequency of each meaningful word that appears in the trial transcript.\nSo, the steps involved might look like this:</p>\n<ul>\n<li>Convert all words to lower case so that “BENJAMIN” and “benjamin”\nare counted as the same word</li>\n<li>Remove any strange or unusual characters</li>\n<li>Count the number of times each word appears</li>\n<li>Remove overly common words such as “it”, “the”, “and”, etc.</li>\n</ul>\n<h2 id=\"convert-to-lower-case\">Convert to Lower Case</h2>\n<p>Typically tokens are <em>folded</em> to lower case when counting frequencies, so\nwe’ll do that using the string method lower which was introduced in\n<a href=\"/lessons/manipulating-strings-in-python\">Manipulating Strings in Python</a>. Since this is a string method we\nwill have to apply it to the string: <em>text</em> in the <em>html-to-list1.py</em>\nprogram. Amend <em>html-to-list1.py</em> by adding the string tag <code>lower()</code> to\nthe the end of the <em>text</em> string.</p>\n<pre><code class=\"language-python\">#html-to-list1.py\nimport urllib.request, urllib.error, urllib.parse, obo\n\nurl = &#39;http://www.oldbaileyonline.org/browse.jsp?id=t17800628-33&amp;div=t17800628-33&#39;\n\nresponse = urllib.request.urlopen(url)\nhtml = str(response.read().decode(&#39;UTF-8&#39;))\ntext = obo.stripTags(html).lower() #add the string method here.\nwordlist = text.split()\n\nprint(wordlist)\n</code></pre>\n<p>You should now see the same list of words as before, but with all\ncharacters changed to lower case.</p>\n<p>By calling methods one after another like this, we can keep our code\nshort and make some pretty significant changes to our program.</p>\n<p>Like we said before, Python makes it easy to do a lot with very little\ncode!</p>\n<p>At this point, we might look through a number of other <em>Old Bailey Online</em>\nentries and a wide range of other potential sources to make sure that\nthere aren’t other special characters that are going to cause problems\nlater. We might also try to anticipate situations where we don’t want to\nget rid of punctuation (e.g., distinguishing monetary amounts like\n“$1629” or “£1295” from dates, or recognizing that “1629-40” has a\ndifferent meaning than “1629 40”.) This is what professional programmers\nget paid to do: try to think of everything that might go wrong and deal\nwith it in advance.</p>\n<p>We’re going to take a different approach. Our main goal is to develop\ntechniques that a working historian can use during the research process.\nThis means that we will almost always prefer approximately correct\nsolutions that can be developed quickly. So rather than taking the time\nnow to make our program robust in the face of exceptions, we’re simply\ngoing to get rid of anything that isn’t an accented or unaccented letter\nor an Arabic numeral. Programming is typically a process of “stepwise\nrefinement”. You start with a problem and part of a solution, and then\nyou keep refining your solution until you have something that works\nbetter.</p>\n<h2 id=\"python-regular-expressions\">Python Regular Expressions</h2>\n<p>We’ve eliminated upper case letters. That just leaves all the\npunctuation to get rid of. Punctuation will throw off our frequency\ncounts if we leave them in. We want “evening?” to be counted as\n“evening” and “1780.” as “1780”, of course.</p>\n<p>It is possible to use the replace string method to remove each type of\npunctuation:</p>\n<pre><code class=\"language-python\">text = text.replace(&#39;[&#39;, &#39;&#39;)\ntext = text.replace(&#39;]&#39;, &#39;&#39;)\ntext = text.replace(&#39;,&#39;, &#39;&#39;)\n#etc...\n</code></pre>\n<p>But that’s not very efficient. In keeping with our goal of creating\nshort, powerful programs, we’re going to use a mechanism called <em>regular</em>\n<em>expressions</em>. Regular expressions are provided by many programming\nlanguages in a range of different forms.</p>\n<p>Regular expressions allow you to search for well defined patterns and\ncan drastically shorten the length of your code. For instance, if you\nwanted to know if a substring matched a letter of the alphabet, rather\nthan use an if/else statement to check if it matched the letter “a” then\n“b” then “c”, and so on, you could use a regular expression to see if\nthe substring matched a letter between “a” and “z”. Or, you could check\nfor the presence of a digit, or a capital letter, or any alphanumeric\ncharacter, or a carriage return, or any combination of the above, and\nmore.</p>\n<p>In Python, regular expressions are available as a Python module. To\nspeed up processing it is not loaded automatically because not all\nprograms require it. So, you will have to <code>import</code> the module (called\n<em>re</em>) in the same way that you imported your <em>obo.py</em> module.</p>\n<p>Since we’re interested in only alphanumeric characters, we’ll create a\nregular expression that will isolate only these and remove the rest.\nCopy the following function and paste it into the <em>obo.py</em> module at\nthe end. You can leave the other functions in the module alone, as we’ll\ncontinue to use those.</p>\n<pre><code class=\"language-python\"># Given a text string, remove all non-alphanumeric\n# characters (using Unicode definition of alphanumeric).\n\ndef stripNonAlphaNum(text):\n    import re\n    return re.compile(r&#39;\\W+&#39;, re.UNICODE).split(text)\n</code></pre>\n<p>The regular expression in the above code is the material inside the\nstring, in other words <code>W+</code>. The <code>W</code> is shorthand for the class of\n<em>non-alphanumeric characters</em>. In a Python regular expression, the plus\nsign (+) matches one or more copies of a given character. The <code>re.UNICODE</code>\ntells the interpreter that we want to include characters from the\nworld’s other languages in our definition of “alphanumeric”, as well as\nthe A to Z, a to z and 0-9 of English. Regular expressions have to be\n<em>compiled</em> before they can be used, which is what the rest of the\nstatement does. Don’t worry about understanding the compilation part\nright now.</p>\n<p>When we refine our <em>html-to-list1.py</em> program, it now looks like this:</p>\n<pre><code class=\"language-python\">#html-to-list1.py\nimport urllib.request, urllib.error, urllib.parse, obo\n\nurl = &#39;http://www.oldbaileyonline.org/browse.jsp?id=t17800628-33&amp;div=t17800628-33&#39;\n\nresponse = urllib.request.urlopen(url)\nhtml = response.read().decode(&#39;UTF-8&#39;)\ntext = obo.stripTags(html).lower()\nwordlist = obo.stripNonAlphaNum(text)\n\nprint(wordlist)\n</code></pre>\n<p>When you execute the program and look through its output in the “Command\nOutput” pane, you’ll see that it has done a pretty good job. This code\nwill split hyphenated forms like “coach-wheels” into two words and turn\nthe possessive “s” or “o’clock” into separate words by losing the\napostrophe. But it is a good enough approximation to what we want that\nwe should move on to counting frequencies before attempting to make it\nbetter. (If you work with sources in more than one language, you need to\nlearn more about the <a href=\"http://unicode.org/\">Unicode</a> standard and about <a href=\"https://web.archive.org/web/20180502053841/http://www.diveintopython.net/xml_processing/unicode.html\">Python support</a>\nfor it.)</p>\n<h2 id=\"suggested-reading\">Suggested Reading</h2>\n<p>For extra practice with Regular Expressions, you may find Chapter 7 of\nMark Pilgrim’s “<a href=\"https://web.archive.org/web/20180416143856/http://www.diveintopython.net/regular_expressions/index.html\">Dive into Python</a>” a useful tutorial.</p>\n<h3 id=\"code-syncing\">Code Syncing</h3>\n<p>To follow along with future lessons it is important that you have the\nright files and programs in your programming-historian directory. At the\nend of each chapter in this series you can download the programming-historian zip file\nto make sure you have the correct code.</p>\n<ul>\n<li>python-lessons4.zip (<a href=\"/assets/python-lessons4.zip\">zip sync</a>)</li>\n</ul>\n"}