<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-a80c730b.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-a80c730b.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/extracting-illustrated-pages"),
					params: {lang:"en",lessons:"lessons",slug:"extracting-illustrated-pages"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Extracting Illustrated Pages from Digital Libraries with Python</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h1 id="overview">Overview</h1>
<p>What if you only wanted to look at the pictures in a book? This is a thought that has occurred to young children and adult researchers alike. If you knew that the book was available through a digital library, it would be nice to download only those pages with pictures and ignore the rest.</p>
<p>Here are the page thumbnails for a HathiTrust volume with unique identifier <code>osu.32435078698222</code>. After the process described in this lesson, only those pages with pictures (31 in total) have been downloaded as JPEGs to a folder.</p>
<p>{% include figure.html filename=&quot;file-explorer-example.png&quot; caption=&quot;View of volume for which only pages with pictures have been downloaded.&quot; %}</p>
<p>To see how many <em>unillustrated</em> pages have been filtered out, compare against the <a href="https://babel.hathitrust.org/cgi/pt?id=osu.32435078698222;view=thumb;seq=1">full set of thumbnails</a> for all 148 pages in this revised 1845 edition of Samuel Griswold Goodrich&#39;s bestselling children&#39;s book <em>Tales of Peter Parley about America</em> (1827).</p>
<p>{% include figure.html filename=&quot;parley-full-thumbnails.png&quot; caption=&quot;View of HathiTrust thumbnails for all pages.&quot; %}</p>
<p>This lesson shows how complete these filtering and downloading steps for public-domain text volumes held by HathiTrust (HT) and Internet Archive (IA), two of the largest digital libraries in the world. It will be of interest to anyone who wants to create image corpora in order to learn about the history of illustration and the layout (<em>mise en page</em>) of books. Visual approaches to digital bibliography are becoming popular, following  the pioneering efforts of <a href="https://ebba.english.ucsb.edu/">EBBA</a> and <a href="http://projectaida.org/">AIDA</a>. Recently completed or funded projects explore ways to <a href="https://web.archive.org/web/20190526050917/http://culturalanalytics.org/2018/12/detecting-footnotes-in-32-million-pages-of-ecco/">identify footnotes</a> and <a href="http://www.ccs.neu.edu/home/dasmith/ichneumon-proposal.pdf">track marginalia</a>, to give just two <a href="https://www.neh.gov/divisions/odh/grant-news/announcing-new-2017-odh-grant-awards">examples</a>.</p>
<p>My own research tries to answer empirical questions about changes in the frequency and mode of illustration in nineteenth-century  medical and educational texts. This involves aggregating counts of pictures per book and trying to estimate what printing process was used to make those pictures. A more targeted use case for extracting picture pages might be the collation of illustrations across <a href="https://www.cambridge.org/core/books/cambridge-companion-to-robinson-crusoe/iconic-crusoe-illustrations-and-images-of-robinson-crusoe/B83352C33FB1A9929A856FFA8E2D0CD0/core-reader">different editions</a> of the same book. Future work might profitably investigate the visual characteristics and <em>meaning</em> of the extracted pictures: their color, size, theme, genre, number of figures, and so on.</p>
<p>How to get <em>localized</em> information about visual regions of interest is beyond the scope of this lesson since the process involves quite a bit of machine learning. However, the yes/no classification of pages with (or without) pictures is a practical first step to shrink the huge space of <em>all</em> pages for each book in a target collection and, thereby making illustration localization feasible. To give a reference point, nineteenth-century medical texts contain (on average) illustrations on 1-3% of their pages. If you are trying to study illustration within a digital-library corpus about which you do not have any preexisting information, it is thus reasonable to assume that 90+% of the pages in that corpus will NOT be illustrated.</p>
<p>HT and IA allow the pictures/no pictures question to be answered indirectly through parsing the data generated by optical character recognition software (OCR is applied after a physical volume is scanned in order to generate an often-noisy transcription of the text). Leveraging OCR output to find illustrated pages was first proposed by Kalev Leetaru in a <a href="https://blog.gdeltproject.org/500-years-of-the-images-of-the-worlds-books-now-on-flickr/">2014 collaboration</a> with Internet Archive and Flickr. This lesson ports Leetaru&#39;s approach to HathiTrust and takes advantage of faster XML-processing libraries in Python as well as IA&#39;s newly-extended range of image file formats.</p>
<p>Since HT and IA expose their OCR-derived information in slightly different ways, I will postpone the details of each library&#39;s &quot;visual features&quot; to their respective sections.</p>
<h1 id="goals">Goals</h1>
<p>By the end of the lesson you will be able to:</p>
<ul>
<li>Set up the &quot;minimal&quot; Anaconda Python distribution (Miniconda) and create an environment</li>
<li>Save and iterate over a list of HT or IA volumes IDs generated by a search</li>
<li>Access the HT and IA data application programmer interfaces (APIs) through Python libraries</li>
<li>Find page-level visual features</li>
<li>Download page JPEGs programmatically</li>
</ul>
<p>The big-picture goal is to strengthen data collection and exploration skills by creating a historical illustration corpus. Combining image data with volume metadata allows the formulation of promising research questions about visual change over time.</p>
<h1 id="requirements">Requirements</h1>
<p>This lesson&#39;s software requirements are minimal: access to a machine running a standard operating system and a web browser. Miniconda is available in both 32- and 64-bit versions for Windows, macOS, and Linux. Python 3 is the current stable release of the language and will be supported indefinitely.</p>
<p>This tutorial assumes basic knowledge of the command line and the Python  programming language. You should understand the conventions for comments and commands in a shell-based tutorial. I recommend Ian Milligan and James Baker&#39;s <a href="/en/lessons/intro-to-bash">Introduction to the Bash Command Line</a> for learning or brushing up on your command line skills.</p>
<h1 id="setup">Setup</h1>
<h2 id="dependencies">Dependencies</h2>
<p>More experienced readers may wish to simply install the dependencies and run the notebooks in their environment of choice. Further information on my own Miniconda setup (and some Windows/*nix differences) is provided.</p>
<ul>
<li><code>hathitrust-api</code> (<a href="https://github.com/rlmv/hathitrust-api">Install docs</a>)</li>
<li><code>internetarchive</code> (<a href="https://archive.org/services/docs/api/internetarchive/">Install docs</a>)</li>
<li><code>jupyter</code> (<a href="https://jupyter.org/install">Install docs</a>)</li>
<li><code>requests</code> (<a href="https://requests.readthedocs.io/en/master/">Install docs</a>) [creator recommends <code>pipenv</code> installation; for <code>pip</code> install see <a href="https://pypi.org/project/requests2/">PyPI</a>]</li>
</ul>
<h2 id="lesson-files">Lesson Files</h2>
<p>Download this compressed <a href="/assets/extracting-illustrated-pages/lesson-files.zip">folder</a> that contains two Jupyter notebooks, one for each of the digital libraries. The folder also contains a sample JSON metadata file describing a HathiTrust collection. Unzip and check that the following files are present: <code>554050894-1535834127.json</code>, <code>hathitrust.ipynb</code>, <code>internetarchive.ipynb</code>.</p>
<div class="alert alert-warning">
All subsequent commands assume that your current working directory is the folder containing the lesson files.
</div>


<h3 id="download-destination">Download Destination</h3>
<p>Here is the default directory that will be created once all the cells in both notebooks have been run (as provided). After getting a list of which pages in a volume contain pictures, the HT and IA download functions request those pages as JPEGs (named by page number) and store them in sub-directories (named by item ID). You can of course use different volume lists or change the <code>out_dir</code> destination to something other than <code>items</code>.</p>
<pre><code>items/
├── hathitrust
│   ├── hvd.32044021161005
│   │   ├── 103.jpg
│   │   └── ...
│   └── osu.32435078698222
│       ├── 100.jpg
│       ├── ...
└── internetarchive
    └── talespeterparle00goodgoog
        ├── 103.jpg
        └── ...

5 directories, 113 files
</code></pre>
<p>The download functions are lazy; if you run the notebooks again, with the <code>items</code> directory looking as it does above, any item that already has its own sub-folder will be skipped.</p>
<h2 id="anaconda-optional">Anaconda (optional)</h2>
<p>Anaconda is the leading scientific Python distribution. Its <code>conda</code> package manager allows you to install libraries such as <code>numpy</code> and <code>tensorflow</code> with ease. The &quot;Miniconda&quot; version does not come with any superfluous packages preinstalled, which encourages you to keep your base environment clean and only install what you need for a project within a named environment.</p>
<p>Download and install <a href="https://conda.io/miniconda.html">Miniconda</a>. Choose the latest stable release of Python 3. If everything goes well, you should be able to run <code>which conda</code> (linux/macOS) or <code>where conda</code> (Windows) in your shell and see the location of the executable program in the output.</p>
<p>Anaconda has a handy <a href="http://web.archive.org/web/20190115051900/https://conda.io/docs/_downloads/conda-cheatsheet.pdf">cheat sheet</a> for frequently used commands.</p>
<h3 id="create-an-environment">Create an Environment</h3>
<p>Environments, among other things, help control the complexity associated with using multiple package managers in tandem. Not all Python libraries can be installed through <code>conda</code>. In some cases we will fall back to the standard Python package manager, <code>pip</code> (or planned replacements like <code>pipenv</code>). However, when we do so, we will use a version of <code>pip</code> installed by <code>conda</code>. This keeps all the packages we need for the project in the same virtual sandbox.</p>
<pre><code class="language-bash"># your current environment is indicated by a preceding asterisk
# (it will be &quot;base&quot; in a new shell)
conda env list

# installed packages in the current environment
conda list
</code></pre>
<p>Now we create a named environment, set it to use Python 3, and activate it.</p>
<pre><code class="language-bash"># note the --name flag which takes a string argument (e.g. &quot;extract-pages&quot;)
# and the syntax for specifying the Python version
conda create --name extract-pages python=3

# enter the new environment (macOS/Linux)
source activate extract-pages
</code></pre>
<pre><code class="language-bash"># Windows command for activating environment is slightly different
conda activate extract-pages
</code></pre>
<p>To exit an environment, run <code>source deactivate</code> on macOS/Linux or <code>deactivate</code> on Windows. But make sure to stay in the <code>extract-pages</code> environment for the duration of the lesson!</p>
<h3 id="install-conda-packages">Install Conda Packages</h3>
<p>We can use <code>conda</code> to install our first couple of packages. All the other required packages (gzip, json, os, sys, and time) are part of the <a href="https://docs.python.org/3/library/">standard Python library</a>. Note how we need to specify a channel in some cases. You can search for packages on <a href="https://anaconda.org/">Anaconda Cloud</a>.</p>
<pre><code class="language-bash"># to ensure we have a local version of pip (see discussion below)
conda install pip

conda install jupyter

conda install --channel anaconda requests
</code></pre>
<p>Jupyter has many dependencies (other packages on which it relies), so this step may take a few minutes. Remember that when <code>conda</code> prompts you with <code>Proceed ([y]/n)?</code> you should type a <code>y</code> or <code>yes</code> and then press Enter to accept the package plan.</p>
<div class="alert alert-warning">
  Behind the scenes, <code>conda</code> is working to make sure all the required packages and dependencies will be installed in a compatible way.
</div>


<h3 id="install-pip-packages">Install Pip Packages</h3>
<p>If you are using a <code>conda</code> environment, it&#39;s best to use the local version of <code>pip</code>. Check that the following commands output a program whose absolute path contains something like <code>/Miniconda/envs/extract-pages/Scripts/pip</code>.</p>
<pre><code class="language-bash">which pip
</code></pre>
<pre><code class="language-bash"># Windows equivalent to &quot;which&quot;
where pip
</code></pre>
<p>If you see two versions of <code>pip</code> in the output above, make sure to type the full path to the <em>local</em> environment version when installing the API wrapper libraries:</p>
<pre><code class="language-bash">pip install hathitrust-api
pip install internetarchive
</code></pre>
<pre><code class="language-bash"># Windows example using the absolute path to the *local* pip executable
C:\Users\stephen-krewson\Miniconda\envs\extract-pages\Scripts\pip.exe install hathitrust-api internetarchive
</code></pre>
<h2 id="jupyter-notebooks">Jupyter Notebooks</h2>
<p>Peter Organisciak and Boris Capitanu&#39;s <a href="/en/lessons/text-mining-with-extracted-features#start-a-notebook">Text Mining in Python through the HTRC Feature Reader</a> explains the benefits of notebooks for development and data exploration. It also contains helpful information on how to effectively run cells. Since we installed the minimalist version of Anaconda, we need to launch Jupyter from the command line. In your shell (from inside the folder containing the lesson files) run <code>jupyter notebook</code>.</p>
<p>This will run the notebook server in your shell and launch your default browser with the Jupyter homepage. The homepage shows all the files in the current working directory.</p>
<p>{% include figure.html filename=&quot;jupyter-home.png&quot; caption=&quot;Jupyter homepage showing lesson files.&quot; %}</p>
<div class="alert alert-warning">
In your shell, make sure you have <code>cd</code>-ed into the unzipped <code>lesson-files</code> directory.
</div>

<p>Click on the <code>hathitrust.ipynb</code> and <code>internetarchive.ipynb</code> notebooks to open them in new browser tabs. From now on, we don&#39;t need to run any commands in the shell. The notebooks allow us to execute Python code and have full access to the computer&#39;s file system. When you are finished, you can stop the notebook server by clicking &quot;Quit&quot; on the Jupyter homepage or executing <code>ctrl+c</code> in the shell.</p>
<h1 id="hathitrust">HathiTrust</h1>
<h2 id="api-access">API Access</h2>
<p>You need to register with HathiTrust before using the Data API. Head over to the <a href="https://babel.hathitrust.org/cgi/kgs/request">registration portal</a> and fill out your name, organization, and email to request access keys. You should receive an email response within a minute or so. Click the link, which will take you to a one-time page with both keys displayed.</p>
<p>In the <code>hathitrust.ipynb</code> notebook, examine the very first cell (shown below). Fill in your API tokens as directed. Then run the cell by clicking &quot;Run&quot; in the notebook navbar.</p>
<pre><code class="language-python"># Import the HT Data API wrapper
from hathitrust_api import DataAPI

# Replace placeholder strings with your HT credentials (leaving the quote marks)
ht_access_key = &quot;YOUR_ACCESS_KEY_HERE&quot;
ht_secret_key = &quot;YOUR_SECRET_KEY_HERE&quot;

# instantiate the Data API connection object
data_api = DataAPI(ht_access_key, ht_secret_key)
</code></pre>
<div class="alert alert-warning">
  Careful! Do not expose your access tokens through a public repo on GitHub (or other version control host). They will be searchable by just about anyone. One good practice for a Python project is to either store your tokens as environment variables or save them in a file that is not versioned.
</div>


<h2 id="create-volume-list">Create Volume List</h2>
<p>HT allows anyone to make a collection of items&mdash;you don&#39;t even have to be logged in! You should register for an account, however, if you want to save your list of volumes. Follow the <a href="https://babel.hathitrust.org/cgi/mb?colltype=updated">instructions</a> to do some full-text searches and then add selected results to a collection. Currently, HathiTrust does not have a public search API for acquiring volumes programmatically; you need to search through their web interface.</p>
<p>As you update a collection, HT keeps track of the associated metadata for each item in it. I have included in the lesson files the metadata for a sample lesson in JSON format. If you wanted to use the file from your own HT collection, you would navigate to your collections page and hover on the metadata link on the left to bring up the option to download as JSON as seen in the following screenshot.</p>
<p>{% include figure.html filename=&quot;download-ht-json.png&quot; caption=&quot;Screenshot of how to download collection metadata in JSON format.&quot; %}</p>
<p>When you have downloaded the JSON file, simply move it to the directory where you placed the Jupyter notebooks. Replace the name of the JSON file in the HT notebook with the name of your collection&#39;s file.</p>
<p>The notebook shows how to use a list comprehension to get all the <code>htitem_id</code> strings within the <code>gathers</code> object that contains all the collection information.</p>
<pre><code class="language-Python"># you can specify your collection metadata file here
metadata_path = &quot;554050894-1535834127.json&quot;

with open(metadata_path, &quot;r&quot;) as fp:
    data = json.load(fp)

# a list of all unique ids in the collection
vol_ids = [item[&#39;htitem_id&#39;] for item in data[&#39;gathers&#39;]]
</code></pre>
<div class="alert alert-warning">
  Tutorials often show you how to process one example item (often of a trivial size or complexity). This is pedagogically convenient, but it means you are less equipped to apply that code to multiple items&mdash;by far the more common use case. In the notebooks you will see how to encapsulate transformations applied to one item into <i>functions</i> that can be called within a loop over a collection of items.
</div>

<h2 id="visual-feature-image_on_page">Visual Feature: IMAGE_ON_PAGE</h2>
<p>Given a list of volumes, we want to explore what visual features they have at the page level. The <a href="https://www.hathitrust.org/documents/hathitrust-data-api-v2_20150526.pdf">most recent documentation</a> (2015) for the Data API describes a metadata object called <code>htd:pfeat</code> on pages 9-10. <code>htd:pfeat</code> is shorthand for &quot;HathiTrust Data API: Page Features.&quot;</p>
<blockquote>
<ul>
<li><code>htd:pfeat</code>­ - the page feature key (if available):<ul>
<li>CHAPTER_START</li>
<li>COPYRIGHT</li>
<li>FIRST_CONTENT_CHAPTER_START</li>
<li>FRONT_COVER</li>
<li>INDEX</li>
<li>REFERENCES</li>
<li>TABLE_OF_CONTENTS</li>
<li>TITLE</li>
</ul>
</li>
</ul>
</blockquote>
<p>What the <code>hathitrust-api</code> wrapper does is make the full metadata for a HT volume available as a Python object. Given a volume&#39;s identifier, we can request its metadata and then drill down through the page <em>sequence</em> into page-level information. The <code>htd:pfeat</code> <em>list</em> is associated with each page in a volume and in theory contains all features that apply to that page. In practice, there a quite a few more feature tags than the eight listed above. The one we will be working with is called <code>IMAGE_ON_PAGE</code> and is more abstractly visual than structural tags such as <code>CHAPTER_START</code>.</p>
<p>Tom Burton-West, a research librarian at the University of Michigan Library, works closely with HathiTrust and HTRC, HathiTrust&#39;s Research Center. Tom told me over email that HathiTrust is provided the <code>htd:pfeat</code> information by Google, with whom they have worked closely since HT&#39;s founding in 2008. A contact at Google gave Tom permission to share the following:</p>
<blockquote>
<p>These tags are derived from a combination of heuristics, machine learning, and human tagging.</p>
</blockquote>
<p>An example heuristic might be that the first element in the volume page sequence is almost always the <code>FRONT_COVER</code>. Machine learning could be used to train models to discriminate, say, between image data that is more typical of lines of prose in a Western script or of the lines in an engraving. Human tagging is the manual assignment of labels to images. The ability to view a volume&#39;s illustrations in the EEBO and ECCO databases is an example of human tagging.</p>
<p>The use of &quot;machine learning&quot; by Google sounds somewhat mysterious. Until Google publicizes their methods, it is impossible to know all the details. However, it&#39;s likely that the <code>IMAGE_ON_PAGE</code> tags were first proposed by detecting &quot;Picture&quot; blocks in the OCR output files (a process discussed below in the Internet Archive section). Further filtering may then be applied.</p>
<h2 id="code-walk-through">Code Walk-through</h2>
<h3 id="find-pictures">Find Pictures</h3>
<p>We have seen how to create a list of volumes and observed that the Data API can be used to get metadata objects containing page-level experimental features. The core function in the HT notebook has the signature <code>ht_picture_download(item_id, out_dir=None)</code>. Given a unique identifier and an optional destination directory, this function will first get the volume&#39;s metadata from the API and convert it into JSON format. Then it loops over the page sequence and checks if the tag <code>IMAGE_ON_PAGE</code> is in the <code>htd:pfeat</code> list (if it exists).</p>
<pre><code class="language-python"># metadata from API in json format (different than HT collection metadata)
meta = json.loads(data_api.getmeta(item_id, json=True))

# sequence gets us each page of the scanned item in order, with any
# additional information that might be available for it
sequence = meta[&#39;htd:seqmap&#39;][0][&#39;htd:seq&#39;]

# list of pages with pictures (empty to start)
img_pages = []

# try/except block handles situation where no &quot;pfeats&quot; exist OR
# the sequence numbers are not numeric
for page in sequence:
    try:
        if &#39;IMAGE_ON_PAGE&#39; in page[&#39;htd:pfeat&#39;]:
            img_pages.append(int(page[&#39;pseq&#39;]))
    except (KeyError, TypeError) as e:
        continue
</code></pre>
<p>Notice that we need to drill down several levels into the top-level object to get the <code>htd:seq</code> object, which we can iterate over.</p>
<p>The two exceptions I want to catch are <code>KeyError</code>, which occurs when the page does not have an page-level features associated with it and <code>TypeError</code>, which occurs when the <code>pseq</code> field for the page is for some reason non-numeric and thus cannot be cast to an <code>int</code>. If something goes wrong with a page, we just <code>continue</code> on to the next one. The idea is to get all the good data we can. Not to clean up inconsistencies or gaps in the item metadata.</p>
<h3 id="download-images">Download Images</h3>
<p>Once <code>img_pages</code> contains the complete list of pages tagged with <code>IMAGE_ON_PAGE</code>, we can download those pages. Note that if no <code>out_dir</code> is supplied to <code>ht_picture_download()</code>, then the function simply returns the <code>img_pages</code> list and does NOT download anything.</p>
<p>The <code>getpageimage()</code> API call returns a JPEG by default. We simply write out the JPEG bytes to a file in the normal way. Within the volume sub-folder (itself inside <code>out_dir</code>), the pages will be named <code>1.jpg</code> for page 1 and so forth.</p>
<p>One thing to consider is our usage rate of the API. We don&#39;t want to abuse our access by making hundreds of requests per minute. To be safe, especially if we intend to run big jobs, we wait two seconds before making each page request. This may be frustrating in the short term, but it helps avoid API throttling or banning.</p>
<pre><code class="language-Python">for i, page in enumerate(img_pages):
    try:
        # simple status message
        print(&quot;[{}] Downloading page {} ({}/{})&quot;.format(item_id, \
            page, i+1, total_pages))

        img = data_api.getpageimage(item_id, page)

        # N.B. loop only executes if out_dir is not None
        img_out = os.path.join(out_dir, str(page) + &quot;.jpg&quot;)

        # write out the image
        with open(img_out, &#39;wb&#39;) as fp:
            fp.write(img)

        # to avoid exceeding the allowed API usage
        time.sleep(2)

    except Exception as e:
        print(&quot;[{}] Error downloading page {}: {}&quot;.format(item_id, page,e))
</code></pre>
<h1 id="internet-archive">Internet Archive</h1>
<h2 id="api-access-1">API Access</h2>
<p>We connect to the Python API library using an Archive.org account email and password rather than API tokens. This is discussed in the <a href="https://archive.org/services/docs/api/internetarchive/quickstart.html">Quickstart Guide</a>. If you do not have an account, <a href="https://archive.org/account/login.createaccount.php">register</a> for your &quot;Virtual Library Card.&quot;</p>
<p>In the first cell of the <code>internetarchive.ipynb</code> notebook, enter your credentials as directed. Run the cell to authenticate to the API.</p>
<h2 id="create-volume-list-1">Create Volume List</h2>
<p>The IA Python library allows you to submit query strings and receive a list of matching key-value pairs where the word &quot;identifier&quot; is the key and the actual identifier is the value. The syntax for a query is explained on the <a href="https://archive.org/advancedsearch.php">Advanced Search page</a> for IA. You can specify parameters by using a keyword like &quot;date&quot; or &quot;mediatype&quot; followed by a colon and the value you want to assign that parameter. For instance, I only want results that are <em>texts</em> (as opposed to video, etc.). Make sure the parameters and options you are trying to use are supported by IA&#39;s search functionality. Otherwise you may get missing or weird results and not know why.</p>
<p>In the notebook, I generate a list of IA ids with the following code:</p>
<pre><code class="language-Python"># sample search (should yield two results)
query = &quot;peter parley date:[1825 TO 1830] mediatype:texts&quot;
vol_ids = [result[&#39;identifier&#39;] for result in ia.search_items(query)]
</code></pre>
<h2 id="visual-feature-picture-blocks">Visual Feature: Picture Blocks</h2>
<p>Internet Archive does not release any page-level features. Instead, it makes a number of raw files from the digitization process available to users. The most important of these for our purposes is the Abbyy XML file. Abbyy is a Russian company whose FineReader software dominates the OCR market.</p>
<p>All recent versions of FineReader produce an <a href="https://en.wikipedia.org/wiki/XML">XML document</a> that associates different &quot;blocks&quot; with each page in the scanned document. The most common type of block is <code>Text</code> but there are <code>Picture</code> blocks as well. Here is an example block taken from an IA Abbyy XML file. The top-left (&quot;t&quot; and &quot;l&quot;) and bottom-right (&quot;b&quot; and &quot;r&quot;) corners are enough to identify the rectangular block region.</p>
<pre><code class="language-xml">&lt;block blockType=&quot;Picture&quot; l=&quot;586&quot; t=&quot;1428&quot; r=&quot;768&quot; b=&quot;1612&quot;&gt;
    &lt;region&gt;&lt;rect l=&quot;586&quot; t=&quot;1428&quot; r=&quot;768&quot; b=&quot;1612&quot;&gt;&lt;/rect&gt;&lt;/region&gt;
&lt;/block&gt;
</code></pre>
<p>The IA equivalent to looking for <code>IMAGE_ON_PAGE</code> tags in HT is parsing the Abbyy XML file and iterating over each page. If there is at least one <code>Picture</code> block on that page, the page is flagged as possibly containing an image.</p>
<p>While HT&#39;s <code>IMAGE_ON_PAGE</code> feature contains no information about the <em>location</em> of that image, the <code>Picture</code> blocks in the XML file are associated with a rectangular region on the page. However, since FineReader specializes in recognizing letters from Western character sets, it is much less accurate at identifying image regions. Leetaru&#39;s project (see Overview) used the region coordinates to crop pictures, but in this lesson we will simply download the whole page.</p>
<p>Part of the intellectual fun of this lesson is using a noisy dataset (OCR block tags) for a largely unintended purpose: identifying pictures and not words. At some point, it will become computationally feasible to run deep learning models on every raw page image in a volume and pick out the desired type(s) of picture(s). But since most pages in most volumes are unillustrated, that is an expensive task. For now, it makes more sense to leverage the existing data we have from the OCR ingest process.</p>
<p>For more information on how OCR itself works and interacts with the scan process, please see Mila Oiva&#39;s PH lesson <a href="/en/lessons/retired/OCR-with-Tesseract-and-ScanTailor">OCR with Tesseract and ScanTailor</a>. Errors can crop up due to skewing, artefacts, and many other problems. These errors end up affecting the reliability and precision of the &quot;Picture&quot; blocks. In many cases, Abbyy will estimate that blank or discolored pages are actually pictures. These incorrect block tags, while undesirable, can be dealt with using retrained convolutional neural networks. Think of the page images downloaded in this lesson as a first pass in a longer process of obtaining a clean and usable dataset of historical illustrations.</p>
<h2 id="code-walk-through-1">Code Walk-through</h2>
<h3 id="find-pictures-1">Find Pictures</h3>
<p>Just as with HT, the core function for IA is <code>ia_picture_download(item_id, out_dir=None)</code>.</p>
<p>Since it involves file I/O, the process for geting the <code>img_pages</code> list is more complicated than that for HT. Using the command line utility <code>ia</code> (which is installed with the library), you can get a sense of the available metadata files for a volume. With very few exceptions, a file with format &quot;Abbyy GZ&quot; should be available for volumes with media type <code>text</code> on Internet Archive.</p>
<p>These files, even when compressed, can easily be hundreds of megabytes in size! If there is an Abbyy file for the volume, we get its name and then download it. The <code>ia.download()</code> call uses some helpful parameters to ignore the request if the file already exists and, if not, download it without creating a nested directory. To save space, we delete the Abbyy file after parsing it.</p>
<pre><code class="language-python"># Use command-line client to see available metadata formats:
# `ia metadata formats VOLUME_ID`

# for this lesson, only the Abbyy file is needed
returned_files = list(ia.get_files(item_id, formats=[&quot;Abbyy GZ&quot;]))

# make sure something got returned
if len(returned_files) &gt; 0:
    abbyy_file = returned_files[0].name
else:
    print(&quot;[{}] Could not get Abbyy file&quot;.format(item_id))
    return None

# download the abbyy file to CWD
ia.download(item_id, formats=[&quot;Abbyy GZ&quot;], ignore_existing=True, \
    destdir=os.getcwd(), no_directory=True)
</code></pre>
<p>Once we have the file, we need to parse the XML using the standard Python library. We take advantage of the fact that we can open the compressed file directly with the <code>gzip</code> library. Abbyy files are zero-indexed so the first page in the scan sequence has index 0. However, we have to filter out 0 since it cannot be requested from IA. IA&#39;s exclusion of index 0 is not documented anywhere; rather, I found out through trial and error. If you see a hard-to-explain error message, try to track down the source and don&#39;t be afraid to ask for help, whether from someone with relevant experience or at the organization itself.</p>
<pre><code class="language-Python"># collect the pages with at least one picture block
img_pages = []

with gzip.open(abbyy_file) as fp:
    tree = ET.parse(fp)
    document = tree.getroot()
    for i, page in enumerate(document):
        for block in page:
            try:
                if block.attrib[&#39;blockType&#39;] == &#39;Picture&#39;:
                    img_pages.append(i)
                    break
            except KeyError:
                continue

# 0 is not a valid page for making GET requests to IA, yet sometimes
# it&#39;s in the zipped Abbyy file
img_pages = [page for page in img_pages if page &gt; 0]

# track for download progress report
total_pages = len(img_pages)

# OCR files are huge, so just delete once we have pagelist
os.remove(abbyy_file)
</code></pre>
<h3 id="download-images-1">Download Images</h3>
<p>IA&#39;s Python wrapper does not provide a single-page download function&mdash;only bulk. This means that we will use IA&#39;s RESTful API to get specific pages. First we construct a URL for each page that we need. Then we use the <code>requests</code> library to send an HTTP <code>GET</code> request and, if everything goes well (i.e. the code 200 is returned in the response), we write out the contents of the response to a JPEG file.</p>
<p>IA has been working on an <a href="https://iiif.archivelab.org/iiif/documentation">alpha version</a> of an API for image cropping and resizing that conforms to the standards of the International Image Interoperability Framework (<a href="https://iiif.io/">IIIF</a>). IIIF represents a vast improvement on the old method for single-page downloads which required downloading JP2 files, a largely unsupported archival format. Now it&#39;s extremely simple to get a single page JPEG:</p>
<pre><code class="language-Python"># See: https://iiif.archivelab.org/iiif/documentation
urls = [&quot;https://iiif.archivelab.org/iiif/{}${}/full/full/0/default.jpg&quot;.format(item_id, page)
    for page in img_pages]

# no direct page download through python library, construct GET request
for i, page, url in zip(range(1,total_pages), img_pages, urls):

    rsp = requests.get(url, allow_redirects=True)

    if rsp.status_code == 200:
        print(&quot;[{}] Downloading page {} ({}/{})&quot;.format(item_id, \
            page, i+1, total_pages))

        with open(os.path.join(out_dir, str(page) + &quot;.jpg&quot;), &quot;wb&quot;) as fp:
            fp.write(rsp.content)
</code></pre>
<h1 id="next-steps">Next Steps</h1>
<p>Once you understand the main functions and the data unpacking code in the notebooks, feel free to run the cells in sequence or &quot;Run All&quot; and watch the picture pages roll in. You are encouraged to adapt these scripts and functions for your own research questions.</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="extracting-illustrated-pages/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Extracting Illustrated Pages from Digital Libraries with Python\",\"collection\":\"lessons\",\"layout\":\"lesson\",\"slug\":\"extracting-illustrated-pages\",\"date\":\"2019-01-14T00:00:00.000Z\",\"authors\":[\"Stephen Krewson\"],\"reviewers\":[\"Catherine DeRose\",\"Taylor Arnold\"],\"editors\":[\"Anandi Silva Knuppel\"],\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F193\",\"difficulty\":2,\"activity\":\"acquiring\",\"topics\":[\"api\"],\"abstract\":\"Machine learning and API extensions by HathiTrust and Internet Archive are making it easier to extract page regions of visual interest from digitized volumes. This lesson shows how to efficiently extract those regions and, in doing so, prompt new, visual research questions.\",\"avatar_alt\":\"Scientific measuring device\",\"doi\":\"10.46430\u002Fphen0084\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"overview\\\"\u003EOverview\u003C\u002Fh1\u003E\\n\u003Cp\u003EWhat if you only wanted to look at the pictures in a book? This is a thought that has occurred to young children and adult researchers alike. If you knew that the book was available through a digital library, it would be nice to download only those pages with pictures and ignore the rest.\u003C\u002Fp\u003E\\n\u003Cp\u003EHere are the page thumbnails for a HathiTrust volume with unique identifier \u003Ccode\u003Eosu.32435078698222\u003C\u002Fcode\u003E. After the process described in this lesson, only those pages with pictures (31 in total) have been downloaded as JPEGs to a folder.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;file-explorer-example.png&quot; caption=&quot;View of volume for which only pages with pictures have been downloaded.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ETo see how many \u003Cem\u003Eunillustrated\u003C\u002Fem\u003E pages have been filtered out, compare against the \u003Ca href=\\\"https:\u002F\u002Fbabel.hathitrust.org\u002Fcgi\u002Fpt?id=osu.32435078698222;view=thumb;seq=1\\\"\u003Efull set of thumbnails\u003C\u002Fa\u003E for all 148 pages in this revised 1845 edition of Samuel Griswold Goodrich&#39;s bestselling children&#39;s book \u003Cem\u003ETales of Peter Parley about America\u003C\u002Fem\u003E (1827).\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;parley-full-thumbnails.png&quot; caption=&quot;View of HathiTrust thumbnails for all pages.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThis lesson shows how complete these filtering and downloading steps for public-domain text volumes held by HathiTrust (HT) and Internet Archive (IA), two of the largest digital libraries in the world. It will be of interest to anyone who wants to create image corpora in order to learn about the history of illustration and the layout (\u003Cem\u003Emise en page\u003C\u002Fem\u003E) of books. Visual approaches to digital bibliography are becoming popular, following  the pioneering efforts of \u003Ca href=\\\"https:\u002F\u002Febba.english.ucsb.edu\u002F\\\"\u003EEBBA\u003C\u002Fa\u003E and \u003Ca href=\\\"http:\u002F\u002Fprojectaida.org\u002F\\\"\u003EAIDA\u003C\u002Fa\u003E. Recently completed or funded projects explore ways to \u003Ca href=\\\"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20190526050917\u002Fhttp:\u002F\u002Fculturalanalytics.org\u002F2018\u002F12\u002Fdetecting-footnotes-in-32-million-pages-of-ecco\u002F\\\"\u003Eidentify footnotes\u003C\u002Fa\u003E and \u003Ca href=\\\"http:\u002F\u002Fwww.ccs.neu.edu\u002Fhome\u002Fdasmith\u002Fichneumon-proposal.pdf\\\"\u003Etrack marginalia\u003C\u002Fa\u003E, to give just two \u003Ca href=\\\"https:\u002F\u002Fwww.neh.gov\u002Fdivisions\u002Fodh\u002Fgrant-news\u002Fannouncing-new-2017-odh-grant-awards\\\"\u003Eexamples\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EMy own research tries to answer empirical questions about changes in the frequency and mode of illustration in nineteenth-century  medical and educational texts. This involves aggregating counts of pictures per book and trying to estimate what printing process was used to make those pictures. A more targeted use case for extracting picture pages might be the collation of illustrations across \u003Ca href=\\\"https:\u002F\u002Fwww.cambridge.org\u002Fcore\u002Fbooks\u002Fcambridge-companion-to-robinson-crusoe\u002Ficonic-crusoe-illustrations-and-images-of-robinson-crusoe\u002FB83352C33FB1A9929A856FFA8E2D0CD0\u002Fcore-reader\\\"\u003Edifferent editions\u003C\u002Fa\u003E of the same book. Future work might profitably investigate the visual characteristics and \u003Cem\u003Emeaning\u003C\u002Fem\u003E of the extracted pictures: their color, size, theme, genre, number of figures, and so on.\u003C\u002Fp\u003E\\n\u003Cp\u003EHow to get \u003Cem\u003Elocalized\u003C\u002Fem\u003E information about visual regions of interest is beyond the scope of this lesson since the process involves quite a bit of machine learning. However, the yes\u002Fno classification of pages with (or without) pictures is a practical first step to shrink the huge space of \u003Cem\u003Eall\u003C\u002Fem\u003E pages for each book in a target collection and, thereby making illustration localization feasible. To give a reference point, nineteenth-century medical texts contain (on average) illustrations on 1-3% of their pages. If you are trying to study illustration within a digital-library corpus about which you do not have any preexisting information, it is thus reasonable to assume that 90+% of the pages in that corpus will NOT be illustrated.\u003C\u002Fp\u003E\\n\u003Cp\u003EHT and IA allow the pictures\u002Fno pictures question to be answered indirectly through parsing the data generated by optical character recognition software (OCR is applied after a physical volume is scanned in order to generate an often-noisy transcription of the text). Leveraging OCR output to find illustrated pages was first proposed by Kalev Leetaru in a \u003Ca href=\\\"https:\u002F\u002Fblog.gdeltproject.org\u002F500-years-of-the-images-of-the-worlds-books-now-on-flickr\u002F\\\"\u003E2014 collaboration\u003C\u002Fa\u003E with Internet Archive and Flickr. This lesson ports Leetaru&#39;s approach to HathiTrust and takes advantage of faster XML-processing libraries in Python as well as IA&#39;s newly-extended range of image file formats.\u003C\u002Fp\u003E\\n\u003Cp\u003ESince HT and IA expose their OCR-derived information in slightly different ways, I will postpone the details of each library&#39;s &quot;visual features&quot; to their respective sections.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"goals\\\"\u003EGoals\u003C\u002Fh1\u003E\\n\u003Cp\u003EBy the end of the lesson you will be able to:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ESet up the &quot;minimal&quot; Anaconda Python distribution (Miniconda) and create an environment\u003C\u002Fli\u003E\\n\u003Cli\u003ESave and iterate over a list of HT or IA volumes IDs generated by a search\u003C\u002Fli\u003E\\n\u003Cli\u003EAccess the HT and IA data application programmer interfaces (APIs) through Python libraries\u003C\u002Fli\u003E\\n\u003Cli\u003EFind page-level visual features\u003C\u002Fli\u003E\\n\u003Cli\u003EDownload page JPEGs programmatically\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EThe big-picture goal is to strengthen data collection and exploration skills by creating a historical illustration corpus. Combining image data with volume metadata allows the formulation of promising research questions about visual change over time.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"requirements\\\"\u003ERequirements\u003C\u002Fh1\u003E\\n\u003Cp\u003EThis lesson&#39;s software requirements are minimal: access to a machine running a standard operating system and a web browser. Miniconda is available in both 32- and 64-bit versions for Windows, macOS, and Linux. Python 3 is the current stable release of the language and will be supported indefinitely.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis tutorial assumes basic knowledge of the command line and the Python  programming language. You should understand the conventions for comments and commands in a shell-based tutorial. I recommend Ian Milligan and James Baker&#39;s \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fintro-to-bash\\\"\u003EIntroduction to the Bash Command Line\u003C\u002Fa\u003E for learning or brushing up on your command line skills.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"setup\\\"\u003ESetup\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"dependencies\\\"\u003EDependencies\u003C\u002Fh2\u003E\\n\u003Cp\u003EMore experienced readers may wish to simply install the dependencies and run the notebooks in their environment of choice. Further information on my own Miniconda setup (and some Windows\u002F*nix differences) is provided.\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Ehathitrust-api\u003C\u002Fcode\u003E (\u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Frlmv\u002Fhathitrust-api\\\"\u003EInstall docs\u003C\u002Fa\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Einternetarchive\u003C\u002Fcode\u003E (\u003Ca href=\\\"https:\u002F\u002Farchive.org\u002Fservices\u002Fdocs\u002Fapi\u002Finternetarchive\u002F\\\"\u003EInstall docs\u003C\u002Fa\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Ejupyter\u003C\u002Fcode\u003E (\u003Ca href=\\\"https:\u002F\u002Fjupyter.org\u002Finstall\\\"\u003EInstall docs\u003C\u002Fa\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Erequests\u003C\u002Fcode\u003E (\u003Ca href=\\\"https:\u002F\u002Frequests.readthedocs.io\u002Fen\u002Fmaster\u002F\\\"\u003EInstall docs\u003C\u002Fa\u003E) [creator recommends \u003Ccode\u003Epipenv\u003C\u002Fcode\u003E installation; for \u003Ccode\u003Epip\u003C\u002Fcode\u003E install see \u003Ca href=\\\"https:\u002F\u002Fpypi.org\u002Fproject\u002Frequests2\u002F\\\"\u003EPyPI\u003C\u002Fa\u003E]\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch2 id=\\\"lesson-files\\\"\u003ELesson Files\u003C\u002Fh2\u003E\\n\u003Cp\u003EDownload this compressed \u003Ca href=\\\"\u002Fassets\u002Fextracting-illustrated-pages\u002Flesson-files.zip\\\"\u003Efolder\u003C\u002Fa\u003E that contains two Jupyter notebooks, one for each of the digital libraries. The folder also contains a sample JSON metadata file describing a HathiTrust collection. Unzip and check that the following files are present: \u003Ccode\u003E554050894-1535834127.json\u003C\u002Fcode\u003E, \u003Ccode\u003Ehathitrust.ipynb\u003C\u002Fcode\u003E, \u003Ccode\u003Einternetarchive.ipynb\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\nAll subsequent commands assume that your current working directory is the folder containing the lesson files.\\n\u003C\u002Fdiv\u003E\\n\\n\\n\u003Ch3 id=\\\"download-destination\\\"\u003EDownload Destination\u003C\u002Fh3\u003E\\n\u003Cp\u003EHere is the default directory that will be created once all the cells in both notebooks have been run (as provided). After getting a list of which pages in a volume contain pictures, the HT and IA download functions request those pages as JPEGs (named by page number) and store them in sub-directories (named by item ID). You can of course use different volume lists or change the \u003Ccode\u003Eout_dir\u003C\u002Fcode\u003E destination to something other than \u003Ccode\u003Eitems\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Eitems\u002F\\n├── hathitrust\\n│   ├── hvd.32044021161005\\n│   │   ├── 103.jpg\\n│   │   └── ...\\n│   └── osu.32435078698222\\n│       ├── 100.jpg\\n│       ├── ...\\n└── internetarchive\\n    └── talespeterparle00goodgoog\\n        ├── 103.jpg\\n        └── ...\\n\\n5 directories, 113 files\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe download functions are lazy; if you run the notebooks again, with the \u003Ccode\u003Eitems\u003C\u002Fcode\u003E directory looking as it does above, any item that already has its own sub-folder will be skipped.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"anaconda-optional\\\"\u003EAnaconda (optional)\u003C\u002Fh2\u003E\\n\u003Cp\u003EAnaconda is the leading scientific Python distribution. Its \u003Ccode\u003Econda\u003C\u002Fcode\u003E package manager allows you to install libraries such as \u003Ccode\u003Enumpy\u003C\u002Fcode\u003E and \u003Ccode\u003Etensorflow\u003C\u002Fcode\u003E with ease. The &quot;Miniconda&quot; version does not come with any superfluous packages preinstalled, which encourages you to keep your base environment clean and only install what you need for a project within a named environment.\u003C\u002Fp\u003E\\n\u003Cp\u003EDownload and install \u003Ca href=\\\"https:\u002F\u002Fconda.io\u002Fminiconda.html\\\"\u003EMiniconda\u003C\u002Fa\u003E. Choose the latest stable release of Python 3. If everything goes well, you should be able to run \u003Ccode\u003Ewhich conda\u003C\u002Fcode\u003E (linux\u002FmacOS) or \u003Ccode\u003Ewhere conda\u003C\u002Fcode\u003E (Windows) in your shell and see the location of the executable program in the output.\u003C\u002Fp\u003E\\n\u003Cp\u003EAnaconda has a handy \u003Ca href=\\\"http:\u002F\u002Fweb.archive.org\u002Fweb\u002F20190115051900\u002Fhttps:\u002F\u002Fconda.io\u002Fdocs\u002F_downloads\u002Fconda-cheatsheet.pdf\\\"\u003Echeat sheet\u003C\u002Fa\u003E for frequently used commands.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"create-an-environment\\\"\u003ECreate an Environment\u003C\u002Fh3\u003E\\n\u003Cp\u003EEnvironments, among other things, help control the complexity associated with using multiple package managers in tandem. Not all Python libraries can be installed through \u003Ccode\u003Econda\u003C\u002Fcode\u003E. In some cases we will fall back to the standard Python package manager, \u003Ccode\u003Epip\u003C\u002Fcode\u003E (or planned replacements like \u003Ccode\u003Epipenv\u003C\u002Fcode\u003E). However, when we do so, we will use a version of \u003Ccode\u003Epip\u003C\u002Fcode\u003E installed by \u003Ccode\u003Econda\u003C\u002Fcode\u003E. This keeps all the packages we need for the project in the same virtual sandbox.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# your current environment is indicated by a preceding asterisk\\n# (it will be &quot;base&quot; in a new shell)\\nconda env list\\n\\n# installed packages in the current environment\\nconda list\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENow we create a named environment, set it to use Python 3, and activate it.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# note the --name flag which takes a string argument (e.g. &quot;extract-pages&quot;)\\n# and the syntax for specifying the Python version\\nconda create --name extract-pages python=3\\n\\n# enter the new environment (macOS\u002FLinux)\\nsource activate extract-pages\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# Windows command for activating environment is slightly different\\nconda activate extract-pages\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ETo exit an environment, run \u003Ccode\u003Esource deactivate\u003C\u002Fcode\u003E on macOS\u002FLinux or \u003Ccode\u003Edeactivate\u003C\u002Fcode\u003E on Windows. But make sure to stay in the \u003Ccode\u003Eextract-pages\u003C\u002Fcode\u003E environment for the duration of the lesson!\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"install-conda-packages\\\"\u003EInstall Conda Packages\u003C\u002Fh3\u003E\\n\u003Cp\u003EWe can use \u003Ccode\u003Econda\u003C\u002Fcode\u003E to install our first couple of packages. All the other required packages (gzip, json, os, sys, and time) are part of the \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F3\u002Flibrary\u002F\\\"\u003Estandard Python library\u003C\u002Fa\u003E. Note how we need to specify a channel in some cases. You can search for packages on \u003Ca href=\\\"https:\u002F\u002Fanaconda.org\u002F\\\"\u003EAnaconda Cloud\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# to ensure we have a local version of pip (see discussion below)\\nconda install pip\\n\\nconda install jupyter\\n\\nconda install --channel anaconda requests\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EJupyter has many dependencies (other packages on which it relies), so this step may take a few minutes. Remember that when \u003Ccode\u003Econda\u003C\u002Fcode\u003E prompts you with \u003Ccode\u003EProceed ([y]\u002Fn)?\u003C\u002Fcode\u003E you should type a \u003Ccode\u003Ey\u003C\u002Fcode\u003E or \u003Ccode\u003Eyes\u003C\u002Fcode\u003E and then press Enter to accept the package plan.\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\n  Behind the scenes, \u003Ccode\u003Econda\u003C\u002Fcode\u003E is working to make sure all the required packages and dependencies will be installed in a compatible way.\\n\u003C\u002Fdiv\u003E\\n\\n\\n\u003Ch3 id=\\\"install-pip-packages\\\"\u003EInstall Pip Packages\u003C\u002Fh3\u003E\\n\u003Cp\u003EIf you are using a \u003Ccode\u003Econda\u003C\u002Fcode\u003E environment, it&#39;s best to use the local version of \u003Ccode\u003Epip\u003C\u002Fcode\u003E. Check that the following commands output a program whose absolute path contains something like \u003Ccode\u003E\u002FMiniconda\u002Fenvs\u002Fextract-pages\u002FScripts\u002Fpip\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Ewhich pip\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# Windows equivalent to &quot;which&quot;\\nwhere pip\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf you see two versions of \u003Ccode\u003Epip\u003C\u002Fcode\u003E in the output above, make sure to type the full path to the \u003Cem\u003Elocal\u003C\u002Fem\u003E environment version when installing the API wrapper libraries:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Epip install hathitrust-api\\npip install internetarchive\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E# Windows example using the absolute path to the *local* pip executable\\nC:\\\\Users\\\\stephen-krewson\\\\Miniconda\\\\envs\\\\extract-pages\\\\Scripts\\\\pip.exe install hathitrust-api internetarchive\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"jupyter-notebooks\\\"\u003EJupyter Notebooks\u003C\u002Fh2\u003E\\n\u003Cp\u003EPeter Organisciak and Boris Capitanu&#39;s \u003Ca href=\\\"\u002Fen\u002Flessons\u002Ftext-mining-with-extracted-features#start-a-notebook\\\"\u003EText Mining in Python through the HTRC Feature Reader\u003C\u002Fa\u003E explains the benefits of notebooks for development and data exploration. It also contains helpful information on how to effectively run cells. Since we installed the minimalist version of Anaconda, we need to launch Jupyter from the command line. In your shell (from inside the folder containing the lesson files) run \u003Ccode\u003Ejupyter notebook\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis will run the notebook server in your shell and launch your default browser with the Jupyter homepage. The homepage shows all the files in the current working directory.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;jupyter-home.png&quot; caption=&quot;Jupyter homepage showing lesson files.&quot; %}\u003C\u002Fp\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\nIn your shell, make sure you have \u003Ccode\u003Ecd\u003C\u002Fcode\u003E-ed into the unzipped \u003Ccode\u003Elesson-files\u003C\u002Fcode\u003E directory.\\n\u003C\u002Fdiv\u003E\\n\\n\u003Cp\u003EClick on the \u003Ccode\u003Ehathitrust.ipynb\u003C\u002Fcode\u003E and \u003Ccode\u003Einternetarchive.ipynb\u003C\u002Fcode\u003E notebooks to open them in new browser tabs. From now on, we don&#39;t need to run any commands in the shell. The notebooks allow us to execute Python code and have full access to the computer&#39;s file system. When you are finished, you can stop the notebook server by clicking &quot;Quit&quot; on the Jupyter homepage or executing \u003Ccode\u003Ectrl+c\u003C\u002Fcode\u003E in the shell.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"hathitrust\\\"\u003EHathiTrust\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"api-access\\\"\u003EAPI Access\u003C\u002Fh2\u003E\\n\u003Cp\u003EYou need to register with HathiTrust before using the Data API. Head over to the \u003Ca href=\\\"https:\u002F\u002Fbabel.hathitrust.org\u002Fcgi\u002Fkgs\u002Frequest\\\"\u003Eregistration portal\u003C\u002Fa\u003E and fill out your name, organization, and email to request access keys. You should receive an email response within a minute or so. Click the link, which will take you to a one-time page with both keys displayed.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the \u003Ccode\u003Ehathitrust.ipynb\u003C\u002Fcode\u003E notebook, examine the very first cell (shown below). Fill in your API tokens as directed. Then run the cell by clicking &quot;Run&quot; in the notebook navbar.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# Import the HT Data API wrapper\\nfrom hathitrust_api import DataAPI\\n\\n# Replace placeholder strings with your HT credentials (leaving the quote marks)\\nht_access_key = &quot;YOUR_ACCESS_KEY_HERE&quot;\\nht_secret_key = &quot;YOUR_SECRET_KEY_HERE&quot;\\n\\n# instantiate the Data API connection object\\ndata_api = DataAPI(ht_access_key, ht_secret_key)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\n  Careful! Do not expose your access tokens through a public repo on GitHub (or other version control host). They will be searchable by just about anyone. One good practice for a Python project is to either store your tokens as environment variables or save them in a file that is not versioned.\\n\u003C\u002Fdiv\u003E\\n\\n\\n\u003Ch2 id=\\\"create-volume-list\\\"\u003ECreate Volume List\u003C\u002Fh2\u003E\\n\u003Cp\u003EHT allows anyone to make a collection of items&mdash;you don&#39;t even have to be logged in! You should register for an account, however, if you want to save your list of volumes. Follow the \u003Ca href=\\\"https:\u002F\u002Fbabel.hathitrust.org\u002Fcgi\u002Fmb?colltype=updated\\\"\u003Einstructions\u003C\u002Fa\u003E to do some full-text searches and then add selected results to a collection. Currently, HathiTrust does not have a public search API for acquiring volumes programmatically; you need to search through their web interface.\u003C\u002Fp\u003E\\n\u003Cp\u003EAs you update a collection, HT keeps track of the associated metadata for each item in it. I have included in the lesson files the metadata for a sample lesson in JSON format. If you wanted to use the file from your own HT collection, you would navigate to your collections page and hover on the metadata link on the left to bring up the option to download as JSON as seen in the following screenshot.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;download-ht-json.png&quot; caption=&quot;Screenshot of how to download collection metadata in JSON format.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EWhen you have downloaded the JSON file, simply move it to the directory where you placed the Jupyter notebooks. Replace the name of the JSON file in the HT notebook with the name of your collection&#39;s file.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe notebook shows how to use a list comprehension to get all the \u003Ccode\u003Ehtitem_id\u003C\u002Fcode\u003E strings within the \u003Ccode\u003Egathers\u003C\u002Fcode\u003E object that contains all the collection information.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-Python\\\"\u003E# you can specify your collection metadata file here\\nmetadata_path = &quot;554050894-1535834127.json&quot;\\n\\nwith open(metadata_path, &quot;r&quot;) as fp:\\n    data = json.load(fp)\\n\\n# a list of all unique ids in the collection\\nvol_ids = [item[&#39;htitem_id&#39;] for item in data[&#39;gathers&#39;]]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cdiv class=\\\"alert alert-warning\\\"\u003E\\n  Tutorials often show you how to process one example item (often of a trivial size or complexity). This is pedagogically convenient, but it means you are less equipped to apply that code to multiple items&mdash;by far the more common use case. In the notebooks you will see how to encapsulate transformations applied to one item into \u003Ci\u003Efunctions\u003C\u002Fi\u003E that can be called within a loop over a collection of items.\\n\u003C\u002Fdiv\u003E\\n\\n\u003Ch2 id=\\\"visual-feature-image_on_page\\\"\u003EVisual Feature: IMAGE_ON_PAGE\u003C\u002Fh2\u003E\\n\u003Cp\u003EGiven a list of volumes, we want to explore what visual features they have at the page level. The \u003Ca href=\\\"https:\u002F\u002Fwww.hathitrust.org\u002Fdocuments\u002Fhathitrust-data-api-v2_20150526.pdf\\\"\u003Emost recent documentation\u003C\u002Fa\u003E (2015) for the Data API describes a metadata object called \u003Ccode\u003Ehtd:pfeat\u003C\u002Fcode\u003E on pages 9-10. \u003Ccode\u003Ehtd:pfeat\u003C\u002Fcode\u003E is shorthand for &quot;HathiTrust Data API: Page Features.&quot;\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Ehtd:pfeat\u003C\u002Fcode\u003E­ - the page feature key (if available):\u003Cul\u003E\\n\u003Cli\u003ECHAPTER_START\u003C\u002Fli\u003E\\n\u003Cli\u003ECOPYRIGHT\u003C\u002Fli\u003E\\n\u003Cli\u003EFIRST_CONTENT_CHAPTER_START\u003C\u002Fli\u003E\\n\u003Cli\u003EFRONT_COVER\u003C\u002Fli\u003E\\n\u003Cli\u003EINDEX\u003C\u002Fli\u003E\\n\u003Cli\u003EREFERENCES\u003C\u002Fli\u003E\\n\u003Cli\u003ETABLE_OF_CONTENTS\u003C\u002Fli\u003E\\n\u003Cli\u003ETITLE\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EWhat the \u003Ccode\u003Ehathitrust-api\u003C\u002Fcode\u003E wrapper does is make the full metadata for a HT volume available as a Python object. Given a volume&#39;s identifier, we can request its metadata and then drill down through the page \u003Cem\u003Esequence\u003C\u002Fem\u003E into page-level information. The \u003Ccode\u003Ehtd:pfeat\u003C\u002Fcode\u003E \u003Cem\u003Elist\u003C\u002Fem\u003E is associated with each page in a volume and in theory contains all features that apply to that page. In practice, there a quite a few more feature tags than the eight listed above. The one we will be working with is called \u003Ccode\u003EIMAGE_ON_PAGE\u003C\u002Fcode\u003E and is more abstractly visual than structural tags such as \u003Ccode\u003ECHAPTER_START\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ETom Burton-West, a research librarian at the University of Michigan Library, works closely with HathiTrust and HTRC, HathiTrust&#39;s Research Center. Tom told me over email that HathiTrust is provided the \u003Ccode\u003Ehtd:pfeat\u003C\u002Fcode\u003E information by Google, with whom they have worked closely since HT&#39;s founding in 2008. A contact at Google gave Tom permission to share the following:\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EThese tags are derived from a combination of heuristics, machine learning, and human tagging.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EAn example heuristic might be that the first element in the volume page sequence is almost always the \u003Ccode\u003EFRONT_COVER\u003C\u002Fcode\u003E. Machine learning could be used to train models to discriminate, say, between image data that is more typical of lines of prose in a Western script or of the lines in an engraving. Human tagging is the manual assignment of labels to images. The ability to view a volume&#39;s illustrations in the EEBO and ECCO databases is an example of human tagging.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe use of &quot;machine learning&quot; by Google sounds somewhat mysterious. Until Google publicizes their methods, it is impossible to know all the details. However, it&#39;s likely that the \u003Ccode\u003EIMAGE_ON_PAGE\u003C\u002Fcode\u003E tags were first proposed by detecting &quot;Picture&quot; blocks in the OCR output files (a process discussed below in the Internet Archive section). Further filtering may then be applied.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"code-walk-through\\\"\u003ECode Walk-through\u003C\u002Fh2\u003E\\n\u003Ch3 id=\\\"find-pictures\\\"\u003EFind Pictures\u003C\u002Fh3\u003E\\n\u003Cp\u003EWe have seen how to create a list of volumes and observed that the Data API can be used to get metadata objects containing page-level experimental features. The core function in the HT notebook has the signature \u003Ccode\u003Eht_picture_download(item_id, out_dir=None)\u003C\u002Fcode\u003E. Given a unique identifier and an optional destination directory, this function will first get the volume&#39;s metadata from the API and convert it into JSON format. Then it loops over the page sequence and checks if the tag \u003Ccode\u003EIMAGE_ON_PAGE\u003C\u002Fcode\u003E is in the \u003Ccode\u003Ehtd:pfeat\u003C\u002Fcode\u003E list (if it exists).\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# metadata from API in json format (different than HT collection metadata)\\nmeta = json.loads(data_api.getmeta(item_id, json=True))\\n\\n# sequence gets us each page of the scanned item in order, with any\\n# additional information that might be available for it\\nsequence = meta[&#39;htd:seqmap&#39;][0][&#39;htd:seq&#39;]\\n\\n# list of pages with pictures (empty to start)\\nimg_pages = []\\n\\n# try\u002Fexcept block handles situation where no &quot;pfeats&quot; exist OR\\n# the sequence numbers are not numeric\\nfor page in sequence:\\n    try:\\n        if &#39;IMAGE_ON_PAGE&#39; in page[&#39;htd:pfeat&#39;]:\\n            img_pages.append(int(page[&#39;pseq&#39;]))\\n    except (KeyError, TypeError) as e:\\n        continue\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENotice that we need to drill down several levels into the top-level object to get the \u003Ccode\u003Ehtd:seq\u003C\u002Fcode\u003E object, which we can iterate over.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe two exceptions I want to catch are \u003Ccode\u003EKeyError\u003C\u002Fcode\u003E, which occurs when the page does not have an page-level features associated with it and \u003Ccode\u003ETypeError\u003C\u002Fcode\u003E, which occurs when the \u003Ccode\u003Epseq\u003C\u002Fcode\u003E field for the page is for some reason non-numeric and thus cannot be cast to an \u003Ccode\u003Eint\u003C\u002Fcode\u003E. If something goes wrong with a page, we just \u003Ccode\u003Econtinue\u003C\u002Fcode\u003E on to the next one. The idea is to get all the good data we can. Not to clean up inconsistencies or gaps in the item metadata.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"download-images\\\"\u003EDownload Images\u003C\u002Fh3\u003E\\n\u003Cp\u003EOnce \u003Ccode\u003Eimg_pages\u003C\u002Fcode\u003E contains the complete list of pages tagged with \u003Ccode\u003EIMAGE_ON_PAGE\u003C\u002Fcode\u003E, we can download those pages. Note that if no \u003Ccode\u003Eout_dir\u003C\u002Fcode\u003E is supplied to \u003Ccode\u003Eht_picture_download()\u003C\u002Fcode\u003E, then the function simply returns the \u003Ccode\u003Eimg_pages\u003C\u002Fcode\u003E list and does NOT download anything.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe \u003Ccode\u003Egetpageimage()\u003C\u002Fcode\u003E API call returns a JPEG by default. We simply write out the JPEG bytes to a file in the normal way. Within the volume sub-folder (itself inside \u003Ccode\u003Eout_dir\u003C\u002Fcode\u003E), the pages will be named \u003Ccode\u003E1.jpg\u003C\u002Fcode\u003E for page 1 and so forth.\u003C\u002Fp\u003E\\n\u003Cp\u003EOne thing to consider is our usage rate of the API. We don&#39;t want to abuse our access by making hundreds of requests per minute. To be safe, especially if we intend to run big jobs, we wait two seconds before making each page request. This may be frustrating in the short term, but it helps avoid API throttling or banning.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-Python\\\"\u003Efor i, page in enumerate(img_pages):\\n    try:\\n        # simple status message\\n        print(&quot;[{}] Downloading page {} ({}\u002F{})&quot;.format(item_id, \\\\\\n            page, i+1, total_pages))\\n\\n        img = data_api.getpageimage(item_id, page)\\n\\n        # N.B. loop only executes if out_dir is not None\\n        img_out = os.path.join(out_dir, str(page) + &quot;.jpg&quot;)\\n\\n        # write out the image\\n        with open(img_out, &#39;wb&#39;) as fp:\\n            fp.write(img)\\n\\n        # to avoid exceeding the allowed API usage\\n        time.sleep(2)\\n\\n    except Exception as e:\\n        print(&quot;[{}] Error downloading page {}: {}&quot;.format(item_id, page,e))\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch1 id=\\\"internet-archive\\\"\u003EInternet Archive\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"api-access-1\\\"\u003EAPI Access\u003C\u002Fh2\u003E\\n\u003Cp\u003EWe connect to the Python API library using an Archive.org account email and password rather than API tokens. This is discussed in the \u003Ca href=\\\"https:\u002F\u002Farchive.org\u002Fservices\u002Fdocs\u002Fapi\u002Finternetarchive\u002Fquickstart.html\\\"\u003EQuickstart Guide\u003C\u002Fa\u003E. If you do not have an account, \u003Ca href=\\\"https:\u002F\u002Farchive.org\u002Faccount\u002Flogin.createaccount.php\\\"\u003Eregister\u003C\u002Fa\u003E for your &quot;Virtual Library Card.&quot;\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the first cell of the \u003Ccode\u003Einternetarchive.ipynb\u003C\u002Fcode\u003E notebook, enter your credentials as directed. Run the cell to authenticate to the API.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"create-volume-list-1\\\"\u003ECreate Volume List\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe IA Python library allows you to submit query strings and receive a list of matching key-value pairs where the word &quot;identifier&quot; is the key and the actual identifier is the value. The syntax for a query is explained on the \u003Ca href=\\\"https:\u002F\u002Farchive.org\u002Fadvancedsearch.php\\\"\u003EAdvanced Search page\u003C\u002Fa\u003E for IA. You can specify parameters by using a keyword like &quot;date&quot; or &quot;mediatype&quot; followed by a colon and the value you want to assign that parameter. For instance, I only want results that are \u003Cem\u003Etexts\u003C\u002Fem\u003E (as opposed to video, etc.). Make sure the parameters and options you are trying to use are supported by IA&#39;s search functionality. Otherwise you may get missing or weird results and not know why.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the notebook, I generate a list of IA ids with the following code:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-Python\\\"\u003E# sample search (should yield two results)\\nquery = &quot;peter parley date:[1825 TO 1830] mediatype:texts&quot;\\nvol_ids = [result[&#39;identifier&#39;] for result in ia.search_items(query)]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"visual-feature-picture-blocks\\\"\u003EVisual Feature: Picture Blocks\u003C\u002Fh2\u003E\\n\u003Cp\u003EInternet Archive does not release any page-level features. Instead, it makes a number of raw files from the digitization process available to users. The most important of these for our purposes is the Abbyy XML file. Abbyy is a Russian company whose FineReader software dominates the OCR market.\u003C\u002Fp\u003E\\n\u003Cp\u003EAll recent versions of FineReader produce an \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FXML\\\"\u003EXML document\u003C\u002Fa\u003E that associates different &quot;blocks&quot; with each page in the scanned document. The most common type of block is \u003Ccode\u003EText\u003C\u002Fcode\u003E but there are \u003Ccode\u003EPicture\u003C\u002Fcode\u003E blocks as well. Here is an example block taken from an IA Abbyy XML file. The top-left (&quot;t&quot; and &quot;l&quot;) and bottom-right (&quot;b&quot; and &quot;r&quot;) corners are enough to identify the rectangular block region.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-xml\\\"\u003E&lt;block blockType=&quot;Picture&quot; l=&quot;586&quot; t=&quot;1428&quot; r=&quot;768&quot; b=&quot;1612&quot;&gt;\\n    &lt;region&gt;&lt;rect l=&quot;586&quot; t=&quot;1428&quot; r=&quot;768&quot; b=&quot;1612&quot;&gt;&lt;\u002Frect&gt;&lt;\u002Fregion&gt;\\n&lt;\u002Fblock&gt;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe IA equivalent to looking for \u003Ccode\u003EIMAGE_ON_PAGE\u003C\u002Fcode\u003E tags in HT is parsing the Abbyy XML file and iterating over each page. If there is at least one \u003Ccode\u003EPicture\u003C\u002Fcode\u003E block on that page, the page is flagged as possibly containing an image.\u003C\u002Fp\u003E\\n\u003Cp\u003EWhile HT&#39;s \u003Ccode\u003EIMAGE_ON_PAGE\u003C\u002Fcode\u003E feature contains no information about the \u003Cem\u003Elocation\u003C\u002Fem\u003E of that image, the \u003Ccode\u003EPicture\u003C\u002Fcode\u003E blocks in the XML file are associated with a rectangular region on the page. However, since FineReader specializes in recognizing letters from Western character sets, it is much less accurate at identifying image regions. Leetaru&#39;s project (see Overview) used the region coordinates to crop pictures, but in this lesson we will simply download the whole page.\u003C\u002Fp\u003E\\n\u003Cp\u003EPart of the intellectual fun of this lesson is using a noisy dataset (OCR block tags) for a largely unintended purpose: identifying pictures and not words. At some point, it will become computationally feasible to run deep learning models on every raw page image in a volume and pick out the desired type(s) of picture(s). But since most pages in most volumes are unillustrated, that is an expensive task. For now, it makes more sense to leverage the existing data we have from the OCR ingest process.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor more information on how OCR itself works and interacts with the scan process, please see Mila Oiva&#39;s PH lesson \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fretired\u002FOCR-with-Tesseract-and-ScanTailor\\\"\u003EOCR with Tesseract and ScanTailor\u003C\u002Fa\u003E. Errors can crop up due to skewing, artefacts, and many other problems. These errors end up affecting the reliability and precision of the &quot;Picture&quot; blocks. In many cases, Abbyy will estimate that blank or discolored pages are actually pictures. These incorrect block tags, while undesirable, can be dealt with using retrained convolutional neural networks. Think of the page images downloaded in this lesson as a first pass in a longer process of obtaining a clean and usable dataset of historical illustrations.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"code-walk-through-1\\\"\u003ECode Walk-through\u003C\u002Fh2\u003E\\n\u003Ch3 id=\\\"find-pictures-1\\\"\u003EFind Pictures\u003C\u002Fh3\u003E\\n\u003Cp\u003EJust as with HT, the core function for IA is \u003Ccode\u003Eia_picture_download(item_id, out_dir=None)\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ESince it involves file I\u002FO, the process for geting the \u003Ccode\u003Eimg_pages\u003C\u002Fcode\u003E list is more complicated than that for HT. Using the command line utility \u003Ccode\u003Eia\u003C\u002Fcode\u003E (which is installed with the library), you can get a sense of the available metadata files for a volume. With very few exceptions, a file with format &quot;Abbyy GZ&quot; should be available for volumes with media type \u003Ccode\u003Etext\u003C\u002Fcode\u003E on Internet Archive.\u003C\u002Fp\u003E\\n\u003Cp\u003EThese files, even when compressed, can easily be hundreds of megabytes in size! If there is an Abbyy file for the volume, we get its name and then download it. The \u003Ccode\u003Eia.download()\u003C\u002Fcode\u003E call uses some helpful parameters to ignore the request if the file already exists and, if not, download it without creating a nested directory. To save space, we delete the Abbyy file after parsing it.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# Use command-line client to see available metadata formats:\\n# `ia metadata formats VOLUME_ID`\\n\\n# for this lesson, only the Abbyy file is needed\\nreturned_files = list(ia.get_files(item_id, formats=[&quot;Abbyy GZ&quot;]))\\n\\n# make sure something got returned\\nif len(returned_files) &gt; 0:\\n    abbyy_file = returned_files[0].name\\nelse:\\n    print(&quot;[{}] Could not get Abbyy file&quot;.format(item_id))\\n    return None\\n\\n# download the abbyy file to CWD\\nia.download(item_id, formats=[&quot;Abbyy GZ&quot;], ignore_existing=True, \\\\\\n    destdir=os.getcwd(), no_directory=True)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EOnce we have the file, we need to parse the XML using the standard Python library. We take advantage of the fact that we can open the compressed file directly with the \u003Ccode\u003Egzip\u003C\u002Fcode\u003E library. Abbyy files are zero-indexed so the first page in the scan sequence has index 0. However, we have to filter out 0 since it cannot be requested from IA. IA&#39;s exclusion of index 0 is not documented anywhere; rather, I found out through trial and error. If you see a hard-to-explain error message, try to track down the source and don&#39;t be afraid to ask for help, whether from someone with relevant experience or at the organization itself.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-Python\\\"\u003E# collect the pages with at least one picture block\\nimg_pages = []\\n\\nwith gzip.open(abbyy_file) as fp:\\n    tree = ET.parse(fp)\\n    document = tree.getroot()\\n    for i, page in enumerate(document):\\n        for block in page:\\n            try:\\n                if block.attrib[&#39;blockType&#39;] == &#39;Picture&#39;:\\n                    img_pages.append(i)\\n                    break\\n            except KeyError:\\n                continue\\n\\n# 0 is not a valid page for making GET requests to IA, yet sometimes\\n# it&#39;s in the zipped Abbyy file\\nimg_pages = [page for page in img_pages if page &gt; 0]\\n\\n# track for download progress report\\ntotal_pages = len(img_pages)\\n\\n# OCR files are huge, so just delete once we have pagelist\\nos.remove(abbyy_file)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch3 id=\\\"download-images-1\\\"\u003EDownload Images\u003C\u002Fh3\u003E\\n\u003Cp\u003EIA&#39;s Python wrapper does not provide a single-page download function&mdash;only bulk. This means that we will use IA&#39;s RESTful API to get specific pages. First we construct a URL for each page that we need. Then we use the \u003Ccode\u003Erequests\u003C\u002Fcode\u003E library to send an HTTP \u003Ccode\u003EGET\u003C\u002Fcode\u003E request and, if everything goes well (i.e. the code 200 is returned in the response), we write out the contents of the response to a JPEG file.\u003C\u002Fp\u003E\\n\u003Cp\u003EIA has been working on an \u003Ca href=\\\"https:\u002F\u002Fiiif.archivelab.org\u002Fiiif\u002Fdocumentation\\\"\u003Ealpha version\u003C\u002Fa\u003E of an API for image cropping and resizing that conforms to the standards of the International Image Interoperability Framework (\u003Ca href=\\\"https:\u002F\u002Fiiif.io\u002F\\\"\u003EIIIF\u003C\u002Fa\u003E). IIIF represents a vast improvement on the old method for single-page downloads which required downloading JP2 files, a largely unsupported archival format. Now it&#39;s extremely simple to get a single page JPEG:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-Python\\\"\u003E# See: https:\u002F\u002Fiiif.archivelab.org\u002Fiiif\u002Fdocumentation\\nurls = [&quot;https:\u002F\u002Fiiif.archivelab.org\u002Fiiif\u002F{}${}\u002Ffull\u002Ffull\u002F0\u002Fdefault.jpg&quot;.format(item_id, page)\\n    for page in img_pages]\\n\\n# no direct page download through python library, construct GET request\\nfor i, page, url in zip(range(1,total_pages), img_pages, urls):\\n\\n    rsp = requests.get(url, allow_redirects=True)\\n\\n    if rsp.status_code == 200:\\n        print(&quot;[{}] Downloading page {} ({}\u002F{})&quot;.format(item_id, \\\\\\n            page, i+1, total_pages))\\n\\n        with open(os.path.join(out_dir, str(page) + &quot;.jpg&quot;), &quot;wb&quot;) as fp:\\n            fp.write(rsp.content)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch1 id=\\\"next-steps\\\"\u003ENext Steps\u003C\u002Fh1\u003E\\n\u003Cp\u003EOnce you understand the main functions and the data unpacking code in the notebooks, feel free to run the cells in sequence or &quot;Run All&quot; and watch the picture pages roll in. You are encouraged to adapt these scripts and functions for your own research questions.\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
