<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-c09d08cd.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-c09d08cd.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/naive-bayesian"),
					params: {lang:"en",lessons:"lessons",slug:"naive-bayesian"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Supervised Classification: The Naive Bayesian Returns to the Old Bailey</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h2 id="introduction">Introduction</h2>
<p>A few years back, William Turkel wrote a series of blog posts called <a href="http://digitalhistoryhacks.blogspot.com/2008/05/naive-bayesian-in-old-bailey-part-1.html">A
Naive Bayesian in the Old Bailey</a>, which showed how one could use
machine learning to extract interesting documents out of a digital archive.
This tutorial is a kind of an update on that blog essay, with roughly the
same data but a slightly different version of the machine learner.</p>
<p>The idea is to show why machine learning methods are of interest to
historians, as well as to present a step-by-step implementation of
a supervised machine learner. This learner is then applied to the <a href="http://www.oldbaileyonline.org/">Old Bailey digital
archive</a>, which contains several centuries&#39; worth of transcripts of
trials held at the Old Bailey in London. We will be using Python for the
implementation.</p>
<p>One obvious use of machine learning for a historian is document
selection. If we can get the computer to &quot;learn&quot; what kinds of documents
we want to see, we can enlist its help in the always-time-consuming task
of finding relevant documents in a digital archive (or any other digital
collection of documents). We&#39;ll still be the ones reading and
interpreting the documents; the computer is just acting as a fetch dog
of sorts, running to the archive, nosing through documents, and bringing
us those that it thinks we&#39;ll find interesting.</p>
<p>What we will do in this tutorial, then, is to apply a machine learner
called Naive Bayesian to data from the Old Bailey digital archive. Our
goals are to learn how a Naive Bayesian works and to evaluate how
effectively it classifies documents into different categories - in
this case, trials into offense categories (theft, assault, etc.). This
will help us determine how useful a machine learner might be to us as
historians: if it does well at this classification task, it might also
do well at finding us documents that belong to a &quot;class&quot; we, given our
particular research interests, want to see.</p>
<p>Step by step, we&#39;ll do the following:</p>
<ol>
<li>learn what machine learners do, and look more closely at a popular
learner called Naive Bayesian.</li>
<li>download a set of trial records from the Old Bailey archive.</li>
<li>write a script that saves the trials as text (removing the XML
markup) and does a couple of other useful things.</li>
<li>write a couple of helper scripts to assist in testing the learners.</li>
<li>write a script that tests the performance of the learner.</li>
</ol>
<h3 id="files-you-will-need">Files you will need</h3>
<ul>
<li><code>save-trialtxts-by-category.py</code></li>
<li><code>tenfold-crossvalidation.py</code></li>
<li><code>count-offense-instances.py</code></li>
<li><code>pretestprocessing.py</code></li>
<li><code>test-nb-learner.py</code></li>
<li><code>naivebayes.py</code></li>
<li><code>english-stopwords.txt</code></li>
</ul>
<p><a href="/assets/baileycode.zip">A zip file of the scripts</a> is available. You can also download
<a href="https://doi.org/10.5281/zenodo.13284">another zip file</a> containing the scripts, the data that we are using and the files that
result from the scripts. (The second option is probably easiest if you want to follow along with the lesson,
since it gives you everything you need in the correct folder structure.)
More information about where to put the files is in the &quot;Preliminaries&quot; section
of the part where we actually begin to code.</p>
<p><em>Note: You will not need any Python modules that don&#39;t come with standard
installations, except for <a href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> (used in the data creation step,
not in the learner code itself).</em></p>
<h2 id="the-old-bailey-digital-archive">The Old Bailey Digital Archive</h2>
<p>The <a href="http://www.oldbaileyonline.org/">Old Bailey digital archive</a>
contains 197,745 criminal trials held at the Old Bailey, aka the Central
Criminal Court in London. The trials were held between 1674 and 1913,
and since the archive provides the full transcript of each trial, many
of which include testimony by defendants, victims, and witnesses, it&#39;s a
great resource for all kinds of historians interested in the lives of
ordinary people in London.</p>
<p>What makes the collection particularly useful for our purposes is that
the text of each trial is richly annotated with such information as what
type of an offense was involved (pocketpicking, assault, robbery,
conspiracy...), the name and gender of each witness, the verdict, etc.
What&#39;s more, this information has been added to the document in XML
markup, which allows us to extract it easily and reliably. That, in
turn, lets us train a machine learner to recognize the things we are
interested in, and then test the learner&#39;s performance.</p>
<p>Of course, in the case of the Old Bailey archive, we might not need this
computer-assisted sorting all that badly, since the archive&#39;s curators,
making use of the XML markup, offer us a ready-made <a href="http://www.oldbaileyonline.org/forms/formMain.jsp">search interface</a> that
lets us look for documents by offense type, verdict, punishment, etc. But
that&#39;s exactly what makes the Old Bailey such a good resource for
testing a machine learner: we can check how well the learner performs by
checking its judgments against the human-annotated information in the
Old Bailey documents. That, in turn, helps us decide how (or whether) a
learner could help us explore other digital document collections, most
of which are not as richly annotated.</p>
<h2 id="machine-learning">Machine learning</h2>
<p>Machine learning can mean a lot of different things, but the most common
tasks are <a href="http://en.wikipedia.org/wiki/Statistical_classification">classification</a> and <a href="http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/">clustering</a>.</p>
<p>Classification is performed by supervised learners — &quot;supervised&quot;
meaning that a human assistant helps them learn, and only then sends
them out to classify by themselves. The basic training procedure is to
give the learner labeled data: that is, we give it a stack of things
(documents, for example) where each of those things is labeled as
belonging to a group. This is called training data. The learner then
looks at each item in the training data, looks at its label, and
learns what distinguishes the groups from each other. To see how well
the learner learned, we then test it by giving it data that is similar
to the training data but that the learner hasn&#39;t seen before and that
is not labeled. This is called (you guessed it!) test data. How well
the learner performs on classifying this previously-unseen data is a
measure of how well it has learned.</p>
<p>The classic case of a supervised classifier is a program that separates
junk email (spam) from regular email (ham). Such a program is &quot;trained&quot;
by giving it a lot of spam and ham to look at, along with the
information of which is which. It then builds a statistical model of
what a spam message looks like versus what a regular email message looks
like. So it learns that a message is more likely to be spam if it
contains sexual terms, or words like &quot;offer&quot; and &quot;deal&quot;, or, as things
turn out, <a href="http://www.paulgraham.com/spam.html">&quot;ff0000,&quot; the HTML code for red</a>. It can then apply that
statistical model to incoming messages and discard the ones it
identifies as spam.</p>
<p>Clustering is usually a task for unsupervised learners. An unsupervised
learner doesn&#39;t get any tips on how the data &quot;ought&quot; to be sorted, but
rather is expected to discover patterns in the data automatically,
grouping the data by the patterns it has discovered. Unlike in
supervised classification, in unsupervised clustering we don&#39;t tell the
learner what the &quot;right&quot; groups are, or give it any hints on what items
in the data set should go together. Rather, we give it data with a bunch
of features, and (often, but not always) we tell it how many groups we
want it to create. The features could be anything: in document
clustering, they are normally words. But clustering isn&#39;t limited to
grouping documents: it could also be used in, say, trying to improve
diagnoses by clustering patient records. In that task, the features
would be various attributes of the patient (age, weight, blood pressure,
presence and quality of various symptoms etc.) and the clustering
algorithm would attempt to create groups that share as many features as
closely as possible.</p>
<p>A side note: Some of you may have come to think of an objection to this supervised/unsupervised distinction: namely, that the clustering method is not entirely &quot;unsupervised&quot; either. After all, we tell it what features it should look at, whether it is words (rather than sentences, or two-word sequences, or something else) in a document, or a list of numeric values in a patient record. The learner never encounters the data entirely unprepared. Quite true. But no matter - the distinction between unsupervised and supervised is useful nevertheless, in that in one we tell the learner what the right answer is, and in the other it comes to us with some pattern it has figured out without an answer key. Each is useful for different kind of tasks, or sometimes for different approaches to the same task.</p>
<p>In this tutorial, we are dealing with a supervised learner that we train
to perform document classification. We give our learner a set of
documents along with their correct classes, and then test it on a set of
documents they haven&#39;t seen, with the hope that it will succeed in
guessing the document&#39;s correct classification.</p>
<h3 id="a-naive-bayesian-learner">A Naive Bayesian learner</h3>
<p>A Naive Bayesian is a supervised learner: we give it things marked with
group labels, and its job is basically to learn the probability that a
thing that looks a particular way belongs in a particular group.</p>
<p>But why &quot;naive&quot;? And what &quot;Bayesian&quot;?</p>
<p>&quot;Naive&quot; simply means that the learner makes the assumption that all the
&quot;features&quot; that make up a document are independent of each other. In our
case, the features are words, and so the learner assumes that the
occurrence of a particular word is completely independent of the
occurrence of another word. This, of course, is often not true, which is
why we call it &quot;naive.&quot; For example, when we put &quot;new&quot; and &quot;york&quot;
together to form &quot;New York,&quot; the result has a very different meaning
than the &quot;new&quot; and &quot;york&quot; in &quot;New clothes for the Prince of York.&quot; If we
were to distinguish &quot;New York&quot; from &quot;New&quot; and &quot;York&quot; occurring
separately, we might find that each tends to occur in very different
types of documents, and thus not identifying the expression &quot;New York&quot;
might throw our classifier off course.</p>
<p>Despite their simplistic assumption that the occurrence of any
particular feature is independent of the occurrence of other features,
Naive Bayesian classifiers do a good enough job to be very useful in
many contexts (much of the real-world junk mail detection is performed
by Naive Bayesian classifiers, for example). Meanwhile, the assumption
of independence means that processing documents is much less
computationally intensive, so a Naive Bayesian classifier can handle far
more documents in a much shorter time than many other, more complex
methods. That in itself is useful. For example, it wouldn’t take too
long retrain a Naive Bayesian learner if we accumulated more data. Or we
could give it a bigger set of data to begin with; a pile of data that a
Naive Bayesian could burrow through in a day might take a more complex
method weeks or even months to process. Especially when it comes to
classification, more data is often as significant as a better method —
as Bob Mercer of IBM famously quipped in 1985, “there is no data like
more data.”</p>
<p>As for the &quot;Bayesian&quot; part, that refers to the 18th-century English
minister, statistician, and philosopher Thomas Bayes. When you google
for &quot;Naive Bayesian,&quot; you will turn up a lot of references to &quot;Bayes&#39;
theorem&quot; or &quot;Bayes&#39; rule,&quot; which is a formula for applying conditional
probabilities (the probability of some thing X, given some other thing
Y).</p>
<p>Bayes&#39; theorem is related to Naive Bayesian classifiers, in that we can
formulate the classification question as &quot;what is the probability of
document X, given class Y?&quot; However, unless you&#39;ve done enough math and
probability to be comfortable with that kind of thinking, it may not
provide the easiest avenue to grasping how a Naive Bayesian classifier
works. Instead, let&#39;s look at the classifier in a more procedural
manner. (Meanwhile, if you prefer, you can check out <a href="http://www.yudkowsky.net/rational/bayes">an explanation
of Bayes&#39; rule and conditional probabilities</a> that does a very nice
job and is also a good read.)</p>
<h4 id="understanding-naive-bayesian-classification-using-a-generative-story">Understanding Naive Bayesian classification using a generative story</h4>
<p>To understand Naive Bayesian classification, we will start by telling a
story about how documents come into being. Telling such a story — called
a &quot;generative story&quot; in the business — often simplifies the
probabilistic analysis and helps us understand the assumptions we&#39;re
making. Telling the story takes a while, so bear with me. There is a
payoff at the end: the story directly informs us how to build a
classifier under the assumptions that the particular generative story
makes.</p>
<p>The fundamental assumption we will make in our generative story is that
documents come into being not as a result of intellectual cogitation but
as a result of a process whereby words are picked at random out of a bag
and then put into a document (known as a bag-of-words model).</p>
<p>So we pretend that historical works, for example, are written in
something like the following manner. Each historian has his or her own
bag of words with a vocabulary specific to that bag. So when Ann the
Historian writes a book, what she does is this:</p>
<ul>
<li>She goes to the bag that is her store of words.</li>
<li>She puts her hand in and pulls out a piece of paper.</li>
<li>She reads the word on the piece of paper, writes it down in her
book, and puts the paper back in the bag.</li>
<li>Then she again puts her hand in the bag and pulls out a piece of
paper.</li>
<li>She writes down that word in the book, and puts the piece of paper
back in the bag.</li>
</ul>
<p>Ann the Historian keeps going until she decides her book (or article, or
blog post, or whatever) is finished. The next time she wants to write
something, she goes back to her bag of words and does the same thing. If
her friend John the Historian were to write a book, he would go to his
own bag, which has a different set of words, and then he would follow
the same procedure of taking out a word, writing it down, putting it
back in. It&#39;s just one damn word after another.</p>
<p>{% include figure.html filename=&quot;naive-bayesian-1.png&quot; caption=&quot;Bags of Words&quot; %}</p>
<p><em>(If this procedure sounds familiar, that may be because it sounds a bit
like the generative story told in explaining how <a href="/lessons/topic-modeling-and-mallet">topic modeling</a> works.
However, the story in topic modeling is a bit different in that,
for instance, each document contains words from more than one class.
Also, you should note that topic modeling is unsupervised — you don&#39;t
tell the modeler what the &quot;right&quot; topics are, it comes up with them all
by itself.)</em></p>
<p>So let&#39;s say you are a curator of a library of historical works, and one
day you discover a huge forgotten trunk in the basement of the library.
It turns out that the trunk contains dozens and dozens of typed book
manuscripts. After some digging, you find a document that explains that
these are transcripts of unpublished book drafts by three historians:
Edward Gibbon, Carl Becker, and Mercy Otis Warren.</p>
<p>What a find! But unfortunately, as you begin sorting through the drafts,
you realize that they are not marked with the author&#39;s name. What can you
do? How can you classify them correctly?</p>
<p>Well, you do have other writings by these authors. And if historians
write their documents in the manner described above — if each historian
has his or her own bag of words with a particular vocabulary and a
particular distribution of words — then we can figure out who wrote each
document by looking at the words it contains and comparing the
distribution of those words to the distribution of words in documents we
<em>know</em> were written by Gibbon, Becker, and Warren, respectively.</p>
<p>So you go to your library stacks and get out all the books by Gibbon,
Becker, and Warren. Then you start counting. You start with Edward
Gibbon&#39;s <em>oeuvre</em>. For each word in a work by Gibbon, you add the word
to a list marked &quot;Gibbon.&quot; If the word is already in the list, you add
to its count. Then you do the same with the works of Mercy Otis Warren
and Carl Becker. Finally, for each author, you add up the total number
of words you&#39;ve seen. You also add up the total number of monographs you
have examined so you&#39;ll have a metric for how much work each author has
published.</p>
<p>So what you end up with is something like this:</p>
<table>
<thead>
<tr>
<th>Edward Gibbon (5)</th>
<th>Carl Becker (18)</th>
<th>Mercy Otis Warren (2)</th>
</tr>
</thead>
<tbody><tr>
<td>empire, 985</td>
<td>everyman, 756</td>
<td>revolution, 989</td>
</tr>
<tr>
<td>rome, 897</td>
<td>revolution, 699</td>
<td>constitution, 920</td>
</tr>
<tr>
<td>fall, 887</td>
<td>philosopher, 613</td>
<td>principles, 899</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>(total), 352,003</td>
<td>(total), 745,532</td>
<td>(total), 300,487</td>
</tr>
</tbody></table>
<p>What you have done, in essence, is to reconstruct each historian&#39;s &quot;bag
of words&quot; — now you know (at least approximately) what words each
historian uses and in what proportions. Armed with this representation
of the word distributions in the works of Gibbons, Becker, and Warren,
you&#39;re ready to tackle the task of figuring out who wrote which
manuscripts.</p>
<p>You&#39;re going to work manuscript by manuscript and author by author,
first pretending that the manuscript you&#39;re currently considering was
written by Gibbons, then that it was written by Becker, and so on. For
each author, you calculate how likely it is that the manuscript really
was written by that author.</p>
<p>So with your first manuscript in hand, you start by assuming that the
manuscript was written by Gibbons. First you figure out the overall
probability of any monograph being written by Gibbons rather than either
of the two others — that is, of the Gibbons bag rather than the Becker
bag or the Warren bag being used to produce a monograph. You do this by
taking the number of books written by Gibbons and dividing it by the
total number of books written by all these authors. That comes out to
5/25, or 0.2 (20 percent).</p>
<p>Then, you start looking at the words in the manuscript. Let&#39;s say the
first word is &quot;fall.&quot; You check how often that word occurred in Gibbons&#39;
published <em>oeuvre</em>, and you find that the answer is 887. Then you check
how many words, overall, there were in Gibbons&#39; total works, and you
note that the answer is 352,003. You divide 887 by 352,003 to get the
proportional frequency (call it <em>p</em>) of &quot;fall&quot; in Gibbons&#39; work
(0.0025). For the next word, you do the same procedure, and then
multiply the probabilities together (you multiply since each action —
picking an author, or picking a word — represents an independent
choice). In the end you end with a tally like this:</p>
<pre><code>p_bag * p_word_1 * p_word_2 * ... * p_word_n
</code></pre>
<p>Note that including the probability of picking the bag (<em>p_bag</em>) is an
important step: if you only go by the words in the manuscript and ignore
how many manuscripts (or rather, published works) each author has
written, you can easily go wrong. If Becker has written ten times the
number of books that Warren has, it should reasonably require much
firmer evidence in the form of an ample presence of &quot;Warrenesque&quot; words
to assume that a manuscript was written by Warren than that it was
written by Becker. &quot;Extraordinary claims require extraordinary
evidence,&quot; as Carl Sagan once said.</p>
<p>OK, so now you have a total probability of the manuscript having been
written by Gibbons. Next, you repeat the whole procedure with the
assumption that maybe it was instead written by Becker (that is, that it
came out of the bag of words that Becker used when writing). That done,
you move on to considering the probability that the author was Warren
(and if you had more authors, you&#39;d keep going until you had covered
each of them).</p>
<p>When you&#39;re done, you have three total probabilities — one probability
per author. Then you just pick out the largest one, and, as they say,
Bob&#39;s your uncle! That&#39;s the author who most probably wrote this
manuscript.</p>
<p>(Minor technical note: when calculating</p>
<pre><code>p_bag * p_word1 * ... * p_word_n
</code></pre>
<p>in a software implementation we actually work with the
<a href="http://betterexplained.com/articles/using-logs-in-the-real-world/">logarithms</a> of the probabilities since the numbers
easily become very small. When doing this, we actually calculate</p>
<pre><code>log(p_bag) + log(p_word1) + ... + log(p_word_n)
</code></pre>
<p>That is, our multiplications turn into additions in line with
the rules of logarithms. But it all works out right: the class
with the highest number at the end wins.)</p>
<p>But wait! What if a manuscript contains a word that we&#39;ve never seen
Gibbons use before, but also lots of words he used all the time? Won&#39;t
that throw off our calculations?</p>
<p>Indeed. We shouldn&#39;t let outliers throw us off the scent. So we do
something very &quot;Bayesian&quot;: we put a &quot;prior&quot; on each word and each class
— we pretend we&#39;ve seen all imaginable words at least (say) once in each
bag, and that each bag has produced at least (say) one document. Then we
add those fake pretend counts — called <a href="http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_introbayes_sect004.htm">priors</a>, or pseudocounts — to
our real counts. Now, no word or bag gets a count of zero.</p>
<p>In fact, we can play around with the priors as much as we like: they&#39;re
simply a way of modeling our &quot;prior belief&quot; in the probability of one
thing over another. They could model our assumptions about a particular
author being more likely than others, or a particular word being more
likely to have come from the bag of a specific author, and so on. Such
beliefs are &quot;prior&quot; in the sense that we hold the belief before we&#39;ve
seen the evidence we are considering in the actual calculation we are
making. So above, for example, we could add a little bit to Mercy Otis
Warren&#39;s <em>p_bag</em> number if we thought it likely that as a woman, she
might well have had a harder time getting published, and so there might
reasonably be more manuscript material from her than one might infer
from a count of her published monographs.</p>
<p>In some cases, priors can make a Naive Bayesian classifier much more
usable. Often when we&#39;re classifying, after all, we&#39;re not after some
abstract &quot;truth&quot; — rather, we simply want a useful categorization. In
some cases, it&#39;s much more desirable to be mistaken one way than
another, and we can model that with proper class priors. The classic
example is, again, sorting email into junk mail and regular mail piles.
Obviously, you really don&#39;t want legitimate messages to be deleted as
spam; that could do much more damage than letting a few junk messages
slip through. So you set a big prior on the &quot;legitimate&quot; class that
causes your classifier to only throw out a message as junk when faced
with some hefty evidence. By the same token, if you&#39;re sorting the
results of a medical test into &quot;positive&quot; and &quot;negative&quot; piles, you may
want to weight the positive more heavily: you can always do a second
test, but if you send the patient home telling them they&#39;re healthy when
they&#39;re not, that might not turn out so well.</p>
<p>So there you have it, step by step. You have applied a Naive Bayesian to
the unattributed manuscripts, and you now have three neat piles. Of
course, you should keep in mind that Naive Bayesian classifiers are not
perfect, so you may want to do some further research before entering the
newfound materials into the library catalog as works by Gibbons, Becker,
and Warren, respectively.</p>
<h2 id="ok-so-lets-code-already">OK, so let&#39;s code already!</h2>
<p>So, our aim is to apply a Naive Bayesian learner to data from the Old
Bailey. First we get the data; then we clean it up and write some
routines to extract information from it; then we write the code that
trains and tests the learner.</p>
<p>Before we get into the nitty-gritty of downloading the files and
examining the training/testing script, let&#39;s just summarize what our aim
is and what the basic procedure looks like.</p>
<p>We want to have our Naive Bayesian read in trial records from the Old
Bailey and do with them the same thing as we did above in the examples
about the works of Gibbons, Becker, and Warren. In that example, we used
the published works of these authors to reconstruct each historian&#39;s bag
of words, and then used that knowledge to decide which historian had
written which unattributed manuscripts. In classifying the Old Bailey
trials, we will give the learner a set of trials labeled with the
offense for which the defendant was indicted so it can figure out the
&quot;bag of words&quot; that is associated with that offense. Then the learner
will use that knowledge to classify another set of trials where we have
not given it any information about the offense involved. The goal is to
see how well the learner can do this: how often does it label an
unmarked trial with the right offense?</p>
<p>The procedure used in the scripts we employ to train the learner is no
more complicated than the one in the historians-and-manuscripts example.
Basically, each trial is represented as a list of words, like so:</p>
<pre><code>michael, carney, was, indicted, for, stealing, on, the, 22nd, of, december, 26lb, weight, of, nails, value, 7s, 18, dozen, of, screws, ...
... , the, prisoners, came, to, my, shop, on, the, night, in, question, and, brought, in, some, ragged, pieces, of, beef, ...
..., i, had, left, my, door, open, and, when, i, returned, i, missed, all, this, property, i, found, it, at, the, pawnbroker, ...
</code></pre>
<p>When we train the learner, we give it a series of such word lists, along
with their correct bag labels (correct offenses). The learner then
creates word lists for each bag (offense), so that it ends up with a set
of counts similar to the counts we created for Gibbons, Becker, and
Warren, one count for each offense type (theft, deception, etc.)</p>
<p>When we test the learner, we feed it the same sort of word lists
representing other trials. But this time we don&#39;t give it the
information about what offense was involved. Instead, the learner does
what we did above: when it gets a list of words, it compares that list
to the word counts for each offense type, calculating which offense type
has a bag of words most similar to this list of words. It works offense
by offense, just like we worked author by author. So first it assumes
that the trial involved, say, the offense &quot;theft&quot;. It looks at the first
word in the trial&#39;s word list, checks how often that word occurred in
the &quot;theft&quot; bag, performs its probability calculations, moves on to the
next word, and so on. Then it checks the trial&#39;s word list against the
next category, and the next, until it has gone through each offense.
Finally it tallies up the probabilities and labels the trial with the
offense category that has the highest probability.</p>
<p>Finally, the testing script evaluates the performance of the learner and
lets us know how good it was at guessing the offense associated with
each trial.</p>
<h3 id="preliminaries">Preliminaries</h3>
<p>Many of the tools we are using to deal with the preliminaries have been
discussed at Programming Historian before. You may find it helpful to
check out (or revisit) the following tutorials:</p>
<ul>
<li>Milligan &amp; Baker, <a href="/lessons/intro-to-bash">Introduction to the Bash Command Line</a></li>
<li>Milligan, <a href="/lessons/automated-downloading-with-wget">Automated Downloading with wget</a></li>
<li>Knox, <a href="/lessons/understanding-regular-expressions">Understanding Regular Expressions</a></li>
<li>Wieringa, <a href="/lessons/intro-to-beautiful-soup">Intro to Beautiful Soup</a></li>
</ul>
<p>A few words about the file structure the scripts assume/create:</p>
<p>I have a &quot;top-level&quot; directory, which I&#39;m calling <em>bailey</em> (you
could call it something else, it&#39;s not referenced in the code). Under
that I have two directories: <em>baileycode</em> and <em>baileyfiles</em>.
The first contains all the scripts; the second contains the files
that are either downloaded or created by the scripts. That in turn
has subdirectories; all except one (for the downloaded XML
files — see below) are created by the scripts.</p>
<p>If you downloaded the complete zip package with all the files and
scripts, you automatically get the right structure; just unpack it in
its own directory. The only files that are omitted from that are the zip
files of trials downloaded below (if you got the complete package, you
already have the unpacked contents of those files, and the zips would
just take up unnecessary space).</p>
<p>If you only downloaded the scripts, you should do the following:</p>
<ul>
<li>Create a directory and name it something sensible (say, <em>bailey</em>).</li>
<li>In that directory, create another directory called <em>baileycode</em> and
unpack the contents of the script zip file into that directory
(make sure you don&#39;t end up with two <em>baileycode</em> directories inside
one another).</li>
<li>In the same directory (<em>bailey</em>), create another directory called
<em>baileyfiles</em>.</li>
</ul>
<p>On my Mac, the structure looks like this:</p>
<p>{% include figure.html filename=&quot;naive-bayesian-2.png&quot; caption=&quot;Bailey Folders&quot; %}</p>
<h4 id="downloading-trials">Downloading trials</h4>
<p>The Old Bailey lets you download trials in zip files of 10 trials each,
so that&#39;s what we&#39;ll do. This is how we do it: we first look at how the
Old Bailey system requests files, and then we write a script that
creates a file with a bunch of those requests. Then we feed that file to
wget, so we don&#39;t have to sit by our computers all day downloading each
set of 10 trials that we want.</p>
<p>As explained on the Old Bailey <a href="http://www.oldbaileyonline.org/static/DocAPI.jsp">documentation for developers</a> page, this
is what the http request for a set of trials looks like:</p>
<pre><code>http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=211&amp;return=zip
</code></pre>
<p>As you see, we can request all trials that took place between two
specified dates (<em>fromdate</em> and <em>todate</em>). The <em>count</em> specifies how
many trials we want, and the <em>start</em> variable says where in the results
to start (in the above, we start with result number 211 and get the ten
following trials). Ten seems to be the highest number allowed for
<em>count</em>, so we need to work around that.</p>
<p>We get around the restriction for how many trials can be in a zip file
with a little script that builds as many of the above type of requests
as we need to get all trials from the 1830s. We can find out how many
trials that is by going to the <a href="http://www.oldbaileyonline.org/forms/formMain.jsp">Old Bailey search page</a> and entering
January 1830 as the start date, December 1839 as the end date, and
choosing &quot;Old Bailey Proceedings &gt; trial accounts&quot; in the &quot;Search In&quot;
field. Turns out there were 22,711 trials in the 1830s.</p>
<p>Here&#39;s the whole script (<code>wgetxmls.py</code>) that creates the list of http requests we need:</p>
<pre><code class="language-python">    mainoutdirname = &#39;../baileyfiles/&#39;
    wgets = &#39;&#39;
    for x in range(0,22711,10):
        getline = &#39;http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=&#39; + str(x+1) + &#39;&amp;return=zip\n&#39;
        wgets += getline
    filename = mainoutdirname + &#39;wget1830s.txt&#39;
    with open(filename,&#39;w&#39;) as f:
        f.write(wgets)
</code></pre>
<p>As you see, we accept the limitation of 10 trials at a time, but
manipulate the start point until we have covered all the trials from the
1830s.</p>
<p>Assuming you&#39;re in the <em>baileycode</em> directory, you can run the script
from the command line like this:</p>
<pre><code class="language-bash">python wgetxmls.py
</code></pre>
<p>What that gets us is a file that looks like this:</p>
<pre><code>http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=1&amp;return=zip
http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=11&amp;return=zip
http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=21&amp;return=zip
...
</code></pre>
<p>This file is saved in the <em>baileyfiles</em> directory; it is called <code>wget1830s.txt</code>.</p>
<p>To download the trials, create a new directory under <em>baileyfiles</em>;
call it <em>trialzips</em>. Then go into that directory and call <em>wget</em> with the
file we just created. So, assuming you are still in the <em>baileycode</em> directory,
you would write the following commands on the command line:</p>
<pre><code class="language-bash">cd ../baileyfiles
mkdir trialzips
cd trialzips
wget -w 2 -i ../wget1830s.txt
</code></pre>
<p>The &quot;-w 2&quot; is just to be polite and not overload the server; it tells
<em>wget</em> to wait 2 seconds between each request. The &quot;-i&quot; flag tells <em>wget</em>
that it should request the URLs found in <code>wget1830s.txt</code>.</p>
<p>What <em>wget</em> returns is a lot of zip files that have unwieldy names and no
extension. You should rename these so that the extension is &quot;.zip&quot;.
Then, in the directory <em>baileyfiles</em>, create a subdirectory called
<em>1830s-trialxmls</em> and then unpack the zips into that so that it
contains 22,170 XML files that each look like <code>t18391216-388.xml</code>. Assuming
you are still in the <em>trialzips</em> directory, you would write:</p>
<pre><code class="language-bash">for f in * ; do mv $f $f.zip; done;
mkdir ../1830s-trialxmls
unzip &quot;*.zip&quot; -d ../1830s-trialxmls/
</code></pre>
<p>If you open one of the trial XMLs in a browser, you can see that it
contains all kinds of useful information: name and gender of defendant,
name and gender of witnesses, type of offense, and so on. Here&#39;s a
snippet from one trial:</p>
<pre><code class="language-xml">&lt;persname id=&quot;t18300114-2-defend110&quot; type=&quot;defendantName&quot;&gt;
THOMAS TAYLOR
    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;surname&quot; value=&quot;TAYLOR&quot;&gt;
    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;given&quot; value=&quot;THOMAS&quot;&gt;
    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;gender&quot; value=&quot;male&quot;&gt;
    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;age&quot; value=&quot;25&quot;&gt;
&lt;/interp&gt;&lt;/interp&gt;&lt;/interp&gt;&lt;/interp&gt;&lt;/persname&gt;
was indicted for
    &lt;rs id=&quot;t18300114-2-off7&quot; type=&quot;offenceDescription&quot;&gt;
        &lt;interp inst=&quot;t18300114-2-off7&quot; type=&quot;offenceCategory&quot; value=&quot;violentTheft&quot;&gt;
        &lt;interp inst=&quot;t18300114-2-off7&quot; type=&quot;offenceSubcategory&quot; value=&quot;robbery&quot;&gt;
            feloniously assaulting
        &lt;persname id=&quot;t18300114-2-victim112&quot; type=&quot;victimName&quot;&gt;
            David Grant
                  &lt;interp inst=&quot;t18300114-2-victim112&quot; type=&quot;surname&quot; value=&quot;Grant&quot;&gt;
            &lt;interp inst=&quot;t18300114-2-victim112&quot; type=&quot;given&quot; value=&quot;David&quot;&gt;
            &lt;interp inst=&quot;t18300114-2-victim112&quot; type=&quot;gender&quot; value=&quot;male&quot;&gt;
            &lt;join result=&quot;offenceVictim&quot; targorder=&quot;Y&quot; targets=&quot;t18300114-2-off7 t18300114-2-victim112&quot;&gt;
        &lt;/join&gt;&lt;/interp&gt;&lt;/interp&gt;&lt;/interp&gt;&lt;/persname&gt;
&lt;/interp&gt;&lt;/interp&gt;&lt;/rs&gt;
</code></pre>
<p>The structured information in the XML lets us reliably extract the
&quot;classes&quot; we want to sort our documents into. We are going to classify
the trials by offense category (and subcategory), so that&#39;s the
information we&#39;re going to extract before converting the XML into a text
file that we can then feed to our learner.</p>
<h4 id="saving-the-trials-into-text-files">Saving the trials into text files</h4>
<p>Now that we have the XML files, we can start extracting information and
plain text from them to feed to our learner. We want to sort the trials
into text files, so that each text file contains all the trials in a
particular offense category (theft-simplelarceny, breakingpeace-riot,
etc.).</p>
<p>We also want to create a text file that contains all the trial IDs
(marked in the XML), so we can use that to easily create
cross-validation samples. The reasons for doing this are discussed below
in the section &quot;Creating the cross-validation samples&quot;.</p>
<p>The script that does these things is called <code>save-txttrials-by-category.py</code>
and it&#39;s pretty extensively commented, so I&#39;ll just note a few things here.</p>
<ol>
<li>We strip the trial text of all punctuation, including quote marks
and parentheses, and we equalize all spaces (newlines, tabs,
multiple spaces) into a single space. This helps us simplify the
coding of the training process (and, incidentally, keep the code
that trains the learner general enough that as long as you have text
files saved in the same format as we use here, you should be able to
apply it more or less directly to your data).</li>
<li>That of course makes the text hard to read for a human. Therefore,
we also save the text of each trial into a file named after the
trial id, so that we can easily examine a particular trial if we
want to (which we will).</li>
</ol>
<p>The script creates the following directories and files under <em>baileyfiles</em>:</p>
<ul>
<li>Directory <em>1830s-trialtxts</em>: this will
contain the text file versions of the trials after they have been
stripped of all XML formatting. Each file
is named after the trial&#39;s ID.</li>
<li>Directory <em>1830s-trialsbycategory</em>: this
will contain the text files that represent all the text in all the
trials belonging to a particular category. These are named after the
category, e.g., <code>theft-simplelarceny.txt</code>. Each category file
contains all the trials in that category, with one trial per line.</li>
<li>File <code>trialids.txt</code>. This contains the
sorted list of trial IDs, one ID per line; we will use it later in
creating cross-validation samples for training the learner (this is
the next step).</li>
<li>Files <code>offensedict.json</code> and <code>trialdict.json</code>. These json files will come
into use in training the learner.</li>
</ul>
<p>So if you&#39;re still in the <em>trialxmls</em> directory, you would write the
following commands to run this script:</p>
<pre><code class="language-bash">cd ../../baileycode/
python save-trialtxts-by-category.py
</code></pre>
<p>This will take a while. After it&#39;s done, you should have the directories
and files described above.</p>
<h4 id="creating-the-cross-validation-samples">Creating the cross-validation samples</h4>
<p>Now that we have all our trials saved where we want them, all we need to
do is to create the cross-validation samples and we&#39;re ready to test our
learners.</p>
<p>Cross-validation simply means repeatedly splitting our data into chunks,
some of which we use for training and others for testing. Since the idea
is to get a learner to extract information from one set of documents
that it can then use to determine the class of documents it has never
seen, we obviously have to reserve a set of documents that are unknown
to the learner if we want to test its performance. Otherwise it&#39;s a bit
like letting your students first read an exam <em>and its answers</em> and then
have them take that same exam. That would only tell you how closely they
read the actual exam, not whether they&#39;ve learned something more
general.</p>
<p>So what you want to do is to test the learner on data it hasn&#39;t seen
before, so that you can tell whether it has learned some general
principles from the training data. You could just split your data into
two sets, using, say, 80 percent for training and 20 percent for
testing. But a common practice is to split your data repeatedly into
different test and train sets, so that you can ensure that your test
results aren&#39;t the consequence of some oddball quirk in the portion of
data you left for testing.</p>
<p>Two scripts are involved in creating the cross-validation sets. The
script <code>tenfold-crossvalidation.py</code> creates
the samples. It reads in the list of trial ids we created in the
previous step, shuffles that list to make it random, and divides it into
10 chunks of roughly equal length (that is, a roughly equal number of
trial ids). Then it writes those 10 chunks each into its own text file,
so we can read them into our learner code later. Next, to be meticulous,
we can run the <code>count-offense-instances.py</code>
to confirm that if we are interested in a particular trial category,
that category is reasonably evenly distributed across the samples.</p>
<p>Before you run the <code>count-offense-instances.py</code> script, you should
edit it to set the category to the one you&#39;re interested in and let the
script know whether we&#39;re looking at a broad or a specific category.
This is what the relevant part of the code looks like:</p>
<pre><code class="language-python">indirname = &#39;../baileyfiles/&#39;
offensedict_fn = indirname + &#39;offensedict.json&#39;
offensecat = &#39;breakingpeace&#39; #change to target category
broadcat = True #set true if category is e.g. &quot;theft&quot; instead of &quot;theft-simplelarceny&quot;
</code></pre>
<p>And here are the commands to run the cross-validation scripts (assuming
you are still in the <em>baileycode</em> directory).</p>
<pre><code class="language-bash">python tenfold-crossvalidation.py
python count-offense-instances.py
</code></pre>
<p>Alternatively, you can run them using <a href="http://pypy.org/">pypy</a>, which is
quite a bit faster.</p>
<pre><code class="language-bash">pypy tenfold-crossvalidation.py
pypy count-offense-instances.py
</code></pre>
<p>The output of the <code>count-offense-instances.py</code> script looks like
this:</p>
<pre><code>Offense category checked for: breakingpeace
sample0.txt: 31
sample1.txt: 25
sample2.txt: 32
sample3.txt: 25
sample4.txt: 36
sample5.txt: 33
sample6.txt: 29
sample7.txt: 35
sample8.txt: 27
sample9.txt: 31
</code></pre>
<p>From the output, we can conclude that the distribution of instances of
&quot;breakingpeace&quot; is more or less even. If it isn&#39;t, we can re-run the
<code>tenfold-crossvalidation.py</code> script, and then check the distribution again.</p>
<h3 id="testing-the-learner">Testing the learner</h3>
<p>All right, we are ready to train and test our Naive Bayesian! The script
that does this is called <code>test-nb-learner.py</code>. It starts by defining a few
variables:</p>
<pre><code class="language-python">categoriesdir = &#39;../baileyfiles/1830s-trialsbycategory/&#39;
sampledirname = &#39;../baileyfiles/Samples_1830s/&#39; #location of 10-fold cross-validation
stopwordfilename = &#39;../baileyfiles/english-stopwords.txt&#39;
# the ones below should be set to None if not using
cattocheck = &#39;breakingpeace&#39; #if evaluating recognition one category against rest
pattern = &#39;[^-]+&#39; #regex pattern to use if category is not complete filename
</code></pre>
<p>Most of these are pretty self-explanatory, but note the two last ones.
The variable &quot;cattocheck&quot; determines whether we are looking to identify
a specific category or to sort each trial into its proper category (the
latter is done if the variable is set to None). The variable &quot;pattern&quot;
tells us whether we are using the whole file name as the category
designation, or only a part of it, and if the latter, how to identify
the part. In the example above, we are focusing on the broad category
&quot;breakingpeace&quot;, and so we are not using the whole file name, which
would be e.g. &quot;breakingpeace-riot&quot; but only the part before the dash.
Before you run the code, you should set these variables to what you want
them to be.</p>
<p>Note that &quot;cattocheck&quot; here should match the &quot;offensecat&quot; that you
checked for with the <code>count-offense-instances.py</code> script. No error is
produced if it does not match, and it&#39;s fairly unlikely that it will
have any real impact, but if the categories don&#39;t match, then of course
you have no assurance that the category you&#39;re actually interested in is
more or less evenly distributed across the ten cross-validation samples.</p>
<p>Note also that you can of course set &quot;cattocheck&quot; to &quot;None&quot; and leave
the pattern as it is, in which case you will be sorting into the broader
categories.</p>
<p>So, with the basic switches set and knobs turned, we begin by reading in
all the trials that we have saved. We do this with the function called
<em>process_data</em> that can be found in the <code>pretestprocessing.py</code> file. (That file
contains functions that are called from the scripts you will run, so it
isn&#39;t something you&#39;ll run directly at any point.)</p>
<pre><code class="language-python">print &#39;Reading in the data...&#39;
trialdata = ptp.process_data(categoriesdir,stopwordfilename,cattocheck,pattern)
</code></pre>
<p>The <em>process_data</em> function reads in all
the files in the directory that contains our trial category files, and
processes them so that we get a list containing all the categories and
the trials belonging to them, with the trial text lowercased and
tokenized (split into a list of words), minus stopwords (common words
like a, the, me, which, etc.) Each trial begins with its id number, so
that&#39;s one of our words (though we ignore it in training and testing).
Like this:</p>
<pre><code>    [
     [breakingpeace,
       [&#39;trialid&#39;,&#39;victim&#39;,&#39;peace&#39;,&#39;disturbed&#39;,&#39;man&#39;,&#39;tree&#39;,...]
       [&#39;trialid&#39;,&#39;dress&#39;,&#39;blood&#39;,&#39;head&#39;,&#39;incited&#39;,...]
      ...]
     [theft,
       [&#39;trialid&#39;,&#39;apples&#39;,&#39;orchard&#39;,&#39;basket&#39;,&#39;screamed&#39;,&#39;guilty&#39;,....]
       [&#39;trialid&#39;,&#39;rotten&#39;,&#39;fish&#39;]
      ...]
    ]
</code></pre>
<p>Next, making use of the results of the ten-fold cross-validation routine
we created, we loop through the files that define the
samples, each time making one sample the test set and the rest the train
set. Then we split &#39;trialdata&#39;, the list of trials-by-category that we
just created, into train and test sets accordingly. The functions that
do these two steps are <em>create_sets</em> and
<em>splittraintest</em>, both in the <code>pretestprocessing.py</code> file.</p>
<p>Now we train our Naive Bayesian classifier on the train set. The
classifier we are using (which is included in the scripts zip file) is
one written by Mans Hulden, and it does pretty much exactly what the
&quot;identify the author of the manuscript&quot; example above
describes.</p>
<pre><code class="language-python">    # split train and test
    print &#39;Creating train and test sets, run {0}&#39;.format(run)
    trainsetids, testsetids = ptp.create_sets(sampledirname,run)
    traindata, testdata = ptp.splittraintest(trainsetids,testsetids,trialdata)

    # train learner
    print &#39;Training learner, run {0}...&#39;.format(run)
    mynb = nb.naivebayes()
    mynb.train(traindata)
</code></pre>
<p>After the learner is trained, we are ready to test how well the it
performs. Here&#39;s the code:</p>
<pre><code class="language-python">    print &#39;Testing learner, run {0}...&#39;.format(run)

    for trialset in testdata:
        correctclass = trialset[0]
        for trial in trialset[1:]:
            result = mynb.classify(trial)
            guessedclass =  max(result, key=result.get)
            # then record correctness of classification result
            # note that first version does a more complex evaluation
            # ... for two-way (one class against rest) classification
            if cattocheck:
                if correctclass == cattocheck:
                    catinsample += 1
                if guessedclass == cattocheck:
                     guesses += 1
                     if guessedclass == correctclass:
                         hits += 1
            if guessedclass == correctclass:
                correctguesses += 1

            total +=1
</code></pre>
<p>So we loop through the categories in the &quot;testdata&quot; list (which is of
the same format as the &quot;trialdata&quot; list). For each
category, we loop through the trials in that category, classifying each
trial with our Naive Bayesian classifier, and comparing the result to
the correct class (saved in the first element of each category list
within the testdata list.) Then we add to various counts to be able to
evaluate the results of the whole classification exercise.</p>
<p>To run the code that trains and tests the learner, first make sure you
have edited it to set the &quot;cattocheck&quot; and &quot;pattern&quot; switches, and then
call it on the command line (assuming you&#39;re still in the directory
<em>baileycode</em>):</p>
<pre><code class="language-bash">    python test-nb-learner.py
</code></pre>
<p>Again, for greater speed, you can also use pypy:</p>
<pre><code class="language-bash">    pypy test-nb-learner.py
</code></pre>
<p>The code will print out some accuracy measures for the classification
task you have chosen. The output should look something like this:</p>
<pre><code>Reading in the data...
Creating train and test sets, run 0
Training learner, run 0...
Testing learner, run 0...
Creating train and test sets, run 1
Training learner, run 1...
...
Training learner, run 9...
Testing learner, run 9...
Saving correctly classified trials and close matches...
Calculating accuracy of classification...
Two-way classification, target category breakingpeace.
And the results are:
Accuracy 99.00%
Precision: 61.59%
Recall: 66.45%
Average number of target category trials in test sample per run: 30.4
Average number of trials in test sample per run: 2271.0
Obtained in 162.74 seconds
</code></pre>
<p>Next, let&#39;s take a look at what these measures of accuracy mean.</p>
<h4 id="measures-of-classification">Measures of classification</h4>
<p>The basic measure of classification prowess is <em>accuracy</em>: how often did
classifier guess the class of a document correctly? This is calculated
by simply dividing the number of correct guesses by the total number of
documents considered.</p>
<p>If we&#39;re interested in a specific category, we can extract a bit more
data. So if we set, for example, cattocheck =
&#39;breakingpeace&#39;, like above, we can then examine how well the classifier
did with respect to the &quot;breakingpeace&quot; category in particular.</p>
<p>So, in the testlearner code, if we&#39;re doing
multiway classification, we only record how many trials we&#39;ve seen
(&quot;total&quot;) and how many of our guesses were correct (&quot;correctguesses&quot;).
But if we&#39;re considering a single category, say &quot;breakingpeace,&quot; we
record a few more numbers. First, we keep track of how many trials
belonging to the category &quot;breakingpeace&quot; there are in our test sample
(this tally is in &quot;catinsample&quot;). We also keep track of how many times
we&#39;ve guessed that a trial belongs to the &quot;breakingpeace&quot; category
(&quot;guesses&quot;). And finally we record how many times we have guessed
<em>correctly</em> that a trial belongs to &quot;breakingpeace&quot; (&quot;hits&quot;).</p>
<p>Now that we have this information, we can use it to calculate a couple
of standard measures of classification efficiency: <em>precision</em> and
<em>recall</em>. Precision tells us how often we correctly guessed that a
trial was in the &quot;breakingpeace&quot; category. Recall lets us know what
proportion of the &quot;breakingpeace&quot; trials we caught.</p>
<p>Let&#39;s take another example to clarify precision and recall. Imagine you
want all the books on a particular topic — World War I, say — from your
university library. You send out one of your many minions (all
historians possess droves of them, as you know) to get the books. The
minion dutifully returns with a big pile.</p>
<p>Now, suppose you were in possession of a list that contained of every
single book in the library on WWI and no books that weren&#39;t related to
the WWI. You could then check the precision and recall of your minion
with regard to the category of &quot;books on WWI.&quot;</p>
<p>Recall is the term for the proportion of books on WWI in the library
that your minion managed to grab. That is, the more books on WWI
remaining in the library after your minion&#39;s visit, the lower your
minion&#39;s recall.</p>
<p>Precision, in turn, is the term for the proportion of books in the pile
brought by your minion that actually had to do with WWI. The more
irrelevant (off-topic) books in the pile, the lower the precision.</p>
<p>So, say the library has 1,000 books on WWI, and your minion lugs you a
pile containing 400 books, of which 300 have nothing to do with WWI. The
minion&#39;s recall would be (400-300)/1,000, or 10 percent. The minion&#39;s
precision, in turn, would be (400-300)/400, or 25 percent.</p>
<p>(Should have gone yourself, eh?)</p>
<p>A side note: the minion&#39;s overall accuracy — correct guesses divided by
actual number of examples — would be:</p>
<pre><code>(the number of books on WWI in your pile - the number of books *not* on
WWI in your pile + the number of books in the library *not* on WWI)
------------------------------------------------------------------------
the total number of books in the library
</code></pre>
<p>So if the library held 100,000 volumes, this would be (100 - 300 +
99,000) / 100,000 — or 98.8 percent. That seems like a great number, but
since it merely means that your minion was smart enough to leave most of
the library books in the library, it&#39;s not very helpful in this case
(except inasmuch as it is nice not to be buried under 100,000 volumes.)</p>
<h4 id="how-well-does-our-naive-bayesian-do">How well does our Naive Bayesian do?</h4>
<p>Our tests on the Naive Bayesian use the data set consisting of all the
trials from the 1830s. It contains 17,549 different trials in 50
different offense categories (which can be grouped into 9 broad
categories).</p>
<p>If we run the Naive Bayesian so that it attempts to sort all trials into
their correct broad categories, its accuracy is pretty good: 94.3
percent. So 94 percent of the time, when it considers how it should sort
trials in the test sample into &quot;breakingpeace,&quot; &quot;deception,&quot; &quot;theft,&quot;
and so on, it chooses correctly.</p>
<p>For the more specific categories (&quot;theft-simplelarceny,&quot;
&quot;breakingpeace-riot,&quot; and so on) the same exercise is much less
accurate: then, the classifier gets it right only 72 percent of the
time. That&#39;s no wonder, really, given that some of the categories are so
small that we barely have any examples. We might do a bit better with
more data (say, all the trials from the whole 19th century, instead of
only all the trials from the 1830s).</p>
<p>The first, overall category results are pretty impressive. They give us
quite a bit of confidence that if what we needed to do was to sort
documents into piles that weren&#39;t all too fine-grained, and we had a
nice bunch of training data, a Naive Bayesian could do the job for us.</p>
<p>But the problem for a historian is often rather different. A historian
using a Naive Bayesian learner is more likely to want to separate
documents that are &quot;interesting&quot; from documents that are &quot;not
interesting&quot; — usually meaning documents dealing with a particular issue
or not dealing with it. So the question is really more one where we have
a mass of &quot;uncategorized&quot; or &quot;other&quot; documents and a much smaller set of
&quot;interesting&quot; documents, and we try to find more of the latter among the
former.</p>
<p>In our current exercise, that situation is fairly well represented by
trying to identify documents from a single category in the mass of the
rest of the documents, set to category &quot;other.&quot; So how well are we able
to do that? In other words, if we set cattocheck
= &#39;breakingpeace&#39; (or another category) so that all trials get
marked as either that category or as &quot;other,&quot; and then run the
classifier, what kinds of results do we get?</p>
<p>Well, our overall accuracy is still high: over 95 percent in all cases
for the broad categories, and usually about that for the detailed ones
as well. But just like the minion going off to the library to get books
on WWI had a pretty high accuracy because he/she didn&#39;t bring back half
the library, in this case, too, our accuracy is mostly just due to the
fact that we manage to not misidentify <em>too</em> many &quot;other&quot; trials as
being in the category we&#39;re interested in. Because there are so many
&quot;other&quot; trials, those correct assessments far outweigh the minus points
we may have gotten from missing interesting trials.</p>
<p>Precision and recall, therefore, are more in this case more interesting
measures than overall accuracy. Here&#39;s a table showing precision and
recall for each of the &quot;broad&quot; categories in our trial sample, and for a
few sample detailed categories. The last column shows how many target
category trials there were in the test set on average (remember, we did
ten runs with different train/test splits, so all our results are
averages of that).</p>
<p><strong>Naive Bayesian classifier, two-way classification, 10-fold
cross-validation</strong></p>
<p><strong>Broad categories</strong></p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Precision (%)</th>
<th>Recall (%)</th>
<th>Avg # trials in cat in TeS</th>
</tr>
</thead>
<tbody><tr>
<td>breakingpeace</td>
<td>63.52</td>
<td>64.05</td>
<td>24.2</td>
</tr>
<tr>
<td>damage</td>
<td>0.00</td>
<td>0.00</td>
<td>1.2</td>
</tr>
<tr>
<td>deception</td>
<td>53.47</td>
<td>61.43</td>
<td>47.7</td>
</tr>
<tr>
<td>kill</td>
<td>62.5</td>
<td>89.39</td>
<td>17.9</td>
</tr>
<tr>
<td>miscellaneous</td>
<td>47.83</td>
<td>4.44</td>
<td>24.8</td>
</tr>
<tr>
<td>royaloffenses</td>
<td>85.56</td>
<td>91.02</td>
<td>42.3</td>
</tr>
<tr>
<td>sexual</td>
<td>93.65</td>
<td>49.17</td>
<td>24.0</td>
</tr>
<tr>
<td>theft</td>
<td>96.26</td>
<td>98.75</td>
<td>1551.8</td>
</tr>
<tr>
<td>violenttheft</td>
<td>68.32</td>
<td>33.01</td>
<td>20.9</td>
</tr>
</tbody></table>
<p><strong>Sample detailed categories</strong></p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Precision (%)</th>
<th>Recall (%)</th>
<th>Avg # trials in cat in TeS</th>
</tr>
</thead>
<tbody><tr>
<td>theft-simplelarceny</td>
<td>64.37</td>
<td>89.03</td>
<td>805.9</td>
</tr>
<tr>
<td>theft-receiving</td>
<td>92.21</td>
<td>61.53</td>
<td>198.1</td>
</tr>
<tr>
<td>deception-forgery</td>
<td>74.29</td>
<td>11.87</td>
<td>21.9</td>
</tr>
<tr>
<td>violenttheft-robbery</td>
<td>68.42</td>
<td>31.86</td>
<td>20.4</td>
</tr>
<tr>
<td>theft-extortion</td>
<td>0.00</td>
<td>0.00</td>
<td>1.3</td>
</tr>
</tbody></table>
<p>There are a few generalizations we can make from these numbers.</p>
<p>First, it&#39;s obvious that if the category is too small, we are out of
luck. So for &quot;damage,&quot; a small enough broad category that our test
samples only held a little over one instance of it on average, we get no
results. Similarly, in the detailed categories, when the occurrence of
cases per test sample drops into the single digits, we fail miserably.
This is no wonder: if the test sample contains about one case on
average, there can&#39;t be much more than ten cases total in the whole data
set. That&#39;s not much to go on.</p>
<p>Second, size isn&#39;t everything. Although we do best for the biggest
category, theft (which in fact accounts for over half our sample), there
are some smaller categories we do very well for. We have very high
recall and precision for &quot;royaloffenses,&quot; a mid-sized category, and very
high recall plus decent precision for &quot;kill,&quot; our smallest
reasonable-sized category. A reasonable guess would be that the language
that occurs in the trials is distinctive and, in the case of
&quot;royaloffenses,&quot; doesn&#39;t occur much anywhere else. Meanwhile,
unsurprisingly, we get low scores for the &quot;miscellaneous&quot; category. We
also have high precision for the &quot;sexual&quot; category, indicating that it
has some language that doesn&#39;t tend to appear anywhere else — though we
miss about half the instances of it, which would lead us to suspect that
many of the trials in that category omit some of the language that most
distinguishes it from others.</p>
<p>Third, in this sample at least, there seems to be no clear pattern
regarding whether the learner has better recall or better precision.
Sometimes it casts a wide net that drags in both a good portion of the
category and some driftwood, and sometimes it handpicks the trials for
good precision but misses a lot that don&#39;t look right enough for its
taste. So in half the cases here, our learner has better precision than
recall, and in half better recall than precision. The differences
between precision and recall are, however, bigger for the cases where
precision is better than recall. That isn&#39;t necessarily a good thing for
us, since as historians we might be happier to see more of the
&quot;interesting&quot; documents and do the additional culling ourselves than to
have our learner miss a lot of good documents. We&#39;ll return to the
question of the meaning of classification errors below.</p>
<h4 id="extracting-the-most-indicative-features">Extracting the most indicative features</h4>
<p>The <code>naivebayes.py</code> script has a feature
that allows you to extract the most (and least) indicative features of
your classification exercise. This allows you to see what weighs a lot
in the learner&#39;s mind — what it has, in effect, learned.</p>
<p>The command to issue is: <em>mynb.topn_print(10)</em> (for the 10 most
indicative; you can put in any number you like). Here are the results
for a multi-way classification of the broad categories in our data:</p>
<pre><code>deception [&#39;norrington&#39;, &#39;election&#39;, &#39;flaum&#39;, &#39;polish&#39;, &#39;caton&#39;, &#39;spicer&#39;, &#39;saltzmaun&#39;, &#39;newcastle&#39;, &#39;stamps&#39;, &#39;rotherham&#39;]
royaloffences [&#39;mould&#39;, &#39;coster&#39;, &#39;coin&#39;, &#39;caleb&#39;, &#39;counterfeit&#39;, &#39;obverse&#39;, &#39;mint&#39;, &#39;moulds&#39;, &#39;plaster-of-paris&#39;, &#39;metal&#39;]
violenttheft [&#39;turfrey&#39;, &#39;stannard&#39;, &#39;millward&#39;, &#39;falcon&#39;, &#39;crawfurd&#39;, &#39;weatherly&#39;, &#39;keith&#39;, &#39;farr&#39;, &#39;ventom&#39;, &#39;shurety&#39;]
damage [&#39;cow-house&#39;, &#39;ewins&#39;, &#39;filtering-room&#39;, &#39;fisk&#39;, &#39;calf&#39;, &#39;skirting&#39;, &#39;girder&#39;, &#39;clipping&#39;, &#39;saturated&#39;, &#39;firemen&#39;]
breakingpeace [&#39;calthorpe-street&#39;, &#39;grievous&#39;, &#39;disable&#39;, &#39;mellish&#39;, &#39;flag&#39;, &#39;bodily&#39;, &#39;banner&#39;, &#39;aforethought&#39;, &#39;fursey&#39;, &#39;emerson&#39;]
miscellaneous [&#39;trevett&#39;, &#39;teuten&#39;, &#39;reitterhoffer&#39;, &#39;quantock&#39;, &#39;feaks&#39;, &#39;boone&#39;, &#39;bray&#39;, &#39;downshire&#39;, &#39;fagnoit&#39;, &#39;ely&#39;]
kill [&#39;vault&#39;, &#39;external&#39;, &#39;appearances&#39;, &#39;slaying&#39;, &#39;deceased&#39;, &#39;marchell&#39;, &#39;disease&#39;, &#39;pedley&#39;, &#39;healthy&#39;, &#39;killing&#39;]
theft [&#39;sheep&#39;, &#39;embezzlement&#39;, &#39;stealing&#39;, &#39;table-cloth&#39;, &#39;fowls&#39;, &#39;dwelling-house&#39;, &#39;missed&#39;, &#39;pairs&#39;, &#39;breaking&#39;, &#39;blankets&#39;]
sexual [&#39;bigamy&#39;, &#39;marriage&#39;, &#39;violate&#39;, &#39;ravish&#39;, &#39;marriages&#39;, &#39;busher&#39;, &#39;register&#39;, &#39;spinster&#39;, &#39;bachelor&#39;, &#39;married&#39;]
</code></pre>
<p>Some of these make sense instantly. In &quot;breakingpeace&quot; (which includes
assaults, riots and woundings) you can see the makings of phrases like
&quot;grievous bodily harm&quot; and &quot;malice aforethought,&quot; along with other
indications of wreaking havoc like &quot;disable&quot; and &quot;harm.&quot; In
royaloffenses, the presence of &quot;mint,&quot; &quot;mould&quot; and &quot;plaster-of-paris&quot;
make sense since the largest subcategory is coining offenses. In
&quot;theft,&quot; one might infer that sheep, fowl, and table-cloths seem to have
been popular objects for stealing (though table-cloth may of course have
been a wrapping for stolen objects; one would have to examine the trials
to know).</p>
<p>Others are more puzzling. Why is violenttheft almost exclusively
composed of what seem to be person or place names? Why is &quot;election&quot;
indicative of deception? Is there a lot of election fraud going on, or
abuse of elected office? Looking at the documents, one finds that 9 of
the words indicative of violent theft are person names, and one is a
pub; why person and pub names should be more indicative here than for
other categories is mildly intriguing and might bear further analysis
(or might just be a quirk of our data set — remember that &quot;violenttheft&quot;
is a fairly small category). As for &quot;election,&quot; it&#39;s hard to distinguish
a clear pattern, though it seems to be linked to fraud attempts on and
by various officials at different levels of government.</p>
<p>The indicative features, then, may be intriguing in themselves (though
obviously, one should not draw any conclusions about them without
closely examining the data first). They are also useful in that they can
help us determine whether something is skewing our results in a way we
don&#39;t wish, something we may be able to correct for with different
weighting or different selection of features (see the section on
<a href="#Tuning">Tuning</a> below).</p>
<h3 id="the-meanings-of-misclassification">The meanings of misclassification</h3>
<p>Again, it&#39;s good to keep in mind that in classifying documents we are
not always after an abstract &quot;true&quot; classification, but simply a useful
or interesting one. Thus, it is a good idea to look a bit more closely
at the &quot;errors&quot; in classification.</p>
<p>We&#39;ll focus on two-way classification, and look at the cases where the
Naive Bayesian incorrectly includes a trial in a category (false
positives) as well as take a look at trials it narrowly excludes from
the category (let&#39;s call them close relatives).</p>
<p>In the script for testing the learner (<code>test-nb-learner.py</code>), we saved the trial ids for
false positives and close relatives so we could examine them later.</p>
<p>Here&#39;s the relevant code bit:</p>
<pre><code class="language-python">result = mynb.classify(trial)
guessedclass =  max(result, key=result.get)
if cattocheck:
    diff = abs(result[cattocheck] - result[&#39;other&#39;])
    if diff &lt; 10 and guessedclass != cattocheck:
        closetrials.append(trial[0])
        difflist.append(diff)
    if correctclass == cattocheck:
        catinsample += 1
    if guessedclass == cattocheck:
         guesses += 1
         if guessedclass == correctclass:
             hits += 1
         else:
             falsepositives.append(trial[0])
if guessedclass == correctclass:
    correctguesses += 1
</code></pre>
<p>False positives are easy to catch: we simply save the cases where we
guessed that a trial belonged to the category but it really did not.</p>
<p>For close relatives, we first check how confident we were that the trial
did not belong in our category. When we issue the call to classify the
trial <em>mynb.classify(trial)</em>, it returns
us a dictionary that looks like this:</p>
<pre><code>{
    &#39;other&#39;: -2358.522248351527,
    &#39;violenttheft-robbery&#39;: -2326.2878233211086
}
</code></pre>
<p>So to find the close relatives, we compare these two values, and if the
difference between them is small, we save the id of the trial we are
currently classifying into a list of close relatives. (In the code chunk
above, we have rather arbitrarily defined a &quot;small&quot; difference as being
under 10).</p>
<p>At the end of the script, we write the results of these operations into
two text files: <code>falsepositives.txt</code> and <code>closerelatives.txt</code>.</p>
<p>Let&#39;s look more closely at misclassifications for the category
&quot;violenttheft-robbery.&quot; Here are the first 10 rows of the close
relatives file and the first 20 rows of the false positives file, sorted
by offense:</p>
<p><strong>Close relatives</strong></p>
<pre><code>breakingpeace-wounding, t18350105-458, 1.899530878
theft-pocketpicking, t18310407-90, 0.282424548
theft-pocketpicking, t18380514-1168, 0.784184742
theft-pocketpicking, t18301028-208, 0.797341405
theft-pocketpicking, t18341016-85, 1.296811989
violenttheft-robbery, t18370102-317, 1.075548985
violenttheft-robbery, t18350921-2011, 1.105672712
violenttheft-robbery, t18310407-204, 1.521788666
violenttheft-robbery, t18370102-425, 1.840718222
violenttheft-robbery, t18330214-13, 2.150018805
</code></pre>
<p><strong>False positives</strong></p>
<pre><code>breakingpeace-assault, t18391021-2933
breakingpeace-wounding, t18350615-1577
breakingpeace-wounding, t18331017-159
breakingpeace-wounding, t18350615-1578
breakingpeace-wounding, t18330704-5
kill-manslaughter, t18350706-1682
kill-manslaughter, t18360919-2161
kill-manslaughter, t18380618-1461
kill-murder, t18330103-7
kill-murder, t18391021-2937
miscellaneous-pervertingjustice, t18340904-144
theft-pocketpicking, t18300114-128
theft-pocketpicking, t18310407-66
theft-pocketpicking, t18330905-92
theft-pocketpicking, t18370703-1639
theft-pocketpicking, t18301028-127
theft-pocketpicking, t18310106-87
theft-pocketpicking, t18331017-109
theft-pocketpicking, t18320216-108
theft-pocketpicking, t18331128-116
</code></pre>
<p>The first thing we notice is that many of the close relatives are in
fact from our target category — they are cases that our classifier has
narrowly missed. So saving these separately could compensate nicely for
an otherwise low recall number.</p>
<p>The second thing we notice is that more of the false positives seem to
have to do with violence, whereas more of the close relatives seem to
have to do with stealing; it seems our classifier has pegged the
violence aspect of robberies as more significant in distinguishing them
than the filching aspect.</p>
<p>The third thing we notice is that theft-pocketpicking is a very common
category among both the close relatives and the false positives. And
indeed, if we look at a sample trial from violenttheft-robbery and
another from among the close pocketpicking relatives, we notice that
there are definitely close similarities.</p>
<p>For example, trial t18310407-90, the closest close relative, involved a
certain Eliza Williams indicted for pocketpicking. Williams was accused
of stealing a watch and some other items from a certain Thomas Turk;
according to Turk and his friend, they had been pub-crawling, Eliza
Williams (whom they did not know from before) had tagged along with
them, and at one point in the evening had pocketed Turk&#39;s watch (Turk,
by this time, was quite tipsy). Williams was found guilty and sentenced
to be confined for one year.</p>
<p>Meanwhile, in trial t18300708-14, correctly classed as
violenttheft-robbery, a man called Edward Overton was accused of
feloniously assaulting a fellow by the name of John Quinlan. Quinlan
explained that he had been out with friends, and when he parted from
them he realized it was too late to get into the hotel where he worked
as a waiter and (apparently) also had lodgings. Having nowhere to go, he
decided to visit a few more public-houses. Along the way, he met
Overton, whom he did not know from before, and treated him to a few
drinks. But then, according to Quinlan, Overton attacked him as they
were walking from one pub to another, and stole his watch as well as
other possessions of his. According to Overton, however, Quinlan had
given him the watch as a guarantee that he would repay Overton if
Overton paid for his lodging for the night. Both men, it seems, were
thoroughly drunk by the end of the evening. Overton was found not
guilty.</p>
<p>Both trials, then, are stories of groups out drinking and losing their
possessions; what made the latter a trial for robbery rather than for
pocketpicking was simply Quinlan&#39;s accusation that Overton had &quot;struck
him down.&quot; For a historian interested in either robberies or
pocketpickings (or pub-crawling in 1830s London), both would probably be
equally interesting.</p>
<p>In fact, the misclassification patterns of the learner indicate that
even when data is richly annotated, such as in the case of the Old
Bailey, using a machine learner to extract documents may be useful for a
historian: in this case, it would help you extract trials from different
offense categories that share features of interest to you, regardless of
the offense label.</p>
<h3 id="tuning">Tuning</h3>
<p>The possibilities for tuning are practically endless.</p>
<p>For example, you might consider tweaking your data. For instance,
instead of giving your classifier all the words in the document, you
might present it with a reduced set.</p>
<p>One way of reducing the number of words is to collapse different words
together through stemming. So the verb forms &quot;killed,&quot; &quot;kills,&quot;
&quot;killing&quot; would all become &quot;kill&quot; (as would the plural noun &quot;kills&quot;). A
popular stemmer is the <a href="http://snowball.tartarus.org/">Snowball Stemmer</a>, and you could add that to the
tokenization step. (I ran a couple of tests with this, and while it made
the process much slower, it didn&#39;t much improve the results. But that
would probably depend a bit on the kind of data you have.)</p>
<p>Another way is to select the words you give to the classifier according
to some principle. One common solution is to pick only the words with a
high <strong>TF-IDF</strong> score. TF-IDF is short for &quot;term frequency - inverse
document frequency,&quot; and a high score means that the term occurs quite
frequently in the document under consideration but rarely in documents
in general. (You can also check out <a href="http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/">a more detailed explanation of
TF-IDF</a>, along with some Python code for calculating it.)</p>
<p>Other options include simply playing with the size of the priors: now,
the Naive Bayesian has a class prior as well as a feature prior of 0.5,
meaning that it pretends to have seen all classes and all words at least
one-half times. Doing test runs with different priors might get you
different results.</p>
<p>In addition to simply changing the general prior sizes, you might
consider having the classifier set a higher prior on the target
category than on the &quot;other&quot; category, in effect requiring less
evidence to include a trial in the target category. It might be worth
a try particularly since we noted above when examining the close
relatives (under Meanings of Misclassification) that many of them were
in fact members of our target category. Setting a larger prior on the
target class would probably catch those cases, boosting the recall. At
the same time, it probably would also lower the precision. (To change
the priors, you need to edit the <code>naivebayes.py</code> script.)</p>
<p>As you can see, there is quite a lot of fuzziness here: how you pick
the features, how you pick the priors, and how you weight various
priors all affect the results you get, and how to pick and weight is
not governed by hard logic but is rather a process of trial and error.
Still, like we noted noted in the section on the meaning of
classification error above, if your goal is to get some interesting
data to do historical analysis on, some fuzziness may not be such a
big problem.</p>
<p>Happy hunting!</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="naive-bayesian/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Supervised Classification: The Naive Bayesian Returns to the Old Bailey\",\"layout\":\"lesson\",\"date\":\"2014-12-17T00:00:00.000Z\",\"authors\":[\"Vilja Hulden\"],\"reviewers\":[\"Adam Crymble\"],\"editors\":[\"William J. Turkel\"],\"difficulty\":3,\"exclude_from_check\":[\"review-ticket\"],\"activity\":\"analyzing\",\"topics\":[\"distant-reading\"],\"abstract\":\"This lesson shows how to use machine learning to extract interesting documents out of a digital archive.\",\"redirect_from\":\"\u002Flessons\u002Fnaive-bayesian\",\"avatar_alt\":\"A man peers through a geometric tool\",\"doi\":\"10.46430\u002Fphen0038\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"introduction\\\"\u003EIntroduction\u003C\u002Fh2\u003E\\n\u003Cp\u003EA few years back, William Turkel wrote a series of blog posts called \u003Ca href=\\\"http:\u002F\u002Fdigitalhistoryhacks.blogspot.com\u002F2008\u002F05\u002Fnaive-bayesian-in-old-bailey-part-1.html\\\"\u003EA\\nNaive Bayesian in the Old Bailey\u003C\u002Fa\u003E, which showed how one could use\\nmachine learning to extract interesting documents out of a digital archive.\\nThis tutorial is a kind of an update on that blog essay, with roughly the\\nsame data but a slightly different version of the machine learner.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe idea is to show why machine learning methods are of interest to\\nhistorians, as well as to present a step-by-step implementation of\\na supervised machine learner. This learner is then applied to the \u003Ca href=\\\"http:\u002F\u002Fwww.oldbaileyonline.org\u002F\\\"\u003EOld Bailey digital\\narchive\u003C\u002Fa\u003E, which contains several centuries&#39; worth of transcripts of\\ntrials held at the Old Bailey in London. We will be using Python for the\\nimplementation.\u003C\u002Fp\u003E\\n\u003Cp\u003EOne obvious use of machine learning for a historian is document\\nselection. If we can get the computer to &quot;learn&quot; what kinds of documents\\nwe want to see, we can enlist its help in the always-time-consuming task\\nof finding relevant documents in a digital archive (or any other digital\\ncollection of documents). We&#39;ll still be the ones reading and\\ninterpreting the documents; the computer is just acting as a fetch dog\\nof sorts, running to the archive, nosing through documents, and bringing\\nus those that it thinks we&#39;ll find interesting.\u003C\u002Fp\u003E\\n\u003Cp\u003EWhat we will do in this tutorial, then, is to apply a machine learner\\ncalled Naive Bayesian to data from the Old Bailey digital archive. Our\\ngoals are to learn how a Naive Bayesian works and to evaluate how\\neffectively it classifies documents into different categories - in\\nthis case, trials into offense categories (theft, assault, etc.). This\\nwill help us determine how useful a machine learner might be to us as\\nhistorians: if it does well at this classification task, it might also\\ndo well at finding us documents that belong to a &quot;class&quot; we, given our\\nparticular research interests, want to see.\u003C\u002Fp\u003E\\n\u003Cp\u003EStep by step, we&#39;ll do the following:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003Elearn what machine learners do, and look more closely at a popular\\nlearner called Naive Bayesian.\u003C\u002Fli\u003E\\n\u003Cli\u003Edownload a set of trial records from the Old Bailey archive.\u003C\u002Fli\u003E\\n\u003Cli\u003Ewrite a script that saves the trials as text (removing the XML\\nmarkup) and does a couple of other useful things.\u003C\u002Fli\u003E\\n\u003Cli\u003Ewrite a couple of helper scripts to assist in testing the learners.\u003C\u002Fli\u003E\\n\u003Cli\u003Ewrite a script that tests the performance of the learner.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch3 id=\\\"files-you-will-need\\\"\u003EFiles you will need\u003C\u002Fh3\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ccode\u003Esave-trialtxts-by-category.py\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Etenfold-crossvalidation.py\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Ecount-offense-instances.py\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Epretestprocessing.py\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Etest-nb-learner.py\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Enaivebayes.py\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ccode\u003Eenglish-stopwords.txt\u003C\u002Fcode\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003E\u003Ca href=\\\"\u002Fassets\u002Fbaileycode.zip\\\"\u003EA zip file of the scripts\u003C\u002Fa\u003E is available. You can also download\\n\u003Ca href=\\\"https:\u002F\u002Fdoi.org\u002F10.5281\u002Fzenodo.13284\\\"\u003Eanother zip file\u003C\u002Fa\u003E containing the scripts, the data that we are using and the files that\\nresult from the scripts. (The second option is probably easiest if you want to follow along with the lesson,\\nsince it gives you everything you need in the correct folder structure.)\\nMore information about where to put the files is in the &quot;Preliminaries&quot; section\\nof the part where we actually begin to code.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003ENote: You will not need any Python modules that don&#39;t come with standard\\ninstallations, except for \u003Ca href=\\\"http:\u002F\u002Fwww.crummy.com\u002Fsoftware\u002FBeautifulSoup\u002F\\\"\u003EBeautifulSoup\u003C\u002Fa\u003E (used in the data creation step,\\nnot in the learner code itself).\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"the-old-bailey-digital-archive\\\"\u003EThe Old Bailey Digital Archive\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe \u003Ca href=\\\"http:\u002F\u002Fwww.oldbaileyonline.org\u002F\\\"\u003EOld Bailey digital archive\u003C\u002Fa\u003E\\ncontains 197,745 criminal trials held at the Old Bailey, aka the Central\\nCriminal Court in London. The trials were held between 1674 and 1913,\\nand since the archive provides the full transcript of each trial, many\\nof which include testimony by defendants, victims, and witnesses, it&#39;s a\\ngreat resource for all kinds of historians interested in the lives of\\nordinary people in London.\u003C\u002Fp\u003E\\n\u003Cp\u003EWhat makes the collection particularly useful for our purposes is that\\nthe text of each trial is richly annotated with such information as what\\ntype of an offense was involved (pocketpicking, assault, robbery,\\nconspiracy...), the name and gender of each witness, the verdict, etc.\\nWhat&#39;s more, this information has been added to the document in XML\\nmarkup, which allows us to extract it easily and reliably. That, in\\nturn, lets us train a machine learner to recognize the things we are\\ninterested in, and then test the learner&#39;s performance.\u003C\u002Fp\u003E\\n\u003Cp\u003EOf course, in the case of the Old Bailey archive, we might not need this\\ncomputer-assisted sorting all that badly, since the archive&#39;s curators,\\nmaking use of the XML markup, offer us a ready-made \u003Ca href=\\\"http:\u002F\u002Fwww.oldbaileyonline.org\u002Fforms\u002FformMain.jsp\\\"\u003Esearch interface\u003C\u002Fa\u003E that\\nlets us look for documents by offense type, verdict, punishment, etc. But\\nthat&#39;s exactly what makes the Old Bailey such a good resource for\\ntesting a machine learner: we can check how well the learner performs by\\nchecking its judgments against the human-annotated information in the\\nOld Bailey documents. That, in turn, helps us decide how (or whether) a\\nlearner could help us explore other digital document collections, most\\nof which are not as richly annotated.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"machine-learning\\\"\u003EMachine learning\u003C\u002Fh2\u003E\\n\u003Cp\u003EMachine learning can mean a lot of different things, but the most common\\ntasks are \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FStatistical_classification\\\"\u003Eclassification\u003C\u002Fa\u003E and \u003Ca href=\\\"http:\u002F\u002Fhome.deib.polimi.it\u002Fmatteucc\u002FClustering\u002Ftutorial_html\u002F\\\"\u003Eclustering\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EClassification is performed by supervised learners — &quot;supervised&quot;\\nmeaning that a human assistant helps them learn, and only then sends\\nthem out to classify by themselves. The basic training procedure is to\\ngive the learner labeled data: that is, we give it a stack of things\\n(documents, for example) where each of those things is labeled as\\nbelonging to a group. This is called training data. The learner then\\nlooks at each item in the training data, looks at its label, and\\nlearns what distinguishes the groups from each other. To see how well\\nthe learner learned, we then test it by giving it data that is similar\\nto the training data but that the learner hasn&#39;t seen before and that\\nis not labeled. This is called (you guessed it!) test data. How well\\nthe learner performs on classifying this previously-unseen data is a\\nmeasure of how well it has learned.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe classic case of a supervised classifier is a program that separates\\njunk email (spam) from regular email (ham). Such a program is &quot;trained&quot;\\nby giving it a lot of spam and ham to look at, along with the\\ninformation of which is which. It then builds a statistical model of\\nwhat a spam message looks like versus what a regular email message looks\\nlike. So it learns that a message is more likely to be spam if it\\ncontains sexual terms, or words like &quot;offer&quot; and &quot;deal&quot;, or, as things\\nturn out, \u003Ca href=\\\"http:\u002F\u002Fwww.paulgraham.com\u002Fspam.html\\\"\u003E&quot;ff0000,&quot; the HTML code for red\u003C\u002Fa\u003E. It can then apply that\\nstatistical model to incoming messages and discard the ones it\\nidentifies as spam.\u003C\u002Fp\u003E\\n\u003Cp\u003EClustering is usually a task for unsupervised learners. An unsupervised\\nlearner doesn&#39;t get any tips on how the data &quot;ought&quot; to be sorted, but\\nrather is expected to discover patterns in the data automatically,\\ngrouping the data by the patterns it has discovered. Unlike in\\nsupervised classification, in unsupervised clustering we don&#39;t tell the\\nlearner what the &quot;right&quot; groups are, or give it any hints on what items\\nin the data set should go together. Rather, we give it data with a bunch\\nof features, and (often, but not always) we tell it how many groups we\\nwant it to create. The features could be anything: in document\\nclustering, they are normally words. But clustering isn&#39;t limited to\\ngrouping documents: it could also be used in, say, trying to improve\\ndiagnoses by clustering patient records. In that task, the features\\nwould be various attributes of the patient (age, weight, blood pressure,\\npresence and quality of various symptoms etc.) and the clustering\\nalgorithm would attempt to create groups that share as many features as\\nclosely as possible.\u003C\u002Fp\u003E\\n\u003Cp\u003EA side note: Some of you may have come to think of an objection to this supervised\u002Funsupervised distinction: namely, that the clustering method is not entirely &quot;unsupervised&quot; either. After all, we tell it what features it should look at, whether it is words (rather than sentences, or two-word sequences, or something else) in a document, or a list of numeric values in a patient record. The learner never encounters the data entirely unprepared. Quite true. But no matter - the distinction between unsupervised and supervised is useful nevertheless, in that in one we tell the learner what the right answer is, and in the other it comes to us with some pattern it has figured out without an answer key. Each is useful for different kind of tasks, or sometimes for different approaches to the same task.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this tutorial, we are dealing with a supervised learner that we train\\nto perform document classification. We give our learner a set of\\ndocuments along with their correct classes, and then test it on a set of\\ndocuments they haven&#39;t seen, with the hope that it will succeed in\\nguessing the document&#39;s correct classification.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"a-naive-bayesian-learner\\\"\u003EA Naive Bayesian learner\u003C\u002Fh3\u003E\\n\u003Cp\u003EA Naive Bayesian is a supervised learner: we give it things marked with\\ngroup labels, and its job is basically to learn the probability that a\\nthing that looks a particular way belongs in a particular group.\u003C\u002Fp\u003E\\n\u003Cp\u003EBut why &quot;naive&quot;? And what &quot;Bayesian&quot;?\u003C\u002Fp\u003E\\n\u003Cp\u003E&quot;Naive&quot; simply means that the learner makes the assumption that all the\\n&quot;features&quot; that make up a document are independent of each other. In our\\ncase, the features are words, and so the learner assumes that the\\noccurrence of a particular word is completely independent of the\\noccurrence of another word. This, of course, is often not true, which is\\nwhy we call it &quot;naive.&quot; For example, when we put &quot;new&quot; and &quot;york&quot;\\ntogether to form &quot;New York,&quot; the result has a very different meaning\\nthan the &quot;new&quot; and &quot;york&quot; in &quot;New clothes for the Prince of York.&quot; If we\\nwere to distinguish &quot;New York&quot; from &quot;New&quot; and &quot;York&quot; occurring\\nseparately, we might find that each tends to occur in very different\\ntypes of documents, and thus not identifying the expression &quot;New York&quot;\\nmight throw our classifier off course.\u003C\u002Fp\u003E\\n\u003Cp\u003EDespite their simplistic assumption that the occurrence of any\\nparticular feature is independent of the occurrence of other features,\\nNaive Bayesian classifiers do a good enough job to be very useful in\\nmany contexts (much of the real-world junk mail detection is performed\\nby Naive Bayesian classifiers, for example). Meanwhile, the assumption\\nof independence means that processing documents is much less\\ncomputationally intensive, so a Naive Bayesian classifier can handle far\\nmore documents in a much shorter time than many other, more complex\\nmethods. That in itself is useful. For example, it wouldn’t take too\\nlong retrain a Naive Bayesian learner if we accumulated more data. Or we\\ncould give it a bigger set of data to begin with; a pile of data that a\\nNaive Bayesian could burrow through in a day might take a more complex\\nmethod weeks or even months to process. Especially when it comes to\\nclassification, more data is often as significant as a better method —\\nas Bob Mercer of IBM famously quipped in 1985, “there is no data like\\nmore data.”\u003C\u002Fp\u003E\\n\u003Cp\u003EAs for the &quot;Bayesian&quot; part, that refers to the 18th-century English\\nminister, statistician, and philosopher Thomas Bayes. When you google\\nfor &quot;Naive Bayesian,&quot; you will turn up a lot of references to &quot;Bayes&#39;\\ntheorem&quot; or &quot;Bayes&#39; rule,&quot; which is a formula for applying conditional\\nprobabilities (the probability of some thing X, given some other thing\\nY).\u003C\u002Fp\u003E\\n\u003Cp\u003EBayes&#39; theorem is related to Naive Bayesian classifiers, in that we can\\nformulate the classification question as &quot;what is the probability of\\ndocument X, given class Y?&quot; However, unless you&#39;ve done enough math and\\nprobability to be comfortable with that kind of thinking, it may not\\nprovide the easiest avenue to grasping how a Naive Bayesian classifier\\nworks. Instead, let&#39;s look at the classifier in a more procedural\\nmanner. (Meanwhile, if you prefer, you can check out \u003Ca href=\\\"http:\u002F\u002Fwww.yudkowsky.net\u002Frational\u002Fbayes\\\"\u003Ean explanation\\nof Bayes&#39; rule and conditional probabilities\u003C\u002Fa\u003E that does a very nice\\njob and is also a good read.)\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"understanding-naive-bayesian-classification-using-a-generative-story\\\"\u003EUnderstanding Naive Bayesian classification using a generative story\u003C\u002Fh4\u003E\\n\u003Cp\u003ETo understand Naive Bayesian classification, we will start by telling a\\nstory about how documents come into being. Telling such a story — called\\na &quot;generative story&quot; in the business — often simplifies the\\nprobabilistic analysis and helps us understand the assumptions we&#39;re\\nmaking. Telling the story takes a while, so bear with me. There is a\\npayoff at the end: the story directly informs us how to build a\\nclassifier under the assumptions that the particular generative story\\nmakes.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe fundamental assumption we will make in our generative story is that\\ndocuments come into being not as a result of intellectual cogitation but\\nas a result of a process whereby words are picked at random out of a bag\\nand then put into a document (known as a bag-of-words model).\u003C\u002Fp\u003E\\n\u003Cp\u003ESo we pretend that historical works, for example, are written in\\nsomething like the following manner. Each historian has his or her own\\nbag of words with a vocabulary specific to that bag. So when Ann the\\nHistorian writes a book, what she does is this:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EShe goes to the bag that is her store of words.\u003C\u002Fli\u003E\\n\u003Cli\u003EShe puts her hand in and pulls out a piece of paper.\u003C\u002Fli\u003E\\n\u003Cli\u003EShe reads the word on the piece of paper, writes it down in her\\nbook, and puts the paper back in the bag.\u003C\u002Fli\u003E\\n\u003Cli\u003EThen she again puts her hand in the bag and pulls out a piece of\\npaper.\u003C\u002Fli\u003E\\n\u003Cli\u003EShe writes down that word in the book, and puts the piece of paper\\nback in the bag.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EAnn the Historian keeps going until she decides her book (or article, or\\nblog post, or whatever) is finished. The next time she wants to write\\nsomething, she goes back to her bag of words and does the same thing. If\\nher friend John the Historian were to write a book, he would go to his\\nown bag, which has a different set of words, and then he would follow\\nthe same procedure of taking out a word, writing it down, putting it\\nback in. It&#39;s just one damn word after another.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;naive-bayesian-1.png&quot; caption=&quot;Bags of Words&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003E(If this procedure sounds familiar, that may be because it sounds a bit\\nlike the generative story told in explaining how \u003Ca href=\\\"\u002Flessons\u002Ftopic-modeling-and-mallet\\\"\u003Etopic modeling\u003C\u002Fa\u003E works.\\nHowever, the story in topic modeling is a bit different in that,\\nfor instance, each document contains words from more than one class.\\nAlso, you should note that topic modeling is unsupervised — you don&#39;t\\ntell the modeler what the &quot;right&quot; topics are, it comes up with them all\\nby itself.)\u003C\u002Fem\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003ESo let&#39;s say you are a curator of a library of historical works, and one\\nday you discover a huge forgotten trunk in the basement of the library.\\nIt turns out that the trunk contains dozens and dozens of typed book\\nmanuscripts. After some digging, you find a document that explains that\\nthese are transcripts of unpublished book drafts by three historians:\\nEdward Gibbon, Carl Becker, and Mercy Otis Warren.\u003C\u002Fp\u003E\\n\u003Cp\u003EWhat a find! But unfortunately, as you begin sorting through the drafts,\\nyou realize that they are not marked with the author&#39;s name. What can you\\ndo? How can you classify them correctly?\u003C\u002Fp\u003E\\n\u003Cp\u003EWell, you do have other writings by these authors. And if historians\\nwrite their documents in the manner described above — if each historian\\nhas his or her own bag of words with a particular vocabulary and a\\nparticular distribution of words — then we can figure out who wrote each\\ndocument by looking at the words it contains and comparing the\\ndistribution of those words to the distribution of words in documents we\\n\u003Cem\u003Eknow\u003C\u002Fem\u003E were written by Gibbon, Becker, and Warren, respectively.\u003C\u002Fp\u003E\\n\u003Cp\u003ESo you go to your library stacks and get out all the books by Gibbon,\\nBecker, and Warren. Then you start counting. You start with Edward\\nGibbon&#39;s \u003Cem\u003Eoeuvre\u003C\u002Fem\u003E. For each word in a work by Gibbon, you add the word\\nto a list marked &quot;Gibbon.&quot; If the word is already in the list, you add\\nto its count. Then you do the same with the works of Mercy Otis Warren\\nand Carl Becker. Finally, for each author, you add up the total number\\nof words you&#39;ve seen. You also add up the total number of monographs you\\nhave examined so you&#39;ll have a metric for how much work each author has\\npublished.\u003C\u002Fp\u003E\\n\u003Cp\u003ESo what you end up with is something like this:\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003EEdward Gibbon (5)\u003C\u002Fth\u003E\\n\u003Cth\u003ECarl Becker (18)\u003C\u002Fth\u003E\\n\u003Cth\u003EMercy Otis Warren (2)\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003Eempire, 985\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eeveryman, 756\u003C\u002Ftd\u003E\\n\u003Ctd\u003Erevolution, 989\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Erome, 897\u003C\u002Ftd\u003E\\n\u003Ctd\u003Erevolution, 699\u003C\u002Ftd\u003E\\n\u003Ctd\u003Econstitution, 920\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Efall, 887\u003C\u002Ftd\u003E\\n\u003Ctd\u003Ephilosopher, 613\u003C\u002Ftd\u003E\\n\u003Ctd\u003Eprinciples, 899\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E…\u003C\u002Ftd\u003E\\n\u003Ctd\u003E…\u003C\u002Ftd\u003E\\n\u003Ctd\u003E…\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003E(total), 352,003\u003C\u002Ftd\u003E\\n\u003Ctd\u003E(total), 745,532\u003C\u002Ftd\u003E\\n\u003Ctd\u003E(total), 300,487\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EWhat you have done, in essence, is to reconstruct each historian&#39;s &quot;bag\\nof words&quot; — now you know (at least approximately) what words each\\nhistorian uses and in what proportions. Armed with this representation\\nof the word distributions in the works of Gibbons, Becker, and Warren,\\nyou&#39;re ready to tackle the task of figuring out who wrote which\\nmanuscripts.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou&#39;re going to work manuscript by manuscript and author by author,\\nfirst pretending that the manuscript you&#39;re currently considering was\\nwritten by Gibbons, then that it was written by Becker, and so on. For\\neach author, you calculate how likely it is that the manuscript really\\nwas written by that author.\u003C\u002Fp\u003E\\n\u003Cp\u003ESo with your first manuscript in hand, you start by assuming that the\\nmanuscript was written by Gibbons. First you figure out the overall\\nprobability of any monograph being written by Gibbons rather than either\\nof the two others — that is, of the Gibbons bag rather than the Becker\\nbag or the Warren bag being used to produce a monograph. You do this by\\ntaking the number of books written by Gibbons and dividing it by the\\ntotal number of books written by all these authors. That comes out to\\n5\u002F25, or 0.2 (20 percent).\u003C\u002Fp\u003E\\n\u003Cp\u003EThen, you start looking at the words in the manuscript. Let&#39;s say the\\nfirst word is &quot;fall.&quot; You check how often that word occurred in Gibbons&#39;\\npublished \u003Cem\u003Eoeuvre\u003C\u002Fem\u003E, and you find that the answer is 887. Then you check\\nhow many words, overall, there were in Gibbons&#39; total works, and you\\nnote that the answer is 352,003. You divide 887 by 352,003 to get the\\nproportional frequency (call it \u003Cem\u003Ep\u003C\u002Fem\u003E) of &quot;fall&quot; in Gibbons&#39; work\\n(0.0025). For the next word, you do the same procedure, and then\\nmultiply the probabilities together (you multiply since each action —\\npicking an author, or picking a word — represents an independent\\nchoice). In the end you end with a tally like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Ep_bag * p_word_1 * p_word_2 * ... * p_word_n\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENote that including the probability of picking the bag (\u003Cem\u003Ep_bag\u003C\u002Fem\u003E) is an\\nimportant step: if you only go by the words in the manuscript and ignore\\nhow many manuscripts (or rather, published works) each author has\\nwritten, you can easily go wrong. If Becker has written ten times the\\nnumber of books that Warren has, it should reasonably require much\\nfirmer evidence in the form of an ample presence of &quot;Warrenesque&quot; words\\nto assume that a manuscript was written by Warren than that it was\\nwritten by Becker. &quot;Extraordinary claims require extraordinary\\nevidence,&quot; as Carl Sagan once said.\u003C\u002Fp\u003E\\n\u003Cp\u003EOK, so now you have a total probability of the manuscript having been\\nwritten by Gibbons. Next, you repeat the whole procedure with the\\nassumption that maybe it was instead written by Becker (that is, that it\\ncame out of the bag of words that Becker used when writing). That done,\\nyou move on to considering the probability that the author was Warren\\n(and if you had more authors, you&#39;d keep going until you had covered\\neach of them).\u003C\u002Fp\u003E\\n\u003Cp\u003EWhen you&#39;re done, you have three total probabilities — one probability\\nper author. Then you just pick out the largest one, and, as they say,\\nBob&#39;s your uncle! That&#39;s the author who most probably wrote this\\nmanuscript.\u003C\u002Fp\u003E\\n\u003Cp\u003E(Minor technical note: when calculating\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Ep_bag * p_word1 * ... * p_word_n\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003Ein a software implementation we actually work with the\\n\u003Ca href=\\\"http:\u002F\u002Fbetterexplained.com\u002Farticles\u002Fusing-logs-in-the-real-world\u002F\\\"\u003Elogarithms\u003C\u002Fa\u003E of the probabilities since the numbers\\neasily become very small. When doing this, we actually calculate\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Elog(p_bag) + log(p_word1) + ... + log(p_word_n)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThat is, our multiplications turn into additions in line with\\nthe rules of logarithms. But it all works out right: the class\\nwith the highest number at the end wins.)\u003C\u002Fp\u003E\\n\u003Cp\u003EBut wait! What if a manuscript contains a word that we&#39;ve never seen\\nGibbons use before, but also lots of words he used all the time? Won&#39;t\\nthat throw off our calculations?\u003C\u002Fp\u003E\\n\u003Cp\u003EIndeed. We shouldn&#39;t let outliers throw us off the scent. So we do\\nsomething very &quot;Bayesian&quot;: we put a &quot;prior&quot; on each word and each class\\n— we pretend we&#39;ve seen all imaginable words at least (say) once in each\\nbag, and that each bag has produced at least (say) one document. Then we\\nadd those fake pretend counts — called \u003Ca href=\\\"http:\u002F\u002Fsupport.sas.com\u002Fdocumentation\u002Fcdl\u002Fen\u002Fstatug\u002F63033\u002FHTML\u002Fdefault\u002Fviewer.htm#statug_introbayes_sect004.htm\\\"\u003Epriors\u003C\u002Fa\u003E, or pseudocounts — to\\nour real counts. Now, no word or bag gets a count of zero.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn fact, we can play around with the priors as much as we like: they&#39;re\\nsimply a way of modeling our &quot;prior belief&quot; in the probability of one\\nthing over another. They could model our assumptions about a particular\\nauthor being more likely than others, or a particular word being more\\nlikely to have come from the bag of a specific author, and so on. Such\\nbeliefs are &quot;prior&quot; in the sense that we hold the belief before we&#39;ve\\nseen the evidence we are considering in the actual calculation we are\\nmaking. So above, for example, we could add a little bit to Mercy Otis\\nWarren&#39;s \u003Cem\u003Ep_bag\u003C\u002Fem\u003E number if we thought it likely that as a woman, she\\nmight well have had a harder time getting published, and so there might\\nreasonably be more manuscript material from her than one might infer\\nfrom a count of her published monographs.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn some cases, priors can make a Naive Bayesian classifier much more\\nusable. Often when we&#39;re classifying, after all, we&#39;re not after some\\nabstract &quot;truth&quot; — rather, we simply want a useful categorization. In\\nsome cases, it&#39;s much more desirable to be mistaken one way than\\nanother, and we can model that with proper class priors. The classic\\nexample is, again, sorting email into junk mail and regular mail piles.\\nObviously, you really don&#39;t want legitimate messages to be deleted as\\nspam; that could do much more damage than letting a few junk messages\\nslip through. So you set a big prior on the &quot;legitimate&quot; class that\\ncauses your classifier to only throw out a message as junk when faced\\nwith some hefty evidence. By the same token, if you&#39;re sorting the\\nresults of a medical test into &quot;positive&quot; and &quot;negative&quot; piles, you may\\nwant to weight the positive more heavily: you can always do a second\\ntest, but if you send the patient home telling them they&#39;re healthy when\\nthey&#39;re not, that might not turn out so well.\u003C\u002Fp\u003E\\n\u003Cp\u003ESo there you have it, step by step. You have applied a Naive Bayesian to\\nthe unattributed manuscripts, and you now have three neat piles. Of\\ncourse, you should keep in mind that Naive Bayesian classifiers are not\\nperfect, so you may want to do some further research before entering the\\nnewfound materials into the library catalog as works by Gibbons, Becker,\\nand Warren, respectively.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"ok-so-lets-code-already\\\"\u003EOK, so let&#39;s code already!\u003C\u002Fh2\u003E\\n\u003Cp\u003ESo, our aim is to apply a Naive Bayesian learner to data from the Old\\nBailey. First we get the data; then we clean it up and write some\\nroutines to extract information from it; then we write the code that\\ntrains and tests the learner.\u003C\u002Fp\u003E\\n\u003Cp\u003EBefore we get into the nitty-gritty of downloading the files and\\nexamining the training\u002Ftesting script, let&#39;s just summarize what our aim\\nis and what the basic procedure looks like.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe want to have our Naive Bayesian read in trial records from the Old\\nBailey and do with them the same thing as we did above in the examples\\nabout the works of Gibbons, Becker, and Warren. In that example, we used\\nthe published works of these authors to reconstruct each historian&#39;s bag\\nof words, and then used that knowledge to decide which historian had\\nwritten which unattributed manuscripts. In classifying the Old Bailey\\ntrials, we will give the learner a set of trials labeled with the\\noffense for which the defendant was indicted so it can figure out the\\n&quot;bag of words&quot; that is associated with that offense. Then the learner\\nwill use that knowledge to classify another set of trials where we have\\nnot given it any information about the offense involved. The goal is to\\nsee how well the learner can do this: how often does it label an\\nunmarked trial with the right offense?\u003C\u002Fp\u003E\\n\u003Cp\u003EThe procedure used in the scripts we employ to train the learner is no\\nmore complicated than the one in the historians-and-manuscripts example.\\nBasically, each trial is represented as a list of words, like so:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Emichael, carney, was, indicted, for, stealing, on, the, 22nd, of, december, 26lb, weight, of, nails, value, 7s, 18, dozen, of, screws, ...\\n... , the, prisoners, came, to, my, shop, on, the, night, in, question, and, brought, in, some, ragged, pieces, of, beef, ...\\n..., i, had, left, my, door, open, and, when, i, returned, i, missed, all, this, property, i, found, it, at, the, pawnbroker, ...\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWhen we train the learner, we give it a series of such word lists, along\\nwith their correct bag labels (correct offenses). The learner then\\ncreates word lists for each bag (offense), so that it ends up with a set\\nof counts similar to the counts we created for Gibbons, Becker, and\\nWarren, one count for each offense type (theft, deception, etc.)\u003C\u002Fp\u003E\\n\u003Cp\u003EWhen we test the learner, we feed it the same sort of word lists\\nrepresenting other trials. But this time we don&#39;t give it the\\ninformation about what offense was involved. Instead, the learner does\\nwhat we did above: when it gets a list of words, it compares that list\\nto the word counts for each offense type, calculating which offense type\\nhas a bag of words most similar to this list of words. It works offense\\nby offense, just like we worked author by author. So first it assumes\\nthat the trial involved, say, the offense &quot;theft&quot;. It looks at the first\\nword in the trial&#39;s word list, checks how often that word occurred in\\nthe &quot;theft&quot; bag, performs its probability calculations, moves on to the\\nnext word, and so on. Then it checks the trial&#39;s word list against the\\nnext category, and the next, until it has gone through each offense.\\nFinally it tallies up the probabilities and labels the trial with the\\noffense category that has the highest probability.\u003C\u002Fp\u003E\\n\u003Cp\u003EFinally, the testing script evaluates the performance of the learner and\\nlets us know how good it was at guessing the offense associated with\\neach trial.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"preliminaries\\\"\u003EPreliminaries\u003C\u002Fh3\u003E\\n\u003Cp\u003EMany of the tools we are using to deal with the preliminaries have been\\ndiscussed at Programming Historian before. You may find it helpful to\\ncheck out (or revisit) the following tutorials:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EMilligan &amp; Baker, \u003Ca href=\\\"\u002Flessons\u002Fintro-to-bash\\\"\u003EIntroduction to the Bash Command Line\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EMilligan, \u003Ca href=\\\"\u002Flessons\u002Fautomated-downloading-with-wget\\\"\u003EAutomated Downloading with wget\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EKnox, \u003Ca href=\\\"\u002Flessons\u002Funderstanding-regular-expressions\\\"\u003EUnderstanding Regular Expressions\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003EWieringa, \u003Ca href=\\\"\u002Flessons\u002Fintro-to-beautiful-soup\\\"\u003EIntro to Beautiful Soup\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EA few words about the file structure the scripts assume\u002Fcreate:\u003C\u002Fp\u003E\\n\u003Cp\u003EI have a &quot;top-level&quot; directory, which I&#39;m calling \u003Cem\u003Ebailey\u003C\u002Fem\u003E (you\\ncould call it something else, it&#39;s not referenced in the code). Under\\nthat I have two directories: \u003Cem\u003Ebaileycode\u003C\u002Fem\u003E and \u003Cem\u003Ebaileyfiles\u003C\u002Fem\u003E.\\nThe first contains all the scripts; the second contains the files\\nthat are either downloaded or created by the scripts. That in turn\\nhas subdirectories; all except one (for the downloaded XML\\nfiles — see below) are created by the scripts.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you downloaded the complete zip package with all the files and\\nscripts, you automatically get the right structure; just unpack it in\\nits own directory. The only files that are omitted from that are the zip\\nfiles of trials downloaded below (if you got the complete package, you\\nalready have the unpacked contents of those files, and the zips would\\njust take up unnecessary space).\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you only downloaded the scripts, you should do the following:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003ECreate a directory and name it something sensible (say, \u003Cem\u003Ebailey\u003C\u002Fem\u003E).\u003C\u002Fli\u003E\\n\u003Cli\u003EIn that directory, create another directory called \u003Cem\u003Ebaileycode\u003C\u002Fem\u003E and\\nunpack the contents of the script zip file into that directory\\n(make sure you don&#39;t end up with two \u003Cem\u003Ebaileycode\u003C\u002Fem\u003E directories inside\\none another).\u003C\u002Fli\u003E\\n\u003Cli\u003EIn the same directory (\u003Cem\u003Ebailey\u003C\u002Fem\u003E), create another directory called\\n\u003Cem\u003Ebaileyfiles\u003C\u002Fem\u003E.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EOn my Mac, the structure looks like this:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;naive-bayesian-2.png&quot; caption=&quot;Bailey Folders&quot; %}\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"downloading-trials\\\"\u003EDownloading trials\u003C\u002Fh4\u003E\\n\u003Cp\u003EThe Old Bailey lets you download trials in zip files of 10 trials each,\\nso that&#39;s what we&#39;ll do. This is how we do it: we first look at how the\\nOld Bailey system requests files, and then we write a script that\\ncreates a file with a bunch of those requests. Then we feed that file to\\nwget, so we don&#39;t have to sit by our computers all day downloading each\\nset of 10 trials that we want.\u003C\u002Fp\u003E\\n\u003Cp\u003EAs explained on the Old Bailey \u003Ca href=\\\"http:\u002F\u002Fwww.oldbaileyonline.org\u002Fstatic\u002FDocAPI.jsp\\\"\u003Edocumentation for developers\u003C\u002Fa\u003E page, this\\nis what the http request for a set of trials looks like:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Ehttp:\u002F\u002Fwww.oldbaileyonline.org\u002Fobapi\u002Fob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=211&amp;return=zip\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAs you see, we can request all trials that took place between two\\nspecified dates (\u003Cem\u003Efromdate\u003C\u002Fem\u003E and \u003Cem\u003Etodate\u003C\u002Fem\u003E). The \u003Cem\u003Ecount\u003C\u002Fem\u003E specifies how\\nmany trials we want, and the \u003Cem\u003Estart\u003C\u002Fem\u003E variable says where in the results\\nto start (in the above, we start with result number 211 and get the ten\\nfollowing trials). Ten seems to be the highest number allowed for\\n\u003Cem\u003Ecount\u003C\u002Fem\u003E, so we need to work around that.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe get around the restriction for how many trials can be in a zip file\\nwith a little script that builds as many of the above type of requests\\nas we need to get all trials from the 1830s. We can find out how many\\ntrials that is by going to the \u003Ca href=\\\"http:\u002F\u002Fwww.oldbaileyonline.org\u002Fforms\u002FformMain.jsp\\\"\u003EOld Bailey search page\u003C\u002Fa\u003E and entering\\nJanuary 1830 as the start date, December 1839 as the end date, and\\nchoosing &quot;Old Bailey Proceedings &gt; trial accounts&quot; in the &quot;Search In&quot;\\nfield. Turns out there were 22,711 trials in the 1830s.\u003C\u002Fp\u003E\\n\u003Cp\u003EHere&#39;s the whole script (\u003Ccode\u003Ewgetxmls.py\u003C\u002Fcode\u003E) that creates the list of http requests we need:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E    mainoutdirname = &#39;..\u002Fbaileyfiles\u002F&#39;\\n    wgets = &#39;&#39;\\n    for x in range(0,22711,10):\\n        getline = &#39;http:\u002F\u002Fwww.oldbaileyonline.org\u002Fobapi\u002Fob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=&#39; + str(x+1) + &#39;&amp;return=zip\\\\n&#39;\\n        wgets += getline\\n    filename = mainoutdirname + &#39;wget1830s.txt&#39;\\n    with open(filename,&#39;w&#39;) as f:\\n        f.write(wgets)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAs you see, we accept the limitation of 10 trials at a time, but\\nmanipulate the start point until we have covered all the trials from the\\n1830s.\u003C\u002Fp\u003E\\n\u003Cp\u003EAssuming you&#39;re in the \u003Cem\u003Ebaileycode\u003C\u002Fem\u003E directory, you can run the script\\nfrom the command line like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Epython wgetxmls.py\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWhat that gets us is a file that looks like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Ehttp:\u002F\u002Fwww.oldbaileyonline.org\u002Fobapi\u002Fob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=1&amp;return=zip\\nhttp:\u002F\u002Fwww.oldbaileyonline.org\u002Fobapi\u002Fob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=11&amp;return=zip\\nhttp:\u002F\u002Fwww.oldbaileyonline.org\u002Fobapi\u002Fob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=21&amp;return=zip\\n...\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis file is saved in the \u003Cem\u003Ebaileyfiles\u003C\u002Fem\u003E directory; it is called \u003Ccode\u003Ewget1830s.txt\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo download the trials, create a new directory under \u003Cem\u003Ebaileyfiles\u003C\u002Fem\u003E;\\ncall it \u003Cem\u003Etrialzips\u003C\u002Fem\u003E. Then go into that directory and call \u003Cem\u003Ewget\u003C\u002Fem\u003E with the\\nfile we just created. So, assuming you are still in the \u003Cem\u003Ebaileycode\u003C\u002Fem\u003E directory,\\nyou would write the following commands on the command line:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Ecd ..\u002Fbaileyfiles\\nmkdir trialzips\\ncd trialzips\\nwget -w 2 -i ..\u002Fwget1830s.txt\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe &quot;-w 2&quot; is just to be polite and not overload the server; it tells\\n\u003Cem\u003Ewget\u003C\u002Fem\u003E to wait 2 seconds between each request. The &quot;-i&quot; flag tells \u003Cem\u003Ewget\u003C\u002Fem\u003E\\nthat it should request the URLs found in \u003Ccode\u003Ewget1830s.txt\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EWhat \u003Cem\u003Ewget\u003C\u002Fem\u003E returns is a lot of zip files that have unwieldy names and no\\nextension. You should rename these so that the extension is &quot;.zip&quot;.\\nThen, in the directory \u003Cem\u003Ebaileyfiles\u003C\u002Fem\u003E, create a subdirectory called\\n\u003Cem\u003E1830s-trialxmls\u003C\u002Fem\u003E and then unpack the zips into that so that it\\ncontains 22,170 XML files that each look like \u003Ccode\u003Et18391216-388.xml\u003C\u002Fcode\u003E. Assuming\\nyou are still in the \u003Cem\u003Etrialzips\u003C\u002Fem\u003E directory, you would write:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Efor f in * ; do mv $f $f.zip; done;\\nmkdir ..\u002F1830s-trialxmls\\nunzip &quot;*.zip&quot; -d ..\u002F1830s-trialxmls\u002F\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf you open one of the trial XMLs in a browser, you can see that it\\ncontains all kinds of useful information: name and gender of defendant,\\nname and gender of witnesses, type of offense, and so on. Here&#39;s a\\nsnippet from one trial:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-xml\\\"\u003E&lt;persname id=&quot;t18300114-2-defend110&quot; type=&quot;defendantName&quot;&gt;\\nTHOMAS TAYLOR\\n    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;surname&quot; value=&quot;TAYLOR&quot;&gt;\\n    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;given&quot; value=&quot;THOMAS&quot;&gt;\\n    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;gender&quot; value=&quot;male&quot;&gt;\\n    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;age&quot; value=&quot;25&quot;&gt;\\n&lt;\u002Finterp&gt;&lt;\u002Finterp&gt;&lt;\u002Finterp&gt;&lt;\u002Finterp&gt;&lt;\u002Fpersname&gt;\\nwas indicted for\\n    &lt;rs id=&quot;t18300114-2-off7&quot; type=&quot;offenceDescription&quot;&gt;\\n        &lt;interp inst=&quot;t18300114-2-off7&quot; type=&quot;offenceCategory&quot; value=&quot;violentTheft&quot;&gt;\\n        &lt;interp inst=&quot;t18300114-2-off7&quot; type=&quot;offenceSubcategory&quot; value=&quot;robbery&quot;&gt;\\n            feloniously assaulting\\n        &lt;persname id=&quot;t18300114-2-victim112&quot; type=&quot;victimName&quot;&gt;\\n            David Grant\\n                  &lt;interp inst=&quot;t18300114-2-victim112&quot; type=&quot;surname&quot; value=&quot;Grant&quot;&gt;\\n            &lt;interp inst=&quot;t18300114-2-victim112&quot; type=&quot;given&quot; value=&quot;David&quot;&gt;\\n            &lt;interp inst=&quot;t18300114-2-victim112&quot; type=&quot;gender&quot; value=&quot;male&quot;&gt;\\n            &lt;join result=&quot;offenceVictim&quot; targorder=&quot;Y&quot; targets=&quot;t18300114-2-off7 t18300114-2-victim112&quot;&gt;\\n        &lt;\u002Fjoin&gt;&lt;\u002Finterp&gt;&lt;\u002Finterp&gt;&lt;\u002Finterp&gt;&lt;\u002Fpersname&gt;\\n&lt;\u002Finterp&gt;&lt;\u002Finterp&gt;&lt;\u002Frs&gt;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe structured information in the XML lets us reliably extract the\\n&quot;classes&quot; we want to sort our documents into. We are going to classify\\nthe trials by offense category (and subcategory), so that&#39;s the\\ninformation we&#39;re going to extract before converting the XML into a text\\nfile that we can then feed to our learner.\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"saving-the-trials-into-text-files\\\"\u003ESaving the trials into text files\u003C\u002Fh4\u003E\\n\u003Cp\u003ENow that we have the XML files, we can start extracting information and\\nplain text from them to feed to our learner. We want to sort the trials\\ninto text files, so that each text file contains all the trials in a\\nparticular offense category (theft-simplelarceny, breakingpeace-riot,\\netc.).\u003C\u002Fp\u003E\\n\u003Cp\u003EWe also want to create a text file that contains all the trial IDs\\n(marked in the XML), so we can use that to easily create\\ncross-validation samples. The reasons for doing this are discussed below\\nin the section &quot;Creating the cross-validation samples&quot;.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe script that does these things is called \u003Ccode\u003Esave-txttrials-by-category.py\u003C\u002Fcode\u003E\\nand it&#39;s pretty extensively commented, so I&#39;ll just note a few things here.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EWe strip the trial text of all punctuation, including quote marks\\nand parentheses, and we equalize all spaces (newlines, tabs,\\nmultiple spaces) into a single space. This helps us simplify the\\ncoding of the training process (and, incidentally, keep the code\\nthat trains the learner general enough that as long as you have text\\nfiles saved in the same format as we use here, you should be able to\\napply it more or less directly to your data).\u003C\u002Fli\u003E\\n\u003Cli\u003EThat of course makes the text hard to read for a human. Therefore,\\nwe also save the text of each trial into a file named after the\\ntrial id, so that we can easily examine a particular trial if we\\nwant to (which we will).\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003EThe script creates the following directories and files under \u003Cem\u003Ebaileyfiles\u003C\u002Fem\u003E:\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EDirectory \u003Cem\u003E1830s-trialtxts\u003C\u002Fem\u003E: this will\\ncontain the text file versions of the trials after they have been\\nstripped of all XML formatting. Each file\\nis named after the trial&#39;s ID.\u003C\u002Fli\u003E\\n\u003Cli\u003EDirectory \u003Cem\u003E1830s-trialsbycategory\u003C\u002Fem\u003E: this\\nwill contain the text files that represent all the text in all the\\ntrials belonging to a particular category. These are named after the\\ncategory, e.g., \u003Ccode\u003Etheft-simplelarceny.txt\u003C\u002Fcode\u003E. Each category file\\ncontains all the trials in that category, with one trial per line.\u003C\u002Fli\u003E\\n\u003Cli\u003EFile \u003Ccode\u003Etrialids.txt\u003C\u002Fcode\u003E. This contains the\\nsorted list of trial IDs, one ID per line; we will use it later in\\ncreating cross-validation samples for training the learner (this is\\nthe next step).\u003C\u002Fli\u003E\\n\u003Cli\u003EFiles \u003Ccode\u003Eoffensedict.json\u003C\u002Fcode\u003E and \u003Ccode\u003Etrialdict.json\u003C\u002Fcode\u003E. These json files will come\\ninto use in training the learner.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003ESo if you&#39;re still in the \u003Cem\u003Etrialxmls\u003C\u002Fem\u003E directory, you would write the\\nfollowing commands to run this script:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Ecd ..\u002F..\u002Fbaileycode\u002F\\npython save-trialtxts-by-category.py\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis will take a while. After it&#39;s done, you should have the directories\\nand files described above.\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"creating-the-cross-validation-samples\\\"\u003ECreating the cross-validation samples\u003C\u002Fh4\u003E\\n\u003Cp\u003ENow that we have all our trials saved where we want them, all we need to\\ndo is to create the cross-validation samples and we&#39;re ready to test our\\nlearners.\u003C\u002Fp\u003E\\n\u003Cp\u003ECross-validation simply means repeatedly splitting our data into chunks,\\nsome of which we use for training and others for testing. Since the idea\\nis to get a learner to extract information from one set of documents\\nthat it can then use to determine the class of documents it has never\\nseen, we obviously have to reserve a set of documents that are unknown\\nto the learner if we want to test its performance. Otherwise it&#39;s a bit\\nlike letting your students first read an exam \u003Cem\u003Eand its answers\u003C\u002Fem\u003E and then\\nhave them take that same exam. That would only tell you how closely they\\nread the actual exam, not whether they&#39;ve learned something more\\ngeneral.\u003C\u002Fp\u003E\\n\u003Cp\u003ESo what you want to do is to test the learner on data it hasn&#39;t seen\\nbefore, so that you can tell whether it has learned some general\\nprinciples from the training data. You could just split your data into\\ntwo sets, using, say, 80 percent for training and 20 percent for\\ntesting. But a common practice is to split your data repeatedly into\\ndifferent test and train sets, so that you can ensure that your test\\nresults aren&#39;t the consequence of some oddball quirk in the portion of\\ndata you left for testing.\u003C\u002Fp\u003E\\n\u003Cp\u003ETwo scripts are involved in creating the cross-validation sets. The\\nscript \u003Ccode\u003Etenfold-crossvalidation.py\u003C\u002Fcode\u003E creates\\nthe samples. It reads in the list of trial ids we created in the\\nprevious step, shuffles that list to make it random, and divides it into\\n10 chunks of roughly equal length (that is, a roughly equal number of\\ntrial ids). Then it writes those 10 chunks each into its own text file,\\nso we can read them into our learner code later. Next, to be meticulous,\\nwe can run the \u003Ccode\u003Ecount-offense-instances.py\u003C\u002Fcode\u003E\\nto confirm that if we are interested in a particular trial category,\\nthat category is reasonably evenly distributed across the samples.\u003C\u002Fp\u003E\\n\u003Cp\u003EBefore you run the \u003Ccode\u003Ecount-offense-instances.py\u003C\u002Fcode\u003E script, you should\\nedit it to set the category to the one you&#39;re interested in and let the\\nscript know whether we&#39;re looking at a broad or a specific category.\\nThis is what the relevant part of the code looks like:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eindirname = &#39;..\u002Fbaileyfiles\u002F&#39;\\noffensedict_fn = indirname + &#39;offensedict.json&#39;\\noffensecat = &#39;breakingpeace&#39; #change to target category\\nbroadcat = True #set true if category is e.g. &quot;theft&quot; instead of &quot;theft-simplelarceny&quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAnd here are the commands to run the cross-validation scripts (assuming\\nyou are still in the \u003Cem\u003Ebaileycode\u003C\u002Fem\u003E directory).\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Epython tenfold-crossvalidation.py\\npython count-offense-instances.py\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAlternatively, you can run them using \u003Ca href=\\\"http:\u002F\u002Fpypy.org\u002F\\\"\u003Epypy\u003C\u002Fa\u003E, which is\\nquite a bit faster.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Epypy tenfold-crossvalidation.py\\npypy count-offense-instances.py\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe output of the \u003Ccode\u003Ecount-offense-instances.py\u003C\u002Fcode\u003E script looks like\\nthis:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003EOffense category checked for: breakingpeace\\nsample0.txt: 31\\nsample1.txt: 25\\nsample2.txt: 32\\nsample3.txt: 25\\nsample4.txt: 36\\nsample5.txt: 33\\nsample6.txt: 29\\nsample7.txt: 35\\nsample8.txt: 27\\nsample9.txt: 31\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EFrom the output, we can conclude that the distribution of instances of\\n&quot;breakingpeace&quot; is more or less even. If it isn&#39;t, we can re-run the\\n\u003Ccode\u003Etenfold-crossvalidation.py\u003C\u002Fcode\u003E script, and then check the distribution again.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"testing-the-learner\\\"\u003ETesting the learner\u003C\u002Fh3\u003E\\n\u003Cp\u003EAll right, we are ready to train and test our Naive Bayesian! The script\\nthat does this is called \u003Ccode\u003Etest-nb-learner.py\u003C\u002Fcode\u003E. It starts by defining a few\\nvariables:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Ecategoriesdir = &#39;..\u002Fbaileyfiles\u002F1830s-trialsbycategory\u002F&#39;\\nsampledirname = &#39;..\u002Fbaileyfiles\u002FSamples_1830s\u002F&#39; #location of 10-fold cross-validation\\nstopwordfilename = &#39;..\u002Fbaileyfiles\u002Fenglish-stopwords.txt&#39;\\n# the ones below should be set to None if not using\\ncattocheck = &#39;breakingpeace&#39; #if evaluating recognition one category against rest\\npattern = &#39;[^-]+&#39; #regex pattern to use if category is not complete filename\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EMost of these are pretty self-explanatory, but note the two last ones.\\nThe variable &quot;cattocheck&quot; determines whether we are looking to identify\\na specific category or to sort each trial into its proper category (the\\nlatter is done if the variable is set to None). The variable &quot;pattern&quot;\\ntells us whether we are using the whole file name as the category\\ndesignation, or only a part of it, and if the latter, how to identify\\nthe part. In the example above, we are focusing on the broad category\\n&quot;breakingpeace&quot;, and so we are not using the whole file name, which\\nwould be e.g. &quot;breakingpeace-riot&quot; but only the part before the dash.\\nBefore you run the code, you should set these variables to what you want\\nthem to be.\u003C\u002Fp\u003E\\n\u003Cp\u003ENote that &quot;cattocheck&quot; here should match the &quot;offensecat&quot; that you\\nchecked for with the \u003Ccode\u003Ecount-offense-instances.py\u003C\u002Fcode\u003E script. No error is\\nproduced if it does not match, and it&#39;s fairly unlikely that it will\\nhave any real impact, but if the categories don&#39;t match, then of course\\nyou have no assurance that the category you&#39;re actually interested in is\\nmore or less evenly distributed across the ten cross-validation samples.\u003C\u002Fp\u003E\\n\u003Cp\u003ENote also that you can of course set &quot;cattocheck&quot; to &quot;None&quot; and leave\\nthe pattern as it is, in which case you will be sorting into the broader\\ncategories.\u003C\u002Fp\u003E\\n\u003Cp\u003ESo, with the basic switches set and knobs turned, we begin by reading in\\nall the trials that we have saved. We do this with the function called\\n\u003Cem\u003Eprocess_data\u003C\u002Fem\u003E that can be found in the \u003Ccode\u003Epretestprocessing.py\u003C\u002Fcode\u003E file. (That file\\ncontains functions that are called from the scripts you will run, so it\\nisn&#39;t something you&#39;ll run directly at any point.)\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eprint &#39;Reading in the data...&#39;\\ntrialdata = ptp.process_data(categoriesdir,stopwordfilename,cattocheck,pattern)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe \u003Cem\u003Eprocess_data\u003C\u002Fem\u003E function reads in all\\nthe files in the directory that contains our trial category files, and\\nprocesses them so that we get a list containing all the categories and\\nthe trials belonging to them, with the trial text lowercased and\\ntokenized (split into a list of words), minus stopwords (common words\\nlike a, the, me, which, etc.) Each trial begins with its id number, so\\nthat&#39;s one of our words (though we ignore it in training and testing).\\nLike this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E    [\\n     [breakingpeace,\\n       [&#39;trialid&#39;,&#39;victim&#39;,&#39;peace&#39;,&#39;disturbed&#39;,&#39;man&#39;,&#39;tree&#39;,...]\\n       [&#39;trialid&#39;,&#39;dress&#39;,&#39;blood&#39;,&#39;head&#39;,&#39;incited&#39;,...]\\n      ...]\\n     [theft,\\n       [&#39;trialid&#39;,&#39;apples&#39;,&#39;orchard&#39;,&#39;basket&#39;,&#39;screamed&#39;,&#39;guilty&#39;,....]\\n       [&#39;trialid&#39;,&#39;rotten&#39;,&#39;fish&#39;]\\n      ...]\\n    ]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENext, making use of the results of the ten-fold cross-validation routine\\nwe created, we loop through the files that define the\\nsamples, each time making one sample the test set and the rest the train\\nset. Then we split &#39;trialdata&#39;, the list of trials-by-category that we\\njust created, into train and test sets accordingly. The functions that\\ndo these two steps are \u003Cem\u003Ecreate_sets\u003C\u002Fem\u003E and\\n\u003Cem\u003Esplittraintest\u003C\u002Fem\u003E, both in the \u003Ccode\u003Epretestprocessing.py\u003C\u002Fcode\u003E file.\u003C\u002Fp\u003E\\n\u003Cp\u003ENow we train our Naive Bayesian classifier on the train set. The\\nclassifier we are using (which is included in the scripts zip file) is\\none written by Mans Hulden, and it does pretty much exactly what the\\n&quot;identify the author of the manuscript&quot; example above\\ndescribes.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E    # split train and test\\n    print &#39;Creating train and test sets, run {0}&#39;.format(run)\\n    trainsetids, testsetids = ptp.create_sets(sampledirname,run)\\n    traindata, testdata = ptp.splittraintest(trainsetids,testsetids,trialdata)\\n\\n    # train learner\\n    print &#39;Training learner, run {0}...&#39;.format(run)\\n    mynb = nb.naivebayes()\\n    mynb.train(traindata)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAfter the learner is trained, we are ready to test how well the it\\nperforms. Here&#39;s the code:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E    print &#39;Testing learner, run {0}...&#39;.format(run)\\n\\n    for trialset in testdata:\\n        correctclass = trialset[0]\\n        for trial in trialset[1:]:\\n            result = mynb.classify(trial)\\n            guessedclass =  max(result, key=result.get)\\n            # then record correctness of classification result\\n            # note that first version does a more complex evaluation\\n            # ... for two-way (one class against rest) classification\\n            if cattocheck:\\n                if correctclass == cattocheck:\\n                    catinsample += 1\\n                if guessedclass == cattocheck:\\n                     guesses += 1\\n                     if guessedclass == correctclass:\\n                         hits += 1\\n            if guessedclass == correctclass:\\n                correctguesses += 1\\n\\n            total +=1\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESo we loop through the categories in the &quot;testdata&quot; list (which is of\\nthe same format as the &quot;trialdata&quot; list). For each\\ncategory, we loop through the trials in that category, classifying each\\ntrial with our Naive Bayesian classifier, and comparing the result to\\nthe correct class (saved in the first element of each category list\\nwithin the testdata list.) Then we add to various counts to be able to\\nevaluate the results of the whole classification exercise.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo run the code that trains and tests the learner, first make sure you\\nhave edited it to set the &quot;cattocheck&quot; and &quot;pattern&quot; switches, and then\\ncall it on the command line (assuming you&#39;re still in the directory\\n\u003Cem\u003Ebaileycode\u003C\u002Fem\u003E):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E    python test-nb-learner.py\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAgain, for greater speed, you can also use pypy:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003E    pypy test-nb-learner.py\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe code will print out some accuracy measures for the classification\\ntask you have chosen. The output should look something like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003EReading in the data...\\nCreating train and test sets, run 0\\nTraining learner, run 0...\\nTesting learner, run 0...\\nCreating train and test sets, run 1\\nTraining learner, run 1...\\n...\\nTraining learner, run 9...\\nTesting learner, run 9...\\nSaving correctly classified trials and close matches...\\nCalculating accuracy of classification...\\nTwo-way classification, target category breakingpeace.\\nAnd the results are:\\nAccuracy 99.00%\\nPrecision: 61.59%\\nRecall: 66.45%\\nAverage number of target category trials in test sample per run: 30.4\\nAverage number of trials in test sample per run: 2271.0\\nObtained in 162.74 seconds\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENext, let&#39;s take a look at what these measures of accuracy mean.\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"measures-of-classification\\\"\u003EMeasures of classification\u003C\u002Fh4\u003E\\n\u003Cp\u003EThe basic measure of classification prowess is \u003Cem\u003Eaccuracy\u003C\u002Fem\u003E: how often did\\nclassifier guess the class of a document correctly? This is calculated\\nby simply dividing the number of correct guesses by the total number of\\ndocuments considered.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf we&#39;re interested in a specific category, we can extract a bit more\\ndata. So if we set, for example, cattocheck =\\n&#39;breakingpeace&#39;, like above, we can then examine how well the classifier\\ndid with respect to the &quot;breakingpeace&quot; category in particular.\u003C\u002Fp\u003E\\n\u003Cp\u003ESo, in the testlearner code, if we&#39;re doing\\nmultiway classification, we only record how many trials we&#39;ve seen\\n(&quot;total&quot;) and how many of our guesses were correct (&quot;correctguesses&quot;).\\nBut if we&#39;re considering a single category, say &quot;breakingpeace,&quot; we\\nrecord a few more numbers. First, we keep track of how many trials\\nbelonging to the category &quot;breakingpeace&quot; there are in our test sample\\n(this tally is in &quot;catinsample&quot;). We also keep track of how many times\\nwe&#39;ve guessed that a trial belongs to the &quot;breakingpeace&quot; category\\n(&quot;guesses&quot;). And finally we record how many times we have guessed\\n\u003Cem\u003Ecorrectly\u003C\u002Fem\u003E that a trial belongs to &quot;breakingpeace&quot; (&quot;hits&quot;).\u003C\u002Fp\u003E\\n\u003Cp\u003ENow that we have this information, we can use it to calculate a couple\\nof standard measures of classification efficiency: \u003Cem\u003Eprecision\u003C\u002Fem\u003E and\\n\u003Cem\u003Erecall\u003C\u002Fem\u003E. Precision tells us how often we correctly guessed that a\\ntrial was in the &quot;breakingpeace&quot; category. Recall lets us know what\\nproportion of the &quot;breakingpeace&quot; trials we caught.\u003C\u002Fp\u003E\\n\u003Cp\u003ELet&#39;s take another example to clarify precision and recall. Imagine you\\nwant all the books on a particular topic — World War I, say — from your\\nuniversity library. You send out one of your many minions (all\\nhistorians possess droves of them, as you know) to get the books. The\\nminion dutifully returns with a big pile.\u003C\u002Fp\u003E\\n\u003Cp\u003ENow, suppose you were in possession of a list that contained of every\\nsingle book in the library on WWI and no books that weren&#39;t related to\\nthe WWI. You could then check the precision and recall of your minion\\nwith regard to the category of &quot;books on WWI.&quot;\u003C\u002Fp\u003E\\n\u003Cp\u003ERecall is the term for the proportion of books on WWI in the library\\nthat your minion managed to grab. That is, the more books on WWI\\nremaining in the library after your minion&#39;s visit, the lower your\\nminion&#39;s recall.\u003C\u002Fp\u003E\\n\u003Cp\u003EPrecision, in turn, is the term for the proportion of books in the pile\\nbrought by your minion that actually had to do with WWI. The more\\nirrelevant (off-topic) books in the pile, the lower the precision.\u003C\u002Fp\u003E\\n\u003Cp\u003ESo, say the library has 1,000 books on WWI, and your minion lugs you a\\npile containing 400 books, of which 300 have nothing to do with WWI. The\\nminion&#39;s recall would be (400-300)\u002F1,000, or 10 percent. The minion&#39;s\\nprecision, in turn, would be (400-300)\u002F400, or 25 percent.\u003C\u002Fp\u003E\\n\u003Cp\u003E(Should have gone yourself, eh?)\u003C\u002Fp\u003E\\n\u003Cp\u003EA side note: the minion&#39;s overall accuracy — correct guesses divided by\\nactual number of examples — would be:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E(the number of books on WWI in your pile - the number of books *not* on\\nWWI in your pile + the number of books in the library *not* on WWI)\\n------------------------------------------------------------------------\\nthe total number of books in the library\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESo if the library held 100,000 volumes, this would be (100 - 300 +\\n99,000) \u002F 100,000 — or 98.8 percent. That seems like a great number, but\\nsince it merely means that your minion was smart enough to leave most of\\nthe library books in the library, it&#39;s not very helpful in this case\\n(except inasmuch as it is nice not to be buried under 100,000 volumes.)\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"how-well-does-our-naive-bayesian-do\\\"\u003EHow well does our Naive Bayesian do?\u003C\u002Fh4\u003E\\n\u003Cp\u003EOur tests on the Naive Bayesian use the data set consisting of all the\\ntrials from the 1830s. It contains 17,549 different trials in 50\\ndifferent offense categories (which can be grouped into 9 broad\\ncategories).\u003C\u002Fp\u003E\\n\u003Cp\u003EIf we run the Naive Bayesian so that it attempts to sort all trials into\\ntheir correct broad categories, its accuracy is pretty good: 94.3\\npercent. So 94 percent of the time, when it considers how it should sort\\ntrials in the test sample into &quot;breakingpeace,&quot; &quot;deception,&quot; &quot;theft,&quot;\\nand so on, it chooses correctly.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor the more specific categories (&quot;theft-simplelarceny,&quot;\\n&quot;breakingpeace-riot,&quot; and so on) the same exercise is much less\\naccurate: then, the classifier gets it right only 72 percent of the\\ntime. That&#39;s no wonder, really, given that some of the categories are so\\nsmall that we barely have any examples. We might do a bit better with\\nmore data (say, all the trials from the whole 19th century, instead of\\nonly all the trials from the 1830s).\u003C\u002Fp\u003E\\n\u003Cp\u003EThe first, overall category results are pretty impressive. They give us\\nquite a bit of confidence that if what we needed to do was to sort\\ndocuments into piles that weren&#39;t all too fine-grained, and we had a\\nnice bunch of training data, a Naive Bayesian could do the job for us.\u003C\u002Fp\u003E\\n\u003Cp\u003EBut the problem for a historian is often rather different. A historian\\nusing a Naive Bayesian learner is more likely to want to separate\\ndocuments that are &quot;interesting&quot; from documents that are &quot;not\\ninteresting&quot; — usually meaning documents dealing with a particular issue\\nor not dealing with it. So the question is really more one where we have\\na mass of &quot;uncategorized&quot; or &quot;other&quot; documents and a much smaller set of\\n&quot;interesting&quot; documents, and we try to find more of the latter among the\\nformer.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn our current exercise, that situation is fairly well represented by\\ntrying to identify documents from a single category in the mass of the\\nrest of the documents, set to category &quot;other.&quot; So how well are we able\\nto do that? In other words, if we set cattocheck\\n= &#39;breakingpeace&#39; (or another category) so that all trials get\\nmarked as either that category or as &quot;other,&quot; and then run the\\nclassifier, what kinds of results do we get?\u003C\u002Fp\u003E\\n\u003Cp\u003EWell, our overall accuracy is still high: over 95 percent in all cases\\nfor the broad categories, and usually about that for the detailed ones\\nas well. But just like the minion going off to the library to get books\\non WWI had a pretty high accuracy because he\u002Fshe didn&#39;t bring back half\\nthe library, in this case, too, our accuracy is mostly just due to the\\nfact that we manage to not misidentify \u003Cem\u003Etoo\u003C\u002Fem\u003E many &quot;other&quot; trials as\\nbeing in the category we&#39;re interested in. Because there are so many\\n&quot;other&quot; trials, those correct assessments far outweigh the minus points\\nwe may have gotten from missing interesting trials.\u003C\u002Fp\u003E\\n\u003Cp\u003EPrecision and recall, therefore, are more in this case more interesting\\nmeasures than overall accuracy. Here&#39;s a table showing precision and\\nrecall for each of the &quot;broad&quot; categories in our trial sample, and for a\\nfew sample detailed categories. The last column shows how many target\\ncategory trials there were in the test set on average (remember, we did\\nten runs with different train\u002Ftest splits, so all our results are\\naverages of that).\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003ENaive Bayesian classifier, two-way classification, 10-fold\\ncross-validation\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003EBroad categories\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003ECategory\u003C\u002Fth\u003E\\n\u003Cth\u003EPrecision (%)\u003C\u002Fth\u003E\\n\u003Cth\u003ERecall (%)\u003C\u002Fth\u003E\\n\u003Cth\u003EAvg # trials in cat in TeS\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003Ebreakingpeace\u003C\u002Ftd\u003E\\n\u003Ctd\u003E63.52\u003C\u002Ftd\u003E\\n\u003Ctd\u003E64.05\u003C\u002Ftd\u003E\\n\u003Ctd\u003E24.2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Edamage\u003C\u002Ftd\u003E\\n\u003Ctd\u003E0.00\u003C\u002Ftd\u003E\\n\u003Ctd\u003E0.00\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Edeception\u003C\u002Ftd\u003E\\n\u003Ctd\u003E53.47\u003C\u002Ftd\u003E\\n\u003Ctd\u003E61.43\u003C\u002Ftd\u003E\\n\u003Ctd\u003E47.7\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Ekill\u003C\u002Ftd\u003E\\n\u003Ctd\u003E62.5\u003C\u002Ftd\u003E\\n\u003Ctd\u003E89.39\u003C\u002Ftd\u003E\\n\u003Ctd\u003E17.9\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Emiscellaneous\u003C\u002Ftd\u003E\\n\u003Ctd\u003E47.83\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4.44\u003C\u002Ftd\u003E\\n\u003Ctd\u003E24.8\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Eroyaloffenses\u003C\u002Ftd\u003E\\n\u003Ctd\u003E85.56\u003C\u002Ftd\u003E\\n\u003Ctd\u003E91.02\u003C\u002Ftd\u003E\\n\u003Ctd\u003E42.3\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Esexual\u003C\u002Ftd\u003E\\n\u003Ctd\u003E93.65\u003C\u002Ftd\u003E\\n\u003Ctd\u003E49.17\u003C\u002Ftd\u003E\\n\u003Ctd\u003E24.0\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Etheft\u003C\u002Ftd\u003E\\n\u003Ctd\u003E96.26\u003C\u002Ftd\u003E\\n\u003Ctd\u003E98.75\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1551.8\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Eviolenttheft\u003C\u002Ftd\u003E\\n\u003Ctd\u003E68.32\u003C\u002Ftd\u003E\\n\u003Ctd\u003E33.01\u003C\u002Ftd\u003E\\n\u003Ctd\u003E20.9\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003E\u003Cstrong\u003ESample detailed categories\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003ECategory\u003C\u002Fth\u003E\\n\u003Cth\u003EPrecision (%)\u003C\u002Fth\u003E\\n\u003Cth\u003ERecall (%)\u003C\u002Fth\u003E\\n\u003Cth\u003EAvg # trials in cat in TeS\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003Etheft-simplelarceny\u003C\u002Ftd\u003E\\n\u003Ctd\u003E64.37\u003C\u002Ftd\u003E\\n\u003Ctd\u003E89.03\u003C\u002Ftd\u003E\\n\u003Ctd\u003E805.9\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Etheft-receiving\u003C\u002Ftd\u003E\\n\u003Ctd\u003E92.21\u003C\u002Ftd\u003E\\n\u003Ctd\u003E61.53\u003C\u002Ftd\u003E\\n\u003Ctd\u003E198.1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Edeception-forgery\u003C\u002Ftd\u003E\\n\u003Ctd\u003E74.29\u003C\u002Ftd\u003E\\n\u003Ctd\u003E11.87\u003C\u002Ftd\u003E\\n\u003Ctd\u003E21.9\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Eviolenttheft-robbery\u003C\u002Ftd\u003E\\n\u003Ctd\u003E68.42\u003C\u002Ftd\u003E\\n\u003Ctd\u003E31.86\u003C\u002Ftd\u003E\\n\u003Ctd\u003E20.4\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Etheft-extortion\u003C\u002Ftd\u003E\\n\u003Ctd\u003E0.00\u003C\u002Ftd\u003E\\n\u003Ctd\u003E0.00\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1.3\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EThere are a few generalizations we can make from these numbers.\u003C\u002Fp\u003E\\n\u003Cp\u003EFirst, it&#39;s obvious that if the category is too small, we are out of\\nluck. So for &quot;damage,&quot; a small enough broad category that our test\\nsamples only held a little over one instance of it on average, we get no\\nresults. Similarly, in the detailed categories, when the occurrence of\\ncases per test sample drops into the single digits, we fail miserably.\\nThis is no wonder: if the test sample contains about one case on\\naverage, there can&#39;t be much more than ten cases total in the whole data\\nset. That&#39;s not much to go on.\u003C\u002Fp\u003E\\n\u003Cp\u003ESecond, size isn&#39;t everything. Although we do best for the biggest\\ncategory, theft (which in fact accounts for over half our sample), there\\nare some smaller categories we do very well for. We have very high\\nrecall and precision for &quot;royaloffenses,&quot; a mid-sized category, and very\\nhigh recall plus decent precision for &quot;kill,&quot; our smallest\\nreasonable-sized category. A reasonable guess would be that the language\\nthat occurs in the trials is distinctive and, in the case of\\n&quot;royaloffenses,&quot; doesn&#39;t occur much anywhere else. Meanwhile,\\nunsurprisingly, we get low scores for the &quot;miscellaneous&quot; category. We\\nalso have high precision for the &quot;sexual&quot; category, indicating that it\\nhas some language that doesn&#39;t tend to appear anywhere else — though we\\nmiss about half the instances of it, which would lead us to suspect that\\nmany of the trials in that category omit some of the language that most\\ndistinguishes it from others.\u003C\u002Fp\u003E\\n\u003Cp\u003EThird, in this sample at least, there seems to be no clear pattern\\nregarding whether the learner has better recall or better precision.\\nSometimes it casts a wide net that drags in both a good portion of the\\ncategory and some driftwood, and sometimes it handpicks the trials for\\ngood precision but misses a lot that don&#39;t look right enough for its\\ntaste. So in half the cases here, our learner has better precision than\\nrecall, and in half better recall than precision. The differences\\nbetween precision and recall are, however, bigger for the cases where\\nprecision is better than recall. That isn&#39;t necessarily a good thing for\\nus, since as historians we might be happier to see more of the\\n&quot;interesting&quot; documents and do the additional culling ourselves than to\\nhave our learner miss a lot of good documents. We&#39;ll return to the\\nquestion of the meaning of classification errors below.\u003C\u002Fp\u003E\\n\u003Ch4 id=\\\"extracting-the-most-indicative-features\\\"\u003EExtracting the most indicative features\u003C\u002Fh4\u003E\\n\u003Cp\u003EThe \u003Ccode\u003Enaivebayes.py\u003C\u002Fcode\u003E script has a feature\\nthat allows you to extract the most (and least) indicative features of\\nyour classification exercise. This allows you to see what weighs a lot\\nin the learner&#39;s mind — what it has, in effect, learned.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe command to issue is: \u003Cem\u003Emynb.topn_print(10)\u003C\u002Fem\u003E (for the 10 most\\nindicative; you can put in any number you like). Here are the results\\nfor a multi-way classification of the broad categories in our data:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Edeception [&#39;norrington&#39;, &#39;election&#39;, &#39;flaum&#39;, &#39;polish&#39;, &#39;caton&#39;, &#39;spicer&#39;, &#39;saltzmaun&#39;, &#39;newcastle&#39;, &#39;stamps&#39;, &#39;rotherham&#39;]\\nroyaloffences [&#39;mould&#39;, &#39;coster&#39;, &#39;coin&#39;, &#39;caleb&#39;, &#39;counterfeit&#39;, &#39;obverse&#39;, &#39;mint&#39;, &#39;moulds&#39;, &#39;plaster-of-paris&#39;, &#39;metal&#39;]\\nviolenttheft [&#39;turfrey&#39;, &#39;stannard&#39;, &#39;millward&#39;, &#39;falcon&#39;, &#39;crawfurd&#39;, &#39;weatherly&#39;, &#39;keith&#39;, &#39;farr&#39;, &#39;ventom&#39;, &#39;shurety&#39;]\\ndamage [&#39;cow-house&#39;, &#39;ewins&#39;, &#39;filtering-room&#39;, &#39;fisk&#39;, &#39;calf&#39;, &#39;skirting&#39;, &#39;girder&#39;, &#39;clipping&#39;, &#39;saturated&#39;, &#39;firemen&#39;]\\nbreakingpeace [&#39;calthorpe-street&#39;, &#39;grievous&#39;, &#39;disable&#39;, &#39;mellish&#39;, &#39;flag&#39;, &#39;bodily&#39;, &#39;banner&#39;, &#39;aforethought&#39;, &#39;fursey&#39;, &#39;emerson&#39;]\\nmiscellaneous [&#39;trevett&#39;, &#39;teuten&#39;, &#39;reitterhoffer&#39;, &#39;quantock&#39;, &#39;feaks&#39;, &#39;boone&#39;, &#39;bray&#39;, &#39;downshire&#39;, &#39;fagnoit&#39;, &#39;ely&#39;]\\nkill [&#39;vault&#39;, &#39;external&#39;, &#39;appearances&#39;, &#39;slaying&#39;, &#39;deceased&#39;, &#39;marchell&#39;, &#39;disease&#39;, &#39;pedley&#39;, &#39;healthy&#39;, &#39;killing&#39;]\\ntheft [&#39;sheep&#39;, &#39;embezzlement&#39;, &#39;stealing&#39;, &#39;table-cloth&#39;, &#39;fowls&#39;, &#39;dwelling-house&#39;, &#39;missed&#39;, &#39;pairs&#39;, &#39;breaking&#39;, &#39;blankets&#39;]\\nsexual [&#39;bigamy&#39;, &#39;marriage&#39;, &#39;violate&#39;, &#39;ravish&#39;, &#39;marriages&#39;, &#39;busher&#39;, &#39;register&#39;, &#39;spinster&#39;, &#39;bachelor&#39;, &#39;married&#39;]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESome of these make sense instantly. In &quot;breakingpeace&quot; (which includes\\nassaults, riots and woundings) you can see the makings of phrases like\\n&quot;grievous bodily harm&quot; and &quot;malice aforethought,&quot; along with other\\nindications of wreaking havoc like &quot;disable&quot; and &quot;harm.&quot; In\\nroyaloffenses, the presence of &quot;mint,&quot; &quot;mould&quot; and &quot;plaster-of-paris&quot;\\nmake sense since the largest subcategory is coining offenses. In\\n&quot;theft,&quot; one might infer that sheep, fowl, and table-cloths seem to have\\nbeen popular objects for stealing (though table-cloth may of course have\\nbeen a wrapping for stolen objects; one would have to examine the trials\\nto know).\u003C\u002Fp\u003E\\n\u003Cp\u003EOthers are more puzzling. Why is violenttheft almost exclusively\\ncomposed of what seem to be person or place names? Why is &quot;election&quot;\\nindicative of deception? Is there a lot of election fraud going on, or\\nabuse of elected office? Looking at the documents, one finds that 9 of\\nthe words indicative of violent theft are person names, and one is a\\npub; why person and pub names should be more indicative here than for\\nother categories is mildly intriguing and might bear further analysis\\n(or might just be a quirk of our data set — remember that &quot;violenttheft&quot;\\nis a fairly small category). As for &quot;election,&quot; it&#39;s hard to distinguish\\na clear pattern, though it seems to be linked to fraud attempts on and\\nby various officials at different levels of government.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe indicative features, then, may be intriguing in themselves (though\\nobviously, one should not draw any conclusions about them without\\nclosely examining the data first). They are also useful in that they can\\nhelp us determine whether something is skewing our results in a way we\\ndon&#39;t wish, something we may be able to correct for with different\\nweighting or different selection of features (see the section on\\n\u003Ca href=\\\"#Tuning\\\"\u003ETuning\u003C\u002Fa\u003E below).\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"the-meanings-of-misclassification\\\"\u003EThe meanings of misclassification\u003C\u002Fh3\u003E\\n\u003Cp\u003EAgain, it&#39;s good to keep in mind that in classifying documents we are\\nnot always after an abstract &quot;true&quot; classification, but simply a useful\\nor interesting one. Thus, it is a good idea to look a bit more closely\\nat the &quot;errors&quot; in classification.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe&#39;ll focus on two-way classification, and look at the cases where the\\nNaive Bayesian incorrectly includes a trial in a category (false\\npositives) as well as take a look at trials it narrowly excludes from\\nthe category (let&#39;s call them close relatives).\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the script for testing the learner (\u003Ccode\u003Etest-nb-learner.py\u003C\u002Fcode\u003E), we saved the trial ids for\\nfalse positives and close relatives so we could examine them later.\u003C\u002Fp\u003E\\n\u003Cp\u003EHere&#39;s the relevant code bit:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eresult = mynb.classify(trial)\\nguessedclass =  max(result, key=result.get)\\nif cattocheck:\\n    diff = abs(result[cattocheck] - result[&#39;other&#39;])\\n    if diff &lt; 10 and guessedclass != cattocheck:\\n        closetrials.append(trial[0])\\n        difflist.append(diff)\\n    if correctclass == cattocheck:\\n        catinsample += 1\\n    if guessedclass == cattocheck:\\n         guesses += 1\\n         if guessedclass == correctclass:\\n             hits += 1\\n         else:\\n             falsepositives.append(trial[0])\\nif guessedclass == correctclass:\\n    correctguesses += 1\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EFalse positives are easy to catch: we simply save the cases where we\\nguessed that a trial belonged to the category but it really did not.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor close relatives, we first check how confident we were that the trial\\ndid not belong in our category. When we issue the call to classify the\\ntrial \u003Cem\u003Emynb.classify(trial)\u003C\u002Fem\u003E, it returns\\nus a dictionary that looks like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003E{\\n    &#39;other&#39;: -2358.522248351527,\\n    &#39;violenttheft-robbery&#39;: -2326.2878233211086\\n}\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ESo to find the close relatives, we compare these two values, and if the\\ndifference between them is small, we save the id of the trial we are\\ncurrently classifying into a list of close relatives. (In the code chunk\\nabove, we have rather arbitrarily defined a &quot;small&quot; difference as being\\nunder 10).\u003C\u002Fp\u003E\\n\u003Cp\u003EAt the end of the script, we write the results of these operations into\\ntwo text files: \u003Ccode\u003Efalsepositives.txt\u003C\u002Fcode\u003E and \u003Ccode\u003Ecloserelatives.txt\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ELet&#39;s look more closely at misclassifications for the category\\n&quot;violenttheft-robbery.&quot; Here are the first 10 rows of the close\\nrelatives file and the first 20 rows of the false positives file, sorted\\nby offense:\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003EClose relatives\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Ebreakingpeace-wounding, t18350105-458, 1.899530878\\ntheft-pocketpicking, t18310407-90, 0.282424548\\ntheft-pocketpicking, t18380514-1168, 0.784184742\\ntheft-pocketpicking, t18301028-208, 0.797341405\\ntheft-pocketpicking, t18341016-85, 1.296811989\\nviolenttheft-robbery, t18370102-317, 1.075548985\\nviolenttheft-robbery, t18350921-2011, 1.105672712\\nviolenttheft-robbery, t18310407-204, 1.521788666\\nviolenttheft-robbery, t18370102-425, 1.840718222\\nviolenttheft-robbery, t18330214-13, 2.150018805\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003E\u003Cstrong\u003EFalse positives\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003Ebreakingpeace-assault, t18391021-2933\\nbreakingpeace-wounding, t18350615-1577\\nbreakingpeace-wounding, t18331017-159\\nbreakingpeace-wounding, t18350615-1578\\nbreakingpeace-wounding, t18330704-5\\nkill-manslaughter, t18350706-1682\\nkill-manslaughter, t18360919-2161\\nkill-manslaughter, t18380618-1461\\nkill-murder, t18330103-7\\nkill-murder, t18391021-2937\\nmiscellaneous-pervertingjustice, t18340904-144\\ntheft-pocketpicking, t18300114-128\\ntheft-pocketpicking, t18310407-66\\ntheft-pocketpicking, t18330905-92\\ntheft-pocketpicking, t18370703-1639\\ntheft-pocketpicking, t18301028-127\\ntheft-pocketpicking, t18310106-87\\ntheft-pocketpicking, t18331017-109\\ntheft-pocketpicking, t18320216-108\\ntheft-pocketpicking, t18331128-116\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe first thing we notice is that many of the close relatives are in\\nfact from our target category — they are cases that our classifier has\\nnarrowly missed. So saving these separately could compensate nicely for\\nan otherwise low recall number.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe second thing we notice is that more of the false positives seem to\\nhave to do with violence, whereas more of the close relatives seem to\\nhave to do with stealing; it seems our classifier has pegged the\\nviolence aspect of robberies as more significant in distinguishing them\\nthan the filching aspect.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe third thing we notice is that theft-pocketpicking is a very common\\ncategory among both the close relatives and the false positives. And\\nindeed, if we look at a sample trial from violenttheft-robbery and\\nanother from among the close pocketpicking relatives, we notice that\\nthere are definitely close similarities.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor example, trial t18310407-90, the closest close relative, involved a\\ncertain Eliza Williams indicted for pocketpicking. Williams was accused\\nof stealing a watch and some other items from a certain Thomas Turk;\\naccording to Turk and his friend, they had been pub-crawling, Eliza\\nWilliams (whom they did not know from before) had tagged along with\\nthem, and at one point in the evening had pocketed Turk&#39;s watch (Turk,\\nby this time, was quite tipsy). Williams was found guilty and sentenced\\nto be confined for one year.\u003C\u002Fp\u003E\\n\u003Cp\u003EMeanwhile, in trial t18300708-14, correctly classed as\\nviolenttheft-robbery, a man called Edward Overton was accused of\\nfeloniously assaulting a fellow by the name of John Quinlan. Quinlan\\nexplained that he had been out with friends, and when he parted from\\nthem he realized it was too late to get into the hotel where he worked\\nas a waiter and (apparently) also had lodgings. Having nowhere to go, he\\ndecided to visit a few more public-houses. Along the way, he met\\nOverton, whom he did not know from before, and treated him to a few\\ndrinks. But then, according to Quinlan, Overton attacked him as they\\nwere walking from one pub to another, and stole his watch as well as\\nother possessions of his. According to Overton, however, Quinlan had\\ngiven him the watch as a guarantee that he would repay Overton if\\nOverton paid for his lodging for the night. Both men, it seems, were\\nthoroughly drunk by the end of the evening. Overton was found not\\nguilty.\u003C\u002Fp\u003E\\n\u003Cp\u003EBoth trials, then, are stories of groups out drinking and losing their\\npossessions; what made the latter a trial for robbery rather than for\\npocketpicking was simply Quinlan&#39;s accusation that Overton had &quot;struck\\nhim down.&quot; For a historian interested in either robberies or\\npocketpickings (or pub-crawling in 1830s London), both would probably be\\nequally interesting.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn fact, the misclassification patterns of the learner indicate that\\neven when data is richly annotated, such as in the case of the Old\\nBailey, using a machine learner to extract documents may be useful for a\\nhistorian: in this case, it would help you extract trials from different\\noffense categories that share features of interest to you, regardless of\\nthe offense label.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"tuning\\\"\u003ETuning\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe possibilities for tuning are practically endless.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor example, you might consider tweaking your data. For instance,\\ninstead of giving your classifier all the words in the document, you\\nmight present it with a reduced set.\u003C\u002Fp\u003E\\n\u003Cp\u003EOne way of reducing the number of words is to collapse different words\\ntogether through stemming. So the verb forms &quot;killed,&quot; &quot;kills,&quot;\\n&quot;killing&quot; would all become &quot;kill&quot; (as would the plural noun &quot;kills&quot;). A\\npopular stemmer is the \u003Ca href=\\\"http:\u002F\u002Fsnowball.tartarus.org\u002F\\\"\u003ESnowball Stemmer\u003C\u002Fa\u003E, and you could add that to the\\ntokenization step. (I ran a couple of tests with this, and while it made\\nthe process much slower, it didn&#39;t much improve the results. But that\\nwould probably depend a bit on the kind of data you have.)\u003C\u002Fp\u003E\\n\u003Cp\u003EAnother way is to select the words you give to the classifier according\\nto some principle. One common solution is to pick only the words with a\\nhigh \u003Cstrong\u003ETF-IDF\u003C\u002Fstrong\u003E score. TF-IDF is short for &quot;term frequency - inverse\\ndocument frequency,&quot; and a high score means that the term occurs quite\\nfrequently in the document under consideration but rarely in documents\\nin general. (You can also check out \u003Ca href=\\\"http:\u002F\u002Fstevenloria.com\u002Ffinding-important-words-in-a-document-using-tf-idf\u002F\\\"\u003Ea more detailed explanation of\\nTF-IDF\u003C\u002Fa\u003E, along with some Python code for calculating it.)\u003C\u002Fp\u003E\\n\u003Cp\u003EOther options include simply playing with the size of the priors: now,\\nthe Naive Bayesian has a class prior as well as a feature prior of 0.5,\\nmeaning that it pretends to have seen all classes and all words at least\\none-half times. Doing test runs with different priors might get you\\ndifferent results.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn addition to simply changing the general prior sizes, you might\\nconsider having the classifier set a higher prior on the target\\ncategory than on the &quot;other&quot; category, in effect requiring less\\nevidence to include a trial in the target category. It might be worth\\na try particularly since we noted above when examining the close\\nrelatives (under Meanings of Misclassification) that many of them were\\nin fact members of our target category. Setting a larger prior on the\\ntarget class would probably catch those cases, boosting the recall. At\\nthe same time, it probably would also lower the precision. (To change\\nthe priors, you need to edit the \u003Ccode\u003Enaivebayes.py\u003C\u002Fcode\u003E script.)\u003C\u002Fp\u003E\\n\u003Cp\u003EAs you can see, there is quite a lot of fuzziness here: how you pick\\nthe features, how you pick the priors, and how you weight various\\npriors all affect the results you get, and how to pick and weight is\\nnot governed by hard logic but is rather a process of trial and error.\\nStill, like we noted noted in the section on the meaning of\\nclassification error above, if your goal is to get some interesting\\ndata to do historical analysis on, some fuzziness may not be such a\\nbig problem.\u003C\u002Fp\u003E\\n\u003Cp\u003EHappy hunting!\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
