{"metadata":{"title":"Supervised Classification: The Naive Bayesian Returns to the Old Bailey","layout":"lesson","date":"2014-12-17T00:00:00.000Z","authors":["Vilja Hulden"],"reviewers":["Adam Crymble"],"editors":["William J. Turkel"],"difficulty":3,"exclude_from_check":["review-ticket"],"activity":"analyzing","topics":["distant-reading"],"abstract":"This lesson shows how to use machine learning to extract interesting documents out of a digital archive.","redirect_from":"/lessons/naive-bayesian","avatar_alt":"A man peers through a geometric tool","doi":"10.46430/phen0038"},"html_body":"<p>{% include toc.html %}</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>A few years back, William Turkel wrote a series of blog posts called <a href=\"http://digitalhistoryhacks.blogspot.com/2008/05/naive-bayesian-in-old-bailey-part-1.html\">A\nNaive Bayesian in the Old Bailey</a>, which showed how one could use\nmachine learning to extract interesting documents out of a digital archive.\nThis tutorial is a kind of an update on that blog essay, with roughly the\nsame data but a slightly different version of the machine learner.</p>\n<p>The idea is to show why machine learning methods are of interest to\nhistorians, as well as to present a step-by-step implementation of\na supervised machine learner. This learner is then applied to the <a href=\"http://www.oldbaileyonline.org/\">Old Bailey digital\narchive</a>, which contains several centuries&#39; worth of transcripts of\ntrials held at the Old Bailey in London. We will be using Python for the\nimplementation.</p>\n<p>One obvious use of machine learning for a historian is document\nselection. If we can get the computer to &quot;learn&quot; what kinds of documents\nwe want to see, we can enlist its help in the always-time-consuming task\nof finding relevant documents in a digital archive (or any other digital\ncollection of documents). We&#39;ll still be the ones reading and\ninterpreting the documents; the computer is just acting as a fetch dog\nof sorts, running to the archive, nosing through documents, and bringing\nus those that it thinks we&#39;ll find interesting.</p>\n<p>What we will do in this tutorial, then, is to apply a machine learner\ncalled Naive Bayesian to data from the Old Bailey digital archive. Our\ngoals are to learn how a Naive Bayesian works and to evaluate how\neffectively it classifies documents into different categories - in\nthis case, trials into offense categories (theft, assault, etc.). This\nwill help us determine how useful a machine learner might be to us as\nhistorians: if it does well at this classification task, it might also\ndo well at finding us documents that belong to a &quot;class&quot; we, given our\nparticular research interests, want to see.</p>\n<p>Step by step, we&#39;ll do the following:</p>\n<ol>\n<li>learn what machine learners do, and look more closely at a popular\nlearner called Naive Bayesian.</li>\n<li>download a set of trial records from the Old Bailey archive.</li>\n<li>write a script that saves the trials as text (removing the XML\nmarkup) and does a couple of other useful things.</li>\n<li>write a couple of helper scripts to assist in testing the learners.</li>\n<li>write a script that tests the performance of the learner.</li>\n</ol>\n<h3 id=\"files-you-will-need\">Files you will need</h3>\n<ul>\n<li><code>save-trialtxts-by-category.py</code></li>\n<li><code>tenfold-crossvalidation.py</code></li>\n<li><code>count-offense-instances.py</code></li>\n<li><code>pretestprocessing.py</code></li>\n<li><code>test-nb-learner.py</code></li>\n<li><code>naivebayes.py</code></li>\n<li><code>english-stopwords.txt</code></li>\n</ul>\n<p><a href=\"/assets/baileycode.zip\">A zip file of the scripts</a> is available. You can also download\n<a href=\"https://doi.org/10.5281/zenodo.13284\">another zip file</a> containing the scripts, the data that we are using and the files that\nresult from the scripts. (The second option is probably easiest if you want to follow along with the lesson,\nsince it gives you everything you need in the correct folder structure.)\nMore information about where to put the files is in the &quot;Preliminaries&quot; section\nof the part where we actually begin to code.</p>\n<p><em>Note: You will not need any Python modules that don&#39;t come with standard\ninstallations, except for <a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup</a> (used in the data creation step,\nnot in the learner code itself).</em></p>\n<h2 id=\"the-old-bailey-digital-archive\">The Old Bailey Digital Archive</h2>\n<p>The <a href=\"http://www.oldbaileyonline.org/\">Old Bailey digital archive</a>\ncontains 197,745 criminal trials held at the Old Bailey, aka the Central\nCriminal Court in London. The trials were held between 1674 and 1913,\nand since the archive provides the full transcript of each trial, many\nof which include testimony by defendants, victims, and witnesses, it&#39;s a\ngreat resource for all kinds of historians interested in the lives of\nordinary people in London.</p>\n<p>What makes the collection particularly useful for our purposes is that\nthe text of each trial is richly annotated with such information as what\ntype of an offense was involved (pocketpicking, assault, robbery,\nconspiracy...), the name and gender of each witness, the verdict, etc.\nWhat&#39;s more, this information has been added to the document in XML\nmarkup, which allows us to extract it easily and reliably. That, in\nturn, lets us train a machine learner to recognize the things we are\ninterested in, and then test the learner&#39;s performance.</p>\n<p>Of course, in the case of the Old Bailey archive, we might not need this\ncomputer-assisted sorting all that badly, since the archive&#39;s curators,\nmaking use of the XML markup, offer us a ready-made <a href=\"http://www.oldbaileyonline.org/forms/formMain.jsp\">search interface</a> that\nlets us look for documents by offense type, verdict, punishment, etc. But\nthat&#39;s exactly what makes the Old Bailey such a good resource for\ntesting a machine learner: we can check how well the learner performs by\nchecking its judgments against the human-annotated information in the\nOld Bailey documents. That, in turn, helps us decide how (or whether) a\nlearner could help us explore other digital document collections, most\nof which are not as richly annotated.</p>\n<h2 id=\"machine-learning\">Machine learning</h2>\n<p>Machine learning can mean a lot of different things, but the most common\ntasks are <a href=\"http://en.wikipedia.org/wiki/Statistical_classification\">classification</a> and <a href=\"http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/\">clustering</a>.</p>\n<p>Classification is performed by supervised learners — &quot;supervised&quot;\nmeaning that a human assistant helps them learn, and only then sends\nthem out to classify by themselves. The basic training procedure is to\ngive the learner labeled data: that is, we give it a stack of things\n(documents, for example) where each of those things is labeled as\nbelonging to a group. This is called training data. The learner then\nlooks at each item in the training data, looks at its label, and\nlearns what distinguishes the groups from each other. To see how well\nthe learner learned, we then test it by giving it data that is similar\nto the training data but that the learner hasn&#39;t seen before and that\nis not labeled. This is called (you guessed it!) test data. How well\nthe learner performs on classifying this previously-unseen data is a\nmeasure of how well it has learned.</p>\n<p>The classic case of a supervised classifier is a program that separates\njunk email (spam) from regular email (ham). Such a program is &quot;trained&quot;\nby giving it a lot of spam and ham to look at, along with the\ninformation of which is which. It then builds a statistical model of\nwhat a spam message looks like versus what a regular email message looks\nlike. So it learns that a message is more likely to be spam if it\ncontains sexual terms, or words like &quot;offer&quot; and &quot;deal&quot;, or, as things\nturn out, <a href=\"http://www.paulgraham.com/spam.html\">&quot;ff0000,&quot; the HTML code for red</a>. It can then apply that\nstatistical model to incoming messages and discard the ones it\nidentifies as spam.</p>\n<p>Clustering is usually a task for unsupervised learners. An unsupervised\nlearner doesn&#39;t get any tips on how the data &quot;ought&quot; to be sorted, but\nrather is expected to discover patterns in the data automatically,\ngrouping the data by the patterns it has discovered. Unlike in\nsupervised classification, in unsupervised clustering we don&#39;t tell the\nlearner what the &quot;right&quot; groups are, or give it any hints on what items\nin the data set should go together. Rather, we give it data with a bunch\nof features, and (often, but not always) we tell it how many groups we\nwant it to create. The features could be anything: in document\nclustering, they are normally words. But clustering isn&#39;t limited to\ngrouping documents: it could also be used in, say, trying to improve\ndiagnoses by clustering patient records. In that task, the features\nwould be various attributes of the patient (age, weight, blood pressure,\npresence and quality of various symptoms etc.) and the clustering\nalgorithm would attempt to create groups that share as many features as\nclosely as possible.</p>\n<p>A side note: Some of you may have come to think of an objection to this supervised/unsupervised distinction: namely, that the clustering method is not entirely &quot;unsupervised&quot; either. After all, we tell it what features it should look at, whether it is words (rather than sentences, or two-word sequences, or something else) in a document, or a list of numeric values in a patient record. The learner never encounters the data entirely unprepared. Quite true. But no matter - the distinction between unsupervised and supervised is useful nevertheless, in that in one we tell the learner what the right answer is, and in the other it comes to us with some pattern it has figured out without an answer key. Each is useful for different kind of tasks, or sometimes for different approaches to the same task.</p>\n<p>In this tutorial, we are dealing with a supervised learner that we train\nto perform document classification. We give our learner a set of\ndocuments along with their correct classes, and then test it on a set of\ndocuments they haven&#39;t seen, with the hope that it will succeed in\nguessing the document&#39;s correct classification.</p>\n<h3 id=\"a-naive-bayesian-learner\">A Naive Bayesian learner</h3>\n<p>A Naive Bayesian is a supervised learner: we give it things marked with\ngroup labels, and its job is basically to learn the probability that a\nthing that looks a particular way belongs in a particular group.</p>\n<p>But why &quot;naive&quot;? And what &quot;Bayesian&quot;?</p>\n<p>&quot;Naive&quot; simply means that the learner makes the assumption that all the\n&quot;features&quot; that make up a document are independent of each other. In our\ncase, the features are words, and so the learner assumes that the\noccurrence of a particular word is completely independent of the\noccurrence of another word. This, of course, is often not true, which is\nwhy we call it &quot;naive.&quot; For example, when we put &quot;new&quot; and &quot;york&quot;\ntogether to form &quot;New York,&quot; the result has a very different meaning\nthan the &quot;new&quot; and &quot;york&quot; in &quot;New clothes for the Prince of York.&quot; If we\nwere to distinguish &quot;New York&quot; from &quot;New&quot; and &quot;York&quot; occurring\nseparately, we might find that each tends to occur in very different\ntypes of documents, and thus not identifying the expression &quot;New York&quot;\nmight throw our classifier off course.</p>\n<p>Despite their simplistic assumption that the occurrence of any\nparticular feature is independent of the occurrence of other features,\nNaive Bayesian classifiers do a good enough job to be very useful in\nmany contexts (much of the real-world junk mail detection is performed\nby Naive Bayesian classifiers, for example). Meanwhile, the assumption\nof independence means that processing documents is much less\ncomputationally intensive, so a Naive Bayesian classifier can handle far\nmore documents in a much shorter time than many other, more complex\nmethods. That in itself is useful. For example, it wouldn’t take too\nlong retrain a Naive Bayesian learner if we accumulated more data. Or we\ncould give it a bigger set of data to begin with; a pile of data that a\nNaive Bayesian could burrow through in a day might take a more complex\nmethod weeks or even months to process. Especially when it comes to\nclassification, more data is often as significant as a better method —\nas Bob Mercer of IBM famously quipped in 1985, “there is no data like\nmore data.”</p>\n<p>As for the &quot;Bayesian&quot; part, that refers to the 18th-century English\nminister, statistician, and philosopher Thomas Bayes. When you google\nfor &quot;Naive Bayesian,&quot; you will turn up a lot of references to &quot;Bayes&#39;\ntheorem&quot; or &quot;Bayes&#39; rule,&quot; which is a formula for applying conditional\nprobabilities (the probability of some thing X, given some other thing\nY).</p>\n<p>Bayes&#39; theorem is related to Naive Bayesian classifiers, in that we can\nformulate the classification question as &quot;what is the probability of\ndocument X, given class Y?&quot; However, unless you&#39;ve done enough math and\nprobability to be comfortable with that kind of thinking, it may not\nprovide the easiest avenue to grasping how a Naive Bayesian classifier\nworks. Instead, let&#39;s look at the classifier in a more procedural\nmanner. (Meanwhile, if you prefer, you can check out <a href=\"http://www.yudkowsky.net/rational/bayes\">an explanation\nof Bayes&#39; rule and conditional probabilities</a> that does a very nice\njob and is also a good read.)</p>\n<h4 id=\"understanding-naive-bayesian-classification-using-a-generative-story\">Understanding Naive Bayesian classification using a generative story</h4>\n<p>To understand Naive Bayesian classification, we will start by telling a\nstory about how documents come into being. Telling such a story — called\na &quot;generative story&quot; in the business — often simplifies the\nprobabilistic analysis and helps us understand the assumptions we&#39;re\nmaking. Telling the story takes a while, so bear with me. There is a\npayoff at the end: the story directly informs us how to build a\nclassifier under the assumptions that the particular generative story\nmakes.</p>\n<p>The fundamental assumption we will make in our generative story is that\ndocuments come into being not as a result of intellectual cogitation but\nas a result of a process whereby words are picked at random out of a bag\nand then put into a document (known as a bag-of-words model).</p>\n<p>So we pretend that historical works, for example, are written in\nsomething like the following manner. Each historian has his or her own\nbag of words with a vocabulary specific to that bag. So when Ann the\nHistorian writes a book, what she does is this:</p>\n<ul>\n<li>She goes to the bag that is her store of words.</li>\n<li>She puts her hand in and pulls out a piece of paper.</li>\n<li>She reads the word on the piece of paper, writes it down in her\nbook, and puts the paper back in the bag.</li>\n<li>Then she again puts her hand in the bag and pulls out a piece of\npaper.</li>\n<li>She writes down that word in the book, and puts the piece of paper\nback in the bag.</li>\n</ul>\n<p>Ann the Historian keeps going until she decides her book (or article, or\nblog post, or whatever) is finished. The next time she wants to write\nsomething, she goes back to her bag of words and does the same thing. If\nher friend John the Historian were to write a book, he would go to his\nown bag, which has a different set of words, and then he would follow\nthe same procedure of taking out a word, writing it down, putting it\nback in. It&#39;s just one damn word after another.</p>\n<p>{% include figure.html filename=&quot;naive-bayesian-1.png&quot; caption=&quot;Bags of Words&quot; %}</p>\n<p><em>(If this procedure sounds familiar, that may be because it sounds a bit\nlike the generative story told in explaining how <a href=\"/lessons/topic-modeling-and-mallet\">topic modeling</a> works.\nHowever, the story in topic modeling is a bit different in that,\nfor instance, each document contains words from more than one class.\nAlso, you should note that topic modeling is unsupervised — you don&#39;t\ntell the modeler what the &quot;right&quot; topics are, it comes up with them all\nby itself.)</em></p>\n<p>So let&#39;s say you are a curator of a library of historical works, and one\nday you discover a huge forgotten trunk in the basement of the library.\nIt turns out that the trunk contains dozens and dozens of typed book\nmanuscripts. After some digging, you find a document that explains that\nthese are transcripts of unpublished book drafts by three historians:\nEdward Gibbon, Carl Becker, and Mercy Otis Warren.</p>\n<p>What a find! But unfortunately, as you begin sorting through the drafts,\nyou realize that they are not marked with the author&#39;s name. What can you\ndo? How can you classify them correctly?</p>\n<p>Well, you do have other writings by these authors. And if historians\nwrite their documents in the manner described above — if each historian\nhas his or her own bag of words with a particular vocabulary and a\nparticular distribution of words — then we can figure out who wrote each\ndocument by looking at the words it contains and comparing the\ndistribution of those words to the distribution of words in documents we\n<em>know</em> were written by Gibbon, Becker, and Warren, respectively.</p>\n<p>So you go to your library stacks and get out all the books by Gibbon,\nBecker, and Warren. Then you start counting. You start with Edward\nGibbon&#39;s <em>oeuvre</em>. For each word in a work by Gibbon, you add the word\nto a list marked &quot;Gibbon.&quot; If the word is already in the list, you add\nto its count. Then you do the same with the works of Mercy Otis Warren\nand Carl Becker. Finally, for each author, you add up the total number\nof words you&#39;ve seen. You also add up the total number of monographs you\nhave examined so you&#39;ll have a metric for how much work each author has\npublished.</p>\n<p>So what you end up with is something like this:</p>\n<table>\n<thead>\n<tr>\n<th>Edward Gibbon (5)</th>\n<th>Carl Becker (18)</th>\n<th>Mercy Otis Warren (2)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>empire, 985</td>\n<td>everyman, 756</td>\n<td>revolution, 989</td>\n</tr>\n<tr>\n<td>rome, 897</td>\n<td>revolution, 699</td>\n<td>constitution, 920</td>\n</tr>\n<tr>\n<td>fall, 887</td>\n<td>philosopher, 613</td>\n<td>principles, 899</td>\n</tr>\n<tr>\n<td>…</td>\n<td>…</td>\n<td>…</td>\n</tr>\n<tr>\n<td>(total), 352,003</td>\n<td>(total), 745,532</td>\n<td>(total), 300,487</td>\n</tr>\n</tbody></table>\n<p>What you have done, in essence, is to reconstruct each historian&#39;s &quot;bag\nof words&quot; — now you know (at least approximately) what words each\nhistorian uses and in what proportions. Armed with this representation\nof the word distributions in the works of Gibbons, Becker, and Warren,\nyou&#39;re ready to tackle the task of figuring out who wrote which\nmanuscripts.</p>\n<p>You&#39;re going to work manuscript by manuscript and author by author,\nfirst pretending that the manuscript you&#39;re currently considering was\nwritten by Gibbons, then that it was written by Becker, and so on. For\neach author, you calculate how likely it is that the manuscript really\nwas written by that author.</p>\n<p>So with your first manuscript in hand, you start by assuming that the\nmanuscript was written by Gibbons. First you figure out the overall\nprobability of any monograph being written by Gibbons rather than either\nof the two others — that is, of the Gibbons bag rather than the Becker\nbag or the Warren bag being used to produce a monograph. You do this by\ntaking the number of books written by Gibbons and dividing it by the\ntotal number of books written by all these authors. That comes out to\n5/25, or 0.2 (20 percent).</p>\n<p>Then, you start looking at the words in the manuscript. Let&#39;s say the\nfirst word is &quot;fall.&quot; You check how often that word occurred in Gibbons&#39;\npublished <em>oeuvre</em>, and you find that the answer is 887. Then you check\nhow many words, overall, there were in Gibbons&#39; total works, and you\nnote that the answer is 352,003. You divide 887 by 352,003 to get the\nproportional frequency (call it <em>p</em>) of &quot;fall&quot; in Gibbons&#39; work\n(0.0025). For the next word, you do the same procedure, and then\nmultiply the probabilities together (you multiply since each action —\npicking an author, or picking a word — represents an independent\nchoice). In the end you end with a tally like this:</p>\n<pre><code>p_bag * p_word_1 * p_word_2 * ... * p_word_n\n</code></pre>\n<p>Note that including the probability of picking the bag (<em>p_bag</em>) is an\nimportant step: if you only go by the words in the manuscript and ignore\nhow many manuscripts (or rather, published works) each author has\nwritten, you can easily go wrong. If Becker has written ten times the\nnumber of books that Warren has, it should reasonably require much\nfirmer evidence in the form of an ample presence of &quot;Warrenesque&quot; words\nto assume that a manuscript was written by Warren than that it was\nwritten by Becker. &quot;Extraordinary claims require extraordinary\nevidence,&quot; as Carl Sagan once said.</p>\n<p>OK, so now you have a total probability of the manuscript having been\nwritten by Gibbons. Next, you repeat the whole procedure with the\nassumption that maybe it was instead written by Becker (that is, that it\ncame out of the bag of words that Becker used when writing). That done,\nyou move on to considering the probability that the author was Warren\n(and if you had more authors, you&#39;d keep going until you had covered\neach of them).</p>\n<p>When you&#39;re done, you have three total probabilities — one probability\nper author. Then you just pick out the largest one, and, as they say,\nBob&#39;s your uncle! That&#39;s the author who most probably wrote this\nmanuscript.</p>\n<p>(Minor technical note: when calculating</p>\n<pre><code>p_bag * p_word1 * ... * p_word_n\n</code></pre>\n<p>in a software implementation we actually work with the\n<a href=\"http://betterexplained.com/articles/using-logs-in-the-real-world/\">logarithms</a> of the probabilities since the numbers\neasily become very small. When doing this, we actually calculate</p>\n<pre><code>log(p_bag) + log(p_word1) + ... + log(p_word_n)\n</code></pre>\n<p>That is, our multiplications turn into additions in line with\nthe rules of logarithms. But it all works out right: the class\nwith the highest number at the end wins.)</p>\n<p>But wait! What if a manuscript contains a word that we&#39;ve never seen\nGibbons use before, but also lots of words he used all the time? Won&#39;t\nthat throw off our calculations?</p>\n<p>Indeed. We shouldn&#39;t let outliers throw us off the scent. So we do\nsomething very &quot;Bayesian&quot;: we put a &quot;prior&quot; on each word and each class\n— we pretend we&#39;ve seen all imaginable words at least (say) once in each\nbag, and that each bag has produced at least (say) one document. Then we\nadd those fake pretend counts — called <a href=\"http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_introbayes_sect004.htm\">priors</a>, or pseudocounts — to\nour real counts. Now, no word or bag gets a count of zero.</p>\n<p>In fact, we can play around with the priors as much as we like: they&#39;re\nsimply a way of modeling our &quot;prior belief&quot; in the probability of one\nthing over another. They could model our assumptions about a particular\nauthor being more likely than others, or a particular word being more\nlikely to have come from the bag of a specific author, and so on. Such\nbeliefs are &quot;prior&quot; in the sense that we hold the belief before we&#39;ve\nseen the evidence we are considering in the actual calculation we are\nmaking. So above, for example, we could add a little bit to Mercy Otis\nWarren&#39;s <em>p_bag</em> number if we thought it likely that as a woman, she\nmight well have had a harder time getting published, and so there might\nreasonably be more manuscript material from her than one might infer\nfrom a count of her published monographs.</p>\n<p>In some cases, priors can make a Naive Bayesian classifier much more\nusable. Often when we&#39;re classifying, after all, we&#39;re not after some\nabstract &quot;truth&quot; — rather, we simply want a useful categorization. In\nsome cases, it&#39;s much more desirable to be mistaken one way than\nanother, and we can model that with proper class priors. The classic\nexample is, again, sorting email into junk mail and regular mail piles.\nObviously, you really don&#39;t want legitimate messages to be deleted as\nspam; that could do much more damage than letting a few junk messages\nslip through. So you set a big prior on the &quot;legitimate&quot; class that\ncauses your classifier to only throw out a message as junk when faced\nwith some hefty evidence. By the same token, if you&#39;re sorting the\nresults of a medical test into &quot;positive&quot; and &quot;negative&quot; piles, you may\nwant to weight the positive more heavily: you can always do a second\ntest, but if you send the patient home telling them they&#39;re healthy when\nthey&#39;re not, that might not turn out so well.</p>\n<p>So there you have it, step by step. You have applied a Naive Bayesian to\nthe unattributed manuscripts, and you now have three neat piles. Of\ncourse, you should keep in mind that Naive Bayesian classifiers are not\nperfect, so you may want to do some further research before entering the\nnewfound materials into the library catalog as works by Gibbons, Becker,\nand Warren, respectively.</p>\n<h2 id=\"ok-so-lets-code-already\">OK, so let&#39;s code already!</h2>\n<p>So, our aim is to apply a Naive Bayesian learner to data from the Old\nBailey. First we get the data; then we clean it up and write some\nroutines to extract information from it; then we write the code that\ntrains and tests the learner.</p>\n<p>Before we get into the nitty-gritty of downloading the files and\nexamining the training/testing script, let&#39;s just summarize what our aim\nis and what the basic procedure looks like.</p>\n<p>We want to have our Naive Bayesian read in trial records from the Old\nBailey and do with them the same thing as we did above in the examples\nabout the works of Gibbons, Becker, and Warren. In that example, we used\nthe published works of these authors to reconstruct each historian&#39;s bag\nof words, and then used that knowledge to decide which historian had\nwritten which unattributed manuscripts. In classifying the Old Bailey\ntrials, we will give the learner a set of trials labeled with the\noffense for which the defendant was indicted so it can figure out the\n&quot;bag of words&quot; that is associated with that offense. Then the learner\nwill use that knowledge to classify another set of trials where we have\nnot given it any information about the offense involved. The goal is to\nsee how well the learner can do this: how often does it label an\nunmarked trial with the right offense?</p>\n<p>The procedure used in the scripts we employ to train the learner is no\nmore complicated than the one in the historians-and-manuscripts example.\nBasically, each trial is represented as a list of words, like so:</p>\n<pre><code>michael, carney, was, indicted, for, stealing, on, the, 22nd, of, december, 26lb, weight, of, nails, value, 7s, 18, dozen, of, screws, ...\n... , the, prisoners, came, to, my, shop, on, the, night, in, question, and, brought, in, some, ragged, pieces, of, beef, ...\n..., i, had, left, my, door, open, and, when, i, returned, i, missed, all, this, property, i, found, it, at, the, pawnbroker, ...\n</code></pre>\n<p>When we train the learner, we give it a series of such word lists, along\nwith their correct bag labels (correct offenses). The learner then\ncreates word lists for each bag (offense), so that it ends up with a set\nof counts similar to the counts we created for Gibbons, Becker, and\nWarren, one count for each offense type (theft, deception, etc.)</p>\n<p>When we test the learner, we feed it the same sort of word lists\nrepresenting other trials. But this time we don&#39;t give it the\ninformation about what offense was involved. Instead, the learner does\nwhat we did above: when it gets a list of words, it compares that list\nto the word counts for each offense type, calculating which offense type\nhas a bag of words most similar to this list of words. It works offense\nby offense, just like we worked author by author. So first it assumes\nthat the trial involved, say, the offense &quot;theft&quot;. It looks at the first\nword in the trial&#39;s word list, checks how often that word occurred in\nthe &quot;theft&quot; bag, performs its probability calculations, moves on to the\nnext word, and so on. Then it checks the trial&#39;s word list against the\nnext category, and the next, until it has gone through each offense.\nFinally it tallies up the probabilities and labels the trial with the\noffense category that has the highest probability.</p>\n<p>Finally, the testing script evaluates the performance of the learner and\nlets us know how good it was at guessing the offense associated with\neach trial.</p>\n<h3 id=\"preliminaries\">Preliminaries</h3>\n<p>Many of the tools we are using to deal with the preliminaries have been\ndiscussed at Programming Historian before. You may find it helpful to\ncheck out (or revisit) the following tutorials:</p>\n<ul>\n<li>Milligan &amp; Baker, <a href=\"/lessons/intro-to-bash\">Introduction to the Bash Command Line</a></li>\n<li>Milligan, <a href=\"/lessons/automated-downloading-with-wget\">Automated Downloading with wget</a></li>\n<li>Knox, <a href=\"/lessons/understanding-regular-expressions\">Understanding Regular Expressions</a></li>\n<li>Wieringa, <a href=\"/lessons/intro-to-beautiful-soup\">Intro to Beautiful Soup</a></li>\n</ul>\n<p>A few words about the file structure the scripts assume/create:</p>\n<p>I have a &quot;top-level&quot; directory, which I&#39;m calling <em>bailey</em> (you\ncould call it something else, it&#39;s not referenced in the code). Under\nthat I have two directories: <em>baileycode</em> and <em>baileyfiles</em>.\nThe first contains all the scripts; the second contains the files\nthat are either downloaded or created by the scripts. That in turn\nhas subdirectories; all except one (for the downloaded XML\nfiles — see below) are created by the scripts.</p>\n<p>If you downloaded the complete zip package with all the files and\nscripts, you automatically get the right structure; just unpack it in\nits own directory. The only files that are omitted from that are the zip\nfiles of trials downloaded below (if you got the complete package, you\nalready have the unpacked contents of those files, and the zips would\njust take up unnecessary space).</p>\n<p>If you only downloaded the scripts, you should do the following:</p>\n<ul>\n<li>Create a directory and name it something sensible (say, <em>bailey</em>).</li>\n<li>In that directory, create another directory called <em>baileycode</em> and\nunpack the contents of the script zip file into that directory\n(make sure you don&#39;t end up with two <em>baileycode</em> directories inside\none another).</li>\n<li>In the same directory (<em>bailey</em>), create another directory called\n<em>baileyfiles</em>.</li>\n</ul>\n<p>On my Mac, the structure looks like this:</p>\n<p>{% include figure.html filename=&quot;naive-bayesian-2.png&quot; caption=&quot;Bailey Folders&quot; %}</p>\n<h4 id=\"downloading-trials\">Downloading trials</h4>\n<p>The Old Bailey lets you download trials in zip files of 10 trials each,\nso that&#39;s what we&#39;ll do. This is how we do it: we first look at how the\nOld Bailey system requests files, and then we write a script that\ncreates a file with a bunch of those requests. Then we feed that file to\nwget, so we don&#39;t have to sit by our computers all day downloading each\nset of 10 trials that we want.</p>\n<p>As explained on the Old Bailey <a href=\"http://www.oldbaileyonline.org/static/DocAPI.jsp\">documentation for developers</a> page, this\nis what the http request for a set of trials looks like:</p>\n<pre><code>http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=211&amp;return=zip\n</code></pre>\n<p>As you see, we can request all trials that took place between two\nspecified dates (<em>fromdate</em> and <em>todate</em>). The <em>count</em> specifies how\nmany trials we want, and the <em>start</em> variable says where in the results\nto start (in the above, we start with result number 211 and get the ten\nfollowing trials). Ten seems to be the highest number allowed for\n<em>count</em>, so we need to work around that.</p>\n<p>We get around the restriction for how many trials can be in a zip file\nwith a little script that builds as many of the above type of requests\nas we need to get all trials from the 1830s. We can find out how many\ntrials that is by going to the <a href=\"http://www.oldbaileyonline.org/forms/formMain.jsp\">Old Bailey search page</a> and entering\nJanuary 1830 as the start date, December 1839 as the end date, and\nchoosing &quot;Old Bailey Proceedings &gt; trial accounts&quot; in the &quot;Search In&quot;\nfield. Turns out there were 22,711 trials in the 1830s.</p>\n<p>Here&#39;s the whole script (<code>wgetxmls.py</code>) that creates the list of http requests we need:</p>\n<pre><code class=\"language-python\">    mainoutdirname = &#39;../baileyfiles/&#39;\n    wgets = &#39;&#39;\n    for x in range(0,22711,10):\n        getline = &#39;http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=&#39; + str(x+1) + &#39;&amp;return=zip\\n&#39;\n        wgets += getline\n    filename = mainoutdirname + &#39;wget1830s.txt&#39;\n    with open(filename,&#39;w&#39;) as f:\n        f.write(wgets)\n</code></pre>\n<p>As you see, we accept the limitation of 10 trials at a time, but\nmanipulate the start point until we have covered all the trials from the\n1830s.</p>\n<p>Assuming you&#39;re in the <em>baileycode</em> directory, you can run the script\nfrom the command line like this:</p>\n<pre><code class=\"language-bash\">python wgetxmls.py\n</code></pre>\n<p>What that gets us is a file that looks like this:</p>\n<pre><code>http://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=1&amp;return=zip\nhttp://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=11&amp;return=zip\nhttp://www.oldbaileyonline.org/obapi/ob?term0=fromdate_18300114&amp;term1=todate_18391216&amp;count=10&amp;start=21&amp;return=zip\n...\n</code></pre>\n<p>This file is saved in the <em>baileyfiles</em> directory; it is called <code>wget1830s.txt</code>.</p>\n<p>To download the trials, create a new directory under <em>baileyfiles</em>;\ncall it <em>trialzips</em>. Then go into that directory and call <em>wget</em> with the\nfile we just created. So, assuming you are still in the <em>baileycode</em> directory,\nyou would write the following commands on the command line:</p>\n<pre><code class=\"language-bash\">cd ../baileyfiles\nmkdir trialzips\ncd trialzips\nwget -w 2 -i ../wget1830s.txt\n</code></pre>\n<p>The &quot;-w 2&quot; is just to be polite and not overload the server; it tells\n<em>wget</em> to wait 2 seconds between each request. The &quot;-i&quot; flag tells <em>wget</em>\nthat it should request the URLs found in <code>wget1830s.txt</code>.</p>\n<p>What <em>wget</em> returns is a lot of zip files that have unwieldy names and no\nextension. You should rename these so that the extension is &quot;.zip&quot;.\nThen, in the directory <em>baileyfiles</em>, create a subdirectory called\n<em>1830s-trialxmls</em> and then unpack the zips into that so that it\ncontains 22,170 XML files that each look like <code>t18391216-388.xml</code>. Assuming\nyou are still in the <em>trialzips</em> directory, you would write:</p>\n<pre><code class=\"language-bash\">for f in * ; do mv $f $f.zip; done;\nmkdir ../1830s-trialxmls\nunzip &quot;*.zip&quot; -d ../1830s-trialxmls/\n</code></pre>\n<p>If you open one of the trial XMLs in a browser, you can see that it\ncontains all kinds of useful information: name and gender of defendant,\nname and gender of witnesses, type of offense, and so on. Here&#39;s a\nsnippet from one trial:</p>\n<pre><code class=\"language-xml\">&lt;persname id=&quot;t18300114-2-defend110&quot; type=&quot;defendantName&quot;&gt;\nTHOMAS TAYLOR\n    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;surname&quot; value=&quot;TAYLOR&quot;&gt;\n    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;given&quot; value=&quot;THOMAS&quot;&gt;\n    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;gender&quot; value=&quot;male&quot;&gt;\n    &lt;interp inst=&quot;t18300114-2-defend110&quot; type=&quot;age&quot; value=&quot;25&quot;&gt;\n&lt;/interp&gt;&lt;/interp&gt;&lt;/interp&gt;&lt;/interp&gt;&lt;/persname&gt;\nwas indicted for\n    &lt;rs id=&quot;t18300114-2-off7&quot; type=&quot;offenceDescription&quot;&gt;\n        &lt;interp inst=&quot;t18300114-2-off7&quot; type=&quot;offenceCategory&quot; value=&quot;violentTheft&quot;&gt;\n        &lt;interp inst=&quot;t18300114-2-off7&quot; type=&quot;offenceSubcategory&quot; value=&quot;robbery&quot;&gt;\n            feloniously assaulting\n        &lt;persname id=&quot;t18300114-2-victim112&quot; type=&quot;victimName&quot;&gt;\n            David Grant\n                  &lt;interp inst=&quot;t18300114-2-victim112&quot; type=&quot;surname&quot; value=&quot;Grant&quot;&gt;\n            &lt;interp inst=&quot;t18300114-2-victim112&quot; type=&quot;given&quot; value=&quot;David&quot;&gt;\n            &lt;interp inst=&quot;t18300114-2-victim112&quot; type=&quot;gender&quot; value=&quot;male&quot;&gt;\n            &lt;join result=&quot;offenceVictim&quot; targorder=&quot;Y&quot; targets=&quot;t18300114-2-off7 t18300114-2-victim112&quot;&gt;\n        &lt;/join&gt;&lt;/interp&gt;&lt;/interp&gt;&lt;/interp&gt;&lt;/persname&gt;\n&lt;/interp&gt;&lt;/interp&gt;&lt;/rs&gt;\n</code></pre>\n<p>The structured information in the XML lets us reliably extract the\n&quot;classes&quot; we want to sort our documents into. We are going to classify\nthe trials by offense category (and subcategory), so that&#39;s the\ninformation we&#39;re going to extract before converting the XML into a text\nfile that we can then feed to our learner.</p>\n<h4 id=\"saving-the-trials-into-text-files\">Saving the trials into text files</h4>\n<p>Now that we have the XML files, we can start extracting information and\nplain text from them to feed to our learner. We want to sort the trials\ninto text files, so that each text file contains all the trials in a\nparticular offense category (theft-simplelarceny, breakingpeace-riot,\netc.).</p>\n<p>We also want to create a text file that contains all the trial IDs\n(marked in the XML), so we can use that to easily create\ncross-validation samples. The reasons for doing this are discussed below\nin the section &quot;Creating the cross-validation samples&quot;.</p>\n<p>The script that does these things is called <code>save-txttrials-by-category.py</code>\nand it&#39;s pretty extensively commented, so I&#39;ll just note a few things here.</p>\n<ol>\n<li>We strip the trial text of all punctuation, including quote marks\nand parentheses, and we equalize all spaces (newlines, tabs,\nmultiple spaces) into a single space. This helps us simplify the\ncoding of the training process (and, incidentally, keep the code\nthat trains the learner general enough that as long as you have text\nfiles saved in the same format as we use here, you should be able to\napply it more or less directly to your data).</li>\n<li>That of course makes the text hard to read for a human. Therefore,\nwe also save the text of each trial into a file named after the\ntrial id, so that we can easily examine a particular trial if we\nwant to (which we will).</li>\n</ol>\n<p>The script creates the following directories and files under <em>baileyfiles</em>:</p>\n<ul>\n<li>Directory <em>1830s-trialtxts</em>: this will\ncontain the text file versions of the trials after they have been\nstripped of all XML formatting. Each file\nis named after the trial&#39;s ID.</li>\n<li>Directory <em>1830s-trialsbycategory</em>: this\nwill contain the text files that represent all the text in all the\ntrials belonging to a particular category. These are named after the\ncategory, e.g., <code>theft-simplelarceny.txt</code>. Each category file\ncontains all the trials in that category, with one trial per line.</li>\n<li>File <code>trialids.txt</code>. This contains the\nsorted list of trial IDs, one ID per line; we will use it later in\ncreating cross-validation samples for training the learner (this is\nthe next step).</li>\n<li>Files <code>offensedict.json</code> and <code>trialdict.json</code>. These json files will come\ninto use in training the learner.</li>\n</ul>\n<p>So if you&#39;re still in the <em>trialxmls</em> directory, you would write the\nfollowing commands to run this script:</p>\n<pre><code class=\"language-bash\">cd ../../baileycode/\npython save-trialtxts-by-category.py\n</code></pre>\n<p>This will take a while. After it&#39;s done, you should have the directories\nand files described above.</p>\n<h4 id=\"creating-the-cross-validation-samples\">Creating the cross-validation samples</h4>\n<p>Now that we have all our trials saved where we want them, all we need to\ndo is to create the cross-validation samples and we&#39;re ready to test our\nlearners.</p>\n<p>Cross-validation simply means repeatedly splitting our data into chunks,\nsome of which we use for training and others for testing. Since the idea\nis to get a learner to extract information from one set of documents\nthat it can then use to determine the class of documents it has never\nseen, we obviously have to reserve a set of documents that are unknown\nto the learner if we want to test its performance. Otherwise it&#39;s a bit\nlike letting your students first read an exam <em>and its answers</em> and then\nhave them take that same exam. That would only tell you how closely they\nread the actual exam, not whether they&#39;ve learned something more\ngeneral.</p>\n<p>So what you want to do is to test the learner on data it hasn&#39;t seen\nbefore, so that you can tell whether it has learned some general\nprinciples from the training data. You could just split your data into\ntwo sets, using, say, 80 percent for training and 20 percent for\ntesting. But a common practice is to split your data repeatedly into\ndifferent test and train sets, so that you can ensure that your test\nresults aren&#39;t the consequence of some oddball quirk in the portion of\ndata you left for testing.</p>\n<p>Two scripts are involved in creating the cross-validation sets. The\nscript <code>tenfold-crossvalidation.py</code> creates\nthe samples. It reads in the list of trial ids we created in the\nprevious step, shuffles that list to make it random, and divides it into\n10 chunks of roughly equal length (that is, a roughly equal number of\ntrial ids). Then it writes those 10 chunks each into its own text file,\nso we can read them into our learner code later. Next, to be meticulous,\nwe can run the <code>count-offense-instances.py</code>\nto confirm that if we are interested in a particular trial category,\nthat category is reasonably evenly distributed across the samples.</p>\n<p>Before you run the <code>count-offense-instances.py</code> script, you should\nedit it to set the category to the one you&#39;re interested in and let the\nscript know whether we&#39;re looking at a broad or a specific category.\nThis is what the relevant part of the code looks like:</p>\n<pre><code class=\"language-python\">indirname = &#39;../baileyfiles/&#39;\noffensedict_fn = indirname + &#39;offensedict.json&#39;\noffensecat = &#39;breakingpeace&#39; #change to target category\nbroadcat = True #set true if category is e.g. &quot;theft&quot; instead of &quot;theft-simplelarceny&quot;\n</code></pre>\n<p>And here are the commands to run the cross-validation scripts (assuming\nyou are still in the <em>baileycode</em> directory).</p>\n<pre><code class=\"language-bash\">python tenfold-crossvalidation.py\npython count-offense-instances.py\n</code></pre>\n<p>Alternatively, you can run them using <a href=\"http://pypy.org/\">pypy</a>, which is\nquite a bit faster.</p>\n<pre><code class=\"language-bash\">pypy tenfold-crossvalidation.py\npypy count-offense-instances.py\n</code></pre>\n<p>The output of the <code>count-offense-instances.py</code> script looks like\nthis:</p>\n<pre><code>Offense category checked for: breakingpeace\nsample0.txt: 31\nsample1.txt: 25\nsample2.txt: 32\nsample3.txt: 25\nsample4.txt: 36\nsample5.txt: 33\nsample6.txt: 29\nsample7.txt: 35\nsample8.txt: 27\nsample9.txt: 31\n</code></pre>\n<p>From the output, we can conclude that the distribution of instances of\n&quot;breakingpeace&quot; is more or less even. If it isn&#39;t, we can re-run the\n<code>tenfold-crossvalidation.py</code> script, and then check the distribution again.</p>\n<h3 id=\"testing-the-learner\">Testing the learner</h3>\n<p>All right, we are ready to train and test our Naive Bayesian! The script\nthat does this is called <code>test-nb-learner.py</code>. It starts by defining a few\nvariables:</p>\n<pre><code class=\"language-python\">categoriesdir = &#39;../baileyfiles/1830s-trialsbycategory/&#39;\nsampledirname = &#39;../baileyfiles/Samples_1830s/&#39; #location of 10-fold cross-validation\nstopwordfilename = &#39;../baileyfiles/english-stopwords.txt&#39;\n# the ones below should be set to None if not using\ncattocheck = &#39;breakingpeace&#39; #if evaluating recognition one category against rest\npattern = &#39;[^-]+&#39; #regex pattern to use if category is not complete filename\n</code></pre>\n<p>Most of these are pretty self-explanatory, but note the two last ones.\nThe variable &quot;cattocheck&quot; determines whether we are looking to identify\na specific category or to sort each trial into its proper category (the\nlatter is done if the variable is set to None). The variable &quot;pattern&quot;\ntells us whether we are using the whole file name as the category\ndesignation, or only a part of it, and if the latter, how to identify\nthe part. In the example above, we are focusing on the broad category\n&quot;breakingpeace&quot;, and so we are not using the whole file name, which\nwould be e.g. &quot;breakingpeace-riot&quot; but only the part before the dash.\nBefore you run the code, you should set these variables to what you want\nthem to be.</p>\n<p>Note that &quot;cattocheck&quot; here should match the &quot;offensecat&quot; that you\nchecked for with the <code>count-offense-instances.py</code> script. No error is\nproduced if it does not match, and it&#39;s fairly unlikely that it will\nhave any real impact, but if the categories don&#39;t match, then of course\nyou have no assurance that the category you&#39;re actually interested in is\nmore or less evenly distributed across the ten cross-validation samples.</p>\n<p>Note also that you can of course set &quot;cattocheck&quot; to &quot;None&quot; and leave\nthe pattern as it is, in which case you will be sorting into the broader\ncategories.</p>\n<p>So, with the basic switches set and knobs turned, we begin by reading in\nall the trials that we have saved. We do this with the function called\n<em>process_data</em> that can be found in the <code>pretestprocessing.py</code> file. (That file\ncontains functions that are called from the scripts you will run, so it\nisn&#39;t something you&#39;ll run directly at any point.)</p>\n<pre><code class=\"language-python\">print &#39;Reading in the data...&#39;\ntrialdata = ptp.process_data(categoriesdir,stopwordfilename,cattocheck,pattern)\n</code></pre>\n<p>The <em>process_data</em> function reads in all\nthe files in the directory that contains our trial category files, and\nprocesses them so that we get a list containing all the categories and\nthe trials belonging to them, with the trial text lowercased and\ntokenized (split into a list of words), minus stopwords (common words\nlike a, the, me, which, etc.) Each trial begins with its id number, so\nthat&#39;s one of our words (though we ignore it in training and testing).\nLike this:</p>\n<pre><code>    [\n     [breakingpeace,\n       [&#39;trialid&#39;,&#39;victim&#39;,&#39;peace&#39;,&#39;disturbed&#39;,&#39;man&#39;,&#39;tree&#39;,...]\n       [&#39;trialid&#39;,&#39;dress&#39;,&#39;blood&#39;,&#39;head&#39;,&#39;incited&#39;,...]\n      ...]\n     [theft,\n       [&#39;trialid&#39;,&#39;apples&#39;,&#39;orchard&#39;,&#39;basket&#39;,&#39;screamed&#39;,&#39;guilty&#39;,....]\n       [&#39;trialid&#39;,&#39;rotten&#39;,&#39;fish&#39;]\n      ...]\n    ]\n</code></pre>\n<p>Next, making use of the results of the ten-fold cross-validation routine\nwe created, we loop through the files that define the\nsamples, each time making one sample the test set and the rest the train\nset. Then we split &#39;trialdata&#39;, the list of trials-by-category that we\njust created, into train and test sets accordingly. The functions that\ndo these two steps are <em>create_sets</em> and\n<em>splittraintest</em>, both in the <code>pretestprocessing.py</code> file.</p>\n<p>Now we train our Naive Bayesian classifier on the train set. The\nclassifier we are using (which is included in the scripts zip file) is\none written by Mans Hulden, and it does pretty much exactly what the\n&quot;identify the author of the manuscript&quot; example above\ndescribes.</p>\n<pre><code class=\"language-python\">    # split train and test\n    print &#39;Creating train and test sets, run {0}&#39;.format(run)\n    trainsetids, testsetids = ptp.create_sets(sampledirname,run)\n    traindata, testdata = ptp.splittraintest(trainsetids,testsetids,trialdata)\n\n    # train learner\n    print &#39;Training learner, run {0}...&#39;.format(run)\n    mynb = nb.naivebayes()\n    mynb.train(traindata)\n</code></pre>\n<p>After the learner is trained, we are ready to test how well the it\nperforms. Here&#39;s the code:</p>\n<pre><code class=\"language-python\">    print &#39;Testing learner, run {0}...&#39;.format(run)\n\n    for trialset in testdata:\n        correctclass = trialset[0]\n        for trial in trialset[1:]:\n            result = mynb.classify(trial)\n            guessedclass =  max(result, key=result.get)\n            # then record correctness of classification result\n            # note that first version does a more complex evaluation\n            # ... for two-way (one class against rest) classification\n            if cattocheck:\n                if correctclass == cattocheck:\n                    catinsample += 1\n                if guessedclass == cattocheck:\n                     guesses += 1\n                     if guessedclass == correctclass:\n                         hits += 1\n            if guessedclass == correctclass:\n                correctguesses += 1\n\n            total +=1\n</code></pre>\n<p>So we loop through the categories in the &quot;testdata&quot; list (which is of\nthe same format as the &quot;trialdata&quot; list). For each\ncategory, we loop through the trials in that category, classifying each\ntrial with our Naive Bayesian classifier, and comparing the result to\nthe correct class (saved in the first element of each category list\nwithin the testdata list.) Then we add to various counts to be able to\nevaluate the results of the whole classification exercise.</p>\n<p>To run the code that trains and tests the learner, first make sure you\nhave edited it to set the &quot;cattocheck&quot; and &quot;pattern&quot; switches, and then\ncall it on the command line (assuming you&#39;re still in the directory\n<em>baileycode</em>):</p>\n<pre><code class=\"language-bash\">    python test-nb-learner.py\n</code></pre>\n<p>Again, for greater speed, you can also use pypy:</p>\n<pre><code class=\"language-bash\">    pypy test-nb-learner.py\n</code></pre>\n<p>The code will print out some accuracy measures for the classification\ntask you have chosen. The output should look something like this:</p>\n<pre><code>Reading in the data...\nCreating train and test sets, run 0\nTraining learner, run 0...\nTesting learner, run 0...\nCreating train and test sets, run 1\nTraining learner, run 1...\n...\nTraining learner, run 9...\nTesting learner, run 9...\nSaving correctly classified trials and close matches...\nCalculating accuracy of classification...\nTwo-way classification, target category breakingpeace.\nAnd the results are:\nAccuracy 99.00%\nPrecision: 61.59%\nRecall: 66.45%\nAverage number of target category trials in test sample per run: 30.4\nAverage number of trials in test sample per run: 2271.0\nObtained in 162.74 seconds\n</code></pre>\n<p>Next, let&#39;s take a look at what these measures of accuracy mean.</p>\n<h4 id=\"measures-of-classification\">Measures of classification</h4>\n<p>The basic measure of classification prowess is <em>accuracy</em>: how often did\nclassifier guess the class of a document correctly? This is calculated\nby simply dividing the number of correct guesses by the total number of\ndocuments considered.</p>\n<p>If we&#39;re interested in a specific category, we can extract a bit more\ndata. So if we set, for example, cattocheck =\n&#39;breakingpeace&#39;, like above, we can then examine how well the classifier\ndid with respect to the &quot;breakingpeace&quot; category in particular.</p>\n<p>So, in the testlearner code, if we&#39;re doing\nmultiway classification, we only record how many trials we&#39;ve seen\n(&quot;total&quot;) and how many of our guesses were correct (&quot;correctguesses&quot;).\nBut if we&#39;re considering a single category, say &quot;breakingpeace,&quot; we\nrecord a few more numbers. First, we keep track of how many trials\nbelonging to the category &quot;breakingpeace&quot; there are in our test sample\n(this tally is in &quot;catinsample&quot;). We also keep track of how many times\nwe&#39;ve guessed that a trial belongs to the &quot;breakingpeace&quot; category\n(&quot;guesses&quot;). And finally we record how many times we have guessed\n<em>correctly</em> that a trial belongs to &quot;breakingpeace&quot; (&quot;hits&quot;).</p>\n<p>Now that we have this information, we can use it to calculate a couple\nof standard measures of classification efficiency: <em>precision</em> and\n<em>recall</em>. Precision tells us how often we correctly guessed that a\ntrial was in the &quot;breakingpeace&quot; category. Recall lets us know what\nproportion of the &quot;breakingpeace&quot; trials we caught.</p>\n<p>Let&#39;s take another example to clarify precision and recall. Imagine you\nwant all the books on a particular topic — World War I, say — from your\nuniversity library. You send out one of your many minions (all\nhistorians possess droves of them, as you know) to get the books. The\nminion dutifully returns with a big pile.</p>\n<p>Now, suppose you were in possession of a list that contained of every\nsingle book in the library on WWI and no books that weren&#39;t related to\nthe WWI. You could then check the precision and recall of your minion\nwith regard to the category of &quot;books on WWI.&quot;</p>\n<p>Recall is the term for the proportion of books on WWI in the library\nthat your minion managed to grab. That is, the more books on WWI\nremaining in the library after your minion&#39;s visit, the lower your\nminion&#39;s recall.</p>\n<p>Precision, in turn, is the term for the proportion of books in the pile\nbrought by your minion that actually had to do with WWI. The more\nirrelevant (off-topic) books in the pile, the lower the precision.</p>\n<p>So, say the library has 1,000 books on WWI, and your minion lugs you a\npile containing 400 books, of which 300 have nothing to do with WWI. The\nminion&#39;s recall would be (400-300)/1,000, or 10 percent. The minion&#39;s\nprecision, in turn, would be (400-300)/400, or 25 percent.</p>\n<p>(Should have gone yourself, eh?)</p>\n<p>A side note: the minion&#39;s overall accuracy — correct guesses divided by\nactual number of examples — would be:</p>\n<pre><code>(the number of books on WWI in your pile - the number of books *not* on\nWWI in your pile + the number of books in the library *not* on WWI)\n------------------------------------------------------------------------\nthe total number of books in the library\n</code></pre>\n<p>So if the library held 100,000 volumes, this would be (100 - 300 +\n99,000) / 100,000 — or 98.8 percent. That seems like a great number, but\nsince it merely means that your minion was smart enough to leave most of\nthe library books in the library, it&#39;s not very helpful in this case\n(except inasmuch as it is nice not to be buried under 100,000 volumes.)</p>\n<h4 id=\"how-well-does-our-naive-bayesian-do\">How well does our Naive Bayesian do?</h4>\n<p>Our tests on the Naive Bayesian use the data set consisting of all the\ntrials from the 1830s. It contains 17,549 different trials in 50\ndifferent offense categories (which can be grouped into 9 broad\ncategories).</p>\n<p>If we run the Naive Bayesian so that it attempts to sort all trials into\ntheir correct broad categories, its accuracy is pretty good: 94.3\npercent. So 94 percent of the time, when it considers how it should sort\ntrials in the test sample into &quot;breakingpeace,&quot; &quot;deception,&quot; &quot;theft,&quot;\nand so on, it chooses correctly.</p>\n<p>For the more specific categories (&quot;theft-simplelarceny,&quot;\n&quot;breakingpeace-riot,&quot; and so on) the same exercise is much less\naccurate: then, the classifier gets it right only 72 percent of the\ntime. That&#39;s no wonder, really, given that some of the categories are so\nsmall that we barely have any examples. We might do a bit better with\nmore data (say, all the trials from the whole 19th century, instead of\nonly all the trials from the 1830s).</p>\n<p>The first, overall category results are pretty impressive. They give us\nquite a bit of confidence that if what we needed to do was to sort\ndocuments into piles that weren&#39;t all too fine-grained, and we had a\nnice bunch of training data, a Naive Bayesian could do the job for us.</p>\n<p>But the problem for a historian is often rather different. A historian\nusing a Naive Bayesian learner is more likely to want to separate\ndocuments that are &quot;interesting&quot; from documents that are &quot;not\ninteresting&quot; — usually meaning documents dealing with a particular issue\nor not dealing with it. So the question is really more one where we have\na mass of &quot;uncategorized&quot; or &quot;other&quot; documents and a much smaller set of\n&quot;interesting&quot; documents, and we try to find more of the latter among the\nformer.</p>\n<p>In our current exercise, that situation is fairly well represented by\ntrying to identify documents from a single category in the mass of the\nrest of the documents, set to category &quot;other.&quot; So how well are we able\nto do that? In other words, if we set cattocheck\n= &#39;breakingpeace&#39; (or another category) so that all trials get\nmarked as either that category or as &quot;other,&quot; and then run the\nclassifier, what kinds of results do we get?</p>\n<p>Well, our overall accuracy is still high: over 95 percent in all cases\nfor the broad categories, and usually about that for the detailed ones\nas well. But just like the minion going off to the library to get books\non WWI had a pretty high accuracy because he/she didn&#39;t bring back half\nthe library, in this case, too, our accuracy is mostly just due to the\nfact that we manage to not misidentify <em>too</em> many &quot;other&quot; trials as\nbeing in the category we&#39;re interested in. Because there are so many\n&quot;other&quot; trials, those correct assessments far outweigh the minus points\nwe may have gotten from missing interesting trials.</p>\n<p>Precision and recall, therefore, are more in this case more interesting\nmeasures than overall accuracy. Here&#39;s a table showing precision and\nrecall for each of the &quot;broad&quot; categories in our trial sample, and for a\nfew sample detailed categories. The last column shows how many target\ncategory trials there were in the test set on average (remember, we did\nten runs with different train/test splits, so all our results are\naverages of that).</p>\n<p><strong>Naive Bayesian classifier, two-way classification, 10-fold\ncross-validation</strong></p>\n<p><strong>Broad categories</strong></p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Precision (%)</th>\n<th>Recall (%)</th>\n<th>Avg # trials in cat in TeS</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>breakingpeace</td>\n<td>63.52</td>\n<td>64.05</td>\n<td>24.2</td>\n</tr>\n<tr>\n<td>damage</td>\n<td>0.00</td>\n<td>0.00</td>\n<td>1.2</td>\n</tr>\n<tr>\n<td>deception</td>\n<td>53.47</td>\n<td>61.43</td>\n<td>47.7</td>\n</tr>\n<tr>\n<td>kill</td>\n<td>62.5</td>\n<td>89.39</td>\n<td>17.9</td>\n</tr>\n<tr>\n<td>miscellaneous</td>\n<td>47.83</td>\n<td>4.44</td>\n<td>24.8</td>\n</tr>\n<tr>\n<td>royaloffenses</td>\n<td>85.56</td>\n<td>91.02</td>\n<td>42.3</td>\n</tr>\n<tr>\n<td>sexual</td>\n<td>93.65</td>\n<td>49.17</td>\n<td>24.0</td>\n</tr>\n<tr>\n<td>theft</td>\n<td>96.26</td>\n<td>98.75</td>\n<td>1551.8</td>\n</tr>\n<tr>\n<td>violenttheft</td>\n<td>68.32</td>\n<td>33.01</td>\n<td>20.9</td>\n</tr>\n</tbody></table>\n<p><strong>Sample detailed categories</strong></p>\n<table>\n<thead>\n<tr>\n<th>Category</th>\n<th>Precision (%)</th>\n<th>Recall (%)</th>\n<th>Avg # trials in cat in TeS</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>theft-simplelarceny</td>\n<td>64.37</td>\n<td>89.03</td>\n<td>805.9</td>\n</tr>\n<tr>\n<td>theft-receiving</td>\n<td>92.21</td>\n<td>61.53</td>\n<td>198.1</td>\n</tr>\n<tr>\n<td>deception-forgery</td>\n<td>74.29</td>\n<td>11.87</td>\n<td>21.9</td>\n</tr>\n<tr>\n<td>violenttheft-robbery</td>\n<td>68.42</td>\n<td>31.86</td>\n<td>20.4</td>\n</tr>\n<tr>\n<td>theft-extortion</td>\n<td>0.00</td>\n<td>0.00</td>\n<td>1.3</td>\n</tr>\n</tbody></table>\n<p>There are a few generalizations we can make from these numbers.</p>\n<p>First, it&#39;s obvious that if the category is too small, we are out of\nluck. So for &quot;damage,&quot; a small enough broad category that our test\nsamples only held a little over one instance of it on average, we get no\nresults. Similarly, in the detailed categories, when the occurrence of\ncases per test sample drops into the single digits, we fail miserably.\nThis is no wonder: if the test sample contains about one case on\naverage, there can&#39;t be much more than ten cases total in the whole data\nset. That&#39;s not much to go on.</p>\n<p>Second, size isn&#39;t everything. Although we do best for the biggest\ncategory, theft (which in fact accounts for over half our sample), there\nare some smaller categories we do very well for. We have very high\nrecall and precision for &quot;royaloffenses,&quot; a mid-sized category, and very\nhigh recall plus decent precision for &quot;kill,&quot; our smallest\nreasonable-sized category. A reasonable guess would be that the language\nthat occurs in the trials is distinctive and, in the case of\n&quot;royaloffenses,&quot; doesn&#39;t occur much anywhere else. Meanwhile,\nunsurprisingly, we get low scores for the &quot;miscellaneous&quot; category. We\nalso have high precision for the &quot;sexual&quot; category, indicating that it\nhas some language that doesn&#39;t tend to appear anywhere else — though we\nmiss about half the instances of it, which would lead us to suspect that\nmany of the trials in that category omit some of the language that most\ndistinguishes it from others.</p>\n<p>Third, in this sample at least, there seems to be no clear pattern\nregarding whether the learner has better recall or better precision.\nSometimes it casts a wide net that drags in both a good portion of the\ncategory and some driftwood, and sometimes it handpicks the trials for\ngood precision but misses a lot that don&#39;t look right enough for its\ntaste. So in half the cases here, our learner has better precision than\nrecall, and in half better recall than precision. The differences\nbetween precision and recall are, however, bigger for the cases where\nprecision is better than recall. That isn&#39;t necessarily a good thing for\nus, since as historians we might be happier to see more of the\n&quot;interesting&quot; documents and do the additional culling ourselves than to\nhave our learner miss a lot of good documents. We&#39;ll return to the\nquestion of the meaning of classification errors below.</p>\n<h4 id=\"extracting-the-most-indicative-features\">Extracting the most indicative features</h4>\n<p>The <code>naivebayes.py</code> script has a feature\nthat allows you to extract the most (and least) indicative features of\nyour classification exercise. This allows you to see what weighs a lot\nin the learner&#39;s mind — what it has, in effect, learned.</p>\n<p>The command to issue is: <em>mynb.topn_print(10)</em> (for the 10 most\nindicative; you can put in any number you like). Here are the results\nfor a multi-way classification of the broad categories in our data:</p>\n<pre><code>deception [&#39;norrington&#39;, &#39;election&#39;, &#39;flaum&#39;, &#39;polish&#39;, &#39;caton&#39;, &#39;spicer&#39;, &#39;saltzmaun&#39;, &#39;newcastle&#39;, &#39;stamps&#39;, &#39;rotherham&#39;]\nroyaloffences [&#39;mould&#39;, &#39;coster&#39;, &#39;coin&#39;, &#39;caleb&#39;, &#39;counterfeit&#39;, &#39;obverse&#39;, &#39;mint&#39;, &#39;moulds&#39;, &#39;plaster-of-paris&#39;, &#39;metal&#39;]\nviolenttheft [&#39;turfrey&#39;, &#39;stannard&#39;, &#39;millward&#39;, &#39;falcon&#39;, &#39;crawfurd&#39;, &#39;weatherly&#39;, &#39;keith&#39;, &#39;farr&#39;, &#39;ventom&#39;, &#39;shurety&#39;]\ndamage [&#39;cow-house&#39;, &#39;ewins&#39;, &#39;filtering-room&#39;, &#39;fisk&#39;, &#39;calf&#39;, &#39;skirting&#39;, &#39;girder&#39;, &#39;clipping&#39;, &#39;saturated&#39;, &#39;firemen&#39;]\nbreakingpeace [&#39;calthorpe-street&#39;, &#39;grievous&#39;, &#39;disable&#39;, &#39;mellish&#39;, &#39;flag&#39;, &#39;bodily&#39;, &#39;banner&#39;, &#39;aforethought&#39;, &#39;fursey&#39;, &#39;emerson&#39;]\nmiscellaneous [&#39;trevett&#39;, &#39;teuten&#39;, &#39;reitterhoffer&#39;, &#39;quantock&#39;, &#39;feaks&#39;, &#39;boone&#39;, &#39;bray&#39;, &#39;downshire&#39;, &#39;fagnoit&#39;, &#39;ely&#39;]\nkill [&#39;vault&#39;, &#39;external&#39;, &#39;appearances&#39;, &#39;slaying&#39;, &#39;deceased&#39;, &#39;marchell&#39;, &#39;disease&#39;, &#39;pedley&#39;, &#39;healthy&#39;, &#39;killing&#39;]\ntheft [&#39;sheep&#39;, &#39;embezzlement&#39;, &#39;stealing&#39;, &#39;table-cloth&#39;, &#39;fowls&#39;, &#39;dwelling-house&#39;, &#39;missed&#39;, &#39;pairs&#39;, &#39;breaking&#39;, &#39;blankets&#39;]\nsexual [&#39;bigamy&#39;, &#39;marriage&#39;, &#39;violate&#39;, &#39;ravish&#39;, &#39;marriages&#39;, &#39;busher&#39;, &#39;register&#39;, &#39;spinster&#39;, &#39;bachelor&#39;, &#39;married&#39;]\n</code></pre>\n<p>Some of these make sense instantly. In &quot;breakingpeace&quot; (which includes\nassaults, riots and woundings) you can see the makings of phrases like\n&quot;grievous bodily harm&quot; and &quot;malice aforethought,&quot; along with other\nindications of wreaking havoc like &quot;disable&quot; and &quot;harm.&quot; In\nroyaloffenses, the presence of &quot;mint,&quot; &quot;mould&quot; and &quot;plaster-of-paris&quot;\nmake sense since the largest subcategory is coining offenses. In\n&quot;theft,&quot; one might infer that sheep, fowl, and table-cloths seem to have\nbeen popular objects for stealing (though table-cloth may of course have\nbeen a wrapping for stolen objects; one would have to examine the trials\nto know).</p>\n<p>Others are more puzzling. Why is violenttheft almost exclusively\ncomposed of what seem to be person or place names? Why is &quot;election&quot;\nindicative of deception? Is there a lot of election fraud going on, or\nabuse of elected office? Looking at the documents, one finds that 9 of\nthe words indicative of violent theft are person names, and one is a\npub; why person and pub names should be more indicative here than for\nother categories is mildly intriguing and might bear further analysis\n(or might just be a quirk of our data set — remember that &quot;violenttheft&quot;\nis a fairly small category). As for &quot;election,&quot; it&#39;s hard to distinguish\na clear pattern, though it seems to be linked to fraud attempts on and\nby various officials at different levels of government.</p>\n<p>The indicative features, then, may be intriguing in themselves (though\nobviously, one should not draw any conclusions about them without\nclosely examining the data first). They are also useful in that they can\nhelp us determine whether something is skewing our results in a way we\ndon&#39;t wish, something we may be able to correct for with different\nweighting or different selection of features (see the section on\n<a href=\"#Tuning\">Tuning</a> below).</p>\n<h3 id=\"the-meanings-of-misclassification\">The meanings of misclassification</h3>\n<p>Again, it&#39;s good to keep in mind that in classifying documents we are\nnot always after an abstract &quot;true&quot; classification, but simply a useful\nor interesting one. Thus, it is a good idea to look a bit more closely\nat the &quot;errors&quot; in classification.</p>\n<p>We&#39;ll focus on two-way classification, and look at the cases where the\nNaive Bayesian incorrectly includes a trial in a category (false\npositives) as well as take a look at trials it narrowly excludes from\nthe category (let&#39;s call them close relatives).</p>\n<p>In the script for testing the learner (<code>test-nb-learner.py</code>), we saved the trial ids for\nfalse positives and close relatives so we could examine them later.</p>\n<p>Here&#39;s the relevant code bit:</p>\n<pre><code class=\"language-python\">result = mynb.classify(trial)\nguessedclass =  max(result, key=result.get)\nif cattocheck:\n    diff = abs(result[cattocheck] - result[&#39;other&#39;])\n    if diff &lt; 10 and guessedclass != cattocheck:\n        closetrials.append(trial[0])\n        difflist.append(diff)\n    if correctclass == cattocheck:\n        catinsample += 1\n    if guessedclass == cattocheck:\n         guesses += 1\n         if guessedclass == correctclass:\n             hits += 1\n         else:\n             falsepositives.append(trial[0])\nif guessedclass == correctclass:\n    correctguesses += 1\n</code></pre>\n<p>False positives are easy to catch: we simply save the cases where we\nguessed that a trial belonged to the category but it really did not.</p>\n<p>For close relatives, we first check how confident we were that the trial\ndid not belong in our category. When we issue the call to classify the\ntrial <em>mynb.classify(trial)</em>, it returns\nus a dictionary that looks like this:</p>\n<pre><code>{\n    &#39;other&#39;: -2358.522248351527,\n    &#39;violenttheft-robbery&#39;: -2326.2878233211086\n}\n</code></pre>\n<p>So to find the close relatives, we compare these two values, and if the\ndifference between them is small, we save the id of the trial we are\ncurrently classifying into a list of close relatives. (In the code chunk\nabove, we have rather arbitrarily defined a &quot;small&quot; difference as being\nunder 10).</p>\n<p>At the end of the script, we write the results of these operations into\ntwo text files: <code>falsepositives.txt</code> and <code>closerelatives.txt</code>.</p>\n<p>Let&#39;s look more closely at misclassifications for the category\n&quot;violenttheft-robbery.&quot; Here are the first 10 rows of the close\nrelatives file and the first 20 rows of the false positives file, sorted\nby offense:</p>\n<p><strong>Close relatives</strong></p>\n<pre><code>breakingpeace-wounding, t18350105-458, 1.899530878\ntheft-pocketpicking, t18310407-90, 0.282424548\ntheft-pocketpicking, t18380514-1168, 0.784184742\ntheft-pocketpicking, t18301028-208, 0.797341405\ntheft-pocketpicking, t18341016-85, 1.296811989\nviolenttheft-robbery, t18370102-317, 1.075548985\nviolenttheft-robbery, t18350921-2011, 1.105672712\nviolenttheft-robbery, t18310407-204, 1.521788666\nviolenttheft-robbery, t18370102-425, 1.840718222\nviolenttheft-robbery, t18330214-13, 2.150018805\n</code></pre>\n<p><strong>False positives</strong></p>\n<pre><code>breakingpeace-assault, t18391021-2933\nbreakingpeace-wounding, t18350615-1577\nbreakingpeace-wounding, t18331017-159\nbreakingpeace-wounding, t18350615-1578\nbreakingpeace-wounding, t18330704-5\nkill-manslaughter, t18350706-1682\nkill-manslaughter, t18360919-2161\nkill-manslaughter, t18380618-1461\nkill-murder, t18330103-7\nkill-murder, t18391021-2937\nmiscellaneous-pervertingjustice, t18340904-144\ntheft-pocketpicking, t18300114-128\ntheft-pocketpicking, t18310407-66\ntheft-pocketpicking, t18330905-92\ntheft-pocketpicking, t18370703-1639\ntheft-pocketpicking, t18301028-127\ntheft-pocketpicking, t18310106-87\ntheft-pocketpicking, t18331017-109\ntheft-pocketpicking, t18320216-108\ntheft-pocketpicking, t18331128-116\n</code></pre>\n<p>The first thing we notice is that many of the close relatives are in\nfact from our target category — they are cases that our classifier has\nnarrowly missed. So saving these separately could compensate nicely for\nan otherwise low recall number.</p>\n<p>The second thing we notice is that more of the false positives seem to\nhave to do with violence, whereas more of the close relatives seem to\nhave to do with stealing; it seems our classifier has pegged the\nviolence aspect of robberies as more significant in distinguishing them\nthan the filching aspect.</p>\n<p>The third thing we notice is that theft-pocketpicking is a very common\ncategory among both the close relatives and the false positives. And\nindeed, if we look at a sample trial from violenttheft-robbery and\nanother from among the close pocketpicking relatives, we notice that\nthere are definitely close similarities.</p>\n<p>For example, trial t18310407-90, the closest close relative, involved a\ncertain Eliza Williams indicted for pocketpicking. Williams was accused\nof stealing a watch and some other items from a certain Thomas Turk;\naccording to Turk and his friend, they had been pub-crawling, Eliza\nWilliams (whom they did not know from before) had tagged along with\nthem, and at one point in the evening had pocketed Turk&#39;s watch (Turk,\nby this time, was quite tipsy). Williams was found guilty and sentenced\nto be confined for one year.</p>\n<p>Meanwhile, in trial t18300708-14, correctly classed as\nviolenttheft-robbery, a man called Edward Overton was accused of\nfeloniously assaulting a fellow by the name of John Quinlan. Quinlan\nexplained that he had been out with friends, and when he parted from\nthem he realized it was too late to get into the hotel where he worked\nas a waiter and (apparently) also had lodgings. Having nowhere to go, he\ndecided to visit a few more public-houses. Along the way, he met\nOverton, whom he did not know from before, and treated him to a few\ndrinks. But then, according to Quinlan, Overton attacked him as they\nwere walking from one pub to another, and stole his watch as well as\nother possessions of his. According to Overton, however, Quinlan had\ngiven him the watch as a guarantee that he would repay Overton if\nOverton paid for his lodging for the night. Both men, it seems, were\nthoroughly drunk by the end of the evening. Overton was found not\nguilty.</p>\n<p>Both trials, then, are stories of groups out drinking and losing their\npossessions; what made the latter a trial for robbery rather than for\npocketpicking was simply Quinlan&#39;s accusation that Overton had &quot;struck\nhim down.&quot; For a historian interested in either robberies or\npocketpickings (or pub-crawling in 1830s London), both would probably be\nequally interesting.</p>\n<p>In fact, the misclassification patterns of the learner indicate that\neven when data is richly annotated, such as in the case of the Old\nBailey, using a machine learner to extract documents may be useful for a\nhistorian: in this case, it would help you extract trials from different\noffense categories that share features of interest to you, regardless of\nthe offense label.</p>\n<h3 id=\"tuning\">Tuning</h3>\n<p>The possibilities for tuning are practically endless.</p>\n<p>For example, you might consider tweaking your data. For instance,\ninstead of giving your classifier all the words in the document, you\nmight present it with a reduced set.</p>\n<p>One way of reducing the number of words is to collapse different words\ntogether through stemming. So the verb forms &quot;killed,&quot; &quot;kills,&quot;\n&quot;killing&quot; would all become &quot;kill&quot; (as would the plural noun &quot;kills&quot;). A\npopular stemmer is the <a href=\"http://snowball.tartarus.org/\">Snowball Stemmer</a>, and you could add that to the\ntokenization step. (I ran a couple of tests with this, and while it made\nthe process much slower, it didn&#39;t much improve the results. But that\nwould probably depend a bit on the kind of data you have.)</p>\n<p>Another way is to select the words you give to the classifier according\nto some principle. One common solution is to pick only the words with a\nhigh <strong>TF-IDF</strong> score. TF-IDF is short for &quot;term frequency - inverse\ndocument frequency,&quot; and a high score means that the term occurs quite\nfrequently in the document under consideration but rarely in documents\nin general. (You can also check out <a href=\"http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/\">a more detailed explanation of\nTF-IDF</a>, along with some Python code for calculating it.)</p>\n<p>Other options include simply playing with the size of the priors: now,\nthe Naive Bayesian has a class prior as well as a feature prior of 0.5,\nmeaning that it pretends to have seen all classes and all words at least\none-half times. Doing test runs with different priors might get you\ndifferent results.</p>\n<p>In addition to simply changing the general prior sizes, you might\nconsider having the classifier set a higher prior on the target\ncategory than on the &quot;other&quot; category, in effect requiring less\nevidence to include a trial in the target category. It might be worth\na try particularly since we noted above when examining the close\nrelatives (under Meanings of Misclassification) that many of them were\nin fact members of our target category. Setting a larger prior on the\ntarget class would probably catch those cases, boosting the recall. At\nthe same time, it probably would also lower the precision. (To change\nthe priors, you need to edit the <code>naivebayes.py</code> script.)</p>\n<p>As you can see, there is quite a lot of fuzziness here: how you pick\nthe features, how you pick the priors, and how you weight various\npriors all affect the results you get, and how to pick and weight is\nnot governed by hard logic but is rather a process of trial and error.\nStill, like we noted noted in the section on the meaning of\nclassification error above, if your goal is to get some interesting\ndata to do historical analysis on, some fuzziness may not be such a\nbig problem.</p>\n<p>Happy hunting!</p>\n"}