<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-a80c730b.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-a80c730b.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/common-similarity-measures"),
					params: {lang:"en",lessons:"lessons",slug:"common-similarity-measures"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Understanding and Using Common Similarity Measures for Text Analysis</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h1 id="overview">Overview</h1>
<p>The first question many researchers want to ask after collecting data is how similar one data sample---a text, a person, an event---is to another. It&#39;s a very common question for humanists and critics of all kinds: given what you know about two things, how alike or how different are they? Non-computational assessments of similarity and difference form the basis of a lot of critical activity. The genre of a text, for example, can be determined by assessing that text&#39;s similarity to other texts already known to be part of the genre. And conversely, knowing that a certain text is very different from others in an established genre might open up productive new avenues for criticism. An object of study&#39;s uniqueness or sameness relative to another object or to a group can be a crucial factor in the scholarly practices of categorization and critical analysis.</p>
<p>Statistical measures of similarity allow scholars to think computationally about how alike or different their objects of study may be, and these measures are the building blocks of many other clustering and classification techniques. In text analysis, the similarity of two texts can be assessed in its most basic form by representing each text as a series of word counts and calculating distance using those word counts as features. This tutorial will focus on measuring distance among texts by describing the advantages and disadvantages of three of the most common distance measures: city block or &quot;Manhattan&quot; distance, Euclidean distance, and cosine distance. In this lesson, you will learn when to use one measure over another and how to calculate these distances using the SciPy library in Python.</p>
<h1 id="preparation">Preparation</h1>
<h2 id="suggested-prior-skills">Suggested Prior Skills</h2>
<p>Though this lesson is primarily geared toward understanding the underlying principles of these calculations, it does assume some familiarity with the Python programming language. Code for this tutorial is written in Python3.6 and uses the Pandas (v0.25.3) and SciPy (v1.3.3) libraries to calculate distances, though it&#39;s possible to calculate these same distances using other libraries and other programming languages. For the text processing tasks, you will also use scikit-learn (v0.21.2). I recommend you work through <a href="/en/lessons/introduction-and-installation">the <em>Programming Historian</em>&#39;s introductory Python lessons</a> if you are not already familiar with Python.</p>
<h2 id="installation-and-setup">Installation and Setup</h2>
<p>You will need to install Python3 as well as the SciPy, Pandas, and scikit-learn libraries, which are all available through the <a href="https://www.anaconda.com/distribution/">Anaconda Distribution</a>. For more information about installing Anaconda, see their <a href="https://docs.anaconda.com/anaconda/">full documentation</a>.</p>
<h2 id="lesson-dataset">Lesson Dataset</h2>
<p>You can run our three common distance measures on almost any data set that uses numerical features to describe specific data samples (more on that in a moment). For the purposes of this tutorial, you will use a selection of 142 texts, all published in 1666, from the <a href="https://earlyprint.org/"><em>EarlyPrint</em> project</a>. This project (of which I am a collaborator) has linguistically-annotated and corrected <a href="https://web.archive.org/web/20200804133429/https://earlyprint.org/intros/intro-to-eebo-tcp.html">EEBO-TCP</a> texts.</p>
<p>Begin by [downloading the zipped set of text files]({{ site.baseurl }}/assets/common-similarity-measures/1666_texts.zip). These texts were created from the XML files provided by the <a href="https://earlyprint.org/"><em>EarlyPrint</em></a> project, and they&#39;ve been converted to plaintext since that is the format readers of this lesson are most likely to be working with. If you&#39;d like to know more about how the XML documents were transformed into plaintext, you can consult <a href="https://earlyprint.org/jupyterbook/ep_xml.html">this tutorial on the <em>EarlyPrint</em> site</a>, which explains the <em>EarlyPrint</em> XML schema and introduces how to work with those files in Python.</p>
<p>You should also [download the metadata CSV]({{ site.baseurl }}/assets/common-similarity-measures/1666_metadata.csv), which you&#39;ll use to associate your results with the authors, titles, and subject keywords of the books. This CSV was created using the <a href="https://earlyprint.org/download/">metadata filtering and download tool</a> available at <em>EarlyPrint</em>.</p>
<h1 id="what-is-similarity-or-distance">What is Similarity or Distance?</h1>
<p>Similarity is a large umbrella term that covers a wide range of scores and measures for assessing the differences among various kinds of data. In fact, similarity refers to much more than one could cover in a single tutorial. For this lesson, you&#39;ll learn one general type of similarity score that is particularly relevant to DH researchers in text analysis. The class of similarity covered in this lesson takes the word-based features of a set of documents and measures the <em>similarity</em> among documents based on their <em>distance</em> from one another in Cartesian space. Specifically, this method determines differences between texts from their word counts.</p>
<h2 id="samples-and-features">Samples and Features</h2>
<p>Measuring distance or similarity first requires understanding your objects of study as <strong>samples</strong> and the parts of those objects you are measuring as <strong>features</strong>. For text analysis, samples are usually texts, but these are abstract categories. Samples and features can be anything. A sample could be a bird species, for example, and a measured feature of that sample could be average wingspan. Though you can have as many samples and as many features as you want, you&#39;d eventually come up against limits in computing power. The mathematical principles will work regardless of the number of features and samples you are dealing with.</p>
<p>We&#39;ll begin with an example. Let&#39;s say you have two texts, the first sentences of Jane Austen&#39;s <em>Pride and Prejudice</em> and Edith Wharton&#39;s <em>Ethan Frome</em>, respectively. You can label your two texts <code>austen</code> and <code>wharton</code>. In Python, they would look like the following:</p>
<pre><code class="language-py">austen = &quot;It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife.&quot;

wharton = &quot;I had the story, bit by bit, from various people, and, as generally happens in such cases, each time it was a different story. &quot;
</code></pre>
<p>In this example, <code>austen</code> and <code>wharton</code> are your two data <strong>samples</strong>, the units of information about which you&#39;d like to know more. These two samples have lots of <strong>features</strong>, attributes of the data samples that we can measure and represent numerically: for example the number of words in each sentence, the number of characters, the number of nouns in each sentence, or the frequency of certain vowel sounds. The features you choose will depend on the nature of your research question.</p>
<p>For this example, you will use individual word counts as features. Consider the frequencies of the word &quot;a&quot; and the word &quot;in&quot; in your two samples. The following figure is an example of a chart you could construct illustrating the frequency of these words:</p>
<table>
<thead>
<tr>
<th></th>
<th>a</th>
<th>in</th>
</tr>
</thead>
<tbody><tr>
<td>austen</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>wharton</td>
<td>1</td>
<td>1</td>
</tr>
</tbody></table>
<p>Later in this lesson, you&#39;ll count the words in the <em>EarlyPrint</em> texts to create a new data set. Like this very small sample data set, the new data will include columns (features) that are individual words and rows (samples) for specific texts. The main difference is that there will be columns for 1000 words instead of 2. As you&#39;re about to see, despite this difference, distance measures are available via the same calculations.</p>
<h2 id="the-cartesian-coordinate-system">The Cartesian Coordinate System</h2>
<p>Once you&#39;ve chosen samples and measured some features of those samples, you can represent that data in a wide variety of ways. One of the oldest and most common is the <a href="https://en.wikipedia.org/wiki/Cartesian_coordinate_system">Cartesian coordinate system</a>, which you may have learned about in introductory algebra and geometry. This system allows you to represent numerical features as <em>coordinates</em>, typically in 2-dimensional space. The Austen-Wharton data table could be represented as the following graph:</p>
<p>{% include figure.html filename=&quot;datapoints.jpg&quot; caption=&quot;&#39;austen&#39; and &#39;wharton&#39; samples represented as data points.&quot; %}</p>
<p>On this graph, the <code>austen</code> and <code>wharton</code> samples are each represented as data points along two axes, or dimensions. The horizontal x-axis represents the values for the word &quot;in&quot; and the vertical y-axis represents the values for the word &quot;a.&quot; Though it may look simple, this representation allows us to imagine a spatial relationship between data points based on their <strong>features</strong>, and this spatial relationship, what we&#39;re calling similarity or distance, can tell you something about which <strong>samples</strong> are alike.</p>
<p>Here&#39;s where it gets cool. You can represent two <strong>features</strong> as two dimensions and visualize your <strong>samples</strong> using the Cartesian coordinate system. Naturally you could also visualize our samples in three dimensions if you had three features. If you had four or more features, you couldn&#39;t visualize the samples anymore: for how could you create a four-dimensional graph? But it doesn&#39;t matter, because <em>no matter how many features or dimensions you have, you can still calculate distance in the same way</em>. If you&#39;re working with word frequencies, as we are here, you can have as many <strong>features</strong>/dimensions as you do words in a text. For the rest of this lesson, the examples of distance measures will use two dimensions, but when you calculate distance with Python later in this tutorial, you&#39;ll calculate over thousands of dimensions using the same equations.</p>
<h2 id="distance-and-similarity">Distance and Similarity</h2>
<p>Now you&#39;ve taken your <strong>samples</strong> and rendered them as points in space. As a way of understanding how these two points are related to each other, you might ask: How far apart or close together are these two points? The answer to &quot;How far apart are these points?&quot; is their <strong>distance</strong> and the answer to &quot;How close together are these points?&quot; is their <strong>similarity</strong>. In addition to this distinction, <strong>similarity</strong>, as I mentioned previously, can refer to a larger category of similarity measures, whereas <strong>distance</strong> usually refers to a more narrow category that measures difference in Cartesian space.</p>
<p>It may seem redundant or confusing to use both terms, but in text analysis these concepts are usually reciprocally related (i.e., distance is merely the opposite of similarity and vice versa). I bring them both up for a simple reason: out in the world you are likely to encounter both terms, sometimes used more or less interchangeably. When you are measuring by distance, <em>the most closely related points will have the lowest distance</em>, but when you are measuring by similarity, <em>the most closely related points will have the highest similarity</em>. For the most part you will encounter distance rather than similarity, but this explanation may come in handy if you encounter a program or algorithm that outputs similarity instead. We will revisit this distinction in the Cosine Similarity and Cosine Distance section.</p>
<p>You might think that calculating distance is as simple as drawing a line between these two points and calculating its length. And it can be! But in fact there are many ways to calculate the distance between two points in Cartesian space, and different distance measures are useful for different purposes. For instance, the SciPy <code>pdist</code> function that you&#39;ll use later on lists 22 distinct measures for distance. In this tutorial, you&#39;ll learn about three of the most common distance measures: <strong>city block distance</strong>, <strong>Euclidean distance</strong>, and <strong>cosine distance</strong>.</p>
<h1 id="three-types-of-distancesimilarity">Three Types of Distance/Similarity</h1>
<h2 id="city-block-manhattan-distance">City Block (Manhattan) Distance</h2>
<p>The simplest way of calculating the distance between two points is, perhaps surprisingly, not to go in a straight line, but to go horizontally and then vertically until you get from one point to the other. This is simpler because it only requires you to subtract rather than do more complicated calculations.</p>
<p>For example, your <code>wharton</code> sample is at point (1,1): its <strong>x-coordinate</strong> is 1 (its value for &quot;in&quot;), and its <strong>y-coordinate</strong> is 1 (its value for &quot;a&quot;). Your <code>austen</code> sample is at point (2,4): its x-coordinate is 2, and its y-coordinate is 4. We want to calculate distance by looking at the differences between the x- and y-coordinates.  The dotted line in the following graph shows what you&#39;re measuring:</p>
<p>{% include figure.html filename=&quot;cityblock.jpg&quot; caption=&quot;The distance between &#39;austen&#39; and &#39;wharton&#39; points by &#39;city block&#39; distance.&quot; %}</p>
<p>You can see here why it&#39;s called city block distance, or &quot;Manhattan distance&quot; if you prefer a more New York-centric pun. &quot;Block&quot; refers to the grid-like layout of North American city streets, especially those found in New York City. The graphs of city block distance, like the previous one, resemble those grid layouts. On this graph it&#39;s easy to tell that the length of the horizontal line is 1 and the length of the vertical line is 3, which means the city block distance is 4. But how would you abstract this measure? As I alluded to above, city block distance is the sum of the differences between the x- and y-coordinates. So for two points with any values (let&#39;s call them $$(x_1, y_1)$$ and $$(x_2, y_2)$$), the city block distance is calculated using the following expression:</p>
<p>$$|x_2 - x_1| + |y_2 - y_1|$$</p>
<p>(The vertical bars you see are for <em>absolute value</em>; they ensure that even if $$x_1$$ is greater than $$x_2$$, your values are still positive.) Try it out with your two points (1,1) and (2,4):</p>
<p>$$|2 - 1| + |4 - 1| = |1| + |3| = 1 + 3 = 4$$</p>
<p>And that&#39;s it! You could add a third coordinate, call it &quot;z,&quot; or as many additional dimensions as you like for each point, and still calculate city block distance fairly easily. Because city block distance is easy to understand and calculate, it&#39;s a good one to start with as you learn the general principles. But it&#39;s less useful for text analysis than the other two distance measures we&#39;re covering. And in most cases, you&#39;re likely to get better results using the next distance measure, <strong>Euclidean distance</strong>.</p>
<h2 id="euclidean-distance">Euclidean Distance</h2>
<p>At this point I can imagine what you&#39;re thinking: Why should you care about &quot;going around the block&quot;? The shortest distance between two points is a straight line, after all.</p>
<p><strong>Euclidean distance</strong>, named for the <a href="https://en.wikipedia.org/wiki/Euclidean_geometry">geometric system attributed to the Greek mathematician Euclid</a>, will allow you to measure the straight line. Look at the graph again, but this time with a line directly between the two points:</p>
<p>{% include figure.html filename=&quot;euclidean.jpg&quot; caption=&quot;The distance between &#39;austen&#39; and &#39;wharton&#39; data points using Euclidean distance.&quot; %}</p>
<p>You&#39;ll notice I left in the city block lines. If we want to measure the distance of the line (&quot;c&quot;) between our two points, you can think about that line as the <a href="https://en.wikipedia.org/wiki/Hypotenuse"><strong>hypotenuse</strong></a> of a right triangle, where the other two sides (&quot;a&quot; and &quot;b&quot;) are the city block lines from our last distance measurement.</p>
<p>You calculate the length of the line &quot;c&quot; in terms of &quot;a&quot; and &quot;b&quot; using the <a href="https://en.wikipedia.org/wiki/Pythagorean_theorem">Pythagorean theorem</a>:</p>
<p>$$a^2 + b^2 = c^2$$</p>
<p>or:</p>
<p>$$c = \sqrt[]{a^2 + b^2}$$</p>
<p>We know that the values of a and b are the differences between x- and y-coordinates, so the full formula for <strong>Euclidean distance</strong> can be written as the following expression:</p>
<p>$$\sqrt[]{(x_2 - x_1)^2 + (y_2 - y_1)^2}$$</p>
<p>If you put the <code>austen</code> and <code>wharton</code> points into this formula, you get:</p>
<p>$$\sqrt[]{(2 - 1)^2 + (4 - 1)^2} = \sqrt[]{1^2 + 3^2} = \sqrt[]{1 + 9} = \sqrt[]{10} = 3.16$$[^1]</p>
<p>The Euclidean distance result is, as you might expect, a little less than the city block distance. Each measure tells you something about how the two points are related, but each also tells you something <em>different</em> about that relationship because what &quot;distance&quot; means for each measure is different. One isn&#39;t inherently better than the other, but it&#39;s important to know that <strong>distance</strong> isn&#39;t a set fact: the distance between two points can be quite different depending on how you define distance in the first place.</p>
<h2 id="cosine-similarity-and-cosine-distance">Cosine Similarity and Cosine Distance</h2>
<p>To emphasize this point, the final similarity/distance measure in this lesson, <a href="https://en.wikipedia.org/wiki/Cosine_similarity"><strong>cosine similarity</strong></a>, is very different from the other two. This measure is more concerned with the <em>orientation</em> of the two points in space than it is with their exact distance from one another.</p>
<p>If you draw a line from the <strong>origin</strong>---the point on the graph at the coordinates (0, 0)---to each point, you can identify an angle, $$\theta$$, between the two points, as in the following graph:</p>
<p>{% include figure.html filename=&quot;cosine.jpg&quot; caption=&quot;The angle between the &#39;austen&#39; and &#39;wharton&#39; data points, from which you will take the cosine.&quot; %}</p>
<p>The <strong>cosine similarity</strong> between the two points is simply the cosine of this angle. <a href="https://en.wikipedia.org/wiki/Trigonometric_functions#cos"><strong>Cosine</strong></a> is a trigonometric function that, in this case, helps you describe the orientation of two points. If two points were 90 degrees apart, that is if they were on the x-axis and y-axis of this graph as far away from each other as they can be in this <a href="https://en.wikipedia.org/wiki/Cartesian_coordinate_system">graph quadrant</a>, their cosine similarity would be zero, because $$cos(90) = 0$$. If two points were 0 degrees apart, that is if they existed along the same line, their cosine similarity would 1, because $$cos(0) = 1$$.</p>
<p>Cosine provides you with a ready-made scale for similarity. Points that have the same orientation have a similarity of 1, the highest possible. Points that have 90 degree orientations have a similarity of 0, the lowest possible.[^2] Any other value will be somewhere in between.</p>
<p>You needn&#39;t worry very much about how to calculate <strong>cosine similarity</strong> algebraically. Any programming environment will calculate it for you. But it&#39;s possible to determine the cosine similarity by beginning only with the coordinates of two points, $$(x_1, y_1)$$ and $$(x_2, y_2)$$:</p>
<p>$$cos(\theta) = (x_1x_2 + y_1y_2)/(\sqrt[]{x_1^2 + y_1^2}\sqrt[]{x_2^2 + y_2^2})$$</p>
<p>If you enter in your two <code>austen</code> and <code>wharton</code> coordinates, you get:</p>
<p>$$(1\times2 + 1\times4)/(\sqrt[]{1^2 + 1^2}\sqrt[]{2^2 + 4^2}) = 6/(\sqrt[]{2}\sqrt[]{20}) = 6/6.32 = 0.95$$[^3]</p>
<p>The <strong>cosine similarity</strong> of our <code>austen</code> sample to our <code>wharton</code> sample is quite high, almost 1. The result is borne out by looking at the graph, on which we can see that the angle $$\theta$$ is fairly small. Because the two points are closely oriented, their <strong>cosine similarity</strong> is high. To put it another way: according to the measures you&#39;ve seen so far, these two texts are pretty similar to one another.</p>
<p>But note that you&#39;re dealing with <strong>similarity</strong> here and not <strong>distance</strong>. The highest value, 1, is reserved for the two points that are most close together, while the lowest value, 0, is reserved for the two points that are the least close together. This is the exact opposite of <strong>Euclidean distance</strong>, in which the lowest values describe the points closest together. To remedy this confusion, most programming environments calculate <strong>cosine distance</strong> by simply subtracting the <strong>cosine similarity</strong> from one. So <strong>cosine distance</strong> is simply $$1 - cos(\theta)$$. In your example, the <strong>cosine distance</strong> would be:</p>
<p>$$1 - 0.95 = 0.05$$</p>
<p>This low <strong>cosine distance</strong> is more easily comparable to the <strong>Euclidean distance</strong> you calculated previously, but it tells you the same thing as the <strong>cosine similarity</strong> result: that the <code>austen</code> and <code>wharton</code> samples, when represented only by the number of times they each use the words &quot;a&quot; and &quot;in,&quot; are fairly similar to one another.</p>
<h1 id="how-to-know-which-distance-measure-to-use">How To Know Which Distance Measure To Use</h1>
<p>These measures aren&#39;t at all the same thing, and they yield quite different results. Yet they&#39;re all types of <strong>distance</strong>, ways of describing the relationship between two data samples. This distinction illustrates the fact that, even at a very basic level, the decisions you make as an investigator can have an outsized effect on your results.</p>
<p>In this case, the question you must ask is: How do I measure the relationship between two points? The answer to that question depends on the nature of the data you start with and what you&#39;re trying to find out.</p>
<p>As you saw in the previous section, <strong>city block distance</strong> and <strong>Euclidean distance</strong> are similar because they are both concerned with the lengths of lines between two points. This fact makes them more interchangeable. In most cases, <strong>Euclidean distance</strong> will be preferable over <strong>city block</strong> because it is more direct in its measurement of a straight line between two points.</p>
<p><strong>Cosine distance</strong> is another story. The choice between <strong>Euclidean</strong> and <strong>cosine</strong> distance is an important one, especially when working with data derived from texts. I&#39;ve already illustrated that <strong>cosine distance</strong> is only concerned with the orientation of two points and not with their exact placement. This means that <strong>cosine distance</strong> is much less effected by <strong>magnitude</strong>, or how large your numbers are.</p>
<p>To illustrate this, say for example that your points are (1,2) and (2,4) (instead of the (1,1) and (2,4) you used in the last section). The internal relationship within the two sets of coordinates is the same: a ratio of 1:2. But the points aren&#39;t identical: the second set of coordinates has twice the <strong>magnitude</strong> of the first.</p>
<p>The <strong>Euclidean distance</strong> between these two points is:</p>
<p>$$\sqrt[]{(2 - 1)^2 + (4 - 2)^2} = \sqrt[]{1^2 + 2^2} = \sqrt[]{1 + 4} = \sqrt[]{5} = 2.24$$</p>
<p>But their <strong>cosine similarity</strong> is:</p>
<p>$$(1\times2 + 2\times4)/(\sqrt[]{1^2 + 2^2}\sqrt[]{2^2 + 4^2}) = 10/(\sqrt[]{5}\sqrt[]{20}) = 10/\sqrt[]{100} = 10/10 = 1$$</p>
<p>So their <strong>cosine distance</strong> is:</p>
<p>$$1 - 1 = 0$$</p>
<p>Where <strong>Euclidean distance</strong> is concerned, these points are only a little distant from one another. While in terms of <strong>cosine distance</strong>, these two points are not at all distant. This is because <strong>Euclidean distance</strong> accounts for <strong>magnitude</strong> while <strong>cosine distance</strong> does not. Another way of putting this is that <strong>cosine distance</strong> measures whether the relationship <em>among your various features</em> is the same, regardless of <em>how much</em> of any one thing is present. This fact would be true if one of your points was (1,2) and the other was (300,600) as well.</p>
<p><strong>Cosine distance</strong> is sometimes very good for text-related data. Often texts are of very different lengths. If words have vastly different counts but exist in the text in roughly the same proportion, <strong>cosine distance</strong> won&#39;t worry about the raw counts, only their proportional relationships to each other. Otherwise, as with <strong>Euclidean distance</strong> you might wind up saying something like, &quot;All the long texts are similar, and all the short texts are similar.&quot; With text, it&#39;s often better to use the distance measure that disregards differences in <strong>magnitude</strong> and focuses on the proportions of features.</p>
<p>However, if you know your sample texts are all roughly the same size (or if you have subdivided all your texts into equally-sized &quot;chunks,&quot; a common pre-processing step), you might prefer to account for relatively small differences in <strong>magnitude</strong> by using <strong>Euclidean distance</strong>. For non-text data where the size of the sample is unlikely to effect the features, <strong>Euclidean distance</strong> is sometimes preferred.</p>
<p>There&#39;s no one answer for which distance measure to choose. As you&#39;ve learned, it&#39;s highly dependent on your data and your research question. That&#39;s why it&#39;s important to know your data well before you start out. If you&#39;re stacking other methods---like clustering or a machine learning algorithm---on top of distance measures, you&#39;ll certainly want to understand the distinction between distance measures and how choosing one over the other may effect your results down the line.</p>
<h1 id="calculating-distance-in-python">Calculating Distance in Python</h1>
<p>Now that you understand city block, Euclidean, and cosine distance, you&#39;re ready to calculate these measures using Python. For your example data, you&#39;ll use the [plain text files of *EarlyPrint* texts published in 1666]({{ site.baseurl }}/assets/common-similarity-measures/1666_texts.zip), and the [metadata for those files]({{ site.baseurl }}/assets/common-similarity-measures/1666_metadata.csv) that you downloaded earlier. First, unzip the text files and place the <code>1666_texts/</code> directory inside your working folder (i.e. The directory <code>1666_texts/</code> file will need to be in the same folder as <code>similarity.py</code> for this to work).</p>
<h2 id="counting-words">Counting Words</h2>
<p>To begin, you&#39;ll need to import the libraries (Pandas, SciPy, and scikit-learn) that you installed in the Setup and Installation section , as well as a built-in library called <code>glob</code>.</p>
<p>Create a new blank file in your text editor of choice and name it <code>similarity.py</code>. (You can also download my [complete version of this script]({{ site.baseurl }}/assets/common-similarity-measures/similarity.py).) At the top of the file, type:</p>
<pre><code class="language-py">import glob
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from scipy.spatial.distance import pdist, squareform
</code></pre>
<p>The scikit-learn and SciPy libraries are both very large, so the <code>from _____ import _____</code> syntax allows you to import only the functions you need.</p>
<p>From this point, scikit-learn&#39;s <code>CountVectorizer</code> class will handle a lot of the work for you, including opening and reading the text files and counting all the words in each text. You&#39;ll first create an instance of the <code>CountVectorizer</code> class with all of the parameters you choose, and then run that model on your texts.</p>
<p>Scikit-learn gives you many parameters to work with, but you&#39;ll need three:</p>
<ol>
<li><p>Set <code>input</code> to <code>&quot;filename&quot;</code> to tell <code>CountVectorizer</code> to accept a list of filenames to open and read.</p>
</li>
<li><p>Set <code>max_features</code> to <code>1000</code> to capture only the 1000 most frequent words. Otherwise, you&#39;ll wind up with hundreds of thousands of features that will make your calculations slower without adding very much additional accuracy.</p>
</li>
<li><p>Set <code>max_df</code> to <code>0.7</code>. DF stands for document frequency. This parameter tells <code>CountVectorizer</code> that you&#39;d like to eliminate words that appear in more than 70% of the documents in the corpus. This setting will eliminate the most common words (articles, pronouns, prepositions, etc.) without the need for a <a href="https://en.wikipedia.org/wiki/Stop_words">stop words</a> list.</p>
</li>
</ol>
<p>You can use the <code>glob</code> library you imported to create the list of file names that <code>CountVectorizer</code> needs. To set the three scikit-learn parameters and run <code>CountVectorizer</code>, type:</p>
<pre><code class="language-py"># Use the glob library to create a list of file names
filenames = glob.glob(&quot;1666_texts/*.txt&quot;)
# Parse those filenames to create a list of file keys (ID numbers)
# You&#39;ll use these later on.
filekeys = [f.split(&#39;/&#39;)[-1].split(&#39;.&#39;)[0] for f in filenames]

# Create a CountVectorizer instance with the parameters you need
vectorizer = CountVectorizer(input=&quot;filename&quot;, max_features=1000, max_df=0.7)
# Run the vectorizer on your list of filenames to create your wordcounts
# Use the toarray() function so that SciPy will accept the results
wordcounts = vectorizer.fit_transform(filenames).toarray()
</code></pre>
<p>And that&#39;s it! You&#39;ve now counted every word in all 142 texts in the test corpus. To interpret the results, you&#39;ll also need to open the metadata file as a <a href="https://pandas.pydata.org/docs/getting_started/dsintro.html#dataframe">Pandas DataFrame</a>. Add the following to the next line of your file:</p>
<pre><code class="language-py">metadata = pd.read_csv(&quot;1666_metadata.csv&quot;, index_col=&quot;TCP ID&quot;)
</code></pre>
<p>Adding the <code>index_col=&quot;TCP ID&quot;</code> setting will ensure that the index labels for your metadata table are the same as the file keys you saved above. Now you&#39;re ready to begin calculating distances.</p>
<h2 id="calculating-distance-using-scipy">Calculating Distance using SciPy</h2>
<p>Calculating distance in SciPy comprises two steps: first you calculate the distances, and then you must expand the results into a <strong>squareform</strong> matrix so that they&#39;re easier to read and process. It&#39;s called <strong>squareform</strong> because the columns and rows are the same, so the matrix is symmetrical, or square.[^4] The distance function in SciPy is called <code>pdist</code> and the squareform function is called <code>squareform</code>. <strong>Euclidean distance</strong> is the default output of <code>pdist</code>, so you&#39;ll use that one first. To calculate distances, call the <code>pdist</code> function on your DataFrame by typing <code>pdist(wordcounts)</code>. To get the squareform results, you can wrap that entire call in the <code>squareform</code> function: <code>squareform(pdist(wordcounts))</code>. To make this more readable, you&#39;ll want to put it all into a Pandas DataFrame. On the next line of your file, type:</p>
<pre><code class="language-py">euclidean_distances = pd.DataFrame(squareform(pdist(wordcounts)), index=filekeys, columns=filekeys)
print(euclidean_distances)
</code></pre>
<p>You need to declare that the <code>index</code> variable for the rows and the <code>column</code> variable will both refer back to the <code>filekeys</code> you saved when you originally read the files. Stop now, save this file, and run it from the command line by navigating to the appropriate directory in your Terminal application and typing <code>python3 similarity.py</code>. The script will print a matrix of the <strong>Euclidean distances</strong> between every text in the dataset!</p>
<p>In this &quot;matrix,&quot; which is really just a table of numbers, the rows and columns are the same. Each row represents a single <em>EarlyPrint</em> document, and the columns represent the exact same documents. The value in every cell is the distance between the text from that row and the text from that column. This configuration creates a diagonal line of zeroes through the center of your matrix: where every text is compared to itself, the distance value is zero.</p>
<p><em>EarlyPrint</em> documents are corrected and annotated versions of documents from <a href="https://earlyprint.org/intros/intro-to-eebo-and-eebo-tcp.html">the Early English Books Online–Text Creation Partnership</a>, which includes a document for almost every book printed in England between 1473 and 1700. This sample dataset includes all the texts published in 1666—the ones that are currently publicly available (the rest will be available after January 2021). What your matrix is showing you, then, is the relationships among books printed in England in 1666. This includes texts from a variety of different genres on all sorts of topics: religious texts, political treatises, and literary works, to name a few. One thing a researcher might want to know right away with a text corpus as thematically diverse as this one is: Is there a computational way to determine the kinds of similarity that a reader cares about? When you calculate the distances among such a wide variety of texts, will the results &quot;make sense&quot; to an expert reader? You&#39;ll try to answer these questions in the exercise that follows.</p>
<p>There&#39;s a lot you could do with this table of distances beyond the kind of sorting illustrated in this example. You could use it as an input for an unsupervised clustering of the texts into groups, and you could employ the same measures to drive a machine learning model. If you wanted to understand these results better, you could create a heatmap of this table itself, either in Python or by exporting this table as a CSV and visualizing it elsewhere.</p>
<p>As an example, let&#39;s take a look at the five texts that are the most similar to Robert Boyle&#39;s <em>Hydrostatical paradoxes made out by new experiments</em>, which is part of this dataset under the ID number <code>A28989</code>. The book is a scientific treatise and one of two works Boyle published in 1666. By comparing distances, you could potentially find books that are either thematically or structurally similar to Boyle&#39;s: either scientific texts (rather than religious works, for instance) or texts that have similar prose sections (rather than poetry collections or plays, for instance).</p>
<p>Let&#39;s see what texts are similar to Boyle&#39;s book according to their <strong>Euclidean distance</strong>. You can do this using Pandas&#39;s <code>nsmallest</code> function. In your working file, remove the line that says <code>print(euclidean_distances)</code>, and in its place type:</p>
<pre><code class="language-py">top5_euclidean = euclidean_distances.nsmallest(6, &#39;A28989&#39;)[&#39;A28989&#39;][1:]
print(top5_euclidean)
</code></pre>
<p>Why six instead of five? Because this is a symmetrical or <strong>square</strong> matrix, one of the possible results is always the same text. Since we know that any text&#39;s distance to itself is zero, it will certainly come up in our results. We need five more in addition to that one, so six total. But you can use the slicing notation <code>[1:]</code> to remove that first redundant text.</p>
<p>The results you get should look like the following:</p>
<pre><code>A62436     988.557029
A43020     988.622274
A29017    1000.024000
A56390    1005.630151
A44061    1012.873141
</code></pre>
<p>Your results will contain only the Text Creation Partnership ID numbers, but you can use the <code>metadata</code> DataFrame you created earlier to get more information about the texts. To do so, you&#39;ll use the <code>.loc</code> method in Pandas to select the rows and columns of the metadata that you need. On the next line of your file, type:</p>
<pre><code class="language-py">print(metadata.loc[top5_euclidean.index, [&#39;Author&#39;,&#39;Title&#39;,&#39;Keywords&#39;]])
</code></pre>
<p>In this step, you&#39;re telling Pandas to limit the rows to the file keys in your Euclidean distance results and limit the columns to author, title, and subject keywords, as in the following table:[^5]</p>
<p>{% include figure.html filename=&quot;euclidean_results.png&quot; caption=&quot;Metadata for the top five similar texts by Euclidean distance.&quot; %}</p>
<p>There&#39;s some initial success on this list, suggesting that our features are successfully finding texts that a human would recognize as similar. The first two texts, George Thomson&#39;s work on plague and Gideon Harvey&#39;s on tuberculosis, are both recognizably scientific and clearly related to Boyle&#39;s. But the next one is the other text written by Boyle, which you might expect to come up before the other two. The next question to ask is: what different results might you get with <strong>cosine distance</strong>?</p>
<p>You can calculate <strong>cosine distance</strong> in exactly the way you calculated <strong>Euclidean distance</strong>, but with a parameter that specifies the type of distance you want to use. On the next lines of your file, type:</p>
<pre><code class="language-py">cosine_distances = pd.DataFrame(squareform(pdist(wordcounts, metric=&#39;cosine&#39;)), index=filekeys, columns=filekeys)

top5_cosine = cosine_distances.nsmallest(6, &#39;A28989&#39;)[&#39;A28989&#39;][1:]
print(top5_cosine)
</code></pre>
<p>Running the script will now output the top five texts for both <strong>Euclidean distance</strong> and <strong>cosine distance</strong>. (You could calculate city block distance by using <code>metric=&#39;cityblock&#39;</code>, but the results are unlikely be substantially different from Euclidean distance.) The results for <strong>cosine distance</strong> should look like the following:</p>
<pre><code>A29017    0.432181
A43020    0.616269
A62436    0.629395
A57484    0.633845
A60482    0.663113
</code></pre>
<p>Right away you&#39;ll notice a big difference. Because <strong>cosine distances</strong> are scaled from 0 to 1 (see the Cosine Similarity and Cosine Distance section for an explanation of why this is the case), we can tell not only what the closest samples are, but <em>how</em> close they are.[^6] Only one of the closest five texts has a cosine distance less than 0.5, which means most of them aren&#39;t <em>that</em> close to Boyle&#39;s text. This observation is helpful to know and puts some of the previous results into context. We&#39;re dealing with an artificially limited corpus of texts published in just a single year; if we had a larger set, it&#39;s likely we&#39;d find texts more similar to Boyle&#39;s.</p>
<p>You can now print the metadata for these results in the same way as in the previous example:</p>
<pre><code class="language-py">print(metadata.loc[top5_cosine.index, [&#39;Author&#39;,&#39;Title&#39;,&#39;Keywords&#39;]])
</code></pre>
<p>The following table shows the metadata for the texts that <strong>cosine distance</strong> identified:</p>
<p>{% include figure.html filename=&quot;cosine_results.png&quot; caption=&quot;Metadata for the top five similar texts by cosine distance.&quot; %}</p>
<p>The first three texts in the list are the same as before, but their order has reversed. Boyle&#39;s other text, as we might expect, is now at the top of the rankings. And as we saw in the numerical results, its cosine distance suggests it&#39;s more similar than the next text down in this list, Harvey&#39;s. The order in this example suggests that perhaps <strong>Euclidean distance</strong> was picking up on a similarity between Thomson and Boyle that had more to do with <strong>magnitude</strong> (i.e. the texts were similar lengths) than it did with their contents (i.e. words used in similar proportions). The final two texts in this list, though it is hard to tell from their titles, are also fairly relevant to Boyle&#39;s. Both of them deal with topics that were part of early modern scientific thought, natural history and aging, respectively. As you might expect, because <strong>cosine distance</strong> is more focused on comparing the proportions of features within individual samples, its results were slightly better for this text corpus. But <strong>Euclidean distance</strong> was on the right track, even if it didn&#39;t capture all the similarity you were looking for. If as a next step you expanded these lists out to ten texts, you&#39;d likely see even more differences between results for the two distance measures.</p>
<p>It&#39;s crucial to note that this exploratory investigation into text similarity didn&#39;t give you a lot of definitive answers. Instead it raises many interesting questions: Which words (features) caused these specific books (samples) to manifest as similar to one another? What does it mean to say that two texts are &quot;similar&quot; according to raw word counts rather than some other feature set? What else can we learn about the texts that appeared in proximity to Boyle&#39;s? Like many computational methods, distance measures provide you with a way to ask new and interesting questions of your data, and initial results like these can lead you down new research paths.</p>
<h1 id="next-steps">Next Steps</h1>
<p>I hope this tutorial gave you a more concrete understanding of basic distance measures as well as a handle on when to choose one over the other. As a next step, and for better results in assessing similarity among texts by their words, you might consider using TF-IDF (Term Frequency–Inverse Document Frequency) instead of raw word counts. TF-IDF is a weighting system that assigns a value to every word in a text based on the relationship between the number of times a word appears in that text (its term frequency) and the number of texts it appears in through the whole corpus (its document frequency). This method is often used as an initial heuristic for a word&#39;s distinctiveness and can give the researcher more information than a simple word count. To understand exactly what TF-IDF is and what calculating it entails, see Matthew J. Lavin&#39;s <a href="/en/lessons/analyzing-documents-with-tfidf">Analyzing Documents with TF-IDF</a>. You could take TF-IDF results you made using Lavin&#39;s procedure and replace the matrix of word counts in this lesson.</p>
<p>In the future you may use distance measures to look at the most similar samples in a large data set as you did in this lesson. But it&#39;s even more likely that you&#39;ll encounter distance measures as a near-invisible part of a larger data mining or text analysis approach. For example, <a href="https://en.wikipedia.org/wiki/K-means_clustering"><strong>k-means clustering</strong></a> uses <strong>Euclidean distance</strong> by default to determine groups or clusters in a large dataset. Understanding the pros and cons of distance measures could help you to better understand and use a method like <strong>k-means clustering</strong>. Or perhaps more importantly, a good foundation in understanding distance measures might help you to assess and evaluate someone else&#39;s digital work more accurately.</p>
<p>Distance measures are a good first step to investigating your data, but a choice between the three different metrics described in this lesson---or the many other available distance measures---is never neutral. Understanding the advantages and trade-offs of each can make you a more insightful researcher and help you better understand your data.</p>
<p>[^1]: I rounded this result to the nearest hundredth place to make it more readable.</p>
<p>[^2]: A similarity lower than 0 is indeed possible. If you move to another quadrant of the graph, two points could have a 180 degree orientation, and then their cosine similarity would be -1. But because you can&#39;t have negative word counts (our basis for this entire exercise), you&#39;ll never have a point outside this quadrant.</p>
<p>[^3]: Once again, I&#39;ve done some rounding in the final two steps to make this operation more readable.</p>
<p>[^4]: SciPy&#39;s <code>pdist</code> function outputs what&#39;s called a &quot;sparse matrix&quot; to save space and processing power. This output is fine if you&#39;re using this as part of a pipeline for another purpose, but we want the &quot;squareform&quot; matrix so that we can see all the results.</p>
<p>[^5]: I made these results a little easier to read by running identical code in a <a href="/en/lessons/jupyter-notebooks">Jupyter Notebook</a>. If you run the code on the command line, the results will be the same, but they will be formatted a little differently.</p>
<p>[^6]: It&#39;s certainly possible to scale the results of Euclidean or city block distance as well, but it&#39;s not done by default.</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="common-similarity-measures/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Understanding and Using Common Similarity Measures for Text Analysis\",\"collection\":\"lessons\",\"layout\":\"lesson\",\"slug\":\"common-similarity-measures\",\"date\":\"2020-05-05T00:00:00.000Z\",\"authors\":[\"John R. Ladd\"],\"reviewers\":[\"Taylor Arnold\",\"Sarah Connell\"],\"editors\":[\"Brandon Walsh\"],\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F275\",\"difficulty\":2,\"activity\":\"analyzing\",\"topics\":[\"distant-reading\"],\"abstract\":\"This lesson introduces three common measures for determining how similar texts are to one another: city block distance, Euclidean distance, and cosine distance. You will learn the general principles behind similarity, the different advantages of these measures, and how to calculate each of them using the SciPy Python library.\",\"mathjax\":true,\"avatar_alt\":\"Image of a partial eclipse.\",\"doi\":\"10.46430\u002Fphen0089\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"overview\\\"\u003EOverview\u003C\u002Fh1\u003E\\n\u003Cp\u003EThe first question many researchers want to ask after collecting data is how similar one data sample---a text, a person, an event---is to another. It&#39;s a very common question for humanists and critics of all kinds: given what you know about two things, how alike or how different are they? Non-computational assessments of similarity and difference form the basis of a lot of critical activity. The genre of a text, for example, can be determined by assessing that text&#39;s similarity to other texts already known to be part of the genre. And conversely, knowing that a certain text is very different from others in an established genre might open up productive new avenues for criticism. An object of study&#39;s uniqueness or sameness relative to another object or to a group can be a crucial factor in the scholarly practices of categorization and critical analysis.\u003C\u002Fp\u003E\\n\u003Cp\u003EStatistical measures of similarity allow scholars to think computationally about how alike or different their objects of study may be, and these measures are the building blocks of many other clustering and classification techniques. In text analysis, the similarity of two texts can be assessed in its most basic form by representing each text as a series of word counts and calculating distance using those word counts as features. This tutorial will focus on measuring distance among texts by describing the advantages and disadvantages of three of the most common distance measures: city block or &quot;Manhattan&quot; distance, Euclidean distance, and cosine distance. In this lesson, you will learn when to use one measure over another and how to calculate these distances using the SciPy library in Python.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"preparation\\\"\u003EPreparation\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"suggested-prior-skills\\\"\u003ESuggested Prior Skills\u003C\u002Fh2\u003E\\n\u003Cp\u003EThough this lesson is primarily geared toward understanding the underlying principles of these calculations, it does assume some familiarity with the Python programming language. Code for this tutorial is written in Python3.6 and uses the Pandas (v0.25.3) and SciPy (v1.3.3) libraries to calculate distances, though it&#39;s possible to calculate these same distances using other libraries and other programming languages. For the text processing tasks, you will also use scikit-learn (v0.21.2). I recommend you work through \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fintroduction-and-installation\\\"\u003Ethe \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E&#39;s introductory Python lessons\u003C\u002Fa\u003E if you are not already familiar with Python.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"installation-and-setup\\\"\u003EInstallation and Setup\u003C\u002Fh2\u003E\\n\u003Cp\u003EYou will need to install Python3 as well as the SciPy, Pandas, and scikit-learn libraries, which are all available through the \u003Ca href=\\\"https:\u002F\u002Fwww.anaconda.com\u002Fdistribution\u002F\\\"\u003EAnaconda Distribution\u003C\u002Fa\u003E. For more information about installing Anaconda, see their \u003Ca href=\\\"https:\u002F\u002Fdocs.anaconda.com\u002Fanaconda\u002F\\\"\u003Efull documentation\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"lesson-dataset\\\"\u003ELesson Dataset\u003C\u002Fh2\u003E\\n\u003Cp\u003EYou can run our three common distance measures on almost any data set that uses numerical features to describe specific data samples (more on that in a moment). For the purposes of this tutorial, you will use a selection of 142 texts, all published in 1666, from the \u003Ca href=\\\"https:\u002F\u002Fearlyprint.org\u002F\\\"\u003E\u003Cem\u003EEarlyPrint\u003C\u002Fem\u003E project\u003C\u002Fa\u003E. This project (of which I am a collaborator) has linguistically-annotated and corrected \u003Ca href=\\\"https:\u002F\u002Fweb.archive.org\u002Fweb\u002F20200804133429\u002Fhttps:\u002F\u002Fearlyprint.org\u002Fintros\u002Fintro-to-eebo-tcp.html\\\"\u003EEEBO-TCP\u003C\u002Fa\u003E texts.\u003C\u002Fp\u003E\\n\u003Cp\u003EBegin by [downloading the zipped set of text files]({{ site.baseurl }}\u002Fassets\u002Fcommon-similarity-measures\u002F1666_texts.zip). These texts were created from the XML files provided by the \u003Ca href=\\\"https:\u002F\u002Fearlyprint.org\u002F\\\"\u003E\u003Cem\u003EEarlyPrint\u003C\u002Fem\u003E\u003C\u002Fa\u003E project, and they&#39;ve been converted to plaintext since that is the format readers of this lesson are most likely to be working with. If you&#39;d like to know more about how the XML documents were transformed into plaintext, you can consult \u003Ca href=\\\"https:\u002F\u002Fearlyprint.org\u002Fjupyterbook\u002Fep_xml.html\\\"\u003Ethis tutorial on the \u003Cem\u003EEarlyPrint\u003C\u002Fem\u003E site\u003C\u002Fa\u003E, which explains the \u003Cem\u003EEarlyPrint\u003C\u002Fem\u003E XML schema and introduces how to work with those files in Python.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou should also [download the metadata CSV]({{ site.baseurl }}\u002Fassets\u002Fcommon-similarity-measures\u002F1666_metadata.csv), which you&#39;ll use to associate your results with the authors, titles, and subject keywords of the books. This CSV was created using the \u003Ca href=\\\"https:\u002F\u002Fearlyprint.org\u002Fdownload\u002F\\\"\u003Emetadata filtering and download tool\u003C\u002Fa\u003E available at \u003Cem\u003EEarlyPrint\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"what-is-similarity-or-distance\\\"\u003EWhat is Similarity or Distance?\u003C\u002Fh1\u003E\\n\u003Cp\u003ESimilarity is a large umbrella term that covers a wide range of scores and measures for assessing the differences among various kinds of data. In fact, similarity refers to much more than one could cover in a single tutorial. For this lesson, you&#39;ll learn one general type of similarity score that is particularly relevant to DH researchers in text analysis. The class of similarity covered in this lesson takes the word-based features of a set of documents and measures the \u003Cem\u003Esimilarity\u003C\u002Fem\u003E among documents based on their \u003Cem\u003Edistance\u003C\u002Fem\u003E from one another in Cartesian space. Specifically, this method determines differences between texts from their word counts.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"samples-and-features\\\"\u003ESamples and Features\u003C\u002Fh2\u003E\\n\u003Cp\u003EMeasuring distance or similarity first requires understanding your objects of study as \u003Cstrong\u003Esamples\u003C\u002Fstrong\u003E and the parts of those objects you are measuring as \u003Cstrong\u003Efeatures\u003C\u002Fstrong\u003E. For text analysis, samples are usually texts, but these are abstract categories. Samples and features can be anything. A sample could be a bird species, for example, and a measured feature of that sample could be average wingspan. Though you can have as many samples and as many features as you want, you&#39;d eventually come up against limits in computing power. The mathematical principles will work regardless of the number of features and samples you are dealing with.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe&#39;ll begin with an example. Let&#39;s say you have two texts, the first sentences of Jane Austen&#39;s \u003Cem\u003EPride and Prejudice\u003C\u002Fem\u003E and Edith Wharton&#39;s \u003Cem\u003EEthan Frome\u003C\u002Fem\u003E, respectively. You can label your two texts \u003Ccode\u003Eausten\u003C\u002Fcode\u003E and \u003Ccode\u003Ewharton\u003C\u002Fcode\u003E. In Python, they would look like the following:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-py\\\"\u003Eausten = &quot;It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife.&quot;\\n\\nwharton = &quot;I had the story, bit by bit, from various people, and, as generally happens in such cases, each time it was a different story. &quot;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIn this example, \u003Ccode\u003Eausten\u003C\u002Fcode\u003E and \u003Ccode\u003Ewharton\u003C\u002Fcode\u003E are your two data \u003Cstrong\u003Esamples\u003C\u002Fstrong\u003E, the units of information about which you&#39;d like to know more. These two samples have lots of \u003Cstrong\u003Efeatures\u003C\u002Fstrong\u003E, attributes of the data samples that we can measure and represent numerically: for example the number of words in each sentence, the number of characters, the number of nouns in each sentence, or the frequency of certain vowel sounds. The features you choose will depend on the nature of your research question.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor this example, you will use individual word counts as features. Consider the frequencies of the word &quot;a&quot; and the word &quot;in&quot; in your two samples. The following figure is an example of a chart you could construct illustrating the frequency of these words:\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth\u003E\u003C\u002Fth\u003E\\n\u003Cth\u003Ea\u003C\u002Fth\u003E\\n\u003Cth\u003Ein\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd\u003Eausten\u003C\u002Ftd\u003E\\n\u003Ctd\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd\u003Ewharton\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003ELater in this lesson, you&#39;ll count the words in the \u003Cem\u003EEarlyPrint\u003C\u002Fem\u003E texts to create a new data set. Like this very small sample data set, the new data will include columns (features) that are individual words and rows (samples) for specific texts. The main difference is that there will be columns for 1000 words instead of 2. As you&#39;re about to see, despite this difference, distance measures are available via the same calculations.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"the-cartesian-coordinate-system\\\"\u003EThe Cartesian Coordinate System\u003C\u002Fh2\u003E\\n\u003Cp\u003EOnce you&#39;ve chosen samples and measured some features of those samples, you can represent that data in a wide variety of ways. One of the oldest and most common is the \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCartesian_coordinate_system\\\"\u003ECartesian coordinate system\u003C\u002Fa\u003E, which you may have learned about in introductory algebra and geometry. This system allows you to represent numerical features as \u003Cem\u003Ecoordinates\u003C\u002Fem\u003E, typically in 2-dimensional space. The Austen-Wharton data table could be represented as the following graph:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;datapoints.jpg&quot; caption=&quot;&#39;austen&#39; and &#39;wharton&#39; samples represented as data points.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EOn this graph, the \u003Ccode\u003Eausten\u003C\u002Fcode\u003E and \u003Ccode\u003Ewharton\u003C\u002Fcode\u003E samples are each represented as data points along two axes, or dimensions. The horizontal x-axis represents the values for the word &quot;in&quot; and the vertical y-axis represents the values for the word &quot;a.&quot; Though it may look simple, this representation allows us to imagine a spatial relationship between data points based on their \u003Cstrong\u003Efeatures\u003C\u002Fstrong\u003E, and this spatial relationship, what we&#39;re calling similarity or distance, can tell you something about which \u003Cstrong\u003Esamples\u003C\u002Fstrong\u003E are alike.\u003C\u002Fp\u003E\\n\u003Cp\u003EHere&#39;s where it gets cool. You can represent two \u003Cstrong\u003Efeatures\u003C\u002Fstrong\u003E as two dimensions and visualize your \u003Cstrong\u003Esamples\u003C\u002Fstrong\u003E using the Cartesian coordinate system. Naturally you could also visualize our samples in three dimensions if you had three features. If you had four or more features, you couldn&#39;t visualize the samples anymore: for how could you create a four-dimensional graph? But it doesn&#39;t matter, because \u003Cem\u003Eno matter how many features or dimensions you have, you can still calculate distance in the same way\u003C\u002Fem\u003E. If you&#39;re working with word frequencies, as we are here, you can have as many \u003Cstrong\u003Efeatures\u003C\u002Fstrong\u003E\u002Fdimensions as you do words in a text. For the rest of this lesson, the examples of distance measures will use two dimensions, but when you calculate distance with Python later in this tutorial, you&#39;ll calculate over thousands of dimensions using the same equations.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"distance-and-similarity\\\"\u003EDistance and Similarity\u003C\u002Fh2\u003E\\n\u003Cp\u003ENow you&#39;ve taken your \u003Cstrong\u003Esamples\u003C\u002Fstrong\u003E and rendered them as points in space. As a way of understanding how these two points are related to each other, you might ask: How far apart or close together are these two points? The answer to &quot;How far apart are these points?&quot; is their \u003Cstrong\u003Edistance\u003C\u002Fstrong\u003E and the answer to &quot;How close together are these points?&quot; is their \u003Cstrong\u003Esimilarity\u003C\u002Fstrong\u003E. In addition to this distinction, \u003Cstrong\u003Esimilarity\u003C\u002Fstrong\u003E, as I mentioned previously, can refer to a larger category of similarity measures, whereas \u003Cstrong\u003Edistance\u003C\u002Fstrong\u003E usually refers to a more narrow category that measures difference in Cartesian space.\u003C\u002Fp\u003E\\n\u003Cp\u003EIt may seem redundant or confusing to use both terms, but in text analysis these concepts are usually reciprocally related (i.e., distance is merely the opposite of similarity and vice versa). I bring them both up for a simple reason: out in the world you are likely to encounter both terms, sometimes used more or less interchangeably. When you are measuring by distance, \u003Cem\u003Ethe most closely related points will have the lowest distance\u003C\u002Fem\u003E, but when you are measuring by similarity, \u003Cem\u003Ethe most closely related points will have the highest similarity\u003C\u002Fem\u003E. For the most part you will encounter distance rather than similarity, but this explanation may come in handy if you encounter a program or algorithm that outputs similarity instead. We will revisit this distinction in the Cosine Similarity and Cosine Distance section.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou might think that calculating distance is as simple as drawing a line between these two points and calculating its length. And it can be! But in fact there are many ways to calculate the distance between two points in Cartesian space, and different distance measures are useful for different purposes. For instance, the SciPy \u003Ccode\u003Epdist\u003C\u002Fcode\u003E function that you&#39;ll use later on lists 22 distinct measures for distance. In this tutorial, you&#39;ll learn about three of the most common distance measures: \u003Cstrong\u003Ecity block distance\u003C\u002Fstrong\u003E, \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E, and \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"three-types-of-distancesimilarity\\\"\u003EThree Types of Distance\u002FSimilarity\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"city-block-manhattan-distance\\\"\u003ECity Block (Manhattan) Distance\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe simplest way of calculating the distance between two points is, perhaps surprisingly, not to go in a straight line, but to go horizontally and then vertically until you get from one point to the other. This is simpler because it only requires you to subtract rather than do more complicated calculations.\u003C\u002Fp\u003E\\n\u003Cp\u003EFor example, your \u003Ccode\u003Ewharton\u003C\u002Fcode\u003E sample is at point (1,1): its \u003Cstrong\u003Ex-coordinate\u003C\u002Fstrong\u003E is 1 (its value for &quot;in&quot;), and its \u003Cstrong\u003Ey-coordinate\u003C\u002Fstrong\u003E is 1 (its value for &quot;a&quot;). Your \u003Ccode\u003Eausten\u003C\u002Fcode\u003E sample is at point (2,4): its x-coordinate is 2, and its y-coordinate is 4. We want to calculate distance by looking at the differences between the x- and y-coordinates.  The dotted line in the following graph shows what you&#39;re measuring:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;cityblock.jpg&quot; caption=&quot;The distance between &#39;austen&#39; and &#39;wharton&#39; points by &#39;city block&#39; distance.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EYou can see here why it&#39;s called city block distance, or &quot;Manhattan distance&quot; if you prefer a more New York-centric pun. &quot;Block&quot; refers to the grid-like layout of North American city streets, especially those found in New York City. The graphs of city block distance, like the previous one, resemble those grid layouts. On this graph it&#39;s easy to tell that the length of the horizontal line is 1 and the length of the vertical line is 3, which means the city block distance is 4. But how would you abstract this measure? As I alluded to above, city block distance is the sum of the differences between the x- and y-coordinates. So for two points with any values (let&#39;s call them $$(x_1, y_1)$$ and $$(x_2, y_2)$$), the city block distance is calculated using the following expression:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$|x_2 - x_1| + |y_2 - y_1|$$\u003C\u002Fp\u003E\\n\u003Cp\u003E(The vertical bars you see are for \u003Cem\u003Eabsolute value\u003C\u002Fem\u003E; they ensure that even if $$x_1$$ is greater than $$x_2$$, your values are still positive.) Try it out with your two points (1,1) and (2,4):\u003C\u002Fp\u003E\\n\u003Cp\u003E$$|2 - 1| + |4 - 1| = |1| + |3| = 1 + 3 = 4$$\u003C\u002Fp\u003E\\n\u003Cp\u003EAnd that&#39;s it! You could add a third coordinate, call it &quot;z,&quot; or as many additional dimensions as you like for each point, and still calculate city block distance fairly easily. Because city block distance is easy to understand and calculate, it&#39;s a good one to start with as you learn the general principles. But it&#39;s less useful for text analysis than the other two distance measures we&#39;re covering. And in most cases, you&#39;re likely to get better results using the next distance measure, \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"euclidean-distance\\\"\u003EEuclidean Distance\u003C\u002Fh2\u003E\\n\u003Cp\u003EAt this point I can imagine what you&#39;re thinking: Why should you care about &quot;going around the block&quot;? The shortest distance between two points is a straight line, after all.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E, named for the \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FEuclidean_geometry\\\"\u003Egeometric system attributed to the Greek mathematician Euclid\u003C\u002Fa\u003E, will allow you to measure the straight line. Look at the graph again, but this time with a line directly between the two points:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;euclidean.jpg&quot; caption=&quot;The distance between &#39;austen&#39; and &#39;wharton&#39; data points using Euclidean distance.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EYou&#39;ll notice I left in the city block lines. If we want to measure the distance of the line (&quot;c&quot;) between our two points, you can think about that line as the \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FHypotenuse\\\"\u003E\u003Cstrong\u003Ehypotenuse\u003C\u002Fstrong\u003E\u003C\u002Fa\u003E of a right triangle, where the other two sides (&quot;a&quot; and &quot;b&quot;) are the city block lines from our last distance measurement.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou calculate the length of the line &quot;c&quot; in terms of &quot;a&quot; and &quot;b&quot; using the \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPythagorean_theorem\\\"\u003EPythagorean theorem\u003C\u002Fa\u003E:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$a^2 + b^2 = c^2$$\u003C\u002Fp\u003E\\n\u003Cp\u003Eor:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$c = \\\\sqrt[]{a^2 + b^2}$$\u003C\u002Fp\u003E\\n\u003Cp\u003EWe know that the values of a and b are the differences between x- and y-coordinates, so the full formula for \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E can be written as the following expression:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$\\\\sqrt[]{(x_2 - x_1)^2 + (y_2 - y_1)^2}$$\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you put the \u003Ccode\u003Eausten\u003C\u002Fcode\u003E and \u003Ccode\u003Ewharton\u003C\u002Fcode\u003E points into this formula, you get:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$\\\\sqrt[]{(2 - 1)^2 + (4 - 1)^2} = \\\\sqrt[]{1^2 + 3^2} = \\\\sqrt[]{1 + 9} = \\\\sqrt[]{10} = 3.16$$[^1]\u003C\u002Fp\u003E\\n\u003Cp\u003EThe Euclidean distance result is, as you might expect, a little less than the city block distance. Each measure tells you something about how the two points are related, but each also tells you something \u003Cem\u003Edifferent\u003C\u002Fem\u003E about that relationship because what &quot;distance&quot; means for each measure is different. One isn&#39;t inherently better than the other, but it&#39;s important to know that \u003Cstrong\u003Edistance\u003C\u002Fstrong\u003E isn&#39;t a set fact: the distance between two points can be quite different depending on how you define distance in the first place.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"cosine-similarity-and-cosine-distance\\\"\u003ECosine Similarity and Cosine Distance\u003C\u002Fh2\u003E\\n\u003Cp\u003ETo emphasize this point, the final similarity\u002Fdistance measure in this lesson, \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCosine_similarity\\\"\u003E\u003Cstrong\u003Ecosine similarity\u003C\u002Fstrong\u003E\u003C\u002Fa\u003E, is very different from the other two. This measure is more concerned with the \u003Cem\u003Eorientation\u003C\u002Fem\u003E of the two points in space than it is with their exact distance from one another.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you draw a line from the \u003Cstrong\u003Eorigin\u003C\u002Fstrong\u003E---the point on the graph at the coordinates (0, 0)---to each point, you can identify an angle, $$\\\\theta$$, between the two points, as in the following graph:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;cosine.jpg&quot; caption=&quot;The angle between the &#39;austen&#39; and &#39;wharton&#39; data points, from which you will take the cosine.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe \u003Cstrong\u003Ecosine similarity\u003C\u002Fstrong\u003E between the two points is simply the cosine of this angle. \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FTrigonometric_functions#cos\\\"\u003E\u003Cstrong\u003ECosine\u003C\u002Fstrong\u003E\u003C\u002Fa\u003E is a trigonometric function that, in this case, helps you describe the orientation of two points. If two points were 90 degrees apart, that is if they were on the x-axis and y-axis of this graph as far away from each other as they can be in this \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCartesian_coordinate_system\\\"\u003Egraph quadrant\u003C\u002Fa\u003E, their cosine similarity would be zero, because $$cos(90) = 0$$. If two points were 0 degrees apart, that is if they existed along the same line, their cosine similarity would 1, because $$cos(0) = 1$$.\u003C\u002Fp\u003E\\n\u003Cp\u003ECosine provides you with a ready-made scale for similarity. Points that have the same orientation have a similarity of 1, the highest possible. Points that have 90 degree orientations have a similarity of 0, the lowest possible.[^2] Any other value will be somewhere in between.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou needn&#39;t worry very much about how to calculate \u003Cstrong\u003Ecosine similarity\u003C\u002Fstrong\u003E algebraically. Any programming environment will calculate it for you. But it&#39;s possible to determine the cosine similarity by beginning only with the coordinates of two points, $$(x_1, y_1)$$ and $$(x_2, y_2)$$:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$cos(\\\\theta) = (x_1x_2 + y_1y_2)\u002F(\\\\sqrt[]{x_1^2 + y_1^2}\\\\sqrt[]{x_2^2 + y_2^2})$$\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you enter in your two \u003Ccode\u003Eausten\u003C\u002Fcode\u003E and \u003Ccode\u003Ewharton\u003C\u002Fcode\u003E coordinates, you get:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$(1\\\\times2 + 1\\\\times4)\u002F(\\\\sqrt[]{1^2 + 1^2}\\\\sqrt[]{2^2 + 4^2}) = 6\u002F(\\\\sqrt[]{2}\\\\sqrt[]{20}) = 6\u002F6.32 = 0.95$$[^3]\u003C\u002Fp\u003E\\n\u003Cp\u003EThe \u003Cstrong\u003Ecosine similarity\u003C\u002Fstrong\u003E of our \u003Ccode\u003Eausten\u003C\u002Fcode\u003E sample to our \u003Ccode\u003Ewharton\u003C\u002Fcode\u003E sample is quite high, almost 1. The result is borne out by looking at the graph, on which we can see that the angle $$\\\\theta$$ is fairly small. Because the two points are closely oriented, their \u003Cstrong\u003Ecosine similarity\u003C\u002Fstrong\u003E is high. To put it another way: according to the measures you&#39;ve seen so far, these two texts are pretty similar to one another.\u003C\u002Fp\u003E\\n\u003Cp\u003EBut note that you&#39;re dealing with \u003Cstrong\u003Esimilarity\u003C\u002Fstrong\u003E here and not \u003Cstrong\u003Edistance\u003C\u002Fstrong\u003E. The highest value, 1, is reserved for the two points that are most close together, while the lowest value, 0, is reserved for the two points that are the least close together. This is the exact opposite of \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E, in which the lowest values describe the points closest together. To remedy this confusion, most programming environments calculate \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E by simply subtracting the \u003Cstrong\u003Ecosine similarity\u003C\u002Fstrong\u003E from one. So \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E is simply $$1 - cos(\\\\theta)$$. In your example, the \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E would be:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$1 - 0.95 = 0.05$$\u003C\u002Fp\u003E\\n\u003Cp\u003EThis low \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E is more easily comparable to the \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E you calculated previously, but it tells you the same thing as the \u003Cstrong\u003Ecosine similarity\u003C\u002Fstrong\u003E result: that the \u003Ccode\u003Eausten\u003C\u002Fcode\u003E and \u003Ccode\u003Ewharton\u003C\u002Fcode\u003E samples, when represented only by the number of times they each use the words &quot;a&quot; and &quot;in,&quot; are fairly similar to one another.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"how-to-know-which-distance-measure-to-use\\\"\u003EHow To Know Which Distance Measure To Use\u003C\u002Fh1\u003E\\n\u003Cp\u003EThese measures aren&#39;t at all the same thing, and they yield quite different results. Yet they&#39;re all types of \u003Cstrong\u003Edistance\u003C\u002Fstrong\u003E, ways of describing the relationship between two data samples. This distinction illustrates the fact that, even at a very basic level, the decisions you make as an investigator can have an outsized effect on your results.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this case, the question you must ask is: How do I measure the relationship between two points? The answer to that question depends on the nature of the data you start with and what you&#39;re trying to find out.\u003C\u002Fp\u003E\\n\u003Cp\u003EAs you saw in the previous section, \u003Cstrong\u003Ecity block distance\u003C\u002Fstrong\u003E and \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E are similar because they are both concerned with the lengths of lines between two points. This fact makes them more interchangeable. In most cases, \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E will be preferable over \u003Cstrong\u003Ecity block\u003C\u002Fstrong\u003E because it is more direct in its measurement of a straight line between two points.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003ECosine distance\u003C\u002Fstrong\u003E is another story. The choice between \u003Cstrong\u003EEuclidean\u003C\u002Fstrong\u003E and \u003Cstrong\u003Ecosine\u003C\u002Fstrong\u003E distance is an important one, especially when working with data derived from texts. I&#39;ve already illustrated that \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E is only concerned with the orientation of two points and not with their exact placement. This means that \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E is much less effected by \u003Cstrong\u003Emagnitude\u003C\u002Fstrong\u003E, or how large your numbers are.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo illustrate this, say for example that your points are (1,2) and (2,4) (instead of the (1,1) and (2,4) you used in the last section). The internal relationship within the two sets of coordinates is the same: a ratio of 1:2. But the points aren&#39;t identical: the second set of coordinates has twice the \u003Cstrong\u003Emagnitude\u003C\u002Fstrong\u003E of the first.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E between these two points is:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$\\\\sqrt[]{(2 - 1)^2 + (4 - 2)^2} = \\\\sqrt[]{1^2 + 2^2} = \\\\sqrt[]{1 + 4} = \\\\sqrt[]{5} = 2.24$$\u003C\u002Fp\u003E\\n\u003Cp\u003EBut their \u003Cstrong\u003Ecosine similarity\u003C\u002Fstrong\u003E is:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$(1\\\\times2 + 2\\\\times4)\u002F(\\\\sqrt[]{1^2 + 2^2}\\\\sqrt[]{2^2 + 4^2}) = 10\u002F(\\\\sqrt[]{5}\\\\sqrt[]{20}) = 10\u002F\\\\sqrt[]{100} = 10\u002F10 = 1$$\u003C\u002Fp\u003E\\n\u003Cp\u003ESo their \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E is:\u003C\u002Fp\u003E\\n\u003Cp\u003E$$1 - 1 = 0$$\u003C\u002Fp\u003E\\n\u003Cp\u003EWhere \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E is concerned, these points are only a little distant from one another. While in terms of \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E, these two points are not at all distant. This is because \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E accounts for \u003Cstrong\u003Emagnitude\u003C\u002Fstrong\u003E while \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E does not. Another way of putting this is that \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E measures whether the relationship \u003Cem\u003Eamong your various features\u003C\u002Fem\u003E is the same, regardless of \u003Cem\u003Ehow much\u003C\u002Fem\u003E of any one thing is present. This fact would be true if one of your points was (1,2) and the other was (300,600) as well.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003ECosine distance\u003C\u002Fstrong\u003E is sometimes very good for text-related data. Often texts are of very different lengths. If words have vastly different counts but exist in the text in roughly the same proportion, \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E won&#39;t worry about the raw counts, only their proportional relationships to each other. Otherwise, as with \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E you might wind up saying something like, &quot;All the long texts are similar, and all the short texts are similar.&quot; With text, it&#39;s often better to use the distance measure that disregards differences in \u003Cstrong\u003Emagnitude\u003C\u002Fstrong\u003E and focuses on the proportions of features.\u003C\u002Fp\u003E\\n\u003Cp\u003EHowever, if you know your sample texts are all roughly the same size (or if you have subdivided all your texts into equally-sized &quot;chunks,&quot; a common pre-processing step), you might prefer to account for relatively small differences in \u003Cstrong\u003Emagnitude\u003C\u002Fstrong\u003E by using \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E. For non-text data where the size of the sample is unlikely to effect the features, \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E is sometimes preferred.\u003C\u002Fp\u003E\\n\u003Cp\u003EThere&#39;s no one answer for which distance measure to choose. As you&#39;ve learned, it&#39;s highly dependent on your data and your research question. That&#39;s why it&#39;s important to know your data well before you start out. If you&#39;re stacking other methods---like clustering or a machine learning algorithm---on top of distance measures, you&#39;ll certainly want to understand the distinction between distance measures and how choosing one over the other may effect your results down the line.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"calculating-distance-in-python\\\"\u003ECalculating Distance in Python\u003C\u002Fh1\u003E\\n\u003Cp\u003ENow that you understand city block, Euclidean, and cosine distance, you&#39;re ready to calculate these measures using Python. For your example data, you&#39;ll use the [plain text files of *EarlyPrint* texts published in 1666]({{ site.baseurl }}\u002Fassets\u002Fcommon-similarity-measures\u002F1666_texts.zip), and the [metadata for those files]({{ site.baseurl }}\u002Fassets\u002Fcommon-similarity-measures\u002F1666_metadata.csv) that you downloaded earlier. First, unzip the text files and place the \u003Ccode\u003E1666_texts\u002F\u003C\u002Fcode\u003E directory inside your working folder (i.e. The directory \u003Ccode\u003E1666_texts\u002F\u003C\u002Fcode\u003E file will need to be in the same folder as \u003Ccode\u003Esimilarity.py\u003C\u002Fcode\u003E for this to work).\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"counting-words\\\"\u003ECounting Words\u003C\u002Fh2\u003E\\n\u003Cp\u003ETo begin, you&#39;ll need to import the libraries (Pandas, SciPy, and scikit-learn) that you installed in the Setup and Installation section , as well as a built-in library called \u003Ccode\u003Eglob\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003ECreate a new blank file in your text editor of choice and name it \u003Ccode\u003Esimilarity.py\u003C\u002Fcode\u003E. (You can also download my [complete version of this script]({{ site.baseurl }}\u002Fassets\u002Fcommon-similarity-measures\u002Fsimilarity.py).) At the top of the file, type:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-py\\\"\u003Eimport glob\\nimport pandas as pd\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom scipy.spatial.distance import pdist, squareform\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe scikit-learn and SciPy libraries are both very large, so the \u003Ccode\u003Efrom _____ import _____\u003C\u002Fcode\u003E syntax allows you to import only the functions you need.\u003C\u002Fp\u003E\\n\u003Cp\u003EFrom this point, scikit-learn&#39;s \u003Ccode\u003ECountVectorizer\u003C\u002Fcode\u003E class will handle a lot of the work for you, including opening and reading the text files and counting all the words in each text. You&#39;ll first create an instance of the \u003Ccode\u003ECountVectorizer\u003C\u002Fcode\u003E class with all of the parameters you choose, and then run that model on your texts.\u003C\u002Fp\u003E\\n\u003Cp\u003EScikit-learn gives you many parameters to work with, but you&#39;ll need three:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003E\u003Cp\u003ESet \u003Ccode\u003Einput\u003C\u002Fcode\u003E to \u003Ccode\u003E&quot;filename&quot;\u003C\u002Fcode\u003E to tell \u003Ccode\u003ECountVectorizer\u003C\u002Fcode\u003E to accept a list of filenames to open and read.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003ESet \u003Ccode\u003Emax_features\u003C\u002Fcode\u003E to \u003Ccode\u003E1000\u003C\u002Fcode\u003E to capture only the 1000 most frequent words. Otherwise, you&#39;ll wind up with hundreds of thousands of features that will make your calculations slower without adding very much additional accuracy.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Cp\u003ESet \u003Ccode\u003Emax_df\u003C\u002Fcode\u003E to \u003Ccode\u003E0.7\u003C\u002Fcode\u003E. DF stands for document frequency. This parameter tells \u003Ccode\u003ECountVectorizer\u003C\u002Fcode\u003E that you&#39;d like to eliminate words that appear in more than 70% of the documents in the corpus. This setting will eliminate the most common words (articles, pronouns, prepositions, etc.) without the need for a \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FStop_words\\\"\u003Estop words\u003C\u002Fa\u003E list.\u003C\u002Fp\u003E\\n\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003EYou can use the \u003Ccode\u003Eglob\u003C\u002Fcode\u003E library you imported to create the list of file names that \u003Ccode\u003ECountVectorizer\u003C\u002Fcode\u003E needs. To set the three scikit-learn parameters and run \u003Ccode\u003ECountVectorizer\u003C\u002Fcode\u003E, type:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-py\\\"\u003E# Use the glob library to create a list of file names\\nfilenames = glob.glob(&quot;1666_texts\u002F*.txt&quot;)\\n# Parse those filenames to create a list of file keys (ID numbers)\\n# You&#39;ll use these later on.\\nfilekeys = [f.split(&#39;\u002F&#39;)[-1].split(&#39;.&#39;)[0] for f in filenames]\\n\\n# Create a CountVectorizer instance with the parameters you need\\nvectorizer = CountVectorizer(input=&quot;filename&quot;, max_features=1000, max_df=0.7)\\n# Run the vectorizer on your list of filenames to create your wordcounts\\n# Use the toarray() function so that SciPy will accept the results\\nwordcounts = vectorizer.fit_transform(filenames).toarray()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAnd that&#39;s it! You&#39;ve now counted every word in all 142 texts in the test corpus. To interpret the results, you&#39;ll also need to open the metadata file as a \u003Ca href=\\\"https:\u002F\u002Fpandas.pydata.org\u002Fdocs\u002Fgetting_started\u002Fdsintro.html#dataframe\\\"\u003EPandas DataFrame\u003C\u002Fa\u003E. Add the following to the next line of your file:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-py\\\"\u003Emetadata = pd.read_csv(&quot;1666_metadata.csv&quot;, index_col=&quot;TCP ID&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAdding the \u003Ccode\u003Eindex_col=&quot;TCP ID&quot;\u003C\u002Fcode\u003E setting will ensure that the index labels for your metadata table are the same as the file keys you saved above. Now you&#39;re ready to begin calculating distances.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"calculating-distance-using-scipy\\\"\u003ECalculating Distance using SciPy\u003C\u002Fh2\u003E\\n\u003Cp\u003ECalculating distance in SciPy comprises two steps: first you calculate the distances, and then you must expand the results into a \u003Cstrong\u003Esquareform\u003C\u002Fstrong\u003E matrix so that they&#39;re easier to read and process. It&#39;s called \u003Cstrong\u003Esquareform\u003C\u002Fstrong\u003E because the columns and rows are the same, so the matrix is symmetrical, or square.[^4] The distance function in SciPy is called \u003Ccode\u003Epdist\u003C\u002Fcode\u003E and the squareform function is called \u003Ccode\u003Esquareform\u003C\u002Fcode\u003E. \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E is the default output of \u003Ccode\u003Epdist\u003C\u002Fcode\u003E, so you&#39;ll use that one first. To calculate distances, call the \u003Ccode\u003Epdist\u003C\u002Fcode\u003E function on your DataFrame by typing \u003Ccode\u003Epdist(wordcounts)\u003C\u002Fcode\u003E. To get the squareform results, you can wrap that entire call in the \u003Ccode\u003Esquareform\u003C\u002Fcode\u003E function: \u003Ccode\u003Esquareform(pdist(wordcounts))\u003C\u002Fcode\u003E. To make this more readable, you&#39;ll want to put it all into a Pandas DataFrame. On the next line of your file, type:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-py\\\"\u003Eeuclidean_distances = pd.DataFrame(squareform(pdist(wordcounts)), index=filekeys, columns=filekeys)\\nprint(euclidean_distances)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EYou need to declare that the \u003Ccode\u003Eindex\u003C\u002Fcode\u003E variable for the rows and the \u003Ccode\u003Ecolumn\u003C\u002Fcode\u003E variable will both refer back to the \u003Ccode\u003Efilekeys\u003C\u002Fcode\u003E you saved when you originally read the files. Stop now, save this file, and run it from the command line by navigating to the appropriate directory in your Terminal application and typing \u003Ccode\u003Epython3 similarity.py\u003C\u002Fcode\u003E. The script will print a matrix of the \u003Cstrong\u003EEuclidean distances\u003C\u002Fstrong\u003E between every text in the dataset!\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this &quot;matrix,&quot; which is really just a table of numbers, the rows and columns are the same. Each row represents a single \u003Cem\u003EEarlyPrint\u003C\u002Fem\u003E document, and the columns represent the exact same documents. The value in every cell is the distance between the text from that row and the text from that column. This configuration creates a diagonal line of zeroes through the center of your matrix: where every text is compared to itself, the distance value is zero.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cem\u003EEarlyPrint\u003C\u002Fem\u003E documents are corrected and annotated versions of documents from \u003Ca href=\\\"https:\u002F\u002Fearlyprint.org\u002Fintros\u002Fintro-to-eebo-and-eebo-tcp.html\\\"\u003Ethe Early English Books Online–Text Creation Partnership\u003C\u002Fa\u003E, which includes a document for almost every book printed in England between 1473 and 1700. This sample dataset includes all the texts published in 1666—the ones that are currently publicly available (the rest will be available after January 2021). What your matrix is showing you, then, is the relationships among books printed in England in 1666. This includes texts from a variety of different genres on all sorts of topics: religious texts, political treatises, and literary works, to name a few. One thing a researcher might want to know right away with a text corpus as thematically diverse as this one is: Is there a computational way to determine the kinds of similarity that a reader cares about? When you calculate the distances among such a wide variety of texts, will the results &quot;make sense&quot; to an expert reader? You&#39;ll try to answer these questions in the exercise that follows.\u003C\u002Fp\u003E\\n\u003Cp\u003EThere&#39;s a lot you could do with this table of distances beyond the kind of sorting illustrated in this example. You could use it as an input for an unsupervised clustering of the texts into groups, and you could employ the same measures to drive a machine learning model. If you wanted to understand these results better, you could create a heatmap of this table itself, either in Python or by exporting this table as a CSV and visualizing it elsewhere.\u003C\u002Fp\u003E\\n\u003Cp\u003EAs an example, let&#39;s take a look at the five texts that are the most similar to Robert Boyle&#39;s \u003Cem\u003EHydrostatical paradoxes made out by new experiments\u003C\u002Fem\u003E, which is part of this dataset under the ID number \u003Ccode\u003EA28989\u003C\u002Fcode\u003E. The book is a scientific treatise and one of two works Boyle published in 1666. By comparing distances, you could potentially find books that are either thematically or structurally similar to Boyle&#39;s: either scientific texts (rather than religious works, for instance) or texts that have similar prose sections (rather than poetry collections or plays, for instance).\u003C\u002Fp\u003E\\n\u003Cp\u003ELet&#39;s see what texts are similar to Boyle&#39;s book according to their \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E. You can do this using Pandas&#39;s \u003Ccode\u003Ensmallest\u003C\u002Fcode\u003E function. In your working file, remove the line that says \u003Ccode\u003Eprint(euclidean_distances)\u003C\u002Fcode\u003E, and in its place type:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-py\\\"\u003Etop5_euclidean = euclidean_distances.nsmallest(6, &#39;A28989&#39;)[&#39;A28989&#39;][1:]\\nprint(top5_euclidean)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWhy six instead of five? Because this is a symmetrical or \u003Cstrong\u003Esquare\u003C\u002Fstrong\u003E matrix, one of the possible results is always the same text. Since we know that any text&#39;s distance to itself is zero, it will certainly come up in our results. We need five more in addition to that one, so six total. But you can use the slicing notation \u003Ccode\u003E[1:]\u003C\u002Fcode\u003E to remove that first redundant text.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe results you get should look like the following:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003EA62436     988.557029\\nA43020     988.622274\\nA29017    1000.024000\\nA56390    1005.630151\\nA44061    1012.873141\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EYour results will contain only the Text Creation Partnership ID numbers, but you can use the \u003Ccode\u003Emetadata\u003C\u002Fcode\u003E DataFrame you created earlier to get more information about the texts. To do so, you&#39;ll use the \u003Ccode\u003E.loc\u003C\u002Fcode\u003E method in Pandas to select the rows and columns of the metadata that you need. On the next line of your file, type:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-py\\\"\u003Eprint(metadata.loc[top5_euclidean.index, [&#39;Author&#39;,&#39;Title&#39;,&#39;Keywords&#39;]])\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIn this step, you&#39;re telling Pandas to limit the rows to the file keys in your Euclidean distance results and limit the columns to author, title, and subject keywords, as in the following table:[^5]\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;euclidean_results.png&quot; caption=&quot;Metadata for the top five similar texts by Euclidean distance.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThere&#39;s some initial success on this list, suggesting that our features are successfully finding texts that a human would recognize as similar. The first two texts, George Thomson&#39;s work on plague and Gideon Harvey&#39;s on tuberculosis, are both recognizably scientific and clearly related to Boyle&#39;s. But the next one is the other text written by Boyle, which you might expect to come up before the other two. The next question to ask is: what different results might you get with \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E?\u003C\u002Fp\u003E\\n\u003Cp\u003EYou can calculate \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E in exactly the way you calculated \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E, but with a parameter that specifies the type of distance you want to use. On the next lines of your file, type:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-py\\\"\u003Ecosine_distances = pd.DataFrame(squareform(pdist(wordcounts, metric=&#39;cosine&#39;)), index=filekeys, columns=filekeys)\\n\\ntop5_cosine = cosine_distances.nsmallest(6, &#39;A28989&#39;)[&#39;A28989&#39;][1:]\\nprint(top5_cosine)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ERunning the script will now output the top five texts for both \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E and \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E. (You could calculate city block distance by using \u003Ccode\u003Emetric=&#39;cityblock&#39;\u003C\u002Fcode\u003E, but the results are unlikely be substantially different from Euclidean distance.) The results for \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E should look like the following:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode\u003EA29017    0.432181\\nA43020    0.616269\\nA62436    0.629395\\nA57484    0.633845\\nA60482    0.663113\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ERight away you&#39;ll notice a big difference. Because \u003Cstrong\u003Ecosine distances\u003C\u002Fstrong\u003E are scaled from 0 to 1 (see the Cosine Similarity and Cosine Distance section for an explanation of why this is the case), we can tell not only what the closest samples are, but \u003Cem\u003Ehow\u003C\u002Fem\u003E close they are.[^6] Only one of the closest five texts has a cosine distance less than 0.5, which means most of them aren&#39;t \u003Cem\u003Ethat\u003C\u002Fem\u003E close to Boyle&#39;s text. This observation is helpful to know and puts some of the previous results into context. We&#39;re dealing with an artificially limited corpus of texts published in just a single year; if we had a larger set, it&#39;s likely we&#39;d find texts more similar to Boyle&#39;s.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou can now print the metadata for these results in the same way as in the previous example:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-py\\\"\u003Eprint(metadata.loc[top5_cosine.index, [&#39;Author&#39;,&#39;Title&#39;,&#39;Keywords&#39;]])\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe following table shows the metadata for the texts that \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E identified:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;cosine_results.png&quot; caption=&quot;Metadata for the top five similar texts by cosine distance.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe first three texts in the list are the same as before, but their order has reversed. Boyle&#39;s other text, as we might expect, is now at the top of the rankings. And as we saw in the numerical results, its cosine distance suggests it&#39;s more similar than the next text down in this list, Harvey&#39;s. The order in this example suggests that perhaps \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E was picking up on a similarity between Thomson and Boyle that had more to do with \u003Cstrong\u003Emagnitude\u003C\u002Fstrong\u003E (i.e. the texts were similar lengths) than it did with their contents (i.e. words used in similar proportions). The final two texts in this list, though it is hard to tell from their titles, are also fairly relevant to Boyle&#39;s. Both of them deal with topics that were part of early modern scientific thought, natural history and aging, respectively. As you might expect, because \u003Cstrong\u003Ecosine distance\u003C\u002Fstrong\u003E is more focused on comparing the proportions of features within individual samples, its results were slightly better for this text corpus. But \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E was on the right track, even if it didn&#39;t capture all the similarity you were looking for. If as a next step you expanded these lists out to ten texts, you&#39;d likely see even more differences between results for the two distance measures.\u003C\u002Fp\u003E\\n\u003Cp\u003EIt&#39;s crucial to note that this exploratory investigation into text similarity didn&#39;t give you a lot of definitive answers. Instead it raises many interesting questions: Which words (features) caused these specific books (samples) to manifest as similar to one another? What does it mean to say that two texts are &quot;similar&quot; according to raw word counts rather than some other feature set? What else can we learn about the texts that appeared in proximity to Boyle&#39;s? Like many computational methods, distance measures provide you with a way to ask new and interesting questions of your data, and initial results like these can lead you down new research paths.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"next-steps\\\"\u003ENext Steps\u003C\u002Fh1\u003E\\n\u003Cp\u003EI hope this tutorial gave you a more concrete understanding of basic distance measures as well as a handle on when to choose one over the other. As a next step, and for better results in assessing similarity among texts by their words, you might consider using TF-IDF (Term Frequency–Inverse Document Frequency) instead of raw word counts. TF-IDF is a weighting system that assigns a value to every word in a text based on the relationship between the number of times a word appears in that text (its term frequency) and the number of texts it appears in through the whole corpus (its document frequency). This method is often used as an initial heuristic for a word&#39;s distinctiveness and can give the researcher more information than a simple word count. To understand exactly what TF-IDF is and what calculating it entails, see Matthew J. Lavin&#39;s \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fanalyzing-documents-with-tfidf\\\"\u003EAnalyzing Documents with TF-IDF\u003C\u002Fa\u003E. You could take TF-IDF results you made using Lavin&#39;s procedure and replace the matrix of word counts in this lesson.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn the future you may use distance measures to look at the most similar samples in a large data set as you did in this lesson. But it&#39;s even more likely that you&#39;ll encounter distance measures as a near-invisible part of a larger data mining or text analysis approach. For example, \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FK-means_clustering\\\"\u003E\u003Cstrong\u003Ek-means clustering\u003C\u002Fstrong\u003E\u003C\u002Fa\u003E uses \u003Cstrong\u003EEuclidean distance\u003C\u002Fstrong\u003E by default to determine groups or clusters in a large dataset. Understanding the pros and cons of distance measures could help you to better understand and use a method like \u003Cstrong\u003Ek-means clustering\u003C\u002Fstrong\u003E. Or perhaps more importantly, a good foundation in understanding distance measures might help you to assess and evaluate someone else&#39;s digital work more accurately.\u003C\u002Fp\u003E\\n\u003Cp\u003EDistance measures are a good first step to investigating your data, but a choice between the three different metrics described in this lesson---or the many other available distance measures---is never neutral. Understanding the advantages and trade-offs of each can make you a more insightful researcher and help you better understand your data.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^1]: I rounded this result to the nearest hundredth place to make it more readable.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^2]: A similarity lower than 0 is indeed possible. If you move to another quadrant of the graph, two points could have a 180 degree orientation, and then their cosine similarity would be -1. But because you can&#39;t have negative word counts (our basis for this entire exercise), you&#39;ll never have a point outside this quadrant.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^3]: Once again, I&#39;ve done some rounding in the final two steps to make this operation more readable.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^4]: SciPy&#39;s \u003Ccode\u003Epdist\u003C\u002Fcode\u003E function outputs what&#39;s called a &quot;sparse matrix&quot; to save space and processing power. This output is fine if you&#39;re using this as part of a pipeline for another purpose, but we want the &quot;squareform&quot; matrix so that we can see all the results.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^5]: I made these results a little easier to read by running identical code in a \u003Ca href=\\\"\u002Fen\u002Flessons\u002Fjupyter-notebooks\\\"\u003EJupyter Notebook\u003C\u002Fa\u003E. If you run the code on the command line, the results will be the same, but they will be formatted a little differently.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^6]: It&#39;s certainly possible to scale the results of Euclidean or city block distance as well, but it&#39;s not done by default.\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
