<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-a80c730b.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-a80c730b.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/clustering-with-scikit-learn-in-python"),
					params: {lang:"en",lessons:"lessons",slug:"clustering-with-scikit-learn-in-python"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Clustering with Scikit-Learn in Python</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h1 id="introduction">Introduction</h1>
<p>This tutorial demonstrates how to implement and apply <a href="https://perma.cc/GL9D-9GRG"><em>k</em>-means clustering</a> and <a href="https://perma.cc/6JNW-DCNT">DBSCAN</a> in Python. <em>K</em>-means and DBSCAN are two popular clustering algorithms that can be used, in combination with others, during the exploratory data analysis to discover (hidden) structures in your data by identifying groups with similar <a href="https://perma.cc/TG79-SQP3">features</a> (see Patel 2019 in the bibliography). We will implement the clustering algorithms using <a href="https://perma.cc/Z9AT-N6SB">scikit-learn</a>, a widely applied and well-documented machine learning framework in Python. Also, scikit-learn has a huge community and offers smooth implementations of various machine learning algorithms. Once you have understood how to implement <em>k</em>-means and DBSCAN with scikit-learn, you can easily use this knowledge to implement other machine learning algorithms with scikit-learn, too.</p>
<p>This tutorial consists of two different case studies. The first case study clusters and analyzes an ancient authors dataset from <em>Brill&#39;s New Pauly</em>. The second case study focuses on clustering textual data, namely abstracts of all published articles in the journal <a href="https://perma.cc/P4VN-6K9K"><em>Religion</em></a> (Taylor &amp; Francis). These two datasets have been selected to illustrate how clustering algorithms can handle different data types (including numerical and textual features) and potentially be applied to a broad range of potential research topics.</p>
<p>The following section will introduce both datasets.</p>
<h2 id="first-case-study-ancient-authors-in-brills-new-pauly">First Case Study: Ancient Authors in <em>Brill&#39;s New Pauly</em></h2>
<p>In this example, we will use <em>k</em>-means to analyze a dataset including information about 238 ancient authors from Greco-Roman antiquity. The data was taken from the official <a href="https://perma.cc/4377-UUE8"><em>Brill&#39;s New Pauly</em></a> website, and originates from <a href="https://perma.cc/GJZ9-9779">Supplement I Volume 2: Dictionary of Greek and Latin Authors and Texts</a>. <em>Der Neue Pauly: Realenzyklopädie der Antike</em> (in English <em>Brill&#39;s New Pauly</em>) (1996–2002) is a well-known encyclopedia of the ancient world with contributions from established international scholars. It should be noted that access to the texts (and thus the data) in the <em>New Pauly</em> is not free of charge. I used my university&#39;s access to obtain the data from the author entries. For the following analyses, I have not copied any texts from the <em>New Pauly</em> to the dataset. However, the numerical data in the dataset was extracted and partially accumulated from the author entries in the <em>New Pauly</em>. The original German version has been translated into English since 2002. I will refer to the text using its German abbreviation (DNP) from here onwards.</p>
<p>This tutorial demonstrates how <em>k</em>-means can help to cluster ancient authors into separate groups. The overall idea is that clustering algorithms either provide us with new insights into our data structure, or falsify/verify existing hypotheses. For instance, there might be groups of authors who are discussed at length but to whom few manuscripts are attributed. Meanwhile, other groups may include authors to whom many surviving manuscripts are ascribed but who only have short entries in the DNP. Another scenario could be that we find groups of authors associated with many early editions but only a few modern ones. This would point to the fact that modern scholars continue to rely on older editions when reading these authors. In the context of this tutorial, we will leave it to the algorithms to highlight such promising clusters for us.</p>
<p>The author data was collected from the official website using Python modules and libraries such as <a href="https://perma.cc/XK5T-JH2Z">requests</a>, <a href="https://perma.cc/5RP2-869V">BeautifulSoup</a>, and <a href="https://perma.cc/VJ62-2AM2">pandas</a>.[^1] The data was then stored in a csv file named <code>DNP_ancient_authors.csv</code> (see also the <a href="https://github.com/programminghistorian/jekyll/tree/gh-pages/assets/clustering-with-scikit-learn-in-python">GitHub repository</a>).</p>
<p>A single observation (row) in the <code>DNP_ancient_authors.csv</code> dataset contains an author name as an index and observations of the following seven features (variables):</p>
<ul>
<li>Word count of the entry in the DNP, used as a measure to evaluate the importance of the author (<code>word_count</code>)</li>
<li>Number of modern translations (<code>modern_translations</code>)</li>
<li>Number of known works (<code>known_works</code>)</li>
<li>Number of existing manuscripts (<code>manuscripts</code>)</li>
<li>Number of early editions (<code>early_editions</code>)</li>
<li>Number of early translations (<code>early_translations</code>)</li>
<li>Number of modern editions (<code>modern_editions</code>)</li>
<li>Number of commentaries (<code>commentaries</code>)</li>
</ul>
<p>So, a single row in the dataset looks like this:</p>
<table>
<thead>
<tr>
<th align="left">authors</th>
<th align="center">word_count</th>
<th align="center">modern_translations</th>
<th align="center">known_works</th>
<th align="center">manuscripts</th>
<th align="center">early_editions</th>
<th align="center">early_translations</th>
<th align="center">modern_editions</th>
<th align="center">commentaries</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Aelianus Tacticus</td>
<td align="center">350</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">3</td>
<td align="center">6</td>
<td align="center">1</td>
</tr>
</tbody></table>
<h2 id="second-case-study-article-abstracts-in-religion-journal">Second Case Study: Article Abstracts in <em>Religion</em> (Journal)</h2>
<p>The second dataset contains abstracts of all published articles in the journal <em>Religion</em> (Taylor &amp; Francis). The abstracts were collected from the official website using Python modules and libraries such as requests, BeautifulSoup, and pandas. The data was stored in a csv file named <code>RELIGION_abstracts.csv</code> (see the GitHub repository). The current dataset includes abstracts from 701 articles published in 51 volumes between 1971–2021. However, some articles, particularly in older volumes, did not posses scrapable abstracts on the website and were thus left out. Other contribution types, including reviews and miscellanea, have also been excluded from this dataset.</p>
<p>A single row in the <code>RELIGION_abstracts.csv</code> dataset contains an numerical index and observations of the following four features (variables):</p>
<ul>
<li>The title of the article (<code>title</code>)</li>
<li>The full abstract (<code>abstract</code>)</li>
<li>A link to the article (<code>link</code>)</li>
<li>A link to the volume in which the article (abstract) was published (<code>volume</code>)</li>
</ul>
<p>So, a single row in this dataset looks like this:</p>
<table>
<thead>
<tr>
<th align="left">title</th>
<th align="center">abstract</th>
<th align="center">link</th>
<th align="center">volume</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Norwegian Muslims denouncing terrorism: beyond ‘moderate’ versus ‘radical’?</td>
<td align="center">In contemporary (...)</td>
<td align="center"><a href="https://www.tandfonline.com/doi/full/10.1080/0048721X.2021.1865600">https://www.tandfonline.com/doi/full/10.1080/0048721X.2021.1865600</a></td>
<td align="center"><a href="https://www.tandfonline.com/loi/rrel20?treeId=vrrel20-51">https://www.tandfonline.com/loi/rrel20?treeId=vrrel20-51</a></td>
</tr>
</tbody></table>
<p>The analysis in this tutorial focuses on clustering the textual data in the <code>abstract</code> column of the dataset. We will apply <em>k</em>-means and DBSCAN to find thematic clusters within the diversity of topics discussed in <em>Religion</em>. To do so, we will first create document vectors of each abstract (via <strong>T</strong>ext <strong>F</strong>requency - <strong>I</strong>nverted <strong>D</strong>ocument <strong>F</strong>requency, or <a href="https://perma.cc/UL2M-GY4A"><strong>TF-IDF</strong></a> for short), reduce the feature space (which initially consists of the entire vocabulary of the abstracts), and then look for thematic clusters. </p>
<p>You can download both datasets as well as a Jupyter notebook containing the code we are writing in this tutorial from the <a href="https://github.com/programminghistorian/jekyll/tree/gh-pages/assets/clustering-with-scikit-learn-in-python">GitHub repository</a>. This lesson will work on any operating system, as long as you follow these instructions to set up an environment with Anaconda or Google Colab to run the Jupyter notebook locally or in the cloud. If you do not know how to set up a Jupyter notebook locally, this <a href="https://perma.cc/DG7B-ASKL">excellent PH tutorial might help you get started</a>.</p>
<h1 id="prerequisites">Prerequisites</h1>
<p>To follow this tutorial, you should have basic programming knowledge (preferably Python) and be familiar with central Python libraries, such as pandas and <a href="https://perma.cc/GY76-324B">matplotlib</a> (or their equivalents in other programming languages). I also assume that you have basic knowledge of descriptive statistics. For instance, you should know what <a href="https://perma.cc/3Z34-DXCW">mean</a>, <a href="https://perma.cc/DH2Q-NP35">standard deviation</a>, and <a href="https://perma.cc/AKA7-HVQC">categorical</a>/<a href="https://perma.cc/WVE4-4WAQ">continuous</a> variables are.</p>
<h1 id="why-k-means-clustering-and-dbscan">Why <em>K</em>-Means clustering and DBSCAN?</h1>
<p>Generally, you can choose between several clustering algorithms to analyze your data, such as <em>k</em>-means clustering, <a href="https://perma.cc/C3UV-SWMN">hierarchical clustering</a>, and <a href="https://perma.cc/VH9X-DTSB">DBSCAN</a>. We focus on <em>k</em>-means clustering in this tutorial since it is a relatively easy-to-understand clustering algorithm with a fast runtime speed that still delivers decent results,[^2] which makes it an excellent model to start with. I have selected DBSCAN as the second clustering algorithm for this tutorial since DBSCAN is an excellent addition to <em>k</em>-means. Among other capabilities, DBSCAN allows you to focus on dense and non-linear clusters in your data while leaving noise points or outliers outside the dense clusters, which is something that <em>k</em>-means cannot do independently (<em>k</em>-means adds the noise points or outliers to the <em>k</em>-clusters).</p>
<p>However, implementing other clustering algorithms in combination with scikit-learn should be fairly straight-forward once you are familiar with the overall workflow. Thus, if you decide to analyze your data with additional clustering algorithms (such as hierarchical clustering), you can easily do so after finishing this tutorial. In general, it is advisable to apply more than one clustering algorithm to get different perspectives on your data and evaluate each model&#39;s results.</p>
<h1 id="what-is-clustering">What is Clustering?</h1>
<p>Clustering is part of the larger field of <a href="https://perma.cc/RCF8-AVH6">machine learning</a>. Machine learning is an artificial intelligence process by which computers can learn from data without being explicitly programmed (see Géron 2019, 2 in the bibliography), meaning that a machine learning model, once it is set up, can independently discover structures in the data or predict new (unknown) data. The field of machine learning can be separated into <a href="https://perma.cc/RS62-NQE3">supervised</a>, <a href="https://perma.cc/6FSL-9N2J">unsupervised</a>, and <a href="https://perma.cc/2LPR-9DJU">reinforcement</a> learning (see Géron 2019, 7-17 in the bibliography).</p>
<p><strong>Supervised machine learning</strong> uses <a href="https://perma.cc/AC8U-DCYD">labeled data</a> to train machine learning algorithms to make accurate predictions for new data. A good example is a spam filter (with emails either labeled as &quot;spam&quot; or &quot;not-spam&quot;). One way to assess a supervised machine learning model&#39;s accuracy is to test it on some pre-labeled data, then compare the machine learning model&#39;s labeling predictions with the original output. Among other things, the model&#39;s accuracy depends on the quantity and quality of the labeled data it has been trained on and its parameters (<a href="https://perma.cc/AX34-ZKA7">hyperparameter tuning</a>). Thus, building a decent supervised machine learning model involves a continuous loop of training, testing, and fine-tuning of the model&#39;s parameters. Common examples of supervised machine learning classifiers are <a href="https://perma.cc/U6CU-5R55"><em>k</em>-nearest neighbors (KNN)</a> and <a href="https://perma.cc/AG5A-AB7M">logistic regression</a>.</p>
<p><strong>Unsupervised learning</strong> is applied to unlabeled data. Among other things, unsupervised learning is used for anomaly detection, dimensionality reduction, and clustering. When applying unsupervised machine learning algorithms, we do not feed our model with prelabeled data to make predictions for new data. Instead, we want the model to discern potential structures in our data. The datasets in this tutorial are a good example: we are only feeding our model either the author or abstract data, and we expect the model to indicate where (potential) clusters exist (for instance, articles in <em>Religion</em> with similar topics). Hyperparameter tuning can also be a part of unsupervised learning; however, in these cases, the results of the clustering cannot be compared to any prelabeled data. Yet, we can apply measures such as the so-called <a href="https://perma.cc/W69A-EUQB">elbow method</a> or the <a href="https://perma.cc/M4TD-VSNU">silhouette score</a> to evaluate the model&#39;s output based on different parameter choices (such as the n number of clusters in <em>k</em>-means).   </p>
<p><strong>Reinforcement learning</strong> is less relevant for scholars in the humanities. Reinforcement learning consists of setting up an agent (for instance, a robot) that performs actions and is either rewarded or punished for their execution. The agent learns how to react to its environment based on the feedback it received from its former actions.</p>
<h1 id="how-does-k-means-work">How Does <em>K</em>-Means Work?</h1>
<p>The following overview of the <em>k</em>-means algorithm focuses on the so-called <a href="https://perma.cc/8WB3-K8NT">naive <em>k</em>-means</a> clustering, in which the cluster centers (so-called <a href="https://perma.cc/T76C-GWQY">centroids</a>) are randomly initialized. However, the <a href="https://perma.cc/K7KK-XUEG">scikit-learn implementation of <em>k</em>-means</a> applied in this tutorial already integrates many improvements to the original algorithm. For instance, instead of randomly distributing the initial cluster centers (centroids), the scikit-learn model uses a different approach called <a href="https://perma.cc/L98W-GWD5"><em>k</em>-means++</a>, which is a smarter way to distribute the initial centroids. Yet, the way <em>k</em>-means++ works is beyond the scope of this tutorial, and I recommend reading this <a href="https://perma.cc/8KPJ-JRZW">article</a> by David Arthur and Sergei Vassilvitskii if you want to learn more.</p>
<h2 id="the-k-means-algorithm">The <em>K</em>-Means Algorithm</h2>
<p>To explain how <em>k</em>-means works, let us review a snippet from our <code>DNP_ancien_authors.csv</code> dataset. Even though we will later include more features, it is helpful to focus on a few key features in this introductory section to explain how the clustering techniques work.</p>
<table>
<thead>
<tr>
<th align="left">authors</th>
<th align="center">word_count</th>
<th align="center">known_works</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Aelianus Tacticus</td>
<td align="center">350</td>
<td align="center">1</td>
</tr>
<tr>
<td align="left">Ambrosius</td>
<td align="center">1221</td>
<td align="center">14</td>
</tr>
<tr>
<td align="left">Anacreontea</td>
<td align="center">544</td>
<td align="center">1</td>
</tr>
<tr>
<td align="left">Aristophanes</td>
<td align="center">1108</td>
<td align="center">11</td>
</tr>
</tbody></table>
<p>To start with the <em>k</em>-means clustering, you first need to define the number of clusters you want to find in your data. In most cases, you will not know how many clusters exist in your data, so choosing the appropriate initial number of clusters is already a tricky question. We will address this issue below, but let us first review how <em>k</em>-means generally functions. </p>
<p>In our example, we will assume that we are trying to identify two clusters. The naive <em>k</em>-means algorithm will now initialize the model with two randomly distributed cluster centers in the two-dimensional space. </p>
<p>The main algorithm consists of two steps. The first step is to measure the distances between every data point and the current cluster centers (in our case, via <a href="https://perma.cc/X3P6-JESJ">Euclidean distance</a> \( \sqrt[]{(x_1-x_2)^{2}+(y_1-y_2)^{2}} \), where \( (x_1,y_1) \) and \( (x_2,y_2) \) are two data points in our two-dimensional space). After measuring the distances between each data point and the cluster centers, every data point is assigned to its nearest cluster center.</p>
<p>The second step consists of creating new cluster centers by calculating the mean of all the data points assigned to each cluster.</p>
<p>After creating the new cluster centers, the algorithm starts again with the data points&#39; reassignment to the newly created cluster centers. The algorithm stops once the cluster centers are more or less stable. The <a href="https://perma.cc/GL9D-9GRG">Wikipedia entry on <em>k</em>-means clustering</a> provides helpful visualizations of this two-step process.</p>
<p>The plotted results when clustering our snippet from the <code>DNP_ancient_authors.csv</code> dataset look like this, including the position of the final centroids:</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig1.png&quot; caption=&quot;Figure 1: The clustered ancient authors data and the centroids using <em>k</em>-means in a two-dimensional space.&quot; %}</p>
<p>This appears satisfactory, and we can quickly see how the centroids are positioned between the data points that we intuitively assume to represent one cluster. However, we can already notice that the scales of both axes differ significantly. The y-axis ranges from 1–14, whereas the x-axis scale ranges from 300–1300. Thus, a change on the x-axis is likely to influence the distance between data points more significantly than a change on the y-axis. This, in turn, also impacts the placement of the centroids and thus the cluster building process. To showcase this problem, let us change the word count of Aristophanes from 1108 to 700.</p>
<table>
<thead>
<tr>
<th align="left">authors</th>
<th align="center">word_count</th>
<th align="center">known_works</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Aelianus Tacticus</td>
<td align="center">350</td>
<td align="center">1</td>
</tr>
<tr>
<td align="left">Ambrosius</td>
<td align="center">1221</td>
<td align="center">14</td>
</tr>
<tr>
<td align="left">Anacreontea</td>
<td align="center">544</td>
<td align="center">1</td>
</tr>
<tr>
<td align="left">Aristophanes</td>
<td align="center">700</td>
<td align="center">11</td>
</tr>
</tbody></table>
<p>If we apply <em>k</em>-means clustering on the changed dataset, we get the following result:</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig2.png&quot; caption=&quot;Figure 2: A new version of the clustered data and the centroids using <em>k</em>-means on the changed ancient authors data.&quot; %}</p>
<p>As you can see, a change of word count resulted in a new cluster of three authors who each have entries of approximately the same word count in the DNP, but who have a significantly different number of known published works. But does this really make sense? Wouldn&#39;t it be more reasonable to leave Ambrosius and Aristophanes in the same cluster since they have both written approximately the same number of documented works? To account for such problems based on different scales, it is advisable to normalize the data before clustering it. There are different ways to do this, among them <a href="https://perma.cc/M73K-8XST">min-max normalization</a> and <a href="https://perma.cc/ZTB8-3K74">z-score normalization</a>, which is also called standardization. In this tutorial, we will focus on the latter. This means that we first subtract the mean from each data point and then divide it by the standard deviation of the data in the respective column. Fortunately, scikit-learn already provides us with implementations of these normalizations, so we do not have to calculate them manually.</p>
<p>The standardized (z-score) snippet of the ancient authors dataset looks like this:</p>
<table>
<thead>
<tr>
<th align="left">authors</th>
<th align="center">word_count</th>
<th align="center">known_works</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Aelianus Tacticus</td>
<td align="center">-1.094016</td>
<td align="center">-0.983409</td>
</tr>
<tr>
<td align="left">Ambrosius</td>
<td align="center">1.599660</td>
<td align="center">1.239950</td>
</tr>
<tr>
<td align="left">Anacreontea</td>
<td align="center">-0.494047</td>
<td align="center">-0.983409</td>
</tr>
<tr>
<td align="left">Aristophanes</td>
<td align="center">-0.011597</td>
<td align="center">0.726868</td>
</tr>
</tbody></table>
<p>If we now apply a <em>k</em>-means clustering on the standardized dataset, we get the following result:</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig3.png&quot; caption=&quot;Figure 3: Using <em>k</em>-means clustering on the standardized dataset.&quot; %}</p>
<p>As you can see, changing the word count now has a less significant influence on the clustering. In our example, working with the standardized dataset results in a more appropriate clustering of the data since the <code>known_works</code> feature would otherwise lose much of its value for the overall analysis.</p>
<h1 id="how-many-clusters-should-i-choose">How Many Clusters Should I Choose?</h1>
<h2 id="elbow-method">Elbow Method</h2>
<p>The question of how many cluster centers to choose is a difficult one. There is no one-size-fits-all solution to this problem. Yet, specific performance measures might help to select the right number of clusters for your data. A helpful example that we will be using in this tutorial is the elbow method. The elbow method is based on measuring the inertia of the clusters for different numbers of clusters. In this context, inertia is defined as:</p>
<blockquote>
<p>Sum of squared distances of samples to their closest cluster center.[^3]</p>
</blockquote>
<p>The inertia decreases with the number of clusters. The extreme is that inertia will be zero when n is equal to the number of data points. But how could this help us find the right amount of clusters? Ideally, you would expect the inertia to decrease more slowly from a certain n onwards, so that a (fictional) plot of the inertia/cluster relation would look like this:</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig4.png&quot; caption=&quot;Figure 4: Fictional example of inertia for k clusters.&quot; %}</p>
<p>In this plot, the &quot;elbow&quot; is found at four clusters. This indicates that four clusters might be a reasonable trade-off between relatively low inertia (meaning the data points assigned to the clusters are not too far away from the centroids) and as few clusters as possible. Again, this method only provides you with an idea of where to start investigating. The final decision is up to you and highly depends on your data and your research question. Figuring out the right amount of clusters should also be accompanied by other steps, such as plotting your data or assessing other statistics. We will see how inertia helps us to discover the right amount of clusters for our <code>DNP_ancient_authors.csv</code> dataset in the following application of <em>k</em>-means.</p>
<h2 id="silhouette-score">Silhouette Score</h2>
<p>Another possible way to evaluate the clustering of your data is to use the silhouette score, a method that allows you to assess how well each data point is associated with its current cluster. The way the silhouette score works is very well described in the Wikipedia article <a href="https://perma.cc/M4TD-VSNU">&quot;Silhouette (clustering)&quot;</a>:</p>
<blockquote>
<p>The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.</p>
</blockquote>
<p>In this tutorial, we will be using the silhouette scores with the machine learning visualization library <a href="https://perma.cc/5P5D-WPW9">yellowbrick</a> in Python. Plotting the average silhouette score of all data points against the silhouette score of each data point in a cluster can help you to evaluate the quality of your model and the suitability of your current choice of parameter values.</p>
<p>To illustrate how a silhouette plot can help you find the correct number of clusters for your data, we can take a dummy example from our ancient author dataset. The data is based on a fictive sample of the number of known works and the word count of selected authors. The data has already been standardized using the z-score.</p>
<table>
<thead>
<tr>
<th align="left">authors</th>
<th align="center">known_works</th>
<th align="center">word_count</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Author A</td>
<td align="center">0.24893051</td>
<td align="center">0.83656758</td>
</tr>
<tr>
<td align="left">Author B</td>
<td align="center">0.38169345</td>
<td align="center">0.04955707</td>
</tr>
<tr>
<td align="left">Author C</td>
<td align="center">0.11616757</td>
<td align="center">0.34468601</td>
</tr>
<tr>
<td align="left">Author D</td>
<td align="center">-0.01659537</td>
<td align="center">0.14793338</td>
</tr>
<tr>
<td align="left">Author E</td>
<td align="center">-1.21146183</td>
<td align="center">-1.18014685</td>
</tr>
<tr>
<td align="left">Author F</td>
<td align="center">-1.07869889</td>
<td align="center">-1.27852317</td>
</tr>
<tr>
<td align="left">Author G</td>
<td align="center">-0.94593595</td>
<td align="center">-1.22933501</td>
</tr>
<tr>
<td align="left">Author H</td>
<td align="center">-1.07869889</td>
<td align="center">-1.1309587</td>
</tr>
<tr>
<td align="left">Author I</td>
<td align="center">-0.68041007</td>
<td align="center">-0.34394819</td>
</tr>
<tr>
<td align="left">Author J</td>
<td align="center">-0.81317301</td>
<td align="center">-0.83582976</td>
</tr>
<tr>
<td align="left">Author K</td>
<td align="center">-0.41488419</td>
<td align="center">-0.54070081</td>
</tr>
<tr>
<td align="left">Author L</td>
<td align="center">-0.54764713</td>
<td align="center">-0.43838945</td>
</tr>
<tr>
<td align="left">Author M</td>
<td align="center">1.1782711</td>
<td align="center">1.62357809</td>
</tr>
<tr>
<td align="left">Author N</td>
<td align="center">1.31103404</td>
<td align="center">1.52520177</td>
</tr>
<tr>
<td align="left">Author O</td>
<td align="center">1.57655992</td>
<td align="center">1.41698783</td>
</tr>
<tr>
<td align="left">Author P</td>
<td align="center">1.97484874</td>
<td align="center">1.03332021</td>
</tr>
</tbody></table>
<p>We can now plot the silhouette score for different cluster numbers n. In this example, we will plot the silhouette scores for two, three, and four clusters using <em>k</em>-means.</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig5.png&quot; caption=&quot;Figure 5: Silhouette plots using <em>k</em>-means with n clusters between two and five.&quot; %}</p>
<p>The vertical dashed line indicates the average silhouette score of all data points. The horizontal “knives” represent an overview of all data points in a cluster and their individual silhouette scores in descending order (from top to bottom). The silhouette plots show us that a cluster number between four and five seems to be most appropriate for our dataset. Particularly the data points with n=4 clusters have a relatively high average silhouette score (over 0.6), and the cluster “knives” seem to have approximately the same size and are not too sharp, which indicates that the cohesion within each cluster is not too bad. Indeed, if we plot our data using <em>k</em>-means with n=4 clusters, we can see that this choice is a reasonable cluster number for our dataset and offers a good impression of the overall distribution of the data points.</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig6.png&quot; caption=&quot;Figure 6: Scatterplot of the dataset using <em>k</em>-means clustering with n=4 clusters.&quot; %}</p>
<h1 id="how-does-dbscan-work">How Does DBSCAN Work?</h1>
<p>DBSCAN is short for &quot;Density-Based Spatial Clustering of Applications with Noise.&quot; Unlike the <em>k</em>-means algorithm, DBSCAN does not try to cluster every single data point in a dataset. Instead, DBSCAN looks for dense regions of data points in a set while classifying data points without any direct neighbors as outliers or ‘noise points’. DBSCAN can be a great choice when dealing with datasets that are not linearly clustered but still include dense regions of data points.</p>
<h2 id="the-dbscan-algorithm">The DBSCAN Algorithm</h2>
<p>The basic DBSCAN algorithm is very well explained in the corresponding <a href="https://perma.cc/6JNW-DCNT">wikipedia article</a>.</p>
<ol>
<li>The first step consists of defining an ε-distance (eps) that defines the neighborhood region (radius) of a data point. Just as in the case of k-means-clustering, <a href="https://perma.cc/W5TT-ZS4N">scikit-learn&#39;s DBSCAN implementation uses Euclidean distance as the standard metric</a> to calculate distances between data points. The second value that needs to be defined is the minimum number of data points that should be located in the neighborhood of data point to define its region as dense (including the data point itself).</li>
<li>The algorithm starts by choosing a random data point in the dataset as a starting point. DBSCAN then looks for other data points within the ε-region around the starting point. Suppose there are at least n datapoints (with n equals the minimum number of data points specified before) in the neighborhood (including the starting point). In that case, the starting point and all the data points in the ε-region of the starting point are defined as core points that define a core cluster. If there are less than n data points found in the starting point&#39;s neighborhood, the datapoint is labeled as an noise point or outlier (yet, it might still become a member of another cluster later on). In this case, the algorithm continues by choosing another unlabeled data point from the dataset and restarts the algorithm at step 2.</li>
<li>If an initial cluster is found, the DBSCAN algorithm analyzes the ε-region of each core point in the initial cluster. If a region includes at least n data points, new core points are created, and the algorithm continues by looking at the neighborhood of these newly assigned core points, and so on. If a core point has less than n data points, some of which are still unlabeled, they will also be included in the cluster (as so-called border points). In cases where border points are part of different clusters, they will be associated with the nearest cluster</li>
<li>Once every datapoint has been visited and labeled as either part of a cluster or as a noise point or outlier, the algorithm stops.</li>
</ol>
<p>Unlike the <em>k</em>-means algorithm, the difficulty does not lie in finding the right amount of clusters to start with but in figuring out which ε-region is most appropriate for the dataset. A helpful method for finding the proper eps value is explained in <a href="https://perma.cc/5H99-4EX6">this article on towardsdatascience.com</a>. In short, DBSCAN enables us to plot the distance between each data point in a dataset and itentify its nearest neighbor. It is then possible to sort by distance in ascending order. Finally, we can look for the point in the plot which initiates the steepest ascent and make a visual evaluation of the eps value, similar to the ‘elbow’ evaluation method described above in the case of <em>k</em>-means)’. We will use this method later in this tutorial.</p>
<p>Now that we know how our clustering algorithms generally work and which methods we can apply to settle on the right amount of clusters let us apply these concepts in the context of our datasets from <em>Brill&#39;s New Pauly</em> and the journal <em>Religion</em>. We will start by analyzing the <code>DNP_ancient_authors.csv</code> dataset.</p>
<h1 id="first-case-study-applying-k-means-to-the-ancient-authors-dataset-from-brills-new-pauly">First Case Study: Applying <em>K</em>-Means to the Ancient Authors Dataset from <em>Brill&#39;s New Pauly</em></h1>
<h2 id="1-exploring-the-dataset">1. Exploring the Dataset</h2>
<p>Before starting with the clustering, we will explore the data by loading <code>DNP_ancient_authors.csv</code> into Python with <em>pandas</em>. Next, we will print out the first five rows and look at some information and overview statistics about each dataset using pandas&#39; <code>info()</code> and <code>describe()</code> methods.</p>
<pre><code class="language-python">import pandas as pd

# load the authors dataset that has been stored as a .csv files in a folder called &quot;data&quot; in the same directory as the Jupyter Notebook
df_authors = pd.read_csv(&quot;data/DNP_ancient_authors.csv&quot;, index_col=&quot;authors&quot;).drop(columns=[&quot;Unnamed: 0&quot;])

# display dataset structure with the pandas .info() method
print(df_authors.info())

# show first 5 rows
print(df_authors.head(5))

# display some statistics
print(df_authors.describe())
</code></pre>
<p>The output of the <code>info()</code> method should look like this:</p>
<pre><code class="language-python">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Index: 238 entries, Achilles Tatius of Alexandria to Zosimus
Data columns (total 8 columns):
 #   Column               Non-Null Count  Dtype
---  ------               --------------  -----
 0   word_count           238 non-null    int64
 1   modern_translations  238 non-null    int64
 2   known_works          238 non-null    int64
 3   manuscripts          238 non-null    int64
 4   early_editions       238 non-null    int64
 5   early_translations   238 non-null    int64
 6   modern_editions      238 non-null    int64
 7   commentaries         238 non-null    int64
dtypes: int64(8)
memory usage: 16.7+ KB
</code></pre>
<p>As we can see, our data consists of 238 entries of type integer. Next, we will examine our data through the output of the <em>pandas</em> <code>describe()</code> method.</p>
<p>The output of <code>df_authors.describe()</code> should look like this:</p>
<pre><code class="language-python">word_count    modern_translations    known_works    manuscripts    early_editions    early_translations    modern_editions    commentaries
count    238.000000    238.000000    238.000000    238.000000    238.000000    238.000000    238.000000    238.000000
mean    904.441176    12.970588    4.735294    4.512605    5.823529    4.794118    10.399160    3.815126
std    804.388666    16.553047    6.784297    4.637702    4.250881    6.681706    11.652326    7.013509
min    99.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000
25%    448.750000    4.250000    1.000000    1.000000    3.000000    0.000000    4.000000    0.000000
50%    704.000000    9.000000    2.000000    3.000000    5.000000    2.500000    7.000000    1.000000
75%    1151.500000    15.750000    6.000000    6.000000    8.000000    8.000000    14.000000    4.000000
max    9406.000000    178.000000    65.000000    34.000000    28.000000    39.000000    115.000000    43.000000
</code></pre>
<p>We can see that the standard deviation and the mean values vary significantly between the <code>word_count</code> column and the other columns. When working with metrics such as Euclidean distance in the <em>k</em>-means algorithm, different scales between the columns can become problematic. Thus, we should standardize the data before applying the clustering algorithm.</p>
<p>Furthermore, we have an significant standard deviation in almost every column and a vast difference between the 75th percentile value and the maximum value, particularly in the <code>word_count</code> column. This indicates that we might have some noise in our dataset, and it might be necessary to get rid of the noisy data points before we continue with our analysis. Therefore, we only keep those data points in our data frame with a word count within the 90th percentile range.</p>
<pre><code class="language-python">ninety_quantile = df_authors[&quot;word_count&quot;].quantile(0.9)
df_authors = df_authors[df_authors[&quot;word_count&quot;] &lt;= ninety_quantile]
</code></pre>
<h2 id="2-imports-and-additional-functions">2. Imports and Additional Functions</h2>
<p>Before we start with the actual clustering process, we first import all the necessary libraries and write a couple of functions that will help us to plot our results during the analysis. We will also use these functions and imports during the second case study in this tutorial (analyzing the <em>Religion</em> abstracts data). Thus, if you decide to skip the analysis of the ancient authors data, you still need to import these functions and libraries to execute the code in the second part of this tutorial.</p>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler as SS # z-score standardization 
from sklearn.cluster import KMeans, DBSCAN # clustering algorithms
from sklearn.decomposition import PCA # dimensionality reduction
from sklearn.metrics import silhouette_score # used as a metric to evaluate the cohesion in a cluster
from sklearn.neighbors import NearestNeighbors # for selecting the optimal eps value when using DBSCAN
import numpy as np

# plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns
from yellowbrick.cluster import SilhouetteVisualizer
</code></pre>
<p>The following function will help us to plot (and save) the silhouette plots.</p>
<pre><code class="language-python">def silhouettePlot(range_, data):
    &#39;&#39;&#39;
    we will use this function to plot a silhouette plot that helps us to evaluate the cohesion in clusters (k-means only)
    &#39;&#39;&#39;
    half_length = int(len(range_)/2)
    range_list = list(range_)
    fig, ax = plt.subplots(half_length, 2, figsize=(15,8))
    for _ in range_:
        kmeans = KMeans(n_clusters=_, random_state=42)
        q, mod = divmod(_ - range_list[0], 2)
        sv = SilhouetteVisualizer(kmeans, colors=&quot;yellowbrick&quot;, ax=ax[q][mod])
        ax[q][mod].set_title(&quot;Silhouette Plot with n={} Cluster&quot;.format(_))
        sv.fit(data)
    fig.tight_layout()
    fig.show()
    fig.savefig(&quot;silhouette_plot.png&quot;)
</code></pre>
<p>The next function will help us to plot (and save) the elbow plots.</p>
<pre><code class="language-python">def elbowPlot(range_, data, figsize=(10,10)):
    &#39;&#39;&#39;
    the elbow plot function helps to figure out the right amount of clusters for a dataset
    &#39;&#39;&#39;
    inertia_list = []
    for n in range_:
        kmeans = KMeans(n_clusters=n, random_state=42)
        kmeans.fit(data)
        inertia_list.append(kmeans.inertia_)
        
    # plotting
    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111)
    sns.lineplot(y=inertia_list, x=range_, ax=ax)
    ax.set_xlabel(&quot;Cluster&quot;)
    ax.set_ylabel(&quot;Inertia&quot;)
    ax.set_xticks(list(range_))
    fig.show()
    fig.savefig(&quot;elbow_plot.png&quot;)
</code></pre>
<p>The next function assists us in finding the right eps value when using DBSCAN.</p>
<pre><code class="language-python">def findOptimalEps(n_neighbors, data):
    &#39;&#39;&#39;
    function to find optimal eps distance when using DBSCAN; based on this article: https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc
    &#39;&#39;&#39;
    neigh = NearestNeighbors(n_neighbors=n_neighbors)
    nbrs = neigh.fit(data)
    distances, indices = nbrs.kneighbors(data)
    distances = np.sort(distances, axis=0)
    distances = distances[:,1]
    plt.plot(distances)
</code></pre>
<p>The last function <code>progressiveFeatureSelection()</code> implements a basic algorithm to select features from our dataset based on the silhouette score and <em>k</em>-means clustering. The algorithm first identifies a single feature with the best silhouette score when using <em>k</em>-means clustering. Afterward, the algorithm trains a <em>k</em>-means instance for each combination of the initially chosen feature and one of the remaining features. Next, it selects the two-feature combination with the best silhouette score. The algorithm uses this newly discovered pair of features to find the optimal combination of these two features with one of the remaining features, and so on. The algorithm continues until it has discovered the optimal combination of n features (where n is the value of the <code>max_features</code> parameter).</p>
<p>The algorithm is inspired by <a href="https://perma.cc/K5PD-GQPQ">this discussion on stackexchange.com</a>. Yet, don&#39;t worry too much about this implementation; there are better solutions for feature selection algorithms out there, as shown in <a href="https://perma.cc/3HQR-RL27">in Manoranjan Dash and Huan Liu&#39;s paper &#39;Feature Selection for Clustering&#39;</a> and <a href="https://perma.cc/25Y9-NS94">Salem Alelyani, Jiliang Tang, and Huan Liu&#39;s &#39;Feature Selection for Clustering: A Review&#39;</a>. However, most of the potential algorithms for feature selection in an unsupervised context are not implemented in scikit-learn, which is why I have decided to implement one myself, albeit basic.</p>
<pre><code class="language-python">def progressiveFeatureSelection(df, n_clusters=3, max_features=4,):
    &#39;&#39;&#39;
    very basic implementation of an algorithm for feature selection (unsupervised clustering); inspired by this post: https://datascience.stackexchange.com/questions/67040/how-to-do-feature-selection-for-clustering-and-implement-it-in-python
    &#39;&#39;&#39;
    feature_list = list(df.columns)
    selected_features = list()
    # select starting feature
    initial_feature = &quot;&quot;
    high_score = 0
    for feature in feature_list:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        data_ = df[feature]
        labels = kmeans.fit_predict(data_.to_frame())
        score_ = silhouette_score(data_.to_frame(), labels)
        print(&quot;Proposed new feature {} with score {}&quot;. format(feature, score_))
        if score_ &gt;= high_score:
            initial_feature = feature
            high_score = score_
    print(&quot;The initial feature is {} with a silhouette score of {}.&quot;.format(initial_feature, high_score))
    feature_list.remove(initial_feature)
    selected_features.append(initial_feature)
    for _ in range(max_features-1):
        high_score = 0
        selected_feature = &quot;&quot;
        print(&quot;Starting selection {}...&quot;.format(_))
        for feature in feature_list:
            selection_ = selected_features.copy()
            selection_.append(feature)
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            data_ = df[selection_]
            labels = kmeans.fit_predict(data_)
            score_ = silhouette_score(data_, labels)
            print(&quot;Proposed new feature {} with score {}&quot;. format(feature, score_))
            if score_ &gt; high_score:
                selected_feature = feature
                high_score = score_
        selected_features.append(selected_feature)
        feature_list.remove(selected_feature)
        print(&quot;Selected new feature {} with score {}&quot;. format(selected_feature, high_score))
    return selected_features
</code></pre>
<p>Note that we have selected n=3 clusters as default for the <em>k</em>-means instance in <code>progressiveFeatureSelection()</code>. In the context of an advanced hyperparameter tuning (which is beyond the scope of this tutorial), it might make sense to train the <code>progressiveFeatureSelection()</code> with different n values for the <em>k</em>-means instance as well. For the sake of simplicity, we stick to n=3 clusters in this tutorial.</p>
<h2 id="3-standardizing-the-dnp-ancient-authors-dataset">3. Standardizing the DNP Ancient Authors Dataset</h2>
<p>Next, we initialize scikit-learn&#39;s <code>StandardScaler()</code> to standardize our data. We apply scikit-learn&#39;s <a href="https://perma.cc/36NS-WUJT"><code>StandardScaler()</code></a> (z-score) to cast the mean of the columns to approximately zero and the standard deviation to one, to account for the huge differences between the <code>word_count</code> and the other columns in <code>df_ancient_authors.csv</code>.</p>
<pre><code class="language-python">scaler = SS()
DNP_authors_standardized = scaler.fit_transform(df_authors)
df_authors_standardized = pd.DataFrame(DNP_authors_standardized, columns=[&quot;word_count_standardized&quot;, &quot;modern_translations_standardized&quot;, &quot;known_works_standardized&quot;, &quot;manuscripts_standardized&quot;, &quot;early_editions_standardized&quot;, &quot;early_translations_standardized&quot;, &quot;modern_editions_standardized&quot;, &quot;commentaries_standardized&quot;])
df_authors_standardized = df_authors_standardized.set_index(df_authors.index)
</code></pre>
<h2 id="4-feature-selection">4. Feature Selection</h2>
<p>If you were to cluster the entire <code>DNP_ancient_authors.csv</code> with <em>k</em>-means, you would not find any reasonable clusters in the dataset. This is frequently the case when working with real-world data. However, in such cases, it might be pertinent to search for subsets of features that help us to structure the data. As we are only dealing with ten features, we could theoretically do this manually. However, because we have already implemented a basic algorithm to help us find potentially interesting combinations of features, we can also use our <code>progressiveFeatureSelection()</code> function. In this tutorial, we will search for three features that might be interesting to look at. Yet, feel free to try out different <code>max_features</code> with the <code>progressiveFeatureSelection()</code> function (as well as <code>n_clusters</code>). The selection of only three features (as well as n=3 clusters for the <em>k</em>-means instance) was a random choice which unexpectedly led to some exciting results; however, this does not mean that there are no other promising combinations which might be worth examining. </p>
<pre><code class="language-python">selected_features = progressiveFeatureSelection(df_authors_standardized, max_features=3, n_clusters=3)
</code></pre>
<p>Running this function, it turns out that the three features <code>known_works_standardized</code>, <code>commentaries_standardized</code>, and <code>modern_editions_standardized</code> might be worth considering when trying to cluster our data. Thus, we next create a new data frame with only these three features.</p>
<pre><code class="language-python">df_standardized_sliced = df_authors_standardized[selected_features]
</code></pre>
<h2 id="5-choosing-the-right-amount-of-clusters">5. Choosing the Right Amount of Clusters</h2>
<p>We will now apply the elbow method and then use silhouette plots to obtain an impression of how many clusters we should choose to analyze our dataset. We will check for two to ten clusters. Note, however, that the feature selection was also made with a pre-defined <em>k</em>-means algorithm using n=3 clusters. Thus, our three selected features might already tend towards this number of clusters.</p>
<pre><code class="language-python">elbowPlot(range(1,11), df_standardized_sliced)
</code></pre>
<p>The elbow plot looks like this:</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig7.png&quot; caption=&quot;Figure 7: Elbow plot of the df_standardized_sliced dataset.&quot; %}</p>
<p>Looking at the elbow plot indeed shows us that we find an “elbow” at n=3 as well as n=5 clusters. Yet, it is still quite challenging to decide whether to use three, four, five, or even six clusters. Therefore, we should also look at the silhouette plots.</p>
<pre><code class="language-python">silhouettePlot(range(3,9), df_standardized_sliced)
</code></pre>
<p>The silhouette plots look like this:</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig8.png&quot; caption=&quot;Figure 8: Silhouette plots of the df_standardized_sliced dataset.&quot; %}</p>
<p>Looking at the silhouette scores underlines our previous intuition that a selection of n=3 or n=5 seems to be the right choice of clusters. The silhouette plot with n=3 clusters in particular has a relatively high average silhouette score. Yet, because the two other clusters are far below the average silhouette score for n=3 clusters, we decide to analyze the dataset with <em>k</em>-means using n=5 clusters. However, the different sizes of the “knives” and their sharp form in both n=3 and n=5 clusters indicate a single dominant cluster and a couple of rather small and less cohesive clusters.</p>
<h2 id="6-n5-k-means-analysis-of-the-dnp-ancient-authors-dataset">6. n=5 <em>K</em>-Means Analysis of the DNP Ancient Authors Dataset</h2>
<p>Eventually, we can now train a <em>k</em>-means instance with n=5 clusters and plot the results using <em>seaborn</em>. I prefer plotting in two dimensions in Python, so we will use <code>PCA()</code> (<strong>Principal Component Analysis</strong>) to reduce the <a href="https://perma.cc/68J8-UFV9">dimensionality</a> of our dataset to two dimensions. <a href="https://perma.cc/E3RE-TKMM">PCA</a> is a great way to reduce the dimensionality of a dataset while keeping the variance from higher dimensions.</p>
<blockquote>
<p>PCA allows us to reduce the dimensionality of the original data substantially while retaining most of the salient information. On the PCA-reduced feature set, other machine learning algorithms—downstream in the machine learning pipeline—will have an easier time separating the data points in space (to perform tasks such as anomaly detection and clustering) and will require fewer computational resources. (quote from the online version of Ankur A. Patel: <em>Hands-On Unsupervised Learning Using Python</em>, O&#39;Reilly Media 2020)</p>
</blockquote>
<p>PCA can be used to reduce high-dimensional datasets for computational reasons. Yet, in this context, we only use PCA to plot the clusters in our dataset in a two-dimensional space. We will also apply PCA in the following text clustering. One huge disadvantage of using PCA is that we lose our initial features and create new ones that are somewhat nebulous to us, as they do not allow us to look at specific aspects of our data anymore (such as word counts or known works).</p>
<p>Before using PCA and plotting the results, we will instantiate a <em>k</em>-means instance with n=5 clusters and a <code>random_state</code> of 42. The latter parameter allows us to reproduce our results. 42 is an arbitrary choice here that refers to <a href="https://perma.cc/33RA-4ZS9">&quot;Hitchhiker&#39;s Guide to the Galaxy&quot;</a>, but you can choose whichever number you like.</p>
<pre><code class="language-python">kmeans = KMeans(n_clusters=5, random_state=42)
cluster_labels = kmeans.fit_predict(df_standardized_sliced)
df_standardized_sliced[&quot;clusters&quot;] = cluster_labels

# using PCA to reduce the dimensionality
pca = PCA(n_components=2, whiten=False, random_state=42)
authors_standardized_pca = pca.fit_transform(df_standardized_sliced)
df_authors_standardized_pca = pd.DataFrame(data=authors_standardized_pca, columns=[&quot;pc_1&quot;, &quot;pc_2&quot;])
df_authors_standardized_pca[&quot;clusters&quot;] = cluster_labels

# plotting the clusters with seaborn
sns.scatterplot(x=&quot;pc_1&quot;, y=&quot;pc_2&quot;, hue=&quot;clusters&quot;, data=df_authors_standardized_pca)
</code></pre>
<p>In the corresponding plot (see figure 9), we can clearly distinguish several clusters in our data. However, we also perceive what was already visible in the silhouette plots, namely that we only have one dense cluster and two to three less cohesive ones with several noise points.</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig9.png&quot; caption=&quot;Figure 9: Final plot of the clustered df_standardized_sliced dataset with seaborn.&quot; %}</p>
<h2 id="7-conclusion">7. Conclusion</h2>
<p>We were able to observe some clear clusters in our data when using <code>known_works_standardized</code>, <code>commentaries_standardized</code>, and <code>modern_editions_standardized</code> as a feature subset. But what does this actually tell us? This is a question that the algorithm cannot answer. The clustering algorithms only demonstrate that there are specific clusters under certain conditions, in this case, when looking for n=5 clusters with <em>k</em>-means and the above-mentioned subset of features. But what are these clusters about? Do they grant us valuable insights into our data? To answer this question, we need to look at the members of each cluster and analyze whether their grouping hints at certain aspects that might be worth exploring further.</p>
<p>In our example, looking at cluster 0 (the dense one in the left part of our plot) reveals that this cluster includes authors with very few known works, few to no commentaries, few modern editions, and rather short entries in the DNP (average word count of 513). Consequently, it largely consists of relatively unknown ancient authors.</p>
<table>
<thead>
<tr>
<th align="left">authors</th>
<th align="right">word_count</th>
<th align="right">modern_translations</th>
<th align="right">known_works</th>
<th align="right">manuscripts</th>
<th align="right">early_editions</th>
<th align="right">early_translations</th>
<th align="right">modern_editions</th>
<th align="right">commentaries</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Achilles Tatius of Alexandria</td>
<td align="right">383</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="right">9</td>
<td align="right">2</td>
<td align="right">1</td>
</tr>
<tr>
<td align="left">Aelianus Tacticus</td>
<td align="right">350</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Aelianus, Claudius (Aelian)</td>
<td align="right">746</td>
<td align="right">8</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="right">10</td>
<td align="right">8</td>
<td align="right">7</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Aeneas Tacticus</td>
<td align="right">304</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Aesop</td>
<td align="right">757</td>
<td align="right">18</td>
<td align="right">1</td>
<td align="right">6</td>
<td align="right">10</td>
<td align="right">2</td>
<td align="right">11</td>
<td align="right">1</td>
</tr>
<tr>
<td align="left">Agatharchides of Cnidus</td>
<td align="right">330</td>
<td align="right">2</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Agathias</td>
<td align="right">427</td>
<td align="right">4</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Alexander of Tralleis</td>
<td align="right">871</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">7</td>
<td align="right">3</td>
<td align="right">3</td>
<td align="right">4</td>
<td align="right">2</td>
</tr>
<tr>
<td align="left">Ammianus Marcellinus</td>
<td align="right">573</td>
<td align="right">8</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">6</td>
</tr>
<tr>
<td align="left">Anacreontea</td>
<td align="right">544</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">10</td>
<td align="right">5</td>
<td align="right">0</td>
</tr>
</tbody></table>
<p>As we can see in this snippet that shows the first ten entries in cluster 0, the author names (except Aesop) are more or less supporting our initial assumption that we are predominately dealing with authors whose work is produced in few modern editions, particularly compared to the authors in cluster 4.</p>
<p>The authors in cluster 4 (the less cohesive cluster at the upper right of our plot) comprise well-known and extensively discussed authors including Plato or Aristophanes, who have all written several works that are still famous and have remained relevant over the centuries, demonstrated by the high number of modern editions and commentaries.</p>
<table>
<thead>
<tr>
<th align="left">authors</th>
<th align="right">word_count</th>
<th align="right">modern_translations</th>
<th align="right">known_works</th>
<th align="right">manuscripts</th>
<th align="right">early_editions</th>
<th align="right">early_translations</th>
<th align="right">modern_editions</th>
<th align="right">commentaries</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Aeschylus of Athens</td>
<td align="right">1758</td>
<td align="right">31</td>
<td align="right">7</td>
<td align="right">5</td>
<td align="right">10</td>
<td align="right">14</td>
<td align="right">15</td>
<td align="right">20</td>
</tr>
<tr>
<td align="left">Aristophanes of Athens</td>
<td align="right">1108</td>
<td align="right">18</td>
<td align="right">11</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">30</td>
<td align="right">7</td>
<td align="right">18</td>
</tr>
<tr>
<td align="left">Lucanus, Marcus Annaeus</td>
<td align="right">1018</td>
<td align="right">17</td>
<td align="right">1</td>
<td align="right">11</td>
<td align="right">8</td>
<td align="right">15</td>
<td align="right">20</td>
<td align="right">25</td>
</tr>
<tr>
<td align="left">Plato</td>
<td align="right">1681</td>
<td align="right">31</td>
<td align="right">18</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">10</td>
<td align="right">20</td>
</tr>
<tr>
<td align="left">Plutarchus of Chaeronea (Plutarch)</td>
<td align="right">1485</td>
<td align="right">37</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">15</td>
<td align="right">42</td>
</tr>
<tr>
<td align="left">Propertius, Sextus</td>
<td align="right">1443</td>
<td align="right">22</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">24</td>
<td align="right">22</td>
</tr>
<tr>
<td align="left">Sallustius Crispus, Gaius (Sallust)</td>
<td align="right">1292</td>
<td align="right">17</td>
<td align="right">5</td>
<td align="right">12</td>
<td align="right">7</td>
<td align="right">15</td>
<td align="right">15</td>
<td align="right">16</td>
</tr>
<tr>
<td align="left">Sophocles</td>
<td align="right">1499</td>
<td align="right">67</td>
<td align="right">8</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">14</td>
<td align="right">18</td>
</tr>
<tr>
<td align="left">Tacitus, (Publius?) Cornelius</td>
<td align="right">1504</td>
<td align="right">29</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="right">10</td>
<td align="right">14</td>
<td align="right">31</td>
<td align="right">20</td>
</tr>
</tbody></table>
<p>If you want to have a closer look at the other clusters, I advise you to check out the Jupyter notebook in the <a href="https://github.com/programminghistorian/jekyll/tree/gh-pages/assets/clustering-with-scikit-learn-in-python">GitHub repository</a>.</p>
<p>Thus, our clustering of the <code>DNP_ancient_authors.csv</code> dataset has resulted in some promising clusters, which might help us develop new research questions. For instance, we could now take these clusters and apply our hypothesis about their relevance to further explore clustering the authors, based on their early and modern translations/editions. However, this is beyond the scope of this tutorial, which is primarily concerned with introducing tools and methods to examine such research questions. </p>
<h1 id="second-case-study-clustering-textual-data">Second Case Study: Clustering Textual Data</h1>
<p>The second section of this tutorial will deal with textual data, namely all abstracts scraped from the <a href="https://perma.cc/P4VN-6K9K"><em>Religion</em> (journal)</a> website. We will try to cluster the abstracts based on their word features in the form of <strong>TF-IDF</strong> vectors (which is short for &quot;<strong>T</strong>ext <strong>F</strong>requency - <strong>I</strong>nverted <strong>D</strong>ocument <strong>F</strong>requency&quot;).</p>
<h2 id="1-loading-the-dataset--exploratory-data-analysis">1. Loading the Dataset &amp; Exploratory Data Analysis</h2>
<p>Using a similar method as that used to analyze the <code>DNP_ancient_authors.csv</code> dataset, we will first load the <code>RELIGION_abstracts.csv</code> into our program and look at some summary statistics.</p>
<pre><code class="language-python">df_abstracts = pd.read_csv(&quot;data/RELIGION_abstracts.csv&quot;).drop(columns=&quot;Unnamed: 0&quot;)
df_abstracts.info()
df_abstracts.describe()
</code></pre>
<p>The result of the <code>describe()</code> method should print out something like this:</p>
<pre><code class="language-python">title    abstract    link    volume
count    701    701    701    701
unique    701    701    701    40
top    From locality to (...) https://www.tandfonline.com/doi/abs/10.1006/reli.1996.9998 Titel anhand dieser DOI in Citavi-Projekt übernehmen    https://www.tandfonline.com/loi/rrel20?treeId=vrrel20-50
freq    1    1    1    41
</code></pre>
<p>Unlike in the previous dataset, we are now dealing with features where every single observation is unique.</p>
<h2 id="2-tf-idf-vectorization">2. TF-IDF Vectorization</h2>
<p>In order to process the textual data with clustering algorithms, we need to convert the texts into vectors. For this purpose, we are using the scikit-learn implementation of <a href="https://perma.cc/Q2JN-YWV6">TF-IDF vectorization</a>. For a good introduction to how TF-IDF works, see this <a href="https://perma.cc/3XT2-DB6X">great tutorial by Melanie Walsh</a>.</p>
<h3 id="optional-step-lemmatization"><em>Optional Step</em>: Lemmatization</h3>
<p>As an optional step, I have implemented a function called <code>lemmatizeAbstracts()</code> that groups, or ‘lemmatizes’ the abstracts using <a href="https://perma.cc/RTM6-8B27">spaCy</a>. Considering that we are not interested in stylistic similarities between the abstracts, this step helps to reduce the overall amount of features (words) in our dataset. As part of the lemmatization function, we also clean the text of all punctuation and other noise such as brackets. In the following analysis, we will continue working with the lemmatized version of the abstracts. However, you can also keep using the original texts and skip the lemmatization, although this might lead to different results.</p>
<pre><code class="language-python"># lemmatization (optional step)
import spacy
import re
nlp = spacy.load(&quot;en_core_web_sm&quot;)

def lemmatizeAbstracts(x):
        doc = nlp(x)
        new_text = []
        for token in doc:
            new_text.append(token.lemma_)
        text_string = &quot; &quot;.join(new_text)
        # getting rid of non-word characters
        text_string = re.sub(r&quot;[^\w\s]+&quot;, &quot;&quot;, text_string)
        text_string = re.sub(r&quot;\s{2,}&quot;, &quot; &quot;, text_string)
        return text_string

df_abstracts[&quot;abstract_lemma&quot;] = df_abstracts[&quot;abstract&quot;].apply(lemmatizeAbstracts)
df_abstracts.to_csv(&quot;data/RELIGION_abstracts_lemmatized.csv&quot;)
</code></pre>
<p>I have decided to save the new lemmatized version of our abstracts as <code>RELIGION_abstracts_lemmatized.csv</code>. This prevents from having to redo the lemmatization each time we restart our notebook.</p>
<h3 id="tf-idf-vectorization">TF-IDF Vectorization</h3>
<p>The first step is to instantiate our TF-IDF model by passing it the <code>argument</code> to ignore stop words, such as &quot;the,&quot; &quot;a,&quot; etc. The second step is rather similar to the training of our <em>k</em>-means instance in the previous part: We are passing the abstracts from our dataset to the vectorizer in order to convert them to machine-readable vectors. For the moment, we are not passing any additional arguments. Finally, we create a new pandas DataFrame object based on the TF-IDF matrix of our textual data.</p>
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(stop_words=&quot;english&quot;)
df_abstracts_tfidf = tfidf.fit_transform(df_abstracts[&quot;abstract_lemma&quot;])
</code></pre>
<p>When printing out the <code>df_abstracts_tfidf</code> object, you can see that our initial matrix is <em>huge</em> and includes over 8,000 words from the overall vocabulary of the 701 abstracts. This is obviously too much, not only from a computational perspective but also because clustering algorithms such as <em>k</em>-means become less efficient due to the so-called <a href="https://perma.cc/S748-FPNG">&quot;curse of dimensionality&quot;</a>. We will thus need to reduce the number of features significantly.</p>
<p>To do so, we first create a new version of our TF-IDF vectorized data. This time, however, we tell the vectorizer that we only want a reduced set of 250 features. We also tell the model to only consider words from the vocabulary that appear in at least five different documents but in no more than 200. We also add the possibility to include single words and bigrams (such as “19th century”). Finally, we tell our model to clean the text of any potential accents.</p>
<p>Secondly, we are also using the <em>Principal Component Analysis</em> (PCA), this time to reduce the dimensionality of the dataset from 250 to 10 dimensions. </p>
<pre><code class="language-python"># creating a new TF-IDF matrix
tfidf = TfidfVectorizer(stop_words=&quot;english&quot;, ngram_range=(1,2), max_features=250, strip_accents=&quot;unicode&quot;, min_df=10, max_df=200)
tfidf_religion_array = tfidf.fit_transform(df_abstracts[&quot;abstract_lemma&quot;])
df_abstracts_tfidf = pd.DataFrame(tfidf_religion_array.toarray(), index=df_abstracts.index, columns=tfidf.get_feature_names())
df_abstracts_tfidf.describe()
</code></pre>
<h2 id="3-dimensionality-reduction-using-pca">3. Dimensionality Reduction Using PCA</h2>
<p>As mentioned above, let us next apply <code>PCA()</code> to caste the dimension from d=250 to d=10 to account for the <em>curse of dimensionality</em> when using <em>k</em>-means. Similar to the selection of n=3 <code>max_features</code> during the analysis of our ancient authors dataset, setting the dimensionality to d=10 was a random choice that happened to produce promising results. However, feel free to play around with these parameters while conducting a more elaborate hyperparameter tuning. Maybe you can find values for these parameters that result in an even more effective clustering of the data. For instance, you might want to use a <a href="https://perma.cc/PYZ5-6QAV">scree plot</a> to figure out the optimal number of principal components in PCA, which works quite similarly to our elbow method in the context of <em>k</em>-means.</p>
<pre><code class="language-python"># using PCA to reduce the dimensionality
pca = PCA(n_components=10, whiten=False, random_state=42)
abstracts_pca = pca.fit_transform(df_abstracts_tfidf)
df_abstracts_pca = pd.DataFrame(data=abstracts_pca)
</code></pre>
<h2 id="4-applying-k-means-clustering-on-textual-data">4. Applying <em>K</em>-Means Clustering on Textual Data</h2>
<p>Next, we try to find a reasonable method for clustering the abstracts using <em>k</em>-means. As we did in the case of the <code>DNP_ancient_authors.csv</code> dataset, we will start by searching for the right amount of clusters applying the elbow method and the silhouette score.</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig10.png&quot; caption=&quot;Figure 10: Elbow plot with 3 to 99 clusters.&quot; %}</p>
<p>As we can see, there is no real elbow in our plot this time. This might imply that there are no big clusters in our <code>RELIGION_abstracts.csv</code> dataset. But is it likely that a journal such as <em>Religion</em> that covers a vast spectrum of phenomena (which are all, of course, related to religion) only comprises a few thematic clusters? Probably not. Therefore, let us continue by skipping the silhouette score plots (which are most likely of no value with such a huge number of clusters) and just train a <em>k</em>-means instance with n=100 clusters and assess the results.</p>
<pre><code class="language-python">kmeans = KMeans(n_clusters=100, random_state=42)
abstracts_labels = kmeans.fit_predict(df_abstracts_pca)
df_abstracts_labeled = df_abstracts.copy()
df_abstracts_labeled[&quot;cluster&quot;] = abstracts_labels
</code></pre>
<p>We will next evaluate the results by printing out some article titles of randomly chosen clusters. For instance, when analyzing the titles in cluster 75, we can perceive that all articles in this cluster are related to Theravāda Buddhism, Karma, and their perception in &quot;the West&quot;:</p>
<pre><code class="language-python">df_abstracts_labeled[df_abstracts_labeled[&quot;cluster&quot;] == 75][[&quot;title&quot;, &quot;cluster&quot;]]
</code></pre>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="left">title</th>
<th align="right">cluster</th>
</tr>
</thead>
<tbody><tr>
<td align="right">210</td>
<td align="left">Checking the heavenly ‘bank account of karma’: cognitive metaphors for karma in Western perception and early Theravāda Buddhism</td>
<td align="right">75</td>
</tr>
<tr>
<td align="right">211</td>
<td align="left">Karma accounts: supplementary thoughts on Theravāda, Madhyamaka, theosophy, and Protestant Buddhism</td>
<td align="right">75</td>
</tr>
<tr>
<td align="right">258</td>
<td align="left">Resonant paradigms in the study of religions and the emergence of Theravāda Buddhism</td>
<td align="right">75</td>
</tr>
</tbody></table>
<p>Cluster 15 includes articles related to the body and its destruction:</p>
<pre><code class="language-python">df_abstracts_labeled[df_abstracts_labeled[&quot;cluster&quot;] == 15][[&quot;title&quot;, &quot;cluster&quot;]]
</code></pre>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="left">title</th>
<th align="right">cluster</th>
</tr>
</thead>
<tbody><tr>
<td align="right">361</td>
<td align="left">Candanbālā&#39;s hair: Fasting, beauty, and the materialization of Jain wives</td>
<td align="right">15</td>
</tr>
<tr>
<td align="right">425</td>
<td align="left">Monkey kings make havoc: Iconoclasm and murder in the Chinese cultural revolution</td>
<td align="right">15</td>
</tr>
<tr>
<td align="right">623</td>
<td align="left">Techniques of body and desire in Kashmir Śaivism</td>
<td align="right">15</td>
</tr>
<tr>
<td align="right">695</td>
<td align="left">Body-symbols and social reality: Resurrection, incarnation and asceticism in early Christianity</td>
<td align="right">15</td>
</tr>
</tbody></table>
<p>To be fair, other clusters are harder to interpret. A good example is cluster 84. Yet, even in the case of cluster 84 there still seems to be a pattern, namely that almost all articles are related to famous scholars and works in the study of religion, such as Durkheim, Tylor, Otto, Said, etc.</p>
<pre><code class="language-python">df_abstracts_labeled[df_abstracts_labeled[&quot;cluster&quot;] == 84][[&quot;title&quot;, &quot;cluster&quot;]]
</code></pre>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="left">title</th>
<th align="right">cluster</th>
</tr>
</thead>
<tbody><tr>
<td align="right">80</td>
<td align="left">Latin America 1520–1600: a page in the history of the study of religion</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">141</td>
<td align="left">On elves and freethinkers: criticism of religion and the emergence of the literary fantastic in Nordic literature</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">262</td>
<td align="left">Is Durkheim&#39;s understanding of religion compatible with believing?</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">302</td>
<td align="left">Dreaming and god concepts</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">426</td>
<td align="left">Orientalism, representation and religion: The reality behind the myth</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">448</td>
<td align="left">The Science of Religions in a Fascist State: Rudolf Otto and Jakob Wilhelm Hauer During the Third Reich</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">458</td>
<td align="left">Religion Within the Limits of History: Schleiermacher and Religion—A Reappraisal</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">570</td>
<td align="left">Cognitive and Ideological Aspects of Divine Anthropomorphism</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">571</td>
<td align="left">Tylor&#39;s Anthropomorphic Theory of Religion</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">614</td>
<td align="left">‘All my relatives’: Persons in Oglala religion</td>
<td align="right">84</td>
</tr>
<tr>
<td align="right">650</td>
<td align="left">Colloquium: Does autonomy entail theology? Autonomy, legitimacy, and the study of religion</td>
<td align="right">84</td>
</tr>
</tbody></table>
<p>As we can see, even a simple implementation of <em>k</em>-means on textual data without much feature-tuning has resulted in a <em>k</em>-means instance that is, despite its shortcomings, able to assist us by doing the work of a basic <a href="https://perma.cc/57JH-G9EZ">recommender system</a>. For example, we could use our trained <em>k</em>-means instance to suggest articles to visitors of our website based on their previous readings. Of course, we can also use our model during our exploratory data analysis to show us thematic clusters discussed in <em>Religion</em>.</p>
<p>Yet, as the textual data in this example is rather difficult to cluster and includes noise points or clusters that contain very few articles, it might make better sense to apply a different clustering algorithm and see how it performs.</p>
<h2 id="5-applying-dbscan-clustering-on-textual-data">5. Applying DBSCAN Clustering on Textual Data</h2>
<p>Even though the <em>k</em>-means clustering of our data already resulted in some valuable insights, it might still be interesting to apply a different clustering algorithm such as DBSCAN. As explained above, DBSCAN excludes noise points and outliers in our data, meaning that it focuses on those regions in our data that may rightfully be called dense.</p>
<p>We will be using the d=10 reduced version of our <code>RELIGION_abstracts.csv</code> dataset, which allows us to use Euclidean distance as a metric. If we were to use the initial TF-IDF matrix with 250 features, we would need to consider changing the underlying metric to <a href="https://perma.cc/HVZ5-9MXU">cosine distance</a>, which is more suitable when dealing with sparse matrices, as in the case of textual data. </p>
<p>The first step will be to use our <code>findOptimalEps()</code> function to figure out which eps value is most suitable for our data.</p>
<pre><code class="language-python">findOptimalEps(2, df_abstracts_tfidf)
</code></pre>
<p>As can be seen in figure 11, the eps-plotting suggests choosing an eps value between 0.2 and 0.25.</p>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig11.png&quot; caption=&quot;Figure 11: Eps plot for the abstracts dataset.&quot; %}</p>
<p>We are selecting 0.2 as eps value and train a DBSCAN instance.</p>
<pre><code class="language-python">dbscan = DBSCAN(eps=0.2, metric=&quot;euclidean&quot;)
dbscan_labels = dbscan.fit_predict(df_abstracts_pca)
df_abstracts_dbscan = df_abstracts.copy()
df_abstracts_dbscan[&quot;cluster&quot;] = dbscan_labels
df_abstracts_dbscan[&quot;cluster&quot;].unique()
</code></pre>
<p>As we can see when looking at the DBSCAN results in our Jupyter notebook, using a DBSCAN instance under these circumstances results in only four clusters and a vast noise points cluster (-1) with more than 150 entries and an even bigger cluster with more than 500 entries (cluster 0). These clusters are plotted in figure 12 (using a PCA-reduced dataset), where the inconclusive results become even more visible. In this case, we could consider using the original TF-IDF matrix with cosine distance instead.</p>
<p>Its shortcomings aside, the current version of our DBSCAN instance does give some promising insights, for example with cluster 3, which collects articles related to gender and women in different religions:</p>
<pre><code class="language-python">df_abstracts_dbscan[df_abstracts_dbscan[&quot;cluster&quot;] == 1][[&quot;title&quot;, &quot;cluster&quot;]]
</code></pre>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="left">title</th>
<th align="right">cluster</th>
</tr>
</thead>
<tbody><tr>
<td align="right">154</td>
<td align="left">Lifelong minority religion: routines and reflexivity: A Bourdieuan perspective on the habitus of elderly Finnish Orthodox Christian women</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">161</td>
<td align="left">Quiet beauty: problems of agency and appearance in evangelical Christianity</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">388</td>
<td align="left">Renunciation feminised? Joint renunciation of female–male pairs in Bengali Vaishnavism</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">398</td>
<td align="left">Conclusion: Construction sites at the juncture of religion and gender</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">502</td>
<td align="left">Gender and the Contest over the Indian Past</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">506</td>
<td align="left">Art as Neglected ‘Text’ for the Study of Gender and Religion in Africa</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">507</td>
<td align="left">A Medieval Feminist Critique of the Chinese World Order: The Case of Wu Zhao (r. 690–705)</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">509</td>
<td align="left">Notions of Destiny in Women&#39;s Self-Construction</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">526</td>
<td align="left">The Fundamental Unity of the Conservative and Revolutionary Tendencies in Venezuelan Evangelicalism: The Case of Conjugal Relations</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">551</td>
<td align="left">Hindu Women, Destiny and Stridharma</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">644</td>
<td align="left">The women around James Nayler, Quaker: A matter of emphasis</td>
<td align="right">1</td>
</tr>
<tr>
<td align="right">668</td>
<td align="left">Women as aspects of the mother Goddess in India: A case study of Ramakrishna</td>
<td align="right">1</td>
</tr>
</tbody></table>
<p>Cluster 2, on the other hand, seems to be related to belief and atheism:</p>
<pre><code class="language-python">df_abstracts_dbscan[df_abstracts_dbscan[&quot;cluster&quot;] == 2][[&quot;title&quot;, &quot;cluster&quot;]]
</code></pre>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="left">title</th>
<th align="right">cluster</th>
</tr>
</thead>
<tbody><tr>
<td align="right">209</td>
<td align="left">Three cognitive routes to atheism: a dual-process account</td>
<td align="right">2</td>
</tr>
<tr>
<td align="right">282</td>
<td align="left">THE CULTURAL TRANSMISSION OF FAITH Why innate intuitions are necessary, but insufficient, to explain religious belief</td>
<td align="right">2</td>
</tr>
<tr>
<td align="right">321</td>
<td align="left">Religion is natural, atheism is not: On why everybody is both right and wrong</td>
<td align="right">2</td>
</tr>
<tr>
<td align="right">322</td>
<td align="left">Atheism is only skin deep: Geertz and Markusson rely mistakenly on sociodemographic data as meaningful indicators of underlying cognition</td>
<td align="right">2</td>
</tr>
<tr>
<td align="right">323</td>
<td align="left">The relative unnaturalness of atheism: On why Geertz and Markússon are both right and wrong</td>
<td align="right">2</td>
</tr>
<tr>
<td align="right">378</td>
<td align="left">The science of religious beliefs</td>
<td align="right">2</td>
</tr>
<tr>
<td align="right">380</td>
<td align="left">Adaptation, evolution, and religion</td>
<td align="right">2</td>
</tr>
</tbody></table>
<p>{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig12.png&quot; caption=&quot;Figure 12: PCA-reduced version of the abstracts dataset displaying the DBSCAN clustering with eps=0.2.&quot; %}</p>
<p>Although the clustering was far from perfect in this case, it did produce some valuable information, which we could use in combination with the more promising results of the <em>k</em>-means clustering. It might also be pertinent to keep tuning the parameters and trying out different feature sets (reduced, non-reduced, maybe by adding some additional feature selection steps of choosing promising word fields, etc.) to achieve better results with DBSCAN. Of course, we could also apply some other clustering algorithms and then combine the results.</p>
<p>As a next step, we could pursue the idea of building a basic recommender system which suggests articles with similar topics to readers based on their previous readings. This recommender system should consider the clustering of the <em>k</em>-means instance but also include suggestions made by DBSCAN and other potential clustering algorithms. When applied in combination, the rather unsatisfactory results of the DBSCAN model might be less problematic because they are now used as additional information only.</p>
<p>Of course, we as scholars in the humanities will be more likely to use these techniques as part of our research during the exploratory data analysis phase. In this case, combining the results of different clustering algorithms helps us to discover structures and thematic clusters in our data. These discoveries could then lead to new research questions. For instance, there might be specific clusters in the <em>Religion</em> abstracts data that include more articles than the other clusters, thereby indicating an overall thematic focus of this journal that might be worth examining to get an overview of research trends in the study of religion throughout recent decades.</p>
<h1 id="summary">Summary</h1>
<p>I hope to have shown that clustering is indeed a valuable step during exploratory data analysis that enables you to gain new insights into your data.</p>
<p>The clustering of the <code>DNP_ancient_authors.csv</code> and the <code>RELIGION_abstracts.csv</code> datasets provided decent results and identified reasonable groupings of authors and articles in the data. In the case of the abstracts dataset, we have even built a basic recommender system that assists us when searching for articles with similar topics. Yet, the discussion of the results also illustrated that there is always room for interpretation and that not every cluster necessarily needs to provide useful insights from a scholarly (or human) perspective. Despite this general ambiguity when applying machine learning algorithms, our analysis demonstrated that <em>k</em>-means and DBSCAN are great tools that can help you to develop or empirically support new research questions. In addition, they may also be implemented for more practical tasks, for instance, when searching for articles related to a specific topic.</p>
<h1 id="bibliography">Bibliography</h1>
<ul>
<li>Géron, Aurélien. <em>Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. Concepts, tools, and techniques to build intelligent systems, 2nd ed</em>. Sebastopol: O’Reilly, 2019.</li>
<li>Mitchell, Ryan. <em>Web scraping with Python. Collecting more data from the modern web, 1st ed</em>. Sebastopol: O’Reilly, 2018.  </li>
<li>Patel, Ankur A. <em>Hands-on unsupervised learning using Python: How to build applied machine learning solutions from unlabeled data, 1st ed</em>. Sebastopol: O’Reilly, 2019.</li>
</ul>
<h1 id="footnotes">Footnotes</h1>
<p>[^1]: For a good introduction to the use of <em>requests</em> and web scraping in general, see the corresponding articles on <em>The Programming Historian</em> such as <a href="https://perma.cc/J5BV-MZPZ">Introduction to BeautifulSoup</a> (last accessed: 2021-04-22) or books such as Mitchell (2018).</p>
<p>[^2]: Yet, there are certain cases where <em>k</em>-means clustering might fail to identify the clusters in your data. Thus, it is usually recommended to apply several clustering algorithms. A good illustration of the restrictions of <em>k</em>-means clustering can be seen in the examples under <a href="https://perma.cc/MH6W-A6UP">this link</a> (last accessed: 2021-04-23) to the scikit-learn website, particularly in the second plot on the first row.</p>
<p>[^3]: <a href="https://perma.cc/DZT5-VPLV">Definition of inertia on scikit-learn</a> (last accessed: 2021-04-23).</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="clustering-with-scikit-learn-in-python/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Clustering with Scikit-Learn in Python\",\"collection\":\"lessons\",\"layout\":\"lesson\",\"slug\":\"clustering-with-scikit-learn-in-python\",\"date\":\"2021-09-29T00:00:00.000Z\",\"mathjax\":true,\"authors\":[\"Thomas Jurczyk\"],\"reviewers\":[\"Melanie Walsh\",\"Luling Huang\"],\"editors\":[\"Alex Wermer-Colan\"],\"review-ticket\":\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fph-submissions\u002Fissues\u002F325\",\"difficulty\":3,\"activity\":\"analyzing\",\"topics\":[\"python\",\"data-manipulation\"],\"avatar_alt\":\"Microscope images of bacteria\",\"doi\":\"10.46430\u002Fphen0094\",\"abstract\":\"This tutorial demonstrates how to apply clustering algorithms with Python to a dataset with two concrete use cases. The first example uses clustering to identify meaningful groups of Greco-Roman authors based on their publications and their reception. The second use case applies clustering algorithms to textual data in order to discover thematic groups. After finishing this tutorial, you will be able to use clustering in Python with Scikit-learn applied to your own data, adding an invaluable method to your toolbox for exploratory data analysis.\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"introduction\\\"\u003EIntroduction\u003C\u002Fh1\u003E\\n\u003Cp\u003EThis tutorial demonstrates how to implement and apply \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FGL9D-9GRG\\\"\u003E\u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering\u003C\u002Fa\u003E and \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F6JNW-DCNT\\\"\u003EDBSCAN\u003C\u002Fa\u003E in Python. \u003Cem\u003EK\u003C\u002Fem\u003E-means and DBSCAN are two popular clustering algorithms that can be used, in combination with others, during the exploratory data analysis to discover (hidden) structures in your data by identifying groups with similar \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FTG79-SQP3\\\"\u003Efeatures\u003C\u002Fa\u003E (see Patel 2019 in the bibliography). We will implement the clustering algorithms using \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FZ9AT-N6SB\\\"\u003Escikit-learn\u003C\u002Fa\u003E, a widely applied and well-documented machine learning framework in Python. Also, scikit-learn has a huge community and offers smooth implementations of various machine learning algorithms. Once you have understood how to implement \u003Cem\u003Ek\u003C\u002Fem\u003E-means and DBSCAN with scikit-learn, you can easily use this knowledge to implement other machine learning algorithms with scikit-learn, too.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis tutorial consists of two different case studies. The first case study clusters and analyzes an ancient authors dataset from \u003Cem\u003EBrill&#39;s New Pauly\u003C\u002Fem\u003E. The second case study focuses on clustering textual data, namely abstracts of all published articles in the journal \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FP4VN-6K9K\\\"\u003E\u003Cem\u003EReligion\u003C\u002Fem\u003E\u003C\u002Fa\u003E (Taylor &amp; Francis). These two datasets have been selected to illustrate how clustering algorithms can handle different data types (including numerical and textual features) and potentially be applied to a broad range of potential research topics.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe following section will introduce both datasets.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"first-case-study-ancient-authors-in-brills-new-pauly\\\"\u003EFirst Case Study: Ancient Authors in \u003Cem\u003EBrill&#39;s New Pauly\u003C\u002Fem\u003E\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn this example, we will use \u003Cem\u003Ek\u003C\u002Fem\u003E-means to analyze a dataset including information about 238 ancient authors from Greco-Roman antiquity. The data was taken from the official \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F4377-UUE8\\\"\u003E\u003Cem\u003EBrill&#39;s New Pauly\u003C\u002Fem\u003E\u003C\u002Fa\u003E website, and originates from \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FGJZ9-9779\\\"\u003ESupplement I Volume 2: Dictionary of Greek and Latin Authors and Texts\u003C\u002Fa\u003E. \u003Cem\u003EDer Neue Pauly: Realenzyklopädie der Antike\u003C\u002Fem\u003E (in English \u003Cem\u003EBrill&#39;s New Pauly\u003C\u002Fem\u003E) (1996–2002) is a well-known encyclopedia of the ancient world with contributions from established international scholars. It should be noted that access to the texts (and thus the data) in the \u003Cem\u003ENew Pauly\u003C\u002Fem\u003E is not free of charge. I used my university&#39;s access to obtain the data from the author entries. For the following analyses, I have not copied any texts from the \u003Cem\u003ENew Pauly\u003C\u002Fem\u003E to the dataset. However, the numerical data in the dataset was extracted and partially accumulated from the author entries in the \u003Cem\u003ENew Pauly\u003C\u002Fem\u003E. The original German version has been translated into English since 2002. I will refer to the text using its German abbreviation (DNP) from here onwards.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis tutorial demonstrates how \u003Cem\u003Ek\u003C\u002Fem\u003E-means can help to cluster ancient authors into separate groups. The overall idea is that clustering algorithms either provide us with new insights into our data structure, or falsify\u002Fverify existing hypotheses. For instance, there might be groups of authors who are discussed at length but to whom few manuscripts are attributed. Meanwhile, other groups may include authors to whom many surviving manuscripts are ascribed but who only have short entries in the DNP. Another scenario could be that we find groups of authors associated with many early editions but only a few modern ones. This would point to the fact that modern scholars continue to rely on older editions when reading these authors. In the context of this tutorial, we will leave it to the algorithms to highlight such promising clusters for us.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe author data was collected from the official website using Python modules and libraries such as \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FXK5T-JH2Z\\\"\u003Erequests\u003C\u002Fa\u003E, \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F5RP2-869V\\\"\u003EBeautifulSoup\u003C\u002Fa\u003E, and \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FVJ62-2AM2\\\"\u003Epandas\u003C\u002Fa\u003E.[^1] The data was then stored in a csv file named \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E (see also the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fjekyll\u002Ftree\u002Fgh-pages\u002Fassets\u002Fclustering-with-scikit-learn-in-python\\\"\u003EGitHub repository\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\\n\u003Cp\u003EA single observation (row) in the \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E dataset contains an author name as an index and observations of the following seven features (variables):\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EWord count of the entry in the DNP, used as a measure to evaluate the importance of the author (\u003Ccode\u003Eword_count\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003ENumber of modern translations (\u003Ccode\u003Emodern_translations\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003ENumber of known works (\u003Ccode\u003Eknown_works\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003ENumber of existing manuscripts (\u003Ccode\u003Emanuscripts\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003ENumber of early editions (\u003Ccode\u003Eearly_editions\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003ENumber of early translations (\u003Ccode\u003Eearly_translations\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003ENumber of modern editions (\u003Ccode\u003Emodern_editions\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003ENumber of commentaries (\u003Ccode\u003Ecommentaries\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003ESo, a single row in the dataset looks like this:\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"left\\\"\u003Eauthors\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eword_count\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Emodern_translations\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eknown_works\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Emanuscripts\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eearly_editions\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eearly_translations\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Emodern_editions\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Ecommentaries\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAelianus Tacticus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E350\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Ch2 id=\\\"second-case-study-article-abstracts-in-religion-journal\\\"\u003ESecond Case Study: Article Abstracts in \u003Cem\u003EReligion\u003C\u002Fem\u003E (Journal)\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe second dataset contains abstracts of all published articles in the journal \u003Cem\u003EReligion\u003C\u002Fem\u003E (Taylor &amp; Francis). The abstracts were collected from the official website using Python modules and libraries such as requests, BeautifulSoup, and pandas. The data was stored in a csv file named \u003Ccode\u003ERELIGION_abstracts.csv\u003C\u002Fcode\u003E (see the GitHub repository). The current dataset includes abstracts from 701 articles published in 51 volumes between 1971–2021. However, some articles, particularly in older volumes, did not posses scrapable abstracts on the website and were thus left out. Other contribution types, including reviews and miscellanea, have also been excluded from this dataset.\u003C\u002Fp\u003E\\n\u003Cp\u003EA single row in the \u003Ccode\u003ERELIGION_abstracts.csv\u003C\u002Fcode\u003E dataset contains an numerical index and observations of the following four features (variables):\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EThe title of the article (\u003Ccode\u003Etitle\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003EThe full abstract (\u003Ccode\u003Eabstract\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003EA link to the article (\u003Ccode\u003Elink\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003Cli\u003EA link to the volume in which the article (abstract) was published (\u003Ccode\u003Evolume\u003C\u002Fcode\u003E)\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003ESo, a single row in this dataset looks like this:\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"left\\\"\u003Etitle\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eabstract\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Elink\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Evolume\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003ENorwegian Muslims denouncing terrorism: beyond ‘moderate’ versus ‘radical’?\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003EIn contemporary (...)\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.tandfonline.com\u002Fdoi\u002Ffull\u002F10.1080\u002F0048721X.2021.1865600\\\"\u003Ehttps:\u002F\u002Fwww.tandfonline.com\u002Fdoi\u002Ffull\u002F10.1080\u002F0048721X.2021.1865600\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E\u003Ca href=\\\"https:\u002F\u002Fwww.tandfonline.com\u002Floi\u002Frrel20?treeId=vrrel20-51\\\"\u003Ehttps:\u002F\u002Fwww.tandfonline.com\u002Floi\u002Frrel20?treeId=vrrel20-51\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EThe analysis in this tutorial focuses on clustering the textual data in the \u003Ccode\u003Eabstract\u003C\u002Fcode\u003E column of the dataset. We will apply \u003Cem\u003Ek\u003C\u002Fem\u003E-means and DBSCAN to find thematic clusters within the diversity of topics discussed in \u003Cem\u003EReligion\u003C\u002Fem\u003E. To do so, we will first create document vectors of each abstract (via \u003Cstrong\u003ET\u003C\u002Fstrong\u003Eext \u003Cstrong\u003EF\u003C\u002Fstrong\u003Erequency - \u003Cstrong\u003EI\u003C\u002Fstrong\u003Enverted \u003Cstrong\u003ED\u003C\u002Fstrong\u003Eocument \u003Cstrong\u003EF\u003C\u002Fstrong\u003Erequency, or \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FUL2M-GY4A\\\"\u003E\u003Cstrong\u003ETF-IDF\u003C\u002Fstrong\u003E\u003C\u002Fa\u003E for short), reduce the feature space (which initially consists of the entire vocabulary of the abstracts), and then look for thematic clusters. \u003C\u002Fp\u003E\\n\u003Cp\u003EYou can download both datasets as well as a Jupyter notebook containing the code we are writing in this tutorial from the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fjekyll\u002Ftree\u002Fgh-pages\u002Fassets\u002Fclustering-with-scikit-learn-in-python\\\"\u003EGitHub repository\u003C\u002Fa\u003E. This lesson will work on any operating system, as long as you follow these instructions to set up an environment with Anaconda or Google Colab to run the Jupyter notebook locally or in the cloud. If you do not know how to set up a Jupyter notebook locally, this \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FDG7B-ASKL\\\"\u003Eexcellent PH tutorial might help you get started\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"prerequisites\\\"\u003EPrerequisites\u003C\u002Fh1\u003E\\n\u003Cp\u003ETo follow this tutorial, you should have basic programming knowledge (preferably Python) and be familiar with central Python libraries, such as pandas and \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FGY76-324B\\\"\u003Ematplotlib\u003C\u002Fa\u003E (or their equivalents in other programming languages). I also assume that you have basic knowledge of descriptive statistics. For instance, you should know what \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F3Z34-DXCW\\\"\u003Emean\u003C\u002Fa\u003E, \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FDH2Q-NP35\\\"\u003Estandard deviation\u003C\u002Fa\u003E, and \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FAKA7-HVQC\\\"\u003Ecategorical\u003C\u002Fa\u003E\u002F\u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FWVE4-4WAQ\\\"\u003Econtinuous\u003C\u002Fa\u003E variables are.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"why-k-means-clustering-and-dbscan\\\"\u003EWhy \u003Cem\u003EK\u003C\u002Fem\u003E-Means clustering and DBSCAN?\u003C\u002Fh1\u003E\\n\u003Cp\u003EGenerally, you can choose between several clustering algorithms to analyze your data, such as \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering, \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FC3UV-SWMN\\\"\u003Ehierarchical clustering\u003C\u002Fa\u003E, and \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FVH9X-DTSB\\\"\u003EDBSCAN\u003C\u002Fa\u003E. We focus on \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering in this tutorial since it is a relatively easy-to-understand clustering algorithm with a fast runtime speed that still delivers decent results,[^2] which makes it an excellent model to start with. I have selected DBSCAN as the second clustering algorithm for this tutorial since DBSCAN is an excellent addition to \u003Cem\u003Ek\u003C\u002Fem\u003E-means. Among other capabilities, DBSCAN allows you to focus on dense and non-linear clusters in your data while leaving noise points or outliers outside the dense clusters, which is something that \u003Cem\u003Ek\u003C\u002Fem\u003E-means cannot do independently (\u003Cem\u003Ek\u003C\u002Fem\u003E-means adds the noise points or outliers to the \u003Cem\u003Ek\u003C\u002Fem\u003E-clusters).\u003C\u002Fp\u003E\\n\u003Cp\u003EHowever, implementing other clustering algorithms in combination with scikit-learn should be fairly straight-forward once you are familiar with the overall workflow. Thus, if you decide to analyze your data with additional clustering algorithms (such as hierarchical clustering), you can easily do so after finishing this tutorial. In general, it is advisable to apply more than one clustering algorithm to get different perspectives on your data and evaluate each model&#39;s results.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"what-is-clustering\\\"\u003EWhat is Clustering?\u003C\u002Fh1\u003E\\n\u003Cp\u003EClustering is part of the larger field of \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FRCF8-AVH6\\\"\u003Emachine learning\u003C\u002Fa\u003E. Machine learning is an artificial intelligence process by which computers can learn from data without being explicitly programmed (see Géron 2019, 2 in the bibliography), meaning that a machine learning model, once it is set up, can independently discover structures in the data or predict new (unknown) data. The field of machine learning can be separated into \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FRS62-NQE3\\\"\u003Esupervised\u003C\u002Fa\u003E, \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F6FSL-9N2J\\\"\u003Eunsupervised\u003C\u002Fa\u003E, and \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F2LPR-9DJU\\\"\u003Ereinforcement\u003C\u002Fa\u003E learning (see Géron 2019, 7-17 in the bibliography).\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003ESupervised machine learning\u003C\u002Fstrong\u003E uses \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FAC8U-DCYD\\\"\u003Elabeled data\u003C\u002Fa\u003E to train machine learning algorithms to make accurate predictions for new data. A good example is a spam filter (with emails either labeled as &quot;spam&quot; or &quot;not-spam&quot;). One way to assess a supervised machine learning model&#39;s accuracy is to test it on some pre-labeled data, then compare the machine learning model&#39;s labeling predictions with the original output. Among other things, the model&#39;s accuracy depends on the quantity and quality of the labeled data it has been trained on and its parameters (\u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FAX34-ZKA7\\\"\u003Ehyperparameter tuning\u003C\u002Fa\u003E). Thus, building a decent supervised machine learning model involves a continuous loop of training, testing, and fine-tuning of the model&#39;s parameters. Common examples of supervised machine learning classifiers are \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FU6CU-5R55\\\"\u003E\u003Cem\u003Ek\u003C\u002Fem\u003E-nearest neighbors (KNN)\u003C\u002Fa\u003E and \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FAG5A-AB7M\\\"\u003Elogistic regression\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003EUnsupervised learning\u003C\u002Fstrong\u003E is applied to unlabeled data. Among other things, unsupervised learning is used for anomaly detection, dimensionality reduction, and clustering. When applying unsupervised machine learning algorithms, we do not feed our model with prelabeled data to make predictions for new data. Instead, we want the model to discern potential structures in our data. The datasets in this tutorial are a good example: we are only feeding our model either the author or abstract data, and we expect the model to indicate where (potential) clusters exist (for instance, articles in \u003Cem\u003EReligion\u003C\u002Fem\u003E with similar topics). Hyperparameter tuning can also be a part of unsupervised learning; however, in these cases, the results of the clustering cannot be compared to any prelabeled data. Yet, we can apply measures such as the so-called \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FW69A-EUQB\\\"\u003Eelbow method\u003C\u002Fa\u003E or the \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FM4TD-VSNU\\\"\u003Esilhouette score\u003C\u002Fa\u003E to evaluate the model&#39;s output based on different parameter choices (such as the n number of clusters in \u003Cem\u003Ek\u003C\u002Fem\u003E-means).   \u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Cstrong\u003EReinforcement learning\u003C\u002Fstrong\u003E is less relevant for scholars in the humanities. Reinforcement learning consists of setting up an agent (for instance, a robot) that performs actions and is either rewarded or punished for their execution. The agent learns how to react to its environment based on the feedback it received from its former actions.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"how-does-k-means-work\\\"\u003EHow Does \u003Cem\u003EK\u003C\u002Fem\u003E-Means Work?\u003C\u002Fh1\u003E\\n\u003Cp\u003EThe following overview of the \u003Cem\u003Ek\u003C\u002Fem\u003E-means algorithm focuses on the so-called \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F8WB3-K8NT\\\"\u003Enaive \u003Cem\u003Ek\u003C\u002Fem\u003E-means\u003C\u002Fa\u003E clustering, in which the cluster centers (so-called \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FT76C-GWQY\\\"\u003Ecentroids\u003C\u002Fa\u003E) are randomly initialized. However, the \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FK7KK-XUEG\\\"\u003Escikit-learn implementation of \u003Cem\u003Ek\u003C\u002Fem\u003E-means\u003C\u002Fa\u003E applied in this tutorial already integrates many improvements to the original algorithm. For instance, instead of randomly distributing the initial cluster centers (centroids), the scikit-learn model uses a different approach called \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FL98W-GWD5\\\"\u003E\u003Cem\u003Ek\u003C\u002Fem\u003E-means++\u003C\u002Fa\u003E, which is a smarter way to distribute the initial centroids. Yet, the way \u003Cem\u003Ek\u003C\u002Fem\u003E-means++ works is beyond the scope of this tutorial, and I recommend reading this \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F8KPJ-JRZW\\\"\u003Earticle\u003C\u002Fa\u003E by David Arthur and Sergei Vassilvitskii if you want to learn more.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"the-k-means-algorithm\\\"\u003EThe \u003Cem\u003EK\u003C\u002Fem\u003E-Means Algorithm\u003C\u002Fh2\u003E\\n\u003Cp\u003ETo explain how \u003Cem\u003Ek\u003C\u002Fem\u003E-means works, let us review a snippet from our \u003Ccode\u003EDNP_ancien_authors.csv\u003C\u002Fcode\u003E dataset. Even though we will later include more features, it is helpful to focus on a few key features in this introductory section to explain how the clustering techniques work.\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"left\\\"\u003Eauthors\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eword_count\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eknown_works\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAelianus Tacticus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E350\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAmbrosius\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1221\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E14\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAnacreontea\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E544\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAristophanes\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1108\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E11\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003ETo start with the \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering, you first need to define the number of clusters you want to find in your data. In most cases, you will not know how many clusters exist in your data, so choosing the appropriate initial number of clusters is already a tricky question. We will address this issue below, but let us first review how \u003Cem\u003Ek\u003C\u002Fem\u003E-means generally functions. \u003C\u002Fp\u003E\\n\u003Cp\u003EIn our example, we will assume that we are trying to identify two clusters. The naive \u003Cem\u003Ek\u003C\u002Fem\u003E-means algorithm will now initialize the model with two randomly distributed cluster centers in the two-dimensional space. \u003C\u002Fp\u003E\\n\u003Cp\u003EThe main algorithm consists of two steps. The first step is to measure the distances between every data point and the current cluster centers (in our case, via \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FX3P6-JESJ\\\"\u003EEuclidean distance\u003C\u002Fa\u003E \\\\( \\\\sqrt[]{(x_1-x_2)^{2}+(y_1-y_2)^{2}} \\\\), where \\\\( (x_1,y_1) \\\\) and \\\\( (x_2,y_2) \\\\) are two data points in our two-dimensional space). After measuring the distances between each data point and the cluster centers, every data point is assigned to its nearest cluster center.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe second step consists of creating new cluster centers by calculating the mean of all the data points assigned to each cluster.\u003C\u002Fp\u003E\\n\u003Cp\u003EAfter creating the new cluster centers, the algorithm starts again with the data points&#39; reassignment to the newly created cluster centers. The algorithm stops once the cluster centers are more or less stable. The \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FGL9D-9GRG\\\"\u003EWikipedia entry on \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering\u003C\u002Fa\u003E provides helpful visualizations of this two-step process.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe plotted results when clustering our snippet from the \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E dataset look like this, including the position of the final centroids:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig1.png&quot; caption=&quot;Figure 1: The clustered ancient authors data and the centroids using \u003Cem\u003Ek\u003C\u002Fem\u003E-means in a two-dimensional space.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThis appears satisfactory, and we can quickly see how the centroids are positioned between the data points that we intuitively assume to represent one cluster. However, we can already notice that the scales of both axes differ significantly. The y-axis ranges from 1–14, whereas the x-axis scale ranges from 300–1300. Thus, a change on the x-axis is likely to influence the distance between data points more significantly than a change on the y-axis. This, in turn, also impacts the placement of the centroids and thus the cluster building process. To showcase this problem, let us change the word count of Aristophanes from 1108 to 700.\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"left\\\"\u003Eauthors\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eword_count\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eknown_works\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAelianus Tacticus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E350\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAmbrosius\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1221\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E14\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAnacreontea\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E544\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAristophanes\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E700\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E11\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EIf we apply \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering on the changed dataset, we get the following result:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig2.png&quot; caption=&quot;Figure 2: A new version of the clustered data and the centroids using \u003Cem\u003Ek\u003C\u002Fem\u003E-means on the changed ancient authors data.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EAs you can see, a change of word count resulted in a new cluster of three authors who each have entries of approximately the same word count in the DNP, but who have a significantly different number of known published works. But does this really make sense? Wouldn&#39;t it be more reasonable to leave Ambrosius and Aristophanes in the same cluster since they have both written approximately the same number of documented works? To account for such problems based on different scales, it is advisable to normalize the data before clustering it. There are different ways to do this, among them \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FM73K-8XST\\\"\u003Emin-max normalization\u003C\u002Fa\u003E and \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FZTB8-3K74\\\"\u003Ez-score normalization\u003C\u002Fa\u003E, which is also called standardization. In this tutorial, we will focus on the latter. This means that we first subtract the mean from each data point and then divide it by the standard deviation of the data in the respective column. Fortunately, scikit-learn already provides us with implementations of these normalizations, so we do not have to calculate them manually.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe standardized (z-score) snippet of the ancient authors dataset looks like this:\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"left\\\"\u003Eauthors\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eword_count\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eknown_works\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAelianus Tacticus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-1.094016\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.983409\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAmbrosius\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.599660\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.239950\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAnacreontea\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.494047\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.983409\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAristophanes\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.011597\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E0.726868\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EIf we now apply a \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering on the standardized dataset, we get the following result:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig3.png&quot; caption=&quot;Figure 3: Using \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering on the standardized dataset.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EAs you can see, changing the word count now has a less significant influence on the clustering. In our example, working with the standardized dataset results in a more appropriate clustering of the data since the \u003Ccode\u003Eknown_works\u003C\u002Fcode\u003E feature would otherwise lose much of its value for the overall analysis.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"how-many-clusters-should-i-choose\\\"\u003EHow Many Clusters Should I Choose?\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"elbow-method\\\"\u003EElbow Method\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe question of how many cluster centers to choose is a difficult one. There is no one-size-fits-all solution to this problem. Yet, specific performance measures might help to select the right number of clusters for your data. A helpful example that we will be using in this tutorial is the elbow method. The elbow method is based on measuring the inertia of the clusters for different numbers of clusters. In this context, inertia is defined as:\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003ESum of squared distances of samples to their closest cluster center.[^3]\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EThe inertia decreases with the number of clusters. The extreme is that inertia will be zero when n is equal to the number of data points. But how could this help us find the right amount of clusters? Ideally, you would expect the inertia to decrease more slowly from a certain n onwards, so that a (fictional) plot of the inertia\u002Fcluster relation would look like this:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig4.png&quot; caption=&quot;Figure 4: Fictional example of inertia for k clusters.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this plot, the &quot;elbow&quot; is found at four clusters. This indicates that four clusters might be a reasonable trade-off between relatively low inertia (meaning the data points assigned to the clusters are not too far away from the centroids) and as few clusters as possible. Again, this method only provides you with an idea of where to start investigating. The final decision is up to you and highly depends on your data and your research question. Figuring out the right amount of clusters should also be accompanied by other steps, such as plotting your data or assessing other statistics. We will see how inertia helps us to discover the right amount of clusters for our \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E dataset in the following application of \u003Cem\u003Ek\u003C\u002Fem\u003E-means.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"silhouette-score\\\"\u003ESilhouette Score\u003C\u002Fh2\u003E\\n\u003Cp\u003EAnother possible way to evaluate the clustering of your data is to use the silhouette score, a method that allows you to assess how well each data point is associated with its current cluster. The way the silhouette score works is very well described in the Wikipedia article \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FM4TD-VSNU\\\"\u003E&quot;Silhouette (clustering)&quot;\u003C\u002Fa\u003E:\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EThe silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EIn this tutorial, we will be using the silhouette scores with the machine learning visualization library \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F5P5D-WPW9\\\"\u003Eyellowbrick\u003C\u002Fa\u003E in Python. Plotting the average silhouette score of all data points against the silhouette score of each data point in a cluster can help you to evaluate the quality of your model and the suitability of your current choice of parameter values.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo illustrate how a silhouette plot can help you find the correct number of clusters for your data, we can take a dummy example from our ancient author dataset. The data is based on a fictive sample of the number of known works and the word count of selected authors. The data has already been standardized using the z-score.\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"left\\\"\u003Eauthors\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eknown_works\u003C\u002Fth\u003E\\n\u003Cth align=\\\"center\\\"\u003Eword_count\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor A\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E0.24893051\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E0.83656758\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor B\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E0.38169345\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E0.04955707\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor C\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E0.11616757\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E0.34468601\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor D\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.01659537\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E0.14793338\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor E\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-1.21146183\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-1.18014685\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor F\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-1.07869889\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-1.27852317\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor G\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.94593595\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-1.22933501\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor H\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-1.07869889\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-1.1309587\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor I\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.68041007\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.34394819\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor J\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.81317301\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.83582976\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor K\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.41488419\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.54070081\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor L\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.54764713\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E-0.43838945\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor M\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.1782711\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.62357809\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor N\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.31103404\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.52520177\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor O\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.57655992\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.41698783\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAuthor P\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.97484874\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"center\\\"\u003E1.03332021\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EWe can now plot the silhouette score for different cluster numbers n. In this example, we will plot the silhouette scores for two, three, and four clusters using \u003Cem\u003Ek\u003C\u002Fem\u003E-means.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig5.png&quot; caption=&quot;Figure 5: Silhouette plots using \u003Cem\u003Ek\u003C\u002Fem\u003E-means with n clusters between two and five.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe vertical dashed line indicates the average silhouette score of all data points. The horizontal “knives” represent an overview of all data points in a cluster and their individual silhouette scores in descending order (from top to bottom). The silhouette plots show us that a cluster number between four and five seems to be most appropriate for our dataset. Particularly the data points with n=4 clusters have a relatively high average silhouette score (over 0.6), and the cluster “knives” seem to have approximately the same size and are not too sharp, which indicates that the cohesion within each cluster is not too bad. Indeed, if we plot our data using \u003Cem\u003Ek\u003C\u002Fem\u003E-means with n=4 clusters, we can see that this choice is a reasonable cluster number for our dataset and offers a good impression of the overall distribution of the data points.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig6.png&quot; caption=&quot;Figure 6: Scatterplot of the dataset using \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering with n=4 clusters.&quot; %}\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"how-does-dbscan-work\\\"\u003EHow Does DBSCAN Work?\u003C\u002Fh1\u003E\\n\u003Cp\u003EDBSCAN is short for &quot;Density-Based Spatial Clustering of Applications with Noise.&quot; Unlike the \u003Cem\u003Ek\u003C\u002Fem\u003E-means algorithm, DBSCAN does not try to cluster every single data point in a dataset. Instead, DBSCAN looks for dense regions of data points in a set while classifying data points without any direct neighbors as outliers or ‘noise points’. DBSCAN can be a great choice when dealing with datasets that are not linearly clustered but still include dense regions of data points.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"the-dbscan-algorithm\\\"\u003EThe DBSCAN Algorithm\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe basic DBSCAN algorithm is very well explained in the corresponding \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F6JNW-DCNT\\\"\u003Ewikipedia article\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EThe first step consists of defining an ε-distance (eps) that defines the neighborhood region (radius) of a data point. Just as in the case of k-means-clustering, \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FW5TT-ZS4N\\\"\u003Escikit-learn&#39;s DBSCAN implementation uses Euclidean distance as the standard metric\u003C\u002Fa\u003E to calculate distances between data points. The second value that needs to be defined is the minimum number of data points that should be located in the neighborhood of data point to define its region as dense (including the data point itself).\u003C\u002Fli\u003E\\n\u003Cli\u003EThe algorithm starts by choosing a random data point in the dataset as a starting point. DBSCAN then looks for other data points within the ε-region around the starting point. Suppose there are at least n datapoints (with n equals the minimum number of data points specified before) in the neighborhood (including the starting point). In that case, the starting point and all the data points in the ε-region of the starting point are defined as core points that define a core cluster. If there are less than n data points found in the starting point&#39;s neighborhood, the datapoint is labeled as an noise point or outlier (yet, it might still become a member of another cluster later on). In this case, the algorithm continues by choosing another unlabeled data point from the dataset and restarts the algorithm at step 2.\u003C\u002Fli\u003E\\n\u003Cli\u003EIf an initial cluster is found, the DBSCAN algorithm analyzes the ε-region of each core point in the initial cluster. If a region includes at least n data points, new core points are created, and the algorithm continues by looking at the neighborhood of these newly assigned core points, and so on. If a core point has less than n data points, some of which are still unlabeled, they will also be included in the cluster (as so-called border points). In cases where border points are part of different clusters, they will be associated with the nearest cluster\u003C\u002Fli\u003E\\n\u003Cli\u003EOnce every datapoint has been visited and labeled as either part of a cluster or as a noise point or outlier, the algorithm stops.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003EUnlike the \u003Cem\u003Ek\u003C\u002Fem\u003E-means algorithm, the difficulty does not lie in finding the right amount of clusters to start with but in figuring out which ε-region is most appropriate for the dataset. A helpful method for finding the proper eps value is explained in \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F5H99-4EX6\\\"\u003Ethis article on towardsdatascience.com\u003C\u002Fa\u003E. In short, DBSCAN enables us to plot the distance between each data point in a dataset and itentify its nearest neighbor. It is then possible to sort by distance in ascending order. Finally, we can look for the point in the plot which initiates the steepest ascent and make a visual evaluation of the eps value, similar to the ‘elbow’ evaluation method described above in the case of \u003Cem\u003Ek\u003C\u002Fem\u003E-means)’. We will use this method later in this tutorial.\u003C\u002Fp\u003E\\n\u003Cp\u003ENow that we know how our clustering algorithms generally work and which methods we can apply to settle on the right amount of clusters let us apply these concepts in the context of our datasets from \u003Cem\u003EBrill&#39;s New Pauly\u003C\u002Fem\u003E and the journal \u003Cem\u003EReligion\u003C\u002Fem\u003E. We will start by analyzing the \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E dataset.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"first-case-study-applying-k-means-to-the-ancient-authors-dataset-from-brills-new-pauly\\\"\u003EFirst Case Study: Applying \u003Cem\u003EK\u003C\u002Fem\u003E-Means to the Ancient Authors Dataset from \u003Cem\u003EBrill&#39;s New Pauly\u003C\u002Fem\u003E\u003C\u002Fh1\u003E\\n\u003Ch2 id=\\\"1-exploring-the-dataset\\\"\u003E1. Exploring the Dataset\u003C\u002Fh2\u003E\\n\u003Cp\u003EBefore starting with the clustering, we will explore the data by loading \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E into Python with \u003Cem\u003Epandas\u003C\u002Fem\u003E. Next, we will print out the first five rows and look at some information and overview statistics about each dataset using pandas&#39; \u003Ccode\u003Einfo()\u003C\u002Fcode\u003E and \u003Ccode\u003Edescribe()\u003C\u002Fcode\u003E methods.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eimport pandas as pd\\n\\n# load the authors dataset that has been stored as a .csv files in a folder called &quot;data&quot; in the same directory as the Jupyter Notebook\\ndf_authors = pd.read_csv(&quot;data\u002FDNP_ancient_authors.csv&quot;, index_col=&quot;authors&quot;).drop(columns=[&quot;Unnamed: 0&quot;])\\n\\n# display dataset structure with the pandas .info() method\\nprint(df_authors.info())\\n\\n# show first 5 rows\\nprint(df_authors.head(5))\\n\\n# display some statistics\\nprint(df_authors.describe())\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe output of the \u003Ccode\u003Einfo()\u003C\u002Fcode\u003E method should look like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\\nIndex: 238 entries, Achilles Tatius of Alexandria to Zosimus\\nData columns (total 8 columns):\\n #   Column               Non-Null Count  Dtype\\n---  ------               --------------  -----\\n 0   word_count           238 non-null    int64\\n 1   modern_translations  238 non-null    int64\\n 2   known_works          238 non-null    int64\\n 3   manuscripts          238 non-null    int64\\n 4   early_editions       238 non-null    int64\\n 5   early_translations   238 non-null    int64\\n 6   modern_editions      238 non-null    int64\\n 7   commentaries         238 non-null    int64\\ndtypes: int64(8)\\nmemory usage: 16.7+ KB\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAs we can see, our data consists of 238 entries of type integer. Next, we will examine our data through the output of the \u003Cem\u003Epandas\u003C\u002Fem\u003E \u003Ccode\u003Edescribe()\u003C\u002Fcode\u003E method.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe output of \u003Ccode\u003Edf_authors.describe()\u003C\u002Fcode\u003E should look like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eword_count    modern_translations    known_works    manuscripts    early_editions    early_translations    modern_editions    commentaries\\ncount    238.000000    238.000000    238.000000    238.000000    238.000000    238.000000    238.000000    238.000000\\nmean    904.441176    12.970588    4.735294    4.512605    5.823529    4.794118    10.399160    3.815126\\nstd    804.388666    16.553047    6.784297    4.637702    4.250881    6.681706    11.652326    7.013509\\nmin    99.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000\\n25%    448.750000    4.250000    1.000000    1.000000    3.000000    0.000000    4.000000    0.000000\\n50%    704.000000    9.000000    2.000000    3.000000    5.000000    2.500000    7.000000    1.000000\\n75%    1151.500000    15.750000    6.000000    6.000000    8.000000    8.000000    14.000000    4.000000\\nmax    9406.000000    178.000000    65.000000    34.000000    28.000000    39.000000    115.000000    43.000000\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWe can see that the standard deviation and the mean values vary significantly between the \u003Ccode\u003Eword_count\u003C\u002Fcode\u003E column and the other columns. When working with metrics such as Euclidean distance in the \u003Cem\u003Ek\u003C\u002Fem\u003E-means algorithm, different scales between the columns can become problematic. Thus, we should standardize the data before applying the clustering algorithm.\u003C\u002Fp\u003E\\n\u003Cp\u003EFurthermore, we have an significant standard deviation in almost every column and a vast difference between the 75th percentile value and the maximum value, particularly in the \u003Ccode\u003Eword_count\u003C\u002Fcode\u003E column. This indicates that we might have some noise in our dataset, and it might be necessary to get rid of the noisy data points before we continue with our analysis. Therefore, we only keep those data points in our data frame with a word count within the 90th percentile range.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eninety_quantile = df_authors[&quot;word_count&quot;].quantile(0.9)\\ndf_authors = df_authors[df_authors[&quot;word_count&quot;] &lt;= ninety_quantile]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"2-imports-and-additional-functions\\\"\u003E2. Imports and Additional Functions\u003C\u002Fh2\u003E\\n\u003Cp\u003EBefore we start with the actual clustering process, we first import all the necessary libraries and write a couple of functions that will help us to plot our results during the analysis. We will also use these functions and imports during the second case study in this tutorial (analyzing the \u003Cem\u003EReligion\u003C\u002Fem\u003E abstracts data). Thus, if you decide to skip the analysis of the ancient authors data, you still need to import these functions and libraries to execute the code in the second part of this tutorial.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efrom sklearn.preprocessing import StandardScaler as SS # z-score standardization \\nfrom sklearn.cluster import KMeans, DBSCAN # clustering algorithms\\nfrom sklearn.decomposition import PCA # dimensionality reduction\\nfrom sklearn.metrics import silhouette_score # used as a metric to evaluate the cohesion in a cluster\\nfrom sklearn.neighbors import NearestNeighbors # for selecting the optimal eps value when using DBSCAN\\nimport numpy as np\\n\\n# plotting libraries\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom yellowbrick.cluster import SilhouetteVisualizer\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe following function will help us to plot (and save) the silhouette plots.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edef silhouettePlot(range_, data):\\n    &#39;&#39;&#39;\\n    we will use this function to plot a silhouette plot that helps us to evaluate the cohesion in clusters (k-means only)\\n    &#39;&#39;&#39;\\n    half_length = int(len(range_)\u002F2)\\n    range_list = list(range_)\\n    fig, ax = plt.subplots(half_length, 2, figsize=(15,8))\\n    for _ in range_:\\n        kmeans = KMeans(n_clusters=_, random_state=42)\\n        q, mod = divmod(_ - range_list[0], 2)\\n        sv = SilhouetteVisualizer(kmeans, colors=&quot;yellowbrick&quot;, ax=ax[q][mod])\\n        ax[q][mod].set_title(&quot;Silhouette Plot with n={} Cluster&quot;.format(_))\\n        sv.fit(data)\\n    fig.tight_layout()\\n    fig.show()\\n    fig.savefig(&quot;silhouette_plot.png&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe next function will help us to plot (and save) the elbow plots.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edef elbowPlot(range_, data, figsize=(10,10)):\\n    &#39;&#39;&#39;\\n    the elbow plot function helps to figure out the right amount of clusters for a dataset\\n    &#39;&#39;&#39;\\n    inertia_list = []\\n    for n in range_:\\n        kmeans = KMeans(n_clusters=n, random_state=42)\\n        kmeans.fit(data)\\n        inertia_list.append(kmeans.inertia_)\\n        \\n    # plotting\\n    fig = plt.figure(figsize=figsize)\\n    ax = fig.add_subplot(111)\\n    sns.lineplot(y=inertia_list, x=range_, ax=ax)\\n    ax.set_xlabel(&quot;Cluster&quot;)\\n    ax.set_ylabel(&quot;Inertia&quot;)\\n    ax.set_xticks(list(range_))\\n    fig.show()\\n    fig.savefig(&quot;elbow_plot.png&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe next function assists us in finding the right eps value when using DBSCAN.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edef findOptimalEps(n_neighbors, data):\\n    &#39;&#39;&#39;\\n    function to find optimal eps distance when using DBSCAN; based on this article: https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc\\n    &#39;&#39;&#39;\\n    neigh = NearestNeighbors(n_neighbors=n_neighbors)\\n    nbrs = neigh.fit(data)\\n    distances, indices = nbrs.kneighbors(data)\\n    distances = np.sort(distances, axis=0)\\n    distances = distances[:,1]\\n    plt.plot(distances)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe last function \u003Ccode\u003EprogressiveFeatureSelection()\u003C\u002Fcode\u003E implements a basic algorithm to select features from our dataset based on the silhouette score and \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering. The algorithm first identifies a single feature with the best silhouette score when using \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering. Afterward, the algorithm trains a \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance for each combination of the initially chosen feature and one of the remaining features. Next, it selects the two-feature combination with the best silhouette score. The algorithm uses this newly discovered pair of features to find the optimal combination of these two features with one of the remaining features, and so on. The algorithm continues until it has discovered the optimal combination of n features (where n is the value of the \u003Ccode\u003Emax_features\u003C\u002Fcode\u003E parameter).\u003C\u002Fp\u003E\\n\u003Cp\u003EThe algorithm is inspired by \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FK5PD-GQPQ\\\"\u003Ethis discussion on stackexchange.com\u003C\u002Fa\u003E. Yet, don&#39;t worry too much about this implementation; there are better solutions for feature selection algorithms out there, as shown in \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F3HQR-RL27\\\"\u003Ein Manoranjan Dash and Huan Liu&#39;s paper &#39;Feature Selection for Clustering&#39;\u003C\u002Fa\u003E and \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F25Y9-NS94\\\"\u003ESalem Alelyani, Jiliang Tang, and Huan Liu&#39;s &#39;Feature Selection for Clustering: A Review&#39;\u003C\u002Fa\u003E. However, most of the potential algorithms for feature selection in an unsupervised context are not implemented in scikit-learn, which is why I have decided to implement one myself, albeit basic.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edef progressiveFeatureSelection(df, n_clusters=3, max_features=4,):\\n    &#39;&#39;&#39;\\n    very basic implementation of an algorithm for feature selection (unsupervised clustering); inspired by this post: https:\u002F\u002Fdatascience.stackexchange.com\u002Fquestions\u002F67040\u002Fhow-to-do-feature-selection-for-clustering-and-implement-it-in-python\\n    &#39;&#39;&#39;\\n    feature_list = list(df.columns)\\n    selected_features = list()\\n    # select starting feature\\n    initial_feature = &quot;&quot;\\n    high_score = 0\\n    for feature in feature_list:\\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\\n        data_ = df[feature]\\n        labels = kmeans.fit_predict(data_.to_frame())\\n        score_ = silhouette_score(data_.to_frame(), labels)\\n        print(&quot;Proposed new feature {} with score {}&quot;. format(feature, score_))\\n        if score_ &gt;= high_score:\\n            initial_feature = feature\\n            high_score = score_\\n    print(&quot;The initial feature is {} with a silhouette score of {}.&quot;.format(initial_feature, high_score))\\n    feature_list.remove(initial_feature)\\n    selected_features.append(initial_feature)\\n    for _ in range(max_features-1):\\n        high_score = 0\\n        selected_feature = &quot;&quot;\\n        print(&quot;Starting selection {}...&quot;.format(_))\\n        for feature in feature_list:\\n            selection_ = selected_features.copy()\\n            selection_.append(feature)\\n            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\\n            data_ = df[selection_]\\n            labels = kmeans.fit_predict(data_)\\n            score_ = silhouette_score(data_, labels)\\n            print(&quot;Proposed new feature {} with score {}&quot;. format(feature, score_))\\n            if score_ &gt; high_score:\\n                selected_feature = feature\\n                high_score = score_\\n        selected_features.append(selected_feature)\\n        feature_list.remove(selected_feature)\\n        print(&quot;Selected new feature {} with score {}&quot;. format(selected_feature, high_score))\\n    return selected_features\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENote that we have selected n=3 clusters as default for the \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance in \u003Ccode\u003EprogressiveFeatureSelection()\u003C\u002Fcode\u003E. In the context of an advanced hyperparameter tuning (which is beyond the scope of this tutorial), it might make sense to train the \u003Ccode\u003EprogressiveFeatureSelection()\u003C\u002Fcode\u003E with different n values for the \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance as well. For the sake of simplicity, we stick to n=3 clusters in this tutorial.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"3-standardizing-the-dnp-ancient-authors-dataset\\\"\u003E3. Standardizing the DNP Ancient Authors Dataset\u003C\u002Fh2\u003E\\n\u003Cp\u003ENext, we initialize scikit-learn&#39;s \u003Ccode\u003EStandardScaler()\u003C\u002Fcode\u003E to standardize our data. We apply scikit-learn&#39;s \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F36NS-WUJT\\\"\u003E\u003Ccode\u003EStandardScaler()\u003C\u002Fcode\u003E\u003C\u002Fa\u003E (z-score) to cast the mean of the columns to approximately zero and the standard deviation to one, to account for the huge differences between the \u003Ccode\u003Eword_count\u003C\u002Fcode\u003E and the other columns in \u003Ccode\u003Edf_ancient_authors.csv\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Escaler = SS()\\nDNP_authors_standardized = scaler.fit_transform(df_authors)\\ndf_authors_standardized = pd.DataFrame(DNP_authors_standardized, columns=[&quot;word_count_standardized&quot;, &quot;modern_translations_standardized&quot;, &quot;known_works_standardized&quot;, &quot;manuscripts_standardized&quot;, &quot;early_editions_standardized&quot;, &quot;early_translations_standardized&quot;, &quot;modern_editions_standardized&quot;, &quot;commentaries_standardized&quot;])\\ndf_authors_standardized = df_authors_standardized.set_index(df_authors.index)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"4-feature-selection\\\"\u003E4. Feature Selection\u003C\u002Fh2\u003E\\n\u003Cp\u003EIf you were to cluster the entire \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E with \u003Cem\u003Ek\u003C\u002Fem\u003E-means, you would not find any reasonable clusters in the dataset. This is frequently the case when working with real-world data. However, in such cases, it might be pertinent to search for subsets of features that help us to structure the data. As we are only dealing with ten features, we could theoretically do this manually. However, because we have already implemented a basic algorithm to help us find potentially interesting combinations of features, we can also use our \u003Ccode\u003EprogressiveFeatureSelection()\u003C\u002Fcode\u003E function. In this tutorial, we will search for three features that might be interesting to look at. Yet, feel free to try out different \u003Ccode\u003Emax_features\u003C\u002Fcode\u003E with the \u003Ccode\u003EprogressiveFeatureSelection()\u003C\u002Fcode\u003E function (as well as \u003Ccode\u003En_clusters\u003C\u002Fcode\u003E). The selection of only three features (as well as n=3 clusters for the \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance) was a random choice which unexpectedly led to some exciting results; however, this does not mean that there are no other promising combinations which might be worth examining. \u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Eselected_features = progressiveFeatureSelection(df_authors_standardized, max_features=3, n_clusters=3)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ERunning this function, it turns out that the three features \u003Ccode\u003Eknown_works_standardized\u003C\u002Fcode\u003E, \u003Ccode\u003Ecommentaries_standardized\u003C\u002Fcode\u003E, and \u003Ccode\u003Emodern_editions_standardized\u003C\u002Fcode\u003E might be worth considering when trying to cluster our data. Thus, we next create a new data frame with only these three features.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edf_standardized_sliced = df_authors_standardized[selected_features]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"5-choosing-the-right-amount-of-clusters\\\"\u003E5. Choosing the Right Amount of Clusters\u003C\u002Fh2\u003E\\n\u003Cp\u003EWe will now apply the elbow method and then use silhouette plots to obtain an impression of how many clusters we should choose to analyze our dataset. We will check for two to ten clusters. Note, however, that the feature selection was also made with a pre-defined \u003Cem\u003Ek\u003C\u002Fem\u003E-means algorithm using n=3 clusters. Thus, our three selected features might already tend towards this number of clusters.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003EelbowPlot(range(1,11), df_standardized_sliced)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe elbow plot looks like this:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig7.png&quot; caption=&quot;Figure 7: Elbow plot of the df_standardized_sliced dataset.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ELooking at the elbow plot indeed shows us that we find an “elbow” at n=3 as well as n=5 clusters. Yet, it is still quite challenging to decide whether to use three, four, five, or even six clusters. Therefore, we should also look at the silhouette plots.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003EsilhouettePlot(range(3,9), df_standardized_sliced)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe silhouette plots look like this:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig8.png&quot; caption=&quot;Figure 8: Silhouette plots of the df_standardized_sliced dataset.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ELooking at the silhouette scores underlines our previous intuition that a selection of n=3 or n=5 seems to be the right choice of clusters. The silhouette plot with n=3 clusters in particular has a relatively high average silhouette score. Yet, because the two other clusters are far below the average silhouette score for n=3 clusters, we decide to analyze the dataset with \u003Cem\u003Ek\u003C\u002Fem\u003E-means using n=5 clusters. However, the different sizes of the “knives” and their sharp form in both n=3 and n=5 clusters indicate a single dominant cluster and a couple of rather small and less cohesive clusters.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"6-n5-k-means-analysis-of-the-dnp-ancient-authors-dataset\\\"\u003E6. n=5 \u003Cem\u003EK\u003C\u002Fem\u003E-Means Analysis of the DNP Ancient Authors Dataset\u003C\u002Fh2\u003E\\n\u003Cp\u003EEventually, we can now train a \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance with n=5 clusters and plot the results using \u003Cem\u003Eseaborn\u003C\u002Fem\u003E. I prefer plotting in two dimensions in Python, so we will use \u003Ccode\u003EPCA()\u003C\u002Fcode\u003E (\u003Cstrong\u003EPrincipal Component Analysis\u003C\u002Fstrong\u003E) to reduce the \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F68J8-UFV9\\\"\u003Edimensionality\u003C\u002Fa\u003E of our dataset to two dimensions. \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FE3RE-TKMM\\\"\u003EPCA\u003C\u002Fa\u003E is a great way to reduce the dimensionality of a dataset while keeping the variance from higher dimensions.\u003C\u002Fp\u003E\\n\u003Cblockquote\u003E\\n\u003Cp\u003EPCA allows us to reduce the dimensionality of the original data substantially while retaining most of the salient information. On the PCA-reduced feature set, other machine learning algorithms—downstream in the machine learning pipeline—will have an easier time separating the data points in space (to perform tasks such as anomaly detection and clustering) and will require fewer computational resources. (quote from the online version of Ankur A. Patel: \u003Cem\u003EHands-On Unsupervised Learning Using Python\u003C\u002Fem\u003E, O&#39;Reilly Media 2020)\u003C\u002Fp\u003E\\n\u003C\u002Fblockquote\u003E\\n\u003Cp\u003EPCA can be used to reduce high-dimensional datasets for computational reasons. Yet, in this context, we only use PCA to plot the clusters in our dataset in a two-dimensional space. We will also apply PCA in the following text clustering. One huge disadvantage of using PCA is that we lose our initial features and create new ones that are somewhat nebulous to us, as they do not allow us to look at specific aspects of our data anymore (such as word counts or known works).\u003C\u002Fp\u003E\\n\u003Cp\u003EBefore using PCA and plotting the results, we will instantiate a \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance with n=5 clusters and a \u003Ccode\u003Erandom_state\u003C\u002Fcode\u003E of 42. The latter parameter allows us to reproduce our results. 42 is an arbitrary choice here that refers to \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F33RA-4ZS9\\\"\u003E&quot;Hitchhiker&#39;s Guide to the Galaxy&quot;\u003C\u002Fa\u003E, but you can choose whichever number you like.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Ekmeans = KMeans(n_clusters=5, random_state=42)\\ncluster_labels = kmeans.fit_predict(df_standardized_sliced)\\ndf_standardized_sliced[&quot;clusters&quot;] = cluster_labels\\n\\n# using PCA to reduce the dimensionality\\npca = PCA(n_components=2, whiten=False, random_state=42)\\nauthors_standardized_pca = pca.fit_transform(df_standardized_sliced)\\ndf_authors_standardized_pca = pd.DataFrame(data=authors_standardized_pca, columns=[&quot;pc_1&quot;, &quot;pc_2&quot;])\\ndf_authors_standardized_pca[&quot;clusters&quot;] = cluster_labels\\n\\n# plotting the clusters with seaborn\\nsns.scatterplot(x=&quot;pc_1&quot;, y=&quot;pc_2&quot;, hue=&quot;clusters&quot;, data=df_authors_standardized_pca)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIn the corresponding plot (see figure 9), we can clearly distinguish several clusters in our data. However, we also perceive what was already visible in the silhouette plots, namely that we only have one dense cluster and two to three less cohesive ones with several noise points.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig9.png&quot; caption=&quot;Figure 9: Final plot of the clustered df_standardized_sliced dataset with seaborn.&quot; %}\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"7-conclusion\\\"\u003E7. Conclusion\u003C\u002Fh2\u003E\\n\u003Cp\u003EWe were able to observe some clear clusters in our data when using \u003Ccode\u003Eknown_works_standardized\u003C\u002Fcode\u003E, \u003Ccode\u003Ecommentaries_standardized\u003C\u002Fcode\u003E, and \u003Ccode\u003Emodern_editions_standardized\u003C\u002Fcode\u003E as a feature subset. But what does this actually tell us? This is a question that the algorithm cannot answer. The clustering algorithms only demonstrate that there are specific clusters under certain conditions, in this case, when looking for n=5 clusters with \u003Cem\u003Ek\u003C\u002Fem\u003E-means and the above-mentioned subset of features. But what are these clusters about? Do they grant us valuable insights into our data? To answer this question, we need to look at the members of each cluster and analyze whether their grouping hints at certain aspects that might be worth exploring further.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn our example, looking at cluster 0 (the dense one in the left part of our plot) reveals that this cluster includes authors with very few known works, few to no commentaries, few modern editions, and rather short entries in the DNP (average word count of 513). Consequently, it largely consists of relatively unknown ancient authors.\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"left\\\"\u003Eauthors\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Eword_count\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Emodern_translations\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Eknown_works\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Emanuscripts\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Eearly_editions\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Eearly_translations\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Emodern_editions\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Ecommentaries\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAchilles Tatius of Alexandria\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E383\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E9\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAelianus Tacticus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E350\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAelianus, Claudius (Aelian)\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E746\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAeneas Tacticus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E304\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAesop\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E757\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E18\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E11\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAgatharchides of Cnidus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E330\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAgathias\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E427\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAlexander of Tralleis\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E871\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAmmianus Marcellinus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E573\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAnacreontea\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E544\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E3\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EAs we can see in this snippet that shows the first ten entries in cluster 0, the author names (except Aesop) are more or less supporting our initial assumption that we are predominately dealing with authors whose work is produced in few modern editions, particularly compared to the authors in cluster 4.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe authors in cluster 4 (the less cohesive cluster at the upper right of our plot) comprise well-known and extensively discussed authors including Plato or Aristophanes, who have all written several works that are still famous and have remained relevant over the centuries, demonstrated by the high number of modern editions and commentaries.\u003C\u002Fp\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"left\\\"\u003Eauthors\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Eword_count\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Emodern_translations\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Eknown_works\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Emanuscripts\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Eearly_editions\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Eearly_translations\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Emodern_editions\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Ecommentaries\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAeschylus of Athens\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1758\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E31\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E14\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E15\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E20\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAristophanes of Athens\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1108\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E18\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E11\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E30\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E18\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003ELucanus, Marcus Annaeus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1018\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E17\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E11\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E15\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E20\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E25\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EPlato\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1681\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E31\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E18\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E20\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EPlutarchus of Chaeronea (Plutarch)\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1485\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E37\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E15\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E42\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003EPropertius, Sextus\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1443\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E22\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E24\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E22\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003ESallustius Crispus, Gaius (Sallust)\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1292\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E17\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E12\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E7\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E15\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E15\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E16\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003ESophocles\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1499\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E67\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E8\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E4\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E0\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E14\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E18\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"left\\\"\u003ETacitus, (Publius?) Cornelius\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1504\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E29\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E5\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E6\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E10\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E14\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E31\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E20\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EIf you want to have a closer look at the other clusters, I advise you to check out the Jupyter notebook in the \u003Ca href=\\\"https:\u002F\u002Fgithub.com\u002Fprogramminghistorian\u002Fjekyll\u002Ftree\u002Fgh-pages\u002Fassets\u002Fclustering-with-scikit-learn-in-python\\\"\u003EGitHub repository\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EThus, our clustering of the \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E dataset has resulted in some promising clusters, which might help us develop new research questions. For instance, we could now take these clusters and apply our hypothesis about their relevance to further explore clustering the authors, based on their early and modern translations\u002Feditions. However, this is beyond the scope of this tutorial, which is primarily concerned with introducing tools and methods to examine such research questions. \u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"second-case-study-clustering-textual-data\\\"\u003ESecond Case Study: Clustering Textual Data\u003C\u002Fh1\u003E\\n\u003Cp\u003EThe second section of this tutorial will deal with textual data, namely all abstracts scraped from the \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FP4VN-6K9K\\\"\u003E\u003Cem\u003EReligion\u003C\u002Fem\u003E (journal)\u003C\u002Fa\u003E website. We will try to cluster the abstracts based on their word features in the form of \u003Cstrong\u003ETF-IDF\u003C\u002Fstrong\u003E vectors (which is short for &quot;\u003Cstrong\u003ET\u003C\u002Fstrong\u003Eext \u003Cstrong\u003EF\u003C\u002Fstrong\u003Erequency - \u003Cstrong\u003EI\u003C\u002Fstrong\u003Enverted \u003Cstrong\u003ED\u003C\u002Fstrong\u003Eocument \u003Cstrong\u003EF\u003C\u002Fstrong\u003Erequency&quot;).\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"1-loading-the-dataset--exploratory-data-analysis\\\"\u003E1. Loading the Dataset &amp; Exploratory Data Analysis\u003C\u002Fh2\u003E\\n\u003Cp\u003EUsing a similar method as that used to analyze the \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E dataset, we will first load the \u003Ccode\u003ERELIGION_abstracts.csv\u003C\u002Fcode\u003E into our program and look at some summary statistics.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edf_abstracts = pd.read_csv(&quot;data\u002FRELIGION_abstracts.csv&quot;).drop(columns=&quot;Unnamed: 0&quot;)\\ndf_abstracts.info()\\ndf_abstracts.describe()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe result of the \u003Ccode\u003Edescribe()\u003C\u002Fcode\u003E method should print out something like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Etitle    abstract    link    volume\\ncount    701    701    701    701\\nunique    701    701    701    40\\ntop    From locality to (...) https:\u002F\u002Fwww.tandfonline.com\u002Fdoi\u002Fabs\u002F10.1006\u002Freli.1996.9998 Titel anhand dieser DOI in Citavi-Projekt übernehmen    https:\u002F\u002Fwww.tandfonline.com\u002Floi\u002Frrel20?treeId=vrrel20-50\\nfreq    1    1    1    41\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EUnlike in the previous dataset, we are now dealing with features where every single observation is unique.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"2-tf-idf-vectorization\\\"\u003E2. TF-IDF Vectorization\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn order to process the textual data with clustering algorithms, we need to convert the texts into vectors. For this purpose, we are using the scikit-learn implementation of \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FQ2JN-YWV6\\\"\u003ETF-IDF vectorization\u003C\u002Fa\u003E. For a good introduction to how TF-IDF works, see this \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F3XT2-DB6X\\\"\u003Egreat tutorial by Melanie Walsh\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"optional-step-lemmatization\\\"\u003E\u003Cem\u003EOptional Step\u003C\u002Fem\u003E: Lemmatization\u003C\u002Fh3\u003E\\n\u003Cp\u003EAs an optional step, I have implemented a function called \u003Ccode\u003ElemmatizeAbstracts()\u003C\u002Fcode\u003E that groups, or ‘lemmatizes’ the abstracts using \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FRTM6-8B27\\\"\u003EspaCy\u003C\u002Fa\u003E. Considering that we are not interested in stylistic similarities between the abstracts, this step helps to reduce the overall amount of features (words) in our dataset. As part of the lemmatization function, we also clean the text of all punctuation and other noise such as brackets. In the following analysis, we will continue working with the lemmatized version of the abstracts. However, you can also keep using the original texts and skip the lemmatization, although this might lead to different results.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# lemmatization (optional step)\\nimport spacy\\nimport re\\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\\n\\ndef lemmatizeAbstracts(x):\\n        doc = nlp(x)\\n        new_text = []\\n        for token in doc:\\n            new_text.append(token.lemma_)\\n        text_string = &quot; &quot;.join(new_text)\\n        # getting rid of non-word characters\\n        text_string = re.sub(r&quot;[^\\\\w\\\\s]+&quot;, &quot;&quot;, text_string)\\n        text_string = re.sub(r&quot;\\\\s{2,}&quot;, &quot; &quot;, text_string)\\n        return text_string\\n\\ndf_abstracts[&quot;abstract_lemma&quot;] = df_abstracts[&quot;abstract&quot;].apply(lemmatizeAbstracts)\\ndf_abstracts.to_csv(&quot;data\u002FRELIGION_abstracts_lemmatized.csv&quot;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EI have decided to save the new lemmatized version of our abstracts as \u003Ccode\u003ERELIGION_abstracts_lemmatized.csv\u003C\u002Fcode\u003E. This prevents from having to redo the lemmatization each time we restart our notebook.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"tf-idf-vectorization\\\"\u003ETF-IDF Vectorization\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe first step is to instantiate our TF-IDF model by passing it the \u003Ccode\u003Eargument\u003C\u002Fcode\u003E to ignore stop words, such as &quot;the,&quot; &quot;a,&quot; etc. The second step is rather similar to the training of our \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance in the previous part: We are passing the abstracts from our dataset to the vectorizer in order to convert them to machine-readable vectors. For the moment, we are not passing any additional arguments. Finally, we create a new pandas DataFrame object based on the TF-IDF matrix of our textual data.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\ntfidf = TfidfVectorizer(stop_words=&quot;english&quot;)\\ndf_abstracts_tfidf = tfidf.fit_transform(df_abstracts[&quot;abstract_lemma&quot;])\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWhen printing out the \u003Ccode\u003Edf_abstracts_tfidf\u003C\u002Fcode\u003E object, you can see that our initial matrix is \u003Cem\u003Ehuge\u003C\u002Fem\u003E and includes over 8,000 words from the overall vocabulary of the 701 abstracts. This is obviously too much, not only from a computational perspective but also because clustering algorithms such as \u003Cem\u003Ek\u003C\u002Fem\u003E-means become less efficient due to the so-called \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FS748-FPNG\\\"\u003E&quot;curse of dimensionality&quot;\u003C\u002Fa\u003E. We will thus need to reduce the number of features significantly.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo do so, we first create a new version of our TF-IDF vectorized data. This time, however, we tell the vectorizer that we only want a reduced set of 250 features. We also tell the model to only consider words from the vocabulary that appear in at least five different documents but in no more than 200. We also add the possibility to include single words and bigrams (such as “19th century”). Finally, we tell our model to clean the text of any potential accents.\u003C\u002Fp\u003E\\n\u003Cp\u003ESecondly, we are also using the \u003Cem\u003EPrincipal Component Analysis\u003C\u002Fem\u003E (PCA), this time to reduce the dimensionality of the dataset from 250 to 10 dimensions. \u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# creating a new TF-IDF matrix\\ntfidf = TfidfVectorizer(stop_words=&quot;english&quot;, ngram_range=(1,2), max_features=250, strip_accents=&quot;unicode&quot;, min_df=10, max_df=200)\\ntfidf_religion_array = tfidf.fit_transform(df_abstracts[&quot;abstract_lemma&quot;])\\ndf_abstracts_tfidf = pd.DataFrame(tfidf_religion_array.toarray(), index=df_abstracts.index, columns=tfidf.get_feature_names())\\ndf_abstracts_tfidf.describe()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"3-dimensionality-reduction-using-pca\\\"\u003E3. Dimensionality Reduction Using PCA\u003C\u002Fh2\u003E\\n\u003Cp\u003EAs mentioned above, let us next apply \u003Ccode\u003EPCA()\u003C\u002Fcode\u003E to caste the dimension from d=250 to d=10 to account for the \u003Cem\u003Ecurse of dimensionality\u003C\u002Fem\u003E when using \u003Cem\u003Ek\u003C\u002Fem\u003E-means. Similar to the selection of n=3 \u003Ccode\u003Emax_features\u003C\u002Fcode\u003E during the analysis of our ancient authors dataset, setting the dimensionality to d=10 was a random choice that happened to produce promising results. However, feel free to play around with these parameters while conducting a more elaborate hyperparameter tuning. Maybe you can find values for these parameters that result in an even more effective clustering of the data. For instance, you might want to use a \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FPYZ5-6QAV\\\"\u003Escree plot\u003C\u002Fa\u003E to figure out the optimal number of principal components in PCA, which works quite similarly to our elbow method in the context of \u003Cem\u003Ek\u003C\u002Fem\u003E-means.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E# using PCA to reduce the dimensionality\\npca = PCA(n_components=10, whiten=False, random_state=42)\\nabstracts_pca = pca.fit_transform(df_abstracts_tfidf)\\ndf_abstracts_pca = pd.DataFrame(data=abstracts_pca)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ch2 id=\\\"4-applying-k-means-clustering-on-textual-data\\\"\u003E4. Applying \u003Cem\u003EK\u003C\u002Fem\u003E-Means Clustering on Textual Data\u003C\u002Fh2\u003E\\n\u003Cp\u003ENext, we try to find a reasonable method for clustering the abstracts using \u003Cem\u003Ek\u003C\u002Fem\u003E-means. As we did in the case of the \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E dataset, we will start by searching for the right amount of clusters applying the elbow method and the silhouette score.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig10.png&quot; caption=&quot;Figure 10: Elbow plot with 3 to 99 clusters.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EAs we can see, there is no real elbow in our plot this time. This might imply that there are no big clusters in our \u003Ccode\u003ERELIGION_abstracts.csv\u003C\u002Fcode\u003E dataset. But is it likely that a journal such as \u003Cem\u003EReligion\u003C\u002Fem\u003E that covers a vast spectrum of phenomena (which are all, of course, related to religion) only comprises a few thematic clusters? Probably not. Therefore, let us continue by skipping the silhouette score plots (which are most likely of no value with such a huge number of clusters) and just train a \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance with n=100 clusters and assess the results.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Ekmeans = KMeans(n_clusters=100, random_state=42)\\nabstracts_labels = kmeans.fit_predict(df_abstracts_pca)\\ndf_abstracts_labeled = df_abstracts.copy()\\ndf_abstracts_labeled[&quot;cluster&quot;] = abstracts_labels\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWe will next evaluate the results by printing out some article titles of randomly chosen clusters. For instance, when analyzing the titles in cluster 75, we can perceive that all articles in this cluster are related to Theravāda Buddhism, Karma, and their perception in &quot;the West&quot;:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edf_abstracts_labeled[df_abstracts_labeled[&quot;cluster&quot;] == 75][[&quot;title&quot;, &quot;cluster&quot;]]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"right\\\"\u003E\u003C\u002Fth\u003E\\n\u003Cth align=\\\"left\\\"\u003Etitle\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Ecluster\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E210\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EChecking the heavenly ‘bank account of karma’: cognitive metaphors for karma in Western perception and early Theravāda Buddhism\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E75\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E211\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EKarma accounts: supplementary thoughts on Theravāda, Madhyamaka, theosophy, and Protestant Buddhism\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E75\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E258\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EResonant paradigms in the study of religions and the emergence of Theravāda Buddhism\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E75\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003ECluster 15 includes articles related to the body and its destruction:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edf_abstracts_labeled[df_abstracts_labeled[&quot;cluster&quot;] == 15][[&quot;title&quot;, &quot;cluster&quot;]]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"right\\\"\u003E\u003C\u002Fth\u003E\\n\u003Cth align=\\\"left\\\"\u003Etitle\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Ecluster\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E361\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003ECandanbālā&#39;s hair: Fasting, beauty, and the materialization of Jain wives\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E15\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E425\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EMonkey kings make havoc: Iconoclasm and murder in the Chinese cultural revolution\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E15\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E623\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003ETechniques of body and desire in Kashmir Śaivism\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E15\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E695\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EBody-symbols and social reality: Resurrection, incarnation and asceticism in early Christianity\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E15\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003ETo be fair, other clusters are harder to interpret. A good example is cluster 84. Yet, even in the case of cluster 84 there still seems to be a pattern, namely that almost all articles are related to famous scholars and works in the study of religion, such as Durkheim, Tylor, Otto, Said, etc.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edf_abstracts_labeled[df_abstracts_labeled[&quot;cluster&quot;] == 84][[&quot;title&quot;, &quot;cluster&quot;]]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"right\\\"\u003E\u003C\u002Fth\u003E\\n\u003Cth align=\\\"left\\\"\u003Etitle\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Ecluster\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E80\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003ELatin America 1520–1600: a page in the history of the study of religion\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E141\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EOn elves and freethinkers: criticism of religion and the emergence of the literary fantastic in Nordic literature\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E262\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EIs Durkheim&#39;s understanding of religion compatible with believing?\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E302\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EDreaming and god concepts\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E426\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EOrientalism, representation and religion: The reality behind the myth\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E448\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EThe Science of Religions in a Fascist State: Rudolf Otto and Jakob Wilhelm Hauer During the Third Reich\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E458\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EReligion Within the Limits of History: Schleiermacher and Religion—A Reappraisal\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E570\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003ECognitive and Ideological Aspects of Divine Anthropomorphism\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E571\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003ETylor&#39;s Anthropomorphic Theory of Religion\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E614\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003E‘All my relatives’: Persons in Oglala religion\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E650\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EColloquium: Does autonomy entail theology? Autonomy, legitimacy, and the study of religion\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E84\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003EAs we can see, even a simple implementation of \u003Cem\u003Ek\u003C\u002Fem\u003E-means on textual data without much feature-tuning has resulted in a \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance that is, despite its shortcomings, able to assist us by doing the work of a basic \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002F57JH-G9EZ\\\"\u003Erecommender system\u003C\u002Fa\u003E. For example, we could use our trained \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance to suggest articles to visitors of our website based on their previous readings. Of course, we can also use our model during our exploratory data analysis to show us thematic clusters discussed in \u003Cem\u003EReligion\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EYet, as the textual data in this example is rather difficult to cluster and includes noise points or clusters that contain very few articles, it might make better sense to apply a different clustering algorithm and see how it performs.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"5-applying-dbscan-clustering-on-textual-data\\\"\u003E5. Applying DBSCAN Clustering on Textual Data\u003C\u002Fh2\u003E\\n\u003Cp\u003EEven though the \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering of our data already resulted in some valuable insights, it might still be interesting to apply a different clustering algorithm such as DBSCAN. As explained above, DBSCAN excludes noise points and outliers in our data, meaning that it focuses on those regions in our data that may rightfully be called dense.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe will be using the d=10 reduced version of our \u003Ccode\u003ERELIGION_abstracts.csv\u003C\u002Fcode\u003E dataset, which allows us to use Euclidean distance as a metric. If we were to use the initial TF-IDF matrix with 250 features, we would need to consider changing the underlying metric to \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FHVZ5-9MXU\\\"\u003Ecosine distance\u003C\u002Fa\u003E, which is more suitable when dealing with sparse matrices, as in the case of textual data. \u003C\u002Fp\u003E\\n\u003Cp\u003EThe first step will be to use our \u003Ccode\u003EfindOptimalEps()\u003C\u002Fcode\u003E function to figure out which eps value is most suitable for our data.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003EfindOptimalEps(2, df_abstracts_tfidf)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAs can be seen in figure 11, the eps-plotting suggests choosing an eps value between 0.2 and 0.25.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig11.png&quot; caption=&quot;Figure 11: Eps plot for the abstracts dataset.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EWe are selecting 0.2 as eps value and train a DBSCAN instance.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edbscan = DBSCAN(eps=0.2, metric=&quot;euclidean&quot;)\\ndbscan_labels = dbscan.fit_predict(df_abstracts_pca)\\ndf_abstracts_dbscan = df_abstracts.copy()\\ndf_abstracts_dbscan[&quot;cluster&quot;] = dbscan_labels\\ndf_abstracts_dbscan[&quot;cluster&quot;].unique()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAs we can see when looking at the DBSCAN results in our Jupyter notebook, using a DBSCAN instance under these circumstances results in only four clusters and a vast noise points cluster (-1) with more than 150 entries and an even bigger cluster with more than 500 entries (cluster 0). These clusters are plotted in figure 12 (using a PCA-reduced dataset), where the inconclusive results become even more visible. In this case, we could consider using the original TF-IDF matrix with cosine distance instead.\u003C\u002Fp\u003E\\n\u003Cp\u003EIts shortcomings aside, the current version of our DBSCAN instance does give some promising insights, for example with cluster 3, which collects articles related to gender and women in different religions:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edf_abstracts_dbscan[df_abstracts_dbscan[&quot;cluster&quot;] == 1][[&quot;title&quot;, &quot;cluster&quot;]]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"right\\\"\u003E\u003C\u002Fth\u003E\\n\u003Cth align=\\\"left\\\"\u003Etitle\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Ecluster\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E154\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003ELifelong minority religion: routines and reflexivity: A Bourdieuan perspective on the habitus of elderly Finnish Orthodox Christian women\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E161\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EQuiet beauty: problems of agency and appearance in evangelical Christianity\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E388\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003ERenunciation feminised? Joint renunciation of female–male pairs in Bengali Vaishnavism\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E398\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EConclusion: Construction sites at the juncture of religion and gender\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E502\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EGender and the Contest over the Indian Past\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E506\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EArt as Neglected ‘Text’ for the Study of Gender and Religion in Africa\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E507\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EA Medieval Feminist Critique of the Chinese World Order: The Case of Wu Zhao (r. 690–705)\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E509\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003ENotions of Destiny in Women&#39;s Self-Construction\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E526\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EThe Fundamental Unity of the Conservative and Revolutionary Tendencies in Venezuelan Evangelicalism: The Case of Conjugal Relations\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E551\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EHindu Women, Destiny and Stridharma\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E644\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EThe women around James Nayler, Quaker: A matter of emphasis\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E668\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EWomen as aspects of the mother Goddess in India: A case study of Ramakrishna\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E1\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003ECluster 2, on the other hand, seems to be related to belief and atheism:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Edf_abstracts_dbscan[df_abstracts_dbscan[&quot;cluster&quot;] == 2][[&quot;title&quot;, &quot;cluster&quot;]]\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Ctable\u003E\\n\u003Cthead\u003E\\n\u003Ctr\u003E\\n\u003Cth align=\\\"right\\\"\u003E\u003C\u002Fth\u003E\\n\u003Cth align=\\\"left\\\"\u003Etitle\u003C\u002Fth\u003E\\n\u003Cth align=\\\"right\\\"\u003Ecluster\u003C\u002Fth\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Fthead\u003E\\n\u003Ctbody\u003E\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E209\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EThree cognitive routes to atheism: a dual-process account\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E282\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003ETHE CULTURAL TRANSMISSION OF FAITH Why innate intuitions are necessary, but insufficient, to explain religious belief\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E321\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EReligion is natural, atheism is not: On why everybody is both right and wrong\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E322\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAtheism is only skin deep: Geertz and Markusson rely mistakenly on sociodemographic data as meaningful indicators of underlying cognition\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E323\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EThe relative unnaturalness of atheism: On why Geertz and Markússon are both right and wrong\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E378\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EThe science of religious beliefs\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003Ctr\u003E\\n\u003Ctd align=\\\"right\\\"\u003E380\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"left\\\"\u003EAdaptation, evolution, and religion\u003C\u002Ftd\u003E\\n\u003Ctd align=\\\"right\\\"\u003E2\u003C\u002Ftd\u003E\\n\u003C\u002Ftr\u003E\\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;clustering-with-sklearn-in-python-fig12.png&quot; caption=&quot;Figure 12: PCA-reduced version of the abstracts dataset displaying the DBSCAN clustering with eps=0.2.&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EAlthough the clustering was far from perfect in this case, it did produce some valuable information, which we could use in combination with the more promising results of the \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering. It might also be pertinent to keep tuning the parameters and trying out different feature sets (reduced, non-reduced, maybe by adding some additional feature selection steps of choosing promising word fields, etc.) to achieve better results with DBSCAN. Of course, we could also apply some other clustering algorithms and then combine the results.\u003C\u002Fp\u003E\\n\u003Cp\u003EAs a next step, we could pursue the idea of building a basic recommender system which suggests articles with similar topics to readers based on their previous readings. This recommender system should consider the clustering of the \u003Cem\u003Ek\u003C\u002Fem\u003E-means instance but also include suggestions made by DBSCAN and other potential clustering algorithms. When applied in combination, the rather unsatisfactory results of the DBSCAN model might be less problematic because they are now used as additional information only.\u003C\u002Fp\u003E\\n\u003Cp\u003EOf course, we as scholars in the humanities will be more likely to use these techniques as part of our research during the exploratory data analysis phase. In this case, combining the results of different clustering algorithms helps us to discover structures and thematic clusters in our data. These discoveries could then lead to new research questions. For instance, there might be specific clusters in the \u003Cem\u003EReligion\u003C\u002Fem\u003E abstracts data that include more articles than the other clusters, thereby indicating an overall thematic focus of this journal that might be worth examining to get an overview of research trends in the study of religion throughout recent decades.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"summary\\\"\u003ESummary\u003C\u002Fh1\u003E\\n\u003Cp\u003EI hope to have shown that clustering is indeed a valuable step during exploratory data analysis that enables you to gain new insights into your data.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe clustering of the \u003Ccode\u003EDNP_ancient_authors.csv\u003C\u002Fcode\u003E and the \u003Ccode\u003ERELIGION_abstracts.csv\u003C\u002Fcode\u003E datasets provided decent results and identified reasonable groupings of authors and articles in the data. In the case of the abstracts dataset, we have even built a basic recommender system that assists us when searching for articles with similar topics. Yet, the discussion of the results also illustrated that there is always room for interpretation and that not every cluster necessarily needs to provide useful insights from a scholarly (or human) perspective. Despite this general ambiguity when applying machine learning algorithms, our analysis demonstrated that \u003Cem\u003Ek\u003C\u002Fem\u003E-means and DBSCAN are great tools that can help you to develop or empirically support new research questions. In addition, they may also be implemented for more practical tasks, for instance, when searching for articles related to a specific topic.\u003C\u002Fp\u003E\\n\u003Ch1 id=\\\"bibliography\\\"\u003EBibliography\u003C\u002Fh1\u003E\\n\u003Cul\u003E\\n\u003Cli\u003EGéron, Aurélien. \u003Cem\u003EHands-on machine learning with Scikit-Learn, Keras, and TensorFlow. Concepts, tools, and techniques to build intelligent systems, 2nd ed\u003C\u002Fem\u003E. Sebastopol: O’Reilly, 2019.\u003C\u002Fli\u003E\\n\u003Cli\u003EMitchell, Ryan. \u003Cem\u003EWeb scraping with Python. Collecting more data from the modern web, 1st ed\u003C\u002Fem\u003E. Sebastopol: O’Reilly, 2018.  \u003C\u002Fli\u003E\\n\u003Cli\u003EPatel, Ankur A. \u003Cem\u003EHands-on unsupervised learning using Python: How to build applied machine learning solutions from unlabeled data, 1st ed\u003C\u002Fem\u003E. Sebastopol: O’Reilly, 2019.\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Ch1 id=\\\"footnotes\\\"\u003EFootnotes\u003C\u002Fh1\u003E\\n\u003Cp\u003E[^1]: For a good introduction to the use of \u003Cem\u003Erequests\u003C\u002Fem\u003E and web scraping in general, see the corresponding articles on \u003Cem\u003EThe Programming Historian\u003C\u002Fem\u003E such as \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FJ5BV-MZPZ\\\"\u003EIntroduction to BeautifulSoup\u003C\u002Fa\u003E (last accessed: 2021-04-22) or books such as Mitchell (2018).\u003C\u002Fp\u003E\\n\u003Cp\u003E[^2]: Yet, there are certain cases where \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering might fail to identify the clusters in your data. Thus, it is usually recommended to apply several clustering algorithms. A good illustration of the restrictions of \u003Cem\u003Ek\u003C\u002Fem\u003E-means clustering can be seen in the examples under \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FMH6W-A6UP\\\"\u003Ethis link\u003C\u002Fa\u003E (last accessed: 2021-04-23) to the scikit-learn website, particularly in the second plot on the first row.\u003C\u002Fp\u003E\\n\u003Cp\u003E[^3]: \u003Ca href=\\\"https:\u002F\u002Fperma.cc\u002FDZT5-VPLV\\\"\u003EDefinition of inertia on scikit-learn\u003C\u002Fa\u003E (last accessed: 2021-04-23).\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
