<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="/favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		

		

		<link rel="stylesheet" href="/_app/assets/start-61d1577b.css">
		<link rel="modulepreload" href="/_app/start-c09d08cd.js">
		<link rel="modulepreload" href="/_app/chunks/vendor-9b9d6288.js">
		<link rel="modulepreload" href="/_app/pages/__layout.svelte-de46afb2.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/__layout.svelte-e75718c6.js">
		<link rel="modulepreload" href="/_app/chunks/stores-191d1505.js">
		<link rel="modulepreload" href="/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js">

		<script type="module">
			import { start } from "/_app/start-c09d08cd.js";
			start({
				target: document.querySelector("#svelte"),
				paths: {"base":"","assets":""},
				session: {},
				route: true,
				spa: false,
				trailing_slash: "never",
				hydrate: {
					status: 200,
					error: null,
					nodes: [
						import("/_app/pages/__layout.svelte-de46afb2.js"),
						import("/_app/pages/_lang_/__layout.svelte-e75718c6.js"),
						import("/_app/pages/_lang_/_lessons_/_slug_/index.svelte-efe77d76.js")
					],
					url: new URL("sveltekit://prerender/en/lessons/extracting-keywords"),
					params: {lang:"en",lessons:"lessons",slug:"extracting-keywords"}
				}
			});
		</script><script>
			if ('serviceWorker' in navigator) {
				navigator.serviceWorker.register('/service-worker.js');
			}
		</script>
	</head>
	<body>
		<div id="svelte">


The Programming Historian <a href="/en">English</a> <a href="/es">Spanish</a>
<br>
This is the en edition.

<h1>Using Gazetteers to Extract Sets of Keywords from Free-Flowing Texts</h1>

<!-- HTML_TAG_START --><p>{% include toc.html %}</p>
<h2 id="lesson-goals">Lesson Goals</h2>
<p>If you have a copy of a text in electronic format stored on your computer, it is relatively easy to keyword search for a single term. Often you can do this by using the built-in search features in your favourite text editor. However, scholars are increasingly needing to find instances of many terms within a text or texts. For example, a scholar may want to use a <a href="http://en.wikipedia.org/wiki/Gazetteer">gazetteer</a> to extract all mentions of English placenames within a collection of texts so that those places can later be plotted on a map. Alternatively, they may want to extract all male given names, all pronouns, <a href="http://en.wikipedia.org/wiki/Stop_words">stop words</a>, or any other set of words. Using those same built-in search features to achieve this more complex goal is time consuming and clunky. This lesson will teach you how to use Python to extract a set of keywords very quickly and systematically from a set of texts.</p>
<p>It is expected that once you have completed this lesson, you will be able to generalise the skills to extract custom sets of keywords from any set of locally saved files.</p>
<h2 id="for-whom-is-this-useful">For Whom is this Useful?</h2>
<p>This lesson is useful for anyone who works with historical sources that are stored locally on their own computer, and that are transcribed into mutable electronic text (eg, .txt, .xml, .rtf, .md). It is particularly useful for people interested in identifying subsets of documents containing one or more of a fairly large number of keywords. This might be useful for identifying a relevant subset for closer reading, or for extracting and structuring the keywords in a format that can be used in another tool: as input for a mapping exercise, for example.</p>
<p>The present tutorial will show users how to extract all mentions of English and Welsh county names from a series of 6,692 mini-biographies of individuals who began their studies at the University of Oxford during the reign of James I of England (1603-1625). These records were transcribed by <a href="http://www.british-history.ac.uk/alumni-oxon/1500-1714">British History Online</a>, from the printed version of <em>Alumni Oxonienses, 1500-1714</em>. These biographies contain information about each graduate, which includes the date of their studies and the college(s) they attended. Often entries contain additional information when known, including date or birth and death, the name or occupation of their father, where they originated, and what they went on to do in later life. The biographies are a rich resource, providing reasonably comparable data about a large number of similar individuals (rich men who went to Oxford). The 6,692 entries have been pre-processed by the author and saved to a <a href="http://en.wikipedia.org/wiki/Comma-separated_values">CSV file</a> with one entry per row.</p>
<p>In this tutorial, the dataset involves geographical keywords. Once extracted, these placenames could be geo-referenced to their place on the globe and then mapped using digital mapping. This might make it possible to discern which colleges attracted students from what parts of the country, or to determine if these patterns changed over time. For a practical tutorial on taking this next step, see the lesson by Fred Gibbs mentioned at the end of this lesson. Readers may also be interested in <a href="/lessons/georeferencing-qgis">georeferencing in QGIS 2.0</a>, also available from the <em>Programming Historian</em>.</p>
<p>This approach is not limited to geographical keywords, however. It could also be used to extract given names, prepositions, food words, or any other set of terms defined by the user. This process could therefore be useful for someone seeking to isolate individual entries containing any of these keywords, or for someone looking to calculate the frequency of their keywords within a corpus of texts. This tutorial provides pathways into textual or geospatial analyses, rather than research answers in its own right.</p>
<p>Data management skills are increasingly crucial for historians and textual scholars, and anyone working with particularly messy or difficult texts might consider looking into <a href="/lessons/cleaning-data-with-openrefine">Cleaning Data with OpenRefine</a> by Seth van Hooland et al. The approach outlined in this tutorial is not optimised for working with poorly transcribed texts such as text converted through <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">Optical Character Recognition</a> if the software has a high error rate. Readers working with early modern texts with non-standardised spelling may also find this approach challenging, as the gazetteer one uses must contain exact matches of the words sought. However, with a good enough gazetteer, this approach could prove quite useful for early modernites, and may exceed what&#39;s practical with traditional keyword searching by making <a href="https://en.wikipedia.org/wiki/Approximate_string_matching">fuzzy searching</a> possible.</p>
<p>This tutorial assumes that you have already installed Python version 3 on your computer. The lesson will use the Command Line to run Python, as this is more flexible and makes it possible for use in classrooms or computer labs in which students do not have the ability to download and install interactive development environments (IDEs) like Komodo Edit. Readers who would prefer to use an IDE might like to first read  <a href="/lessons/introduction-and-installation">Python Introduction and Installation</a>, but this is optional. The tutorial also makes some basic assumptions about your Python skills. It assumes you know what the following Python data structures are (though not knowing will not prevent the code from working should you follow all of the steps in the tutorial):</p>
<ul>
<li><a href="https://docs.python.org/3/tutorial/datastructures.html">List</a></li>
<li><a href="https://docs.python.org/3/tutorial/controlflow.html">For Loop</a></li>
<li><a href="https://docs.python.org/3/library/string.html">String</a></li>
</ul>
<p>The lesson touches on Regular Expressions, so some readers may find it handy to have the relevant Programming Historian lessons by <a href="/lessons/understanding-regular-expressions">Doug Knox</a> or <a href="/lessons/cleaning-ocrd-text-with-regular-expressions">Laura Turner O&#39;Hara</a> open to consult as needed.</p>
<h2 id="familiarising-yourself-with-the-data">Familiarising yourself with the data</h2>
<p>The first step of this process is to take a look at the data that we will be using in the lesson. As mentioned, the data includes biographical details of approximately 6,692 graduates who began study at the University of Oxford in the early seventeenth century.</p>
<p><a href="/assets/The_Dataset_-_Alumni_Oxonienses-Jas1.csv">The_Dataset_-_Alumni_Oxonienses-Jas1.csv</a> (1.4MB)</p>
<p>{% include figure.html filename=&quot;extracting-keywords-1.png&quot; caption=&quot;Screenshot of the first forty entries in the dataset&quot; %}</p>
<p>Download the dataset and spend a couple of minutes looking at the types of information available. You should notice three columns of information. The first, &#39;Name&#39;, contains the name of the graduate. The second: &#39;Details&#39;, contains the biographical information known about that person. The final column, &#39;Matriculation Year&#39;, contains the year in which the person matriculated (began their studies). This final column was extracted from the details column in the pre-processing stage of this tutorial. The first two columns are as you would find them on the British History Online version of the <em>Alumni Oxonienses</em>. If you notice more than three columns then your spreadsheet programme has incorrectly set the <a href="https://en.wikipedia.org/wiki/Delimiter">delimiter</a> between columns. It should be set to &quot;,&quot; (double quotes, comma). How you do this depends on your spreadsheet programme, but you should be able to find the solution online.</p>
<p>Most (but not all) of these bibliographic entries contain enough information to tell us what county the graduate came from. Notice that a large number of entries contain placenames that correspond to either major cities (&#39;of London&#39;, in the first entry) or English counties (&#39;of Middlesex&#39; in entry 5 or &#39;of Wilts&#39; - short for Wiltshire in entry 6). If you are not British you may not be familiar with these county names. You can find a list of <a href="http://en.wikipedia.org/wiki/Historic_counties_of_England">historic counties of England</a> on Wikipedia.</p>
<p>Unfortunately, the information is not always available in the same format. Sometimes it&#39;s the first thing mentioned in an entry. Sometimes it&#39;s in the middle. Our challenge is to extract those counties of origin from within this messy text, and store it in a new column next to that person&#39;s entry.</p>
<h2 id="build-your-gazetteer">Build your gazetteer</h2>
<p>In order to extract the relevant place names, we first have to decide what they are. We need a list of places, often called a gazetteer. Many of the place names mentioned in the records are shortforms, such as &#39;Wilts&#39; instead of &#39;Wiltshire&#39;, or &#39;Salop&#39; instead of &#39;Shropshire&#39;. Getting all of these variations may be tricky. For a start, let&#39;s build a basic gazetteer of English counties.</p>
<p>Make a new directory (folder) on your computer where you will save all of your work. Create a text file called <code>gazetteer.txt</code> and using the entries listed on the Wikipedia page listed above, add each county to a new line on the text file. It should look something like this:</p>
<pre><code class="language-text">Bedfordshire
Berkshire
Buckinghamshire
Cambridgeshire
Cheshire
Cornwall
Cumberland
Derbyshire
Devon
Dorset
Durham
Essex
Gloucestershire
Hampshire
Herefordshire
Hertfordshire
Huntingdonshire
Kent
Lancashire
Leicestershire
Lincolnshire
Middlesex
Norfolk
Northamptonshire
Northumberland
Nottinghamshire
Oxfordshire
Rutland
Shropshire
Somerset
Staffordshire
Suffolk
Surrey
Sussex
Warwickshire
Westmorland
Wiltshire
Worcestershire
Yorkshire
</code></pre>
<p>Make sure that there are no blank lines in the gazetteer file. If there are, your program will think all spaces are a matching keyword. Some text editing programs (particularly in Linux) will want to add a blank line at the end of your file. If this is the case, try another text editor. It&#39;s best to use software that puts you in control. For more on this problem, see <a href="http://stackoverflow.com/questions/3056740/gedit-adds-line-at-end-of-file">the explanation on Stack Overflow</a> - with thanks to John Levin for the link.</p>
<p>If you ever need to add to this set of keywords, you can open this file in your text editor and add new words, each on their own line. Komodo Edit is a good text editor for this task, especially if you have set it up to run with Python, but you can also use any plain text editor as long as it is <em>not</em> a <a href="http://en.wikipedia.org/wiki/Word_processor">word processor</a> such as Microsoft Word or Open Office. Word processors are inappropriate for writing code because of how they stylise apostrophes and quotes, causing havoc for your code.</p>
<h2 id="loading-your-texts">Loading your texts</h2>
<p>The next step is to put the texts that you want to search into another text file, with one entry per line. The easiest way to do that is to open the spreadsheet and select all of the second (details) column, then paste the contents into a .txt file. Call the file <code>texts.txt</code> and save it to the same directory as your <code>gazetteer.txt</code> file. Your directory should look something like this:</p>
<p>{% include figure.html filename=&quot;extracting-keywords-2.png&quot; caption=&quot;Contents of your working directory&quot; %}</p>
<p>The reason we do this is to keep the original data (the .CSV file) away from the Python program we are about to write, on the off-chance that we accidentally change something without noticing. In my opinion, this approach also makes for easier to read code, which is important when learning. It is not strictly necessary to use a .txt file for this step, as Python does have ways of opening and reading CSV files. At the end of this lesson we will look at how to use the CSV reading and writing features in Python, but this is an optional advanced step.</p>
<h2 id="write-your-python-program">Write your Python program</h2>
<p>The last step is to write a program that will check each of the texts for each of the keywords in the gazetteer, and then to provide an output that will tell us which entries in our spreadsheet contain which of those words. There are lots of ways that this could be achieved. When planning to write a program, it is always a good idea to devise an algorithm. An algorithm is a set of human-readable steps that will solve the problem. It&#39;s a list of what you are going to do, which you then convert into the appropriate programmatic instructions. The approach we will take here uses the following algorithm:</p>
<ol>
<li>Load the list of keywords that you&#39;ve created in <code>gazetteer.txt</code> and save them each to a Python list</li>
<li>Load the texts from <code>texts.txt</code> and save each one to another Python list</li>
<li>Then for each biographical entry, remove the unwanted punctuation (periods, commas, etc)</li>
<li>Then check for the presence of one or more of the keywords from your list. If it finds a match, store it while it checks for other matches. If it finds no match, move on to the next entry</li>
<li>Finally, output the results in a format that can be easily transferred back to the CSV file.</li>
</ol>
<h3 id="step-1-load-the-keywords">Step 1: Load the Keywords</h3>
<p>Using your text editor, create a new empty file, and add the following lines:</p>
<pre><code class="language-python">
#Import the keywords
f = open(&#39;gazetteer.txt&#39;, &#39;r&#39;)
allKeywords = f.read().lower().split(&quot;\n&quot;)
f.close()

print(allKeywords)
print(len(allKeywords))
</code></pre>
<p>The first line is a comment for our own benefit, to tells us (the human) what the code does. All Python lines beginning with a # are a comment. When the code runs it will ignore the comments. They are for human use only. A well commented piece of code is easier to return to later because you will have the means of decyphering your earlier creation.</p>
<p>The second line opens the <code>gazetteer.txt</code> file, and reads it, which is signified by the &#39;r&#39; (as opposed to &#39;w&#39; for write, or &#39;a&#39; for append). That means we will not be changing the contents of the file. Only reading it.</p>
<p>The third line reads everything in that file, converts it to <code>lower()</code> case, and splits the contents into a Python list, using the <a href="http://stackoverflow.com/questions/11497376/new-line-python">newline character</a> as the delimiter. Effectively that means each time the program comes across a new line, it stores it as a new entry. We then save that Python list containing the 39 counties into a variable that we have called <code>allKeywords</code>.</p>
<p>The fourth line closes the open text file. The fifth line prints out the results, and the sixth line tells us how many results were found.</p>
<p>Save this file as <code>extractKeywords.py</code>, again to the same folder as the other files, and then run it with Python. To do this from the command line, first you need to launch your command line terminal.</p>
<p>On Windows it is called <code>Command Prompt</code>. Windows users may find it easier to launch Python by opening the folder containing your <code>extractKeywords.py</code> file, then press <code>shift</code> + <code>right-click</code> and then select &#39;open command window here&#39;. Assuming you have Python installed, you should be able to run your programme using the command beginning with &#39;python&#39; below.</p>
<p>On Mac OS X, this is found in the <code>Applications</code> folder and is called <code>Terminal</code>. Once the Terminal window is open, you need to point your Terminal at the directory that contains all of the files you have just created. I have called my directory &#39;ExtractingKeywordSets&#39; and I have it on my computer&#39;s Desktop. To change the Terminal to this directory, I use the following command:</p>
<pre><code class="language-bash">cd Desktop/ExtractingKeywordSets
</code></pre>
<p>You would need to change the above to reflect the name you gave your directory, and where you put it on your machine. Note that Windows uses &#39;&#39; instead of &#39;/&#39; in file paths. If you get stuck, rename your directory to <code>ExtractingKeywordSets</code> and place it on the Desktop so that you can follow along.</p>
<p>You can now run the program you&#39;ve written with the following command:</p>
<pre><code class="language-bash">python extractKeywords.py
</code></pre>
<p>Once you have run the program you should see your gazetteer printed as a Python list in the command output, along with the number of entries in your list (39). If you can, great! Move on to step 2. If the last line of your output tells you that there was 1 result, that means the code has not worked properly, since we know that there should be 39 keywords in your gazetteer. Double check your code to make sure you havn&#39;t included any typos. If you still can&#39;t solve the problem, try changing &quot;\n&quot; to &quot;\r&quot; on line three. Some text editors will use <a href="http://en.wikipedia.org/wiki/Carriage_return">carriage returns</a> instead of &#39;newline characters&#39; when creating a new line. The \r means &#39;carriage return&#39; and should solve your problem if you&#39;re experiencing one.</p>
<h3 id="step-2-load-the-texts">Step 2: Load the texts</h3>
<p>The second step is very similar to the first. Except this time we will load the <code>texts.txt</code> in addition to the <code>gazetteer.txt</code> file</p>
<p>Add the following lines to the end of your code:</p>
<pre><code class="language-python">#Import the texts you want to search
f = open(&#39;texts.txt&#39;, &#39;r&#39;)
allTexts = f.read().lower().split(&quot;\n&quot;)
f.close()

print(allTexts)
</code></pre>
<p>If you got step 1 to work, you should understand this bit as well. Before you run the code, make sure that you have saved your program or you may accidentally run the OLD version and will be confused with the result. Once you&#39;ve done that, rerun the code. As a shortcut, instead of writing out the command again in the Terminal, you can press the up arrow, which should display the last command you entered. If you keep pressing the up or down arrows, you can scroll through previous commands, saving yourself the time needed to retype them. Once you&#39;ve found the command for running the program, press the return key to run the code.</p>
<p>If the code worked, you should see a big wall of text. Those are the texts we input into the program. As long as you see them, you&#39;re ok. Before moving on to the next step, delete the three lines from your code beginning with &#39;print&#39;. Now that we know they are printing the contents of these files properly we do not need to continue to check. Move on to step 3.</p>
<h3 id="step-3-remove-unwanted-punctuation">Step 3: Remove unwanted punctuation</h3>
<p>When matching strings, you have to make sure the punctuation doesn&#39;t get in the way. Technically, &#39;London.&#39; is a different string than &#39;London&#39; or &#39;;London&#39; because of the added punctuation. These three strings which all mean the same thing to us as human readers will be viewed by the computer as distinct entities. To solve that problem, the easiest thing to do is just to remove all of the punctuation. You can do this with <a href="http://en.wikipedia.org/wiki/Regular_expression">regular expressions</a>, and <a href="/lessons/understanding-regular-expressions">Doug Knox</a> and <a href="/lessons/cleaning-ocrd-text-with-regular-expressions">Laura Turner O&#39;Hara</a> have provided great introductions at <em>Programming Historian</em> for doing so.</p>
<p>To keep things simple, this program will just replace the most common types of punctuation with nothing instead (effectively deleting punctuation).</p>
<p>Add the following lines to your program:</p>
<pre><code class="language-python">for entry in allTexts:
    #for each entry:
    allWords = entry.split(&#39; &#39;)
    for words in allWords:

        #remove punctuation that will interfere with matching
        words = words.replace(&#39;,&#39;, &#39;&#39;)
        words = words.replace(&#39;.&#39;, &#39;&#39;)
        words = words.replace(&#39;;&#39;, &#39;&#39;)
</code></pre>
<p>The &#39;allTexts&#39; list variable contains all of our texts. Using a for loop, we will look at each entry in turn.</p>
<p>Since we are interested in single words, we will split the text into single words by using the .split() method, looking explicitly for spaces: <code>entry.split(&#39; &#39;)</code>. Note that there is a single space between those quotation marks. Since words are generally separated by spaces, this should work fairly well. This means we now have a Python list called &#39;allWords&#39; that contains each word in a single bibliographic entry.</p>
<p>We use another for loop to look through each word in that list, and wherever we find a comma, period, or semi-colon, we replace it with nothing, effectively deleting it. Note that there is no space between those quotation marks in the last three lines.</p>
<p>We now have a clean set of words that we can compare against our gazetteer entries, looking for matches.</p>
<h3 id="step-4-look-for-matching-keywords">Step 4: Look for matching keywords</h3>
<p>As the words from our text are already in a list called &#39;allWords&#39;, and all of our keywords are in a list called &#39;allKeywords&#39;, all we have to do now is check our texts for the keywords.</p>
<p>First, we need somewhere to store details of any matches we have. Immediately after the &#39;for entry in allTexts:&#39; line, at one level of indentation, add the following two lines of code:</p>
<pre><code class="language-python">    matches = 0
    storedMatches = []
</code></pre>
<p>Indentation is important in Python. The above two lines should be indented one tab deeper than the for loop above it. That means the code is to run every time the for loop runs - it is part of the loop. If your text editor does not allow tabs, you can use spaces instead.</p>
<p>The &#39;storedMatches&#39; variable is a blank list, where we can store our matching keywords. The &#39;matches&#39; variable is known as a &#39;flag&#39;, which we will use in the next step when we start printing the output.</p>
<p>To do the actual matching, add the following lines of code to the bottom of your program, again minding the indentation (2 levels from the left margin), making sure you save:</p>
<pre><code class="language-python">        #if a keyword match is found, store the result.
        if words in allKeywords:
            if words in storedMatches:
                continue
            else:
                storedMatches.append(words)
            matches += 1
    print(matches)
</code></pre>
<p>If you are worried that you have your indentation wrong, scroll ahead towards the bottom of the lesson and check the finished code.</p>
<p>Take a look at your whole program. These lines follow immediately after the last section in which you removed the punctuation. So each time a word had its punctuation removed (if it had punctuation to remove in the first place) it was immediately checked to see if it was in the list of keywords in your gazetteer file. If it was a match, we check that we do not already have this word recorded in our &#39;storedMatches&#39; variable. If we do, we skip ahead to the next word. If it is not already recorded, we append it to the &#39;storedMatches&#39; list. This is keeping track of the matching words for us for each text. When we find a match, we also increase our &#39;matches&#39; flag by 1. This lets us know how many matches we have found for that entry.</p>
<p>This code will automatically check each word in a text, keeping track of matches in the &#39;storedMatches&#39; list. When it gets to the end of a text, it will empty out the &#39;storedMatches&#39; variable and start again. Printing out the &#39;matches&#39; variable just lets us see how many matches we got for each text. When you run this code you should see somewhere between 0 and 2 for most entries. If it says 0 for everything then check your code again. If you only have one entry outputting then go back to step one and make sure your program is identifying the right number of keywords (39).</p>
<p>{% include figure.html filename=&quot;extracting-keywords-3.png&quot; caption=&quot;Correct output of the code to this point&quot; %}</p>
<p>If it looks like it worked, delete the &#39;print matches&#39; line and move to the next step.</p>
<h3 id="step-5-output-results">Step 5: Output results</h3>
<p>If you have got to this stage, then your Python program is already finding the matching keywords from your gazetteer. All we need to do now is print them out to the command output pane in a format that&#39;s easy to work with.</p>
<p>Add the following lines to your program, minding the indentation as always:</p>
<pre><code class="language-python">    #if there is a stored result, print it out
    if matches == 0:
        print(&#39; &#39;)
    else:
        matchString = &#39;&#39;
        for matches in storedMatches:
            matchString = matchString + matches + &quot;\t&quot;

        print(matchString)
</code></pre>
<p>This code checks if the number of matches is equal to 0. If so, then we havn&#39;t found any keywords and we don&#39;t need to print them out. However, we are going to print a blank space, because we want our output to contain the same number of lines as did our input (we want 1 line of output per line of text that we searched). This will make it easier to paste the output directly into our CSV file and have all of the entries line up properly with their corresponding text.</p>
<p>If there IS a match, then the program creates a new variable called &#39;matchString&#39; (it could have been called just about anything. That&#39;s just the name I chose because it&#39;s a string of matches). Then for each of the matching keywords that were kept in &#39;storedMatches&#39;, it appends the keyword to &#39;matchString&#39;, along with a tab (&quot;\t&quot;) character. The tab character is useful for CSV files because when you paste it into a spreadsheet, content separated by a tab will automatically go into an adjacent cell. This means that if a single text has more than one match, we&#39;ll be able to automatically paste one match per cell. This makes it easier to keep the keywords separate once we have them back in our CSV file.</p>
<p>When all of the matching keywords have been added to &#39;matchString&#39;, the program prints it out to the command output before moving on to the next text.</p>
<p>If you save your work and run the program, you should now have code that achieves all of the steps from the algorithm and outputs the results to your command output.</p>
<p>The finished code should look like this:</p>
<h2 id="finished-code">Finished Code</h2>
<pre><code class="language-python">#Import the keywords
f = open(&#39;gazetteer.txt&#39;, &#39;r&#39;)
allKeywords = f.read().lower().split(&quot;\n&quot;)
f.close()

#Import the texts you want to search
f = open(&#39;texts.txt&#39;, &#39;r&#39;)
allTexts = f.read().lower().split(&quot;\n&quot;)
f.close()

#Our programme:
for entry in allTexts:
    matches = 0
    storedMatches = []

    #for each entry:
    allWords = entry.split(&#39; &#39;)
    for words in allWords:

        #remove punctuation that will interfere with matching
        words = words.replace(&#39;,&#39;, &#39;&#39;)
        words = words.replace(&#39;.&#39;, &#39;&#39;)
        words = words.replace(&#39;;&#39;, &#39;&#39;)


        #if a keyword match is found, store the result.
        if words in allKeywords:
            if words in storedMatches:
                continue
            else:
                storedMatches.append(words)
            matches += 1

    #if there is a stored result, print it out
    if matches == 0:
        print(&#39; &#39;)
    else:
        matchString = &#39;&#39;
        for matches in storedMatches:
            matchString = matchString + matches + &quot;\t&quot;

        print(matchString)
</code></pre>
<p>If you do not like the output format, you can change it by adjusting the second last line of code. For example, you could save each entry to a new line in a .txt file rather than to the screen. To do that you would replace &#39;print matchString&#39; with the following code (make sure it is at one level of indentation, just as was the replaced line):</p>
<pre><code class="language-python">    f = open(&#39;output.txt&#39;, &#39;a&#39;)
    f.write(matchString)
    f.close()
</code></pre>
<p>Note the &#39;a&#39; instead of the &#39;r&#39; we used earlier. This &#39;appends&#39; the text to the file called <code>output.txt</code>, which will be saved in your working directory. You will have to take care, because running the program several times will continue to append all of the outputs to this file, creating a very long file. There are ways around this, which we will cover in a moment, and you might consider looking into how the &#39;w&#39; (write) feature works, and experimenting with output formats. There is more information related to these features in <a href="/lessons/working-with-text-files">&#39;Working with Text Files in Python&#39;</a>.</p>
<h2 id="refining-the-gazetteer">Refining the Gazetteer</h2>
<p>You can copy and paste that output directly into your spreadsheet next to the first entry. Check that the matches lined up properly. Your last entry of your spreadsheet should correspond to the correctly extracted keywords. In this case, the last entry should be blank, but the second last one should read &#39;dorset&#39;.</p>
<p>{% include figure.html filename=&quot;extracting-keywords-4.png&quot; caption=&quot;The output pasted back into the CSV file&quot; %}</p>
<p>At this point, you might like to refine the gazetteer, as a lot of place names have been missed. Many of them are shortforms, or archaic spellings (Wilts, Salop, Sarum, Northants, etc). You could go through looking at all the empty cells and seeing if you can find keywords that you&#39;ve missed. It may help to know that you can find the next empty cell in a column in Excel by pressing CTRL + down arrow (CMD + down arrow on Mac).</p>
<p>One of the easiest ways to find all of the missing entries is to sort your spreadsheet by the new columns you&#39;ve just added. If you sort the matches alphabetically for each of the new columns, then the entries at the bottom of the spreadsheet will all be unclassified. You can do this by selecting the whole spreadsheet and clicking on the Data -&gt; Sort menu item. You can then sort a-z for each of the new columns.</p>
<p>Before you sort a spreadsheet, it&#39;s often a good idea to add an &#39;original order&#39; column in case you want to sort them back. To do this, add a new column, and in the first 3 rows, type 1, 2, and 3 respectively. Then highlight the three cells and put your cursor over the bottom right corner. If you are using Microsoft Excel your cursor will change into a black cross. When you see this, click and hold the mouse button and drag the cursor down until you reach the bottom of the spreadsheet (down to the last entry) before you let go. This should automatically number the rows consecutively so that you can always re-sort your entries back to the original order.</p>
<p>{% include figure.html filename=&quot;extracting-keywords-5.png&quot; caption=&quot;Adding an original order column and sorting the entries&quot; %}</p>
<p>Now you can sort the data and read some of the entries for which no match was found. If you find there is a place name in there, add it to your &#39;gazetteer.txt&#39; file, one entry per line. You don&#39;t have to be exhaustive at this stage. You could add a handful more entries and then try the code again to see what impact they had on the result.</p>
<p>{% include figure.html filename=&quot;extracting-keywords-6.png&quot; caption=&quot;Missed place name words highlighted in yellow&quot; %}</p>
<p>Before you re-run your Python code, you&#39;ll have to update your <code>texts.txt</code> file so that the program runs on the texts in the correct order. Since the code will output the matches in the same order that it receives the files in <code>texts.txt</code>, it&#39;s important not to get this jumbled up if you&#39;ve been sorting your spreadsheet where you intend to store your outputs. You can either re-sort the spreadsheet back to the original order before you run the program, or you can copy all of the cells in the &#39;details&#39; column again and paste and save them into the <code>texts.txt</code> file.</p>
<p>I&#39;d challenge you to make a few refinements to your gazetteer before moving ahead, just to make sure you have the hang of it.</p>
<p>Once you are happy with that, you can snag my <a href="/assets/extracting-keywords-final-gazetteer.txt">completed list of English and Welsh counties, shortforms, and various other cities (London, Bristol etc) and places (Jersey, Ireland, etc)</a>. My completed list contains 157 entries, and should get you all of the entries that can be extracted from the texts in this collection.</p>
<p>At this point you could stop, as you&#39;ve achieved what you set out to do. This lesson taught you how to use a short Python program to search a fairly large number of texts for a set of keywords defined by you.</p>
<p>With the outputs from this lesson, you could fairly quickly map these entries by geolocating the place names. This might reveal new insights into spatial patterns of Oxford alumni.</p>
<p>Having the ability to search for large numbers of keywords at the same time opens up flexibility for your research process, and makes it feasible to do work that might otherwise just have seemed like it would take too long. You could try a completely different set of words, or use this technique on another set of texts entirely. The research questions are of course, endless.</p>
<p>If you would like to refine the program further, we can use Python to read directly from the CSV file and to print the results to a new CSV file so that everything happens automatically from the Terminal window without the need for cutting and pasting.</p>
<h2 id="printing-the-results-back-to-the-csv-file-using-python">Printing the Results Back to the CSV File Using Python</h2>
<p>Python has a built in code library that can handle working with CSV files, called <code>csv</code></p>
<p>To use it and its features, you first have to import it. At the top of your <code>extractKeywords.py</code> program, add the following line:</p>
<pre><code class="language-python">    import csv
</code></pre>
<p>Now we are going to make some changes to our original program. Instead of cutting all of the texts into a <code>texts.txt</code> file, we&#39;ll use Python to read the data directly into our &#39;allTexts&#39; variable. Replace:</p>
<pre><code class="language-python">#Import the texts you want to search
f = open(&#39;texts.txt&#39;, &#39;r&#39;)
allTexts = f.read().lower().split(&quot;\n&quot;)
f.close()
</code></pre>
<p>With this:</p>
<pre><code class="language-python">
#Import the &#39;Details&#39; column from the CSV file
allTexts = []
fullRow = []
with open(&#39;The_Dataset_-_Alumni_Oxonienses-Jas1.csv&#39;) as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        #the full row for each entry, which will be used to recreate the improved CSV file in a moment
        fullRow.append((row[&#39;Name&#39;], row[&#39;Details&#39;], row[&#39;Matriculation Year&#39;]))

        #the column we want to parse for our keywords
        row = row[&#39;Details&#39;].lower()
        allTexts.append(row)
</code></pre>
<p>As this is an advanced option, I won&#39;t explain what every line does in detail, but you can take a look at the comments in the code to get an idea. Effectively this uses Python to read the CSV file and stores all of the information in the &#39;Details&#39; column in the same &#39;allTexts&#39; variable that we had it in previously, in exactly the same format as before. This code also stores each row of the CSV file into another list called &#39;fullRow&#39;, which will be used for writing a new CSV file containing our program&#39;s outputs.</p>
<p>There are a few extra lines of code here, but you didn&#39;t need to cut and paste anything into the <code>texts.txt</code> file, and there&#39;s no risk here of your sorting of your spreadsheet causing any issues about the order of inputs and outputs. This is therefore a more robust option. You can print out either of these variables using the &#39;print&#39; feature, to make sure they contain what you&#39;d expect of them.</p>
<hr>
<p><strong>TROUBLESHOOTING</strong>: If you get the following error when you attempt to read your CSV file using Python, the CSV file may have been saved on a Mac, and the Python CSV library is only able to read Windows-compatible CSV files</p>
<pre><code class="language-text">(Error: new-line character seen in unquoted field - do you need to open the file in universal-newline mode?).
</code></pre>
<p>To solve this problem, open your CSV file in a spreadsheet program (eg., Excel) and &#39;Save As&#39; and under format chose &#39;Windows Comma Separated (csv)&#39;. This should solve the problem. To read more on this issue, see <a href="http://stackoverflow.com/questions/17315635/csv-new-line-character-seen-in-unquoted-field-error">Stack Overflow</a></p>
<hr>
<h2 id="creating-a-new-csv-file">Creating a new CSV file</h2>
<p>Next we need to create a new CSV file where the results of the analysis can be stored. It&#39;s always a good idea to make a new file rather than try to append it to your only copy of the original data. It&#39;s also a good idea to append the current date and time to the filename for your new file. That way you can run the code lots of times as you refine everything and it will always be clear which file contains your most recent ouputs.</p>
<p>To do this, import the &#39;time&#39; library just below where you imported the &#39;csv&#39; library.</p>
<pre><code class="language-python">
import time
</code></pre>
<p>And then add the following two lines of code right below where you were just working with the new CSV code:</p>
<pre><code class="language-python">
#use the current date and time to create a unique output filename
timestr = time.strftime(&quot;%Y-%m-%d-(%H-%M-%S)&quot;)
filename = &#39;output-&#39; + str(timestr) + &#39;.csv&#39;
</code></pre>
<p>This will create a variable called &#39;filename&#39;, which we&#39;ll use when we make the new output file.</p>
<p>The rest of the process involves creating that new output file, putting in the correct headers, pasting in the original data, and then pasting in our new outputs from our gazetteer matching. That involves quite a few tweaks to the original code, so to keep everything as clear as possible, I&#39;ve included the finished code below. I have appended &#39;NEW!&#39;, &#39;OLD!&#39; and &#39;CHANGED!&#39; in the comments for each section so that you can see at a glance which bits have changed:</p>
<pre><code class="language-python">
#NEW!
import csv
import time

#OLD! Import the keywords
f = open(&#39;gazetteer.txt&#39;, &#39;r&#39;)
allKeywords = f.read().lower().split(&quot;\n&quot;)
f.close()


#CHANGED! Import the &#39;Details&#39; column from the CSV file
allTexts = []
fullRow = []
with open(&#39;The_Dataset_-_Alumni_Oxonienses-Jas1.csv&#39;) as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        #the full row for each entry, which will be used to recreate the improved CSV file in a moment
        fullRow.append((row[&#39;Name&#39;], row[&#39;Details&#39;], row[&#39;Matriculation Year&#39;]))

        #the column we want to parse for our keywords
        row = row[&#39;Details&#39;].lower()
        allTexts.append(row)

#NEW! a flag used to keep track of which row is being printed to the CSV file
counter = 0

#NEW! use the current date and time to create a unique output filename
timestr = time.strftime(&quot;%Y-%m-%d-(%H-%M-%S)&quot;)
filename = &#39;output-&#39; + str(timestr) + &#39;.csv&#39;

#NEW! Open the new output CSV file to append (&#39;a&#39;) rows one at a time.
with open(filename, &#39;a&#39;) as csvfile:

    #NEW! define the column headers and write them to the new file
    fieldnames = [&#39;Name&#39;, &#39;Details&#39;, &#39;Matriculation Year&#39;, &#39;Placename&#39;]
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()

    #NEW! define the output for each row and then print to the output csv file
    writer = csv.writer(csvfile)

    #OLD! this is the same as before, for currentRow in fullRow:
    for entry in allTexts:

        matches = 0
        storedMatches = []

        #for each entry:
        allWords = entry.split(&#39; &#39;)
        for words in allWords:

            #remove punctuation that will interfere with matching
            words = words.replace(&#39;,&#39;, &#39;&#39;)
            words = words.replace(&#39;.&#39;, &#39;&#39;)
            words = words.replace(&#39;;&#39;, &#39;&#39;)

            #if a keyword match is found, store the result.
            if words in allKeywords:
                if words in storedMatches:
                    continue
                else:
                    storedMatches.append(words)
                matches += 1

        #CHANGED! send any matches to a new row of the csv file.
        if matches == 0:
            newRow = fullRow[counter]
        else:
            matchTuple = tuple(storedMatches)
            newRow = fullRow[counter] + matchTuple

        #NEW! write the result of each row to the csv file
        writer.writerows([newRow])
        counter += 1
</code></pre>
<p>The code is heavily commented so if you spend some time, you should be able to figure it out. Save this code and rerun it using Python and you should get a file called <code>output.csv</code> appearing in your working directory, which if you open it should contain all of the same information as you had before, but without the need to do any cutting or pasting.</p>
<p>To give a brief outline of what has been changed from the original version:</p>
<ol>
<li>The texts were extracted automatically from the original datafile instead of having to paste them into a <code>texts.txt</code> file.</li>
<li>Using the &#39;time&#39; library, we used the current date and time to create a unique and easily decypherable filename for our output file.</li>
<li>Using the &#39;csv&#39; library we created a new .csv file using that filename, and put in the column headers we wanted to use.</li>
<li>We then ran the same matching code as before, checking &#39;allTexts&#39; against &#39;allWords&#39; and storing the results.</li>
<li>Instead of printing the results to the screen, we stored each row&#39;s original data (Name, Details, Matriculation Year) + the matches to a <a href="https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences">tuple</a> called &#39;newRow&#39;.</li>
<li>Using the &#39;csv&#39; library we wrote the &#39;newRow&#39; data to the new CSV file, one row at a time.</li>
</ol>
<p>This approach created longer and more complex code, but the result is a powerful program that reads from a CSV file, matches the texts against the contents of a gazetteer, and then automatically writes the output to a clean new CSV file with no intermediary steps for you the user. You didn&#39;t have to go that extra mile, but hopefully you can see the advantages if you made it all the way through.</p>
<h2 id="suggested-further-reading">Suggested Further Reading</h2>
<p>Readers who have completed this lesson might be interested in then geo-referencing the output using the Google API and mapping the results. You can learn more about this process from Fred Gibbs&#39;s tutorial, <a href="http://fredgibbs.net/tutorials/extract-geocode-placenames-from-text-file.html">Extract and Geocode Placenames from a Text File</a>. This will let you visualise the practical outputs of this tutorial. Alternatively, readers may be interested in <a href="/lessons/georeferencing-qgis">Jim Clifford et. al&#39;s tutorial on georeferencing in QGIS 2.0</a>, an open source <a href="https://en.wikipedia.org/wiki/Geographic_information_system">GIS</a> program.</p>
<!-- HTML_TAG_END -->

<script type="application/json" data-type="svelte-data" data-url="extracting-keywords/raw.json">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"metadata\":{\"title\":\"Using Gazetteers to Extract Sets of Keywords from Free-Flowing Texts\",\"layout\":\"lesson\",\"date\":\"2015-12-01T00:00:00.000Z\",\"authors\":[\"Adam Crymble\"],\"reviewers\":[\"Guy McClellan\",\"Amanda Morton\",\"Frederik Elwert\"],\"editors\":[\"Fred Gibbs\"],\"difficulty\":2,\"exclude_from_check\":[\"review-ticket\"],\"activity\":\"acquiring\",\"topics\":[\"data-manipulation\"],\"abstract\":\"This lesson will teach you how to use Python to extract a set of keywords very quickly and systematically from a set of texts.\",\"python_warning\":false,\"redirect_from\":\"\u002Flessons\u002Fextracting-keywords\",\"avatar_alt\":\"Woman churning butter or milk\",\"doi\":\"10.46430\u002Fphen0045\"},\"html_body\":\"\u003Cp\u003E{% include toc.html %}\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"lesson-goals\\\"\u003ELesson Goals\u003C\u002Fh2\u003E\\n\u003Cp\u003EIf you have a copy of a text in electronic format stored on your computer, it is relatively easy to keyword search for a single term. Often you can do this by using the built-in search features in your favourite text editor. However, scholars are increasingly needing to find instances of many terms within a text or texts. For example, a scholar may want to use a \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGazetteer\\\"\u003Egazetteer\u003C\u002Fa\u003E to extract all mentions of English placenames within a collection of texts so that those places can later be plotted on a map. Alternatively, they may want to extract all male given names, all pronouns, \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FStop_words\\\"\u003Estop words\u003C\u002Fa\u003E, or any other set of words. Using those same built-in search features to achieve this more complex goal is time consuming and clunky. This lesson will teach you how to use Python to extract a set of keywords very quickly and systematically from a set of texts.\u003C\u002Fp\u003E\\n\u003Cp\u003EIt is expected that once you have completed this lesson, you will be able to generalise the skills to extract custom sets of keywords from any set of locally saved files.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"for-whom-is-this-useful\\\"\u003EFor Whom is this Useful?\u003C\u002Fh2\u003E\\n\u003Cp\u003EThis lesson is useful for anyone who works with historical sources that are stored locally on their own computer, and that are transcribed into mutable electronic text (eg, .txt, .xml, .rtf, .md). It is particularly useful for people interested in identifying subsets of documents containing one or more of a fairly large number of keywords. This might be useful for identifying a relevant subset for closer reading, or for extracting and structuring the keywords in a format that can be used in another tool: as input for a mapping exercise, for example.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe present tutorial will show users how to extract all mentions of English and Welsh county names from a series of 6,692 mini-biographies of individuals who began their studies at the University of Oxford during the reign of James I of England (1603-1625). These records were transcribed by \u003Ca href=\\\"http:\u002F\u002Fwww.british-history.ac.uk\u002Falumni-oxon\u002F1500-1714\\\"\u003EBritish History Online\u003C\u002Fa\u003E, from the printed version of \u003Cem\u003EAlumni Oxonienses, 1500-1714\u003C\u002Fem\u003E. These biographies contain information about each graduate, which includes the date of their studies and the college(s) they attended. Often entries contain additional information when known, including date or birth and death, the name or occupation of their father, where they originated, and what they went on to do in later life. The biographies are a rich resource, providing reasonably comparable data about a large number of similar individuals (rich men who went to Oxford). The 6,692 entries have been pre-processed by the author and saved to a \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FComma-separated_values\\\"\u003ECSV file\u003C\u002Fa\u003E with one entry per row.\u003C\u002Fp\u003E\\n\u003Cp\u003EIn this tutorial, the dataset involves geographical keywords. Once extracted, these placenames could be geo-referenced to their place on the globe and then mapped using digital mapping. This might make it possible to discern which colleges attracted students from what parts of the country, or to determine if these patterns changed over time. For a practical tutorial on taking this next step, see the lesson by Fred Gibbs mentioned at the end of this lesson. Readers may also be interested in \u003Ca href=\\\"\u002Flessons\u002Fgeoreferencing-qgis\\\"\u003Egeoreferencing in QGIS 2.0\u003C\u002Fa\u003E, also available from the \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis approach is not limited to geographical keywords, however. It could also be used to extract given names, prepositions, food words, or any other set of terms defined by the user. This process could therefore be useful for someone seeking to isolate individual entries containing any of these keywords, or for someone looking to calculate the frequency of their keywords within a corpus of texts. This tutorial provides pathways into textual or geospatial analyses, rather than research answers in its own right.\u003C\u002Fp\u003E\\n\u003Cp\u003EData management skills are increasingly crucial for historians and textual scholars, and anyone working with particularly messy or difficult texts might consider looking into \u003Ca href=\\\"\u002Flessons\u002Fcleaning-data-with-openrefine\\\"\u003ECleaning Data with OpenRefine\u003C\u002Fa\u003E by Seth van Hooland et al. The approach outlined in this tutorial is not optimised for working with poorly transcribed texts such as text converted through \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FOptical_character_recognition\\\"\u003EOptical Character Recognition\u003C\u002Fa\u003E if the software has a high error rate. Readers working with early modern texts with non-standardised spelling may also find this approach challenging, as the gazetteer one uses must contain exact matches of the words sought. However, with a good enough gazetteer, this approach could prove quite useful for early modernites, and may exceed what&#39;s practical with traditional keyword searching by making \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FApproximate_string_matching\\\"\u003Efuzzy searching\u003C\u002Fa\u003E possible.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis tutorial assumes that you have already installed Python version 3 on your computer. The lesson will use the Command Line to run Python, as this is more flexible and makes it possible for use in classrooms or computer labs in which students do not have the ability to download and install interactive development environments (IDEs) like Komodo Edit. Readers who would prefer to use an IDE might like to first read  \u003Ca href=\\\"\u002Flessons\u002Fintroduction-and-installation\\\"\u003EPython Introduction and Installation\u003C\u002Fa\u003E, but this is optional. The tutorial also makes some basic assumptions about your Python skills. It assumes you know what the following Python data structures are (though not knowing will not prevent the code from working should you follow all of the steps in the tutorial):\u003C\u002Fp\u003E\\n\u003Cul\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F3\u002Ftutorial\u002Fdatastructures.html\\\"\u003EList\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F3\u002Ftutorial\u002Fcontrolflow.html\\\"\u003EFor Loop\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003Cli\u003E\u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F3\u002Flibrary\u002Fstring.html\\\"\u003EString\u003C\u002Fa\u003E\u003C\u002Fli\u003E\\n\u003C\u002Ful\u003E\\n\u003Cp\u003EThe lesson touches on Regular Expressions, so some readers may find it handy to have the relevant Programming Historian lessons by \u003Ca href=\\\"\u002Flessons\u002Funderstanding-regular-expressions\\\"\u003EDoug Knox\u003C\u002Fa\u003E or \u003Ca href=\\\"\u002Flessons\u002Fcleaning-ocrd-text-with-regular-expressions\\\"\u003ELaura Turner O&#39;Hara\u003C\u002Fa\u003E open to consult as needed.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"familiarising-yourself-with-the-data\\\"\u003EFamiliarising yourself with the data\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe first step of this process is to take a look at the data that we will be using in the lesson. As mentioned, the data includes biographical details of approximately 6,692 graduates who began study at the University of Oxford in the early seventeenth century.\u003C\u002Fp\u003E\\n\u003Cp\u003E\u003Ca href=\\\"\u002Fassets\u002FThe_Dataset_-_Alumni_Oxonienses-Jas1.csv\\\"\u003EThe_Dataset_-_Alumni_Oxonienses-Jas1.csv\u003C\u002Fa\u003E (1.4MB)\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;extracting-keywords-1.png&quot; caption=&quot;Screenshot of the first forty entries in the dataset&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EDownload the dataset and spend a couple of minutes looking at the types of information available. You should notice three columns of information. The first, &#39;Name&#39;, contains the name of the graduate. The second: &#39;Details&#39;, contains the biographical information known about that person. The final column, &#39;Matriculation Year&#39;, contains the year in which the person matriculated (began their studies). This final column was extracted from the details column in the pre-processing stage of this tutorial. The first two columns are as you would find them on the British History Online version of the \u003Cem\u003EAlumni Oxonienses\u003C\u002Fem\u003E. If you notice more than three columns then your spreadsheet programme has incorrectly set the \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDelimiter\\\"\u003Edelimiter\u003C\u002Fa\u003E between columns. It should be set to &quot;,&quot; (double quotes, comma). How you do this depends on your spreadsheet programme, but you should be able to find the solution online.\u003C\u002Fp\u003E\\n\u003Cp\u003EMost (but not all) of these bibliographic entries contain enough information to tell us what county the graduate came from. Notice that a large number of entries contain placenames that correspond to either major cities (&#39;of London&#39;, in the first entry) or English counties (&#39;of Middlesex&#39; in entry 5 or &#39;of Wilts&#39; - short for Wiltshire in entry 6). If you are not British you may not be familiar with these county names. You can find a list of \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FHistoric_counties_of_England\\\"\u003Ehistoric counties of England\u003C\u002Fa\u003E on Wikipedia.\u003C\u002Fp\u003E\\n\u003Cp\u003EUnfortunately, the information is not always available in the same format. Sometimes it&#39;s the first thing mentioned in an entry. Sometimes it&#39;s in the middle. Our challenge is to extract those counties of origin from within this messy text, and store it in a new column next to that person&#39;s entry.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"build-your-gazetteer\\\"\u003EBuild your gazetteer\u003C\u002Fh2\u003E\\n\u003Cp\u003EIn order to extract the relevant place names, we first have to decide what they are. We need a list of places, often called a gazetteer. Many of the place names mentioned in the records are shortforms, such as &#39;Wilts&#39; instead of &#39;Wiltshire&#39;, or &#39;Salop&#39; instead of &#39;Shropshire&#39;. Getting all of these variations may be tricky. For a start, let&#39;s build a basic gazetteer of English counties.\u003C\u002Fp\u003E\\n\u003Cp\u003EMake a new directory (folder) on your computer where you will save all of your work. Create a text file called \u003Ccode\u003Egazetteer.txt\u003C\u002Fcode\u003E and using the entries listed on the Wikipedia page listed above, add each county to a new line on the text file. It should look something like this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-text\\\"\u003EBedfordshire\\nBerkshire\\nBuckinghamshire\\nCambridgeshire\\nCheshire\\nCornwall\\nCumberland\\nDerbyshire\\nDevon\\nDorset\\nDurham\\nEssex\\nGloucestershire\\nHampshire\\nHerefordshire\\nHertfordshire\\nHuntingdonshire\\nKent\\nLancashire\\nLeicestershire\\nLincolnshire\\nMiddlesex\\nNorfolk\\nNorthamptonshire\\nNorthumberland\\nNottinghamshire\\nOxfordshire\\nRutland\\nShropshire\\nSomerset\\nStaffordshire\\nSuffolk\\nSurrey\\nSussex\\nWarwickshire\\nWestmorland\\nWiltshire\\nWorcestershire\\nYorkshire\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EMake sure that there are no blank lines in the gazetteer file. If there are, your program will think all spaces are a matching keyword. Some text editing programs (particularly in Linux) will want to add a blank line at the end of your file. If this is the case, try another text editor. It&#39;s best to use software that puts you in control. For more on this problem, see \u003Ca href=\\\"http:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F3056740\u002Fgedit-adds-line-at-end-of-file\\\"\u003Ethe explanation on Stack Overflow\u003C\u002Fa\u003E - with thanks to John Levin for the link.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you ever need to add to this set of keywords, you can open this file in your text editor and add new words, each on their own line. Komodo Edit is a good text editor for this task, especially if you have set it up to run with Python, but you can also use any plain text editor as long as it is \u003Cem\u003Enot\u003C\u002Fem\u003E a \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FWord_processor\\\"\u003Eword processor\u003C\u002Fa\u003E such as Microsoft Word or Open Office. Word processors are inappropriate for writing code because of how they stylise apostrophes and quotes, causing havoc for your code.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"loading-your-texts\\\"\u003ELoading your texts\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe next step is to put the texts that you want to search into another text file, with one entry per line. The easiest way to do that is to open the spreadsheet and select all of the second (details) column, then paste the contents into a .txt file. Call the file \u003Ccode\u003Etexts.txt\u003C\u002Fcode\u003E and save it to the same directory as your \u003Ccode\u003Egazetteer.txt\u003C\u002Fcode\u003E file. Your directory should look something like this:\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;extracting-keywords-2.png&quot; caption=&quot;Contents of your working directory&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EThe reason we do this is to keep the original data (the .CSV file) away from the Python program we are about to write, on the off-chance that we accidentally change something without noticing. In my opinion, this approach also makes for easier to read code, which is important when learning. It is not strictly necessary to use a .txt file for this step, as Python does have ways of opening and reading CSV files. At the end of this lesson we will look at how to use the CSV reading and writing features in Python, but this is an optional advanced step.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"write-your-python-program\\\"\u003EWrite your Python program\u003C\u002Fh2\u003E\\n\u003Cp\u003EThe last step is to write a program that will check each of the texts for each of the keywords in the gazetteer, and then to provide an output that will tell us which entries in our spreadsheet contain which of those words. There are lots of ways that this could be achieved. When planning to write a program, it is always a good idea to devise an algorithm. An algorithm is a set of human-readable steps that will solve the problem. It&#39;s a list of what you are going to do, which you then convert into the appropriate programmatic instructions. The approach we will take here uses the following algorithm:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003ELoad the list of keywords that you&#39;ve created in \u003Ccode\u003Egazetteer.txt\u003C\u002Fcode\u003E and save them each to a Python list\u003C\u002Fli\u003E\\n\u003Cli\u003ELoad the texts from \u003Ccode\u003Etexts.txt\u003C\u002Fcode\u003E and save each one to another Python list\u003C\u002Fli\u003E\\n\u003Cli\u003EThen for each biographical entry, remove the unwanted punctuation (periods, commas, etc)\u003C\u002Fli\u003E\\n\u003Cli\u003EThen check for the presence of one or more of the keywords from your list. If it finds a match, store it while it checks for other matches. If it finds no match, move on to the next entry\u003C\u002Fli\u003E\\n\u003Cli\u003EFinally, output the results in a format that can be easily transferred back to the CSV file.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Ch3 id=\\\"step-1-load-the-keywords\\\"\u003EStep 1: Load the Keywords\u003C\u002Fh3\u003E\\n\u003Cp\u003EUsing your text editor, create a new empty file, and add the following lines:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E\\n#Import the keywords\\nf = open(&#39;gazetteer.txt&#39;, &#39;r&#39;)\\nallKeywords = f.read().lower().split(&quot;\\\\n&quot;)\\nf.close()\\n\\nprint(allKeywords)\\nprint(len(allKeywords))\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe first line is a comment for our own benefit, to tells us (the human) what the code does. All Python lines beginning with a # are a comment. When the code runs it will ignore the comments. They are for human use only. A well commented piece of code is easier to return to later because you will have the means of decyphering your earlier creation.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe second line opens the \u003Ccode\u003Egazetteer.txt\u003C\u002Fcode\u003E file, and reads it, which is signified by the &#39;r&#39; (as opposed to &#39;w&#39; for write, or &#39;a&#39; for append). That means we will not be changing the contents of the file. Only reading it.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe third line reads everything in that file, converts it to \u003Ccode\u003Elower()\u003C\u002Fcode\u003E case, and splits the contents into a Python list, using the \u003Ca href=\\\"http:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F11497376\u002Fnew-line-python\\\"\u003Enewline character\u003C\u002Fa\u003E as the delimiter. Effectively that means each time the program comes across a new line, it stores it as a new entry. We then save that Python list containing the 39 counties into a variable that we have called \u003Ccode\u003EallKeywords\u003C\u002Fcode\u003E.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe fourth line closes the open text file. The fifth line prints out the results, and the sixth line tells us how many results were found.\u003C\u002Fp\u003E\\n\u003Cp\u003ESave this file as \u003Ccode\u003EextractKeywords.py\u003C\u002Fcode\u003E, again to the same folder as the other files, and then run it with Python. To do this from the command line, first you need to launch your command line terminal.\u003C\u002Fp\u003E\\n\u003Cp\u003EOn Windows it is called \u003Ccode\u003ECommand Prompt\u003C\u002Fcode\u003E. Windows users may find it easier to launch Python by opening the folder containing your \u003Ccode\u003EextractKeywords.py\u003C\u002Fcode\u003E file, then press \u003Ccode\u003Eshift\u003C\u002Fcode\u003E + \u003Ccode\u003Eright-click\u003C\u002Fcode\u003E and then select &#39;open command window here&#39;. Assuming you have Python installed, you should be able to run your programme using the command beginning with &#39;python&#39; below.\u003C\u002Fp\u003E\\n\u003Cp\u003EOn Mac OS X, this is found in the \u003Ccode\u003EApplications\u003C\u002Fcode\u003E folder and is called \u003Ccode\u003ETerminal\u003C\u002Fcode\u003E. Once the Terminal window is open, you need to point your Terminal at the directory that contains all of the files you have just created. I have called my directory &#39;ExtractingKeywordSets&#39; and I have it on my computer&#39;s Desktop. To change the Terminal to this directory, I use the following command:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Ecd Desktop\u002FExtractingKeywordSets\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EYou would need to change the above to reflect the name you gave your directory, and where you put it on your machine. Note that Windows uses &#39;&#39; instead of &#39;\u002F&#39; in file paths. If you get stuck, rename your directory to \u003Ccode\u003EExtractingKeywordSets\u003C\u002Fcode\u003E and place it on the Desktop so that you can follow along.\u003C\u002Fp\u003E\\n\u003Cp\u003EYou can now run the program you&#39;ve written with the following command:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-bash\\\"\u003Epython extractKeywords.py\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EOnce you have run the program you should see your gazetteer printed as a Python list in the command output, along with the number of entries in your list (39). If you can, great! Move on to step 2. If the last line of your output tells you that there was 1 result, that means the code has not worked properly, since we know that there should be 39 keywords in your gazetteer. Double check your code to make sure you havn&#39;t included any typos. If you still can&#39;t solve the problem, try changing &quot;\\\\n&quot; to &quot;\\\\r&quot; on line three. Some text editors will use \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCarriage_return\\\"\u003Ecarriage returns\u003C\u002Fa\u003E instead of &#39;newline characters&#39; when creating a new line. The \\\\r means &#39;carriage return&#39; and should solve your problem if you&#39;re experiencing one.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"step-2-load-the-texts\\\"\u003EStep 2: Load the texts\u003C\u002Fh3\u003E\\n\u003Cp\u003EThe second step is very similar to the first. Except this time we will load the \u003Ccode\u003Etexts.txt\u003C\u002Fcode\u003E in addition to the \u003Ccode\u003Egazetteer.txt\u003C\u002Fcode\u003E file\u003C\u002Fp\u003E\\n\u003Cp\u003EAdd the following lines to the end of your code:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E#Import the texts you want to search\\nf = open(&#39;texts.txt&#39;, &#39;r&#39;)\\nallTexts = f.read().lower().split(&quot;\\\\n&quot;)\\nf.close()\\n\\nprint(allTexts)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf you got step 1 to work, you should understand this bit as well. Before you run the code, make sure that you have saved your program or you may accidentally run the OLD version and will be confused with the result. Once you&#39;ve done that, rerun the code. As a shortcut, instead of writing out the command again in the Terminal, you can press the up arrow, which should display the last command you entered. If you keep pressing the up or down arrows, you can scroll through previous commands, saving yourself the time needed to retype them. Once you&#39;ve found the command for running the program, press the return key to run the code.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf the code worked, you should see a big wall of text. Those are the texts we input into the program. As long as you see them, you&#39;re ok. Before moving on to the next step, delete the three lines from your code beginning with &#39;print&#39;. Now that we know they are printing the contents of these files properly we do not need to continue to check. Move on to step 3.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"step-3-remove-unwanted-punctuation\\\"\u003EStep 3: Remove unwanted punctuation\u003C\u002Fh3\u003E\\n\u003Cp\u003EWhen matching strings, you have to make sure the punctuation doesn&#39;t get in the way. Technically, &#39;London.&#39; is a different string than &#39;London&#39; or &#39;;London&#39; because of the added punctuation. These three strings which all mean the same thing to us as human readers will be viewed by the computer as distinct entities. To solve that problem, the easiest thing to do is just to remove all of the punctuation. You can do this with \u003Ca href=\\\"http:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FRegular_expression\\\"\u003Eregular expressions\u003C\u002Fa\u003E, and \u003Ca href=\\\"\u002Flessons\u002Funderstanding-regular-expressions\\\"\u003EDoug Knox\u003C\u002Fa\u003E and \u003Ca href=\\\"\u002Flessons\u002Fcleaning-ocrd-text-with-regular-expressions\\\"\u003ELaura Turner O&#39;Hara\u003C\u002Fa\u003E have provided great introductions at \u003Cem\u003EProgramming Historian\u003C\u002Fem\u003E for doing so.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo keep things simple, this program will just replace the most common types of punctuation with nothing instead (effectively deleting punctuation).\u003C\u002Fp\u003E\\n\u003Cp\u003EAdd the following lines to your program:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003Efor entry in allTexts:\\n    #for each entry:\\n    allWords = entry.split(&#39; &#39;)\\n    for words in allWords:\\n\\n        #remove punctuation that will interfere with matching\\n        words = words.replace(&#39;,&#39;, &#39;&#39;)\\n        words = words.replace(&#39;.&#39;, &#39;&#39;)\\n        words = words.replace(&#39;;&#39;, &#39;&#39;)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe &#39;allTexts&#39; list variable contains all of our texts. Using a for loop, we will look at each entry in turn.\u003C\u002Fp\u003E\\n\u003Cp\u003ESince we are interested in single words, we will split the text into single words by using the .split() method, looking explicitly for spaces: \u003Ccode\u003Eentry.split(&#39; &#39;)\u003C\u002Fcode\u003E. Note that there is a single space between those quotation marks. Since words are generally separated by spaces, this should work fairly well. This means we now have a Python list called &#39;allWords&#39; that contains each word in a single bibliographic entry.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe use another for loop to look through each word in that list, and wherever we find a comma, period, or semi-colon, we replace it with nothing, effectively deleting it. Note that there is no space between those quotation marks in the last three lines.\u003C\u002Fp\u003E\\n\u003Cp\u003EWe now have a clean set of words that we can compare against our gazetteer entries, looking for matches.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"step-4-look-for-matching-keywords\\\"\u003EStep 4: Look for matching keywords\u003C\u002Fh3\u003E\\n\u003Cp\u003EAs the words from our text are already in a list called &#39;allWords&#39;, and all of our keywords are in a list called &#39;allKeywords&#39;, all we have to do now is check our texts for the keywords.\u003C\u002Fp\u003E\\n\u003Cp\u003EFirst, we need somewhere to store details of any matches we have. Immediately after the &#39;for entry in allTexts:&#39; line, at one level of indentation, add the following two lines of code:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E    matches = 0\\n    storedMatches = []\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIndentation is important in Python. The above two lines should be indented one tab deeper than the for loop above it. That means the code is to run every time the for loop runs - it is part of the loop. If your text editor does not allow tabs, you can use spaces instead.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe &#39;storedMatches&#39; variable is a blank list, where we can store our matching keywords. The &#39;matches&#39; variable is known as a &#39;flag&#39;, which we will use in the next step when we start printing the output.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo do the actual matching, add the following lines of code to the bottom of your program, again minding the indentation (2 levels from the left margin), making sure you save:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E        #if a keyword match is found, store the result.\\n        if words in allKeywords:\\n            if words in storedMatches:\\n                continue\\n            else:\\n                storedMatches.append(words)\\n            matches += 1\\n    print(matches)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf you are worried that you have your indentation wrong, scroll ahead towards the bottom of the lesson and check the finished code.\u003C\u002Fp\u003E\\n\u003Cp\u003ETake a look at your whole program. These lines follow immediately after the last section in which you removed the punctuation. So each time a word had its punctuation removed (if it had punctuation to remove in the first place) it was immediately checked to see if it was in the list of keywords in your gazetteer file. If it was a match, we check that we do not already have this word recorded in our &#39;storedMatches&#39; variable. If we do, we skip ahead to the next word. If it is not already recorded, we append it to the &#39;storedMatches&#39; list. This is keeping track of the matching words for us for each text. When we find a match, we also increase our &#39;matches&#39; flag by 1. This lets us know how many matches we have found for that entry.\u003C\u002Fp\u003E\\n\u003Cp\u003EThis code will automatically check each word in a text, keeping track of matches in the &#39;storedMatches&#39; list. When it gets to the end of a text, it will empty out the &#39;storedMatches&#39; variable and start again. Printing out the &#39;matches&#39; variable just lets us see how many matches we got for each text. When you run this code you should see somewhere between 0 and 2 for most entries. If it says 0 for everything then check your code again. If you only have one entry outputting then go back to step one and make sure your program is identifying the right number of keywords (39).\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;extracting-keywords-3.png&quot; caption=&quot;Correct output of the code to this point&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EIf it looks like it worked, delete the &#39;print matches&#39; line and move to the next step.\u003C\u002Fp\u003E\\n\u003Ch3 id=\\\"step-5-output-results\\\"\u003EStep 5: Output results\u003C\u002Fh3\u003E\\n\u003Cp\u003EIf you have got to this stage, then your Python program is already finding the matching keywords from your gazetteer. All we need to do now is print them out to the command output pane in a format that&#39;s easy to work with.\u003C\u002Fp\u003E\\n\u003Cp\u003EAdd the following lines to your program, minding the indentation as always:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E    #if there is a stored result, print it out\\n    if matches == 0:\\n        print(&#39; &#39;)\\n    else:\\n        matchString = &#39;&#39;\\n        for matches in storedMatches:\\n            matchString = matchString + matches + &quot;\\\\t&quot;\\n\\n        print(matchString)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis code checks if the number of matches is equal to 0. If so, then we havn&#39;t found any keywords and we don&#39;t need to print them out. However, we are going to print a blank space, because we want our output to contain the same number of lines as did our input (we want 1 line of output per line of text that we searched). This will make it easier to paste the output directly into our CSV file and have all of the entries line up properly with their corresponding text.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf there IS a match, then the program creates a new variable called &#39;matchString&#39; (it could have been called just about anything. That&#39;s just the name I chose because it&#39;s a string of matches). Then for each of the matching keywords that were kept in &#39;storedMatches&#39;, it appends the keyword to &#39;matchString&#39;, along with a tab (&quot;\\\\t&quot;) character. The tab character is useful for CSV files because when you paste it into a spreadsheet, content separated by a tab will automatically go into an adjacent cell. This means that if a single text has more than one match, we&#39;ll be able to automatically paste one match per cell. This makes it easier to keep the keywords separate once we have them back in our CSV file.\u003C\u002Fp\u003E\\n\u003Cp\u003EWhen all of the matching keywords have been added to &#39;matchString&#39;, the program prints it out to the command output before moving on to the next text.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you save your work and run the program, you should now have code that achieves all of the steps from the algorithm and outputs the results to your command output.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe finished code should look like this:\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"finished-code\\\"\u003EFinished Code\u003C\u002Fh2\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E#Import the keywords\\nf = open(&#39;gazetteer.txt&#39;, &#39;r&#39;)\\nallKeywords = f.read().lower().split(&quot;\\\\n&quot;)\\nf.close()\\n\\n#Import the texts you want to search\\nf = open(&#39;texts.txt&#39;, &#39;r&#39;)\\nallTexts = f.read().lower().split(&quot;\\\\n&quot;)\\nf.close()\\n\\n#Our programme:\\nfor entry in allTexts:\\n    matches = 0\\n    storedMatches = []\\n\\n    #for each entry:\\n    allWords = entry.split(&#39; &#39;)\\n    for words in allWords:\\n\\n        #remove punctuation that will interfere with matching\\n        words = words.replace(&#39;,&#39;, &#39;&#39;)\\n        words = words.replace(&#39;.&#39;, &#39;&#39;)\\n        words = words.replace(&#39;;&#39;, &#39;&#39;)\\n\\n\\n        #if a keyword match is found, store the result.\\n        if words in allKeywords:\\n            if words in storedMatches:\\n                continue\\n            else:\\n                storedMatches.append(words)\\n            matches += 1\\n\\n    #if there is a stored result, print it out\\n    if matches == 0:\\n        print(&#39; &#39;)\\n    else:\\n        matchString = &#39;&#39;\\n        for matches in storedMatches:\\n            matchString = matchString + matches + &quot;\\\\t&quot;\\n\\n        print(matchString)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EIf you do not like the output format, you can change it by adjusting the second last line of code. For example, you could save each entry to a new line in a .txt file rather than to the screen. To do that you would replace &#39;print matchString&#39; with the following code (make sure it is at one level of indentation, just as was the replaced line):\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E    f = open(&#39;output.txt&#39;, &#39;a&#39;)\\n    f.write(matchString)\\n    f.close()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENote the &#39;a&#39; instead of the &#39;r&#39; we used earlier. This &#39;appends&#39; the text to the file called \u003Ccode\u003Eoutput.txt\u003C\u002Fcode\u003E, which will be saved in your working directory. You will have to take care, because running the program several times will continue to append all of the outputs to this file, creating a very long file. There are ways around this, which we will cover in a moment, and you might consider looking into how the &#39;w&#39; (write) feature works, and experimenting with output formats. There is more information related to these features in \u003Ca href=\\\"\u002Flessons\u002Fworking-with-text-files\\\"\u003E&#39;Working with Text Files in Python&#39;\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"refining-the-gazetteer\\\"\u003ERefining the Gazetteer\u003C\u002Fh2\u003E\\n\u003Cp\u003EYou can copy and paste that output directly into your spreadsheet next to the first entry. Check that the matches lined up properly. Your last entry of your spreadsheet should correspond to the correctly extracted keywords. In this case, the last entry should be blank, but the second last one should read &#39;dorset&#39;.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;extracting-keywords-4.png&quot; caption=&quot;The output pasted back into the CSV file&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EAt this point, you might like to refine the gazetteer, as a lot of place names have been missed. Many of them are shortforms, or archaic spellings (Wilts, Salop, Sarum, Northants, etc). You could go through looking at all the empty cells and seeing if you can find keywords that you&#39;ve missed. It may help to know that you can find the next empty cell in a column in Excel by pressing CTRL + down arrow (CMD + down arrow on Mac).\u003C\u002Fp\u003E\\n\u003Cp\u003EOne of the easiest ways to find all of the missing entries is to sort your spreadsheet by the new columns you&#39;ve just added. If you sort the matches alphabetically for each of the new columns, then the entries at the bottom of the spreadsheet will all be unclassified. You can do this by selecting the whole spreadsheet and clicking on the Data -&gt; Sort menu item. You can then sort a-z for each of the new columns.\u003C\u002Fp\u003E\\n\u003Cp\u003EBefore you sort a spreadsheet, it&#39;s often a good idea to add an &#39;original order&#39; column in case you want to sort them back. To do this, add a new column, and in the first 3 rows, type 1, 2, and 3 respectively. Then highlight the three cells and put your cursor over the bottom right corner. If you are using Microsoft Excel your cursor will change into a black cross. When you see this, click and hold the mouse button and drag the cursor down until you reach the bottom of the spreadsheet (down to the last entry) before you let go. This should automatically number the rows consecutively so that you can always re-sort your entries back to the original order.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;extracting-keywords-5.png&quot; caption=&quot;Adding an original order column and sorting the entries&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003ENow you can sort the data and read some of the entries for which no match was found. If you find there is a place name in there, add it to your &#39;gazetteer.txt&#39; file, one entry per line. You don&#39;t have to be exhaustive at this stage. You could add a handful more entries and then try the code again to see what impact they had on the result.\u003C\u002Fp\u003E\\n\u003Cp\u003E{% include figure.html filename=&quot;extracting-keywords-6.png&quot; caption=&quot;Missed place name words highlighted in yellow&quot; %}\u003C\u002Fp\u003E\\n\u003Cp\u003EBefore you re-run your Python code, you&#39;ll have to update your \u003Ccode\u003Etexts.txt\u003C\u002Fcode\u003E file so that the program runs on the texts in the correct order. Since the code will output the matches in the same order that it receives the files in \u003Ccode\u003Etexts.txt\u003C\u002Fcode\u003E, it&#39;s important not to get this jumbled up if you&#39;ve been sorting your spreadsheet where you intend to store your outputs. You can either re-sort the spreadsheet back to the original order before you run the program, or you can copy all of the cells in the &#39;details&#39; column again and paste and save them into the \u003Ccode\u003Etexts.txt\u003C\u002Fcode\u003E file.\u003C\u002Fp\u003E\\n\u003Cp\u003EI&#39;d challenge you to make a few refinements to your gazetteer before moving ahead, just to make sure you have the hang of it.\u003C\u002Fp\u003E\\n\u003Cp\u003EOnce you are happy with that, you can snag my \u003Ca href=\\\"\u002Fassets\u002Fextracting-keywords-final-gazetteer.txt\\\"\u003Ecompleted list of English and Welsh counties, shortforms, and various other cities (London, Bristol etc) and places (Jersey, Ireland, etc)\u003C\u002Fa\u003E. My completed list contains 157 entries, and should get you all of the entries that can be extracted from the texts in this collection.\u003C\u002Fp\u003E\\n\u003Cp\u003EAt this point you could stop, as you&#39;ve achieved what you set out to do. This lesson taught you how to use a short Python program to search a fairly large number of texts for a set of keywords defined by you.\u003C\u002Fp\u003E\\n\u003Cp\u003EWith the outputs from this lesson, you could fairly quickly map these entries by geolocating the place names. This might reveal new insights into spatial patterns of Oxford alumni.\u003C\u002Fp\u003E\\n\u003Cp\u003EHaving the ability to search for large numbers of keywords at the same time opens up flexibility for your research process, and makes it feasible to do work that might otherwise just have seemed like it would take too long. You could try a completely different set of words, or use this technique on another set of texts entirely. The research questions are of course, endless.\u003C\u002Fp\u003E\\n\u003Cp\u003EIf you would like to refine the program further, we can use Python to read directly from the CSV file and to print the results to a new CSV file so that everything happens automatically from the Terminal window without the need for cutting and pasting.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"printing-the-results-back-to-the-csv-file-using-python\\\"\u003EPrinting the Results Back to the CSV File Using Python\u003C\u002Fh2\u003E\\n\u003Cp\u003EPython has a built in code library that can handle working with CSV files, called \u003Ccode\u003Ecsv\u003C\u002Fcode\u003E\u003C\u002Fp\u003E\\n\u003Cp\u003ETo use it and its features, you first have to import it. At the top of your \u003Ccode\u003EextractKeywords.py\u003C\u002Fcode\u003E program, add the following line:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E    import csv\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ENow we are going to make some changes to our original program. Instead of cutting all of the texts into a \u003Ccode\u003Etexts.txt\u003C\u002Fcode\u003E file, we&#39;ll use Python to read the data directly into our &#39;allTexts&#39; variable. Replace:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E#Import the texts you want to search\\nf = open(&#39;texts.txt&#39;, &#39;r&#39;)\\nallTexts = f.read().lower().split(&quot;\\\\n&quot;)\\nf.close()\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EWith this:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E\\n#Import the &#39;Details&#39; column from the CSV file\\nallTexts = []\\nfullRow = []\\nwith open(&#39;The_Dataset_-_Alumni_Oxonienses-Jas1.csv&#39;) as csvfile:\\n    reader = csv.DictReader(csvfile)\\n    for row in reader:\\n        #the full row for each entry, which will be used to recreate the improved CSV file in a moment\\n        fullRow.append((row[&#39;Name&#39;], row[&#39;Details&#39;], row[&#39;Matriculation Year&#39;]))\\n\\n        #the column we want to parse for our keywords\\n        row = row[&#39;Details&#39;].lower()\\n        allTexts.append(row)\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAs this is an advanced option, I won&#39;t explain what every line does in detail, but you can take a look at the comments in the code to get an idea. Effectively this uses Python to read the CSV file and stores all of the information in the &#39;Details&#39; column in the same &#39;allTexts&#39; variable that we had it in previously, in exactly the same format as before. This code also stores each row of the CSV file into another list called &#39;fullRow&#39;, which will be used for writing a new CSV file containing our program&#39;s outputs.\u003C\u002Fp\u003E\\n\u003Cp\u003EThere are a few extra lines of code here, but you didn&#39;t need to cut and paste anything into the \u003Ccode\u003Etexts.txt\u003C\u002Fcode\u003E file, and there&#39;s no risk here of your sorting of your spreadsheet causing any issues about the order of inputs and outputs. This is therefore a more robust option. You can print out either of these variables using the &#39;print&#39; feature, to make sure they contain what you&#39;d expect of them.\u003C\u002Fp\u003E\\n\u003Chr\u003E\\n\u003Cp\u003E\u003Cstrong\u003ETROUBLESHOOTING\u003C\u002Fstrong\u003E: If you get the following error when you attempt to read your CSV file using Python, the CSV file may have been saved on a Mac, and the Python CSV library is only able to read Windows-compatible CSV files\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-text\\\"\u003E(Error: new-line character seen in unquoted field - do you need to open the file in universal-newline mode?).\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003ETo solve this problem, open your CSV file in a spreadsheet program (eg., Excel) and &#39;Save As&#39; and under format chose &#39;Windows Comma Separated (csv)&#39;. This should solve the problem. To read more on this issue, see \u003Ca href=\\\"http:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F17315635\u002Fcsv-new-line-character-seen-in-unquoted-field-error\\\"\u003EStack Overflow\u003C\u002Fa\u003E\u003C\u002Fp\u003E\\n\u003Chr\u003E\\n\u003Ch2 id=\\\"creating-a-new-csv-file\\\"\u003ECreating a new CSV file\u003C\u002Fh2\u003E\\n\u003Cp\u003ENext we need to create a new CSV file where the results of the analysis can be stored. It&#39;s always a good idea to make a new file rather than try to append it to your only copy of the original data. It&#39;s also a good idea to append the current date and time to the filename for your new file. That way you can run the code lots of times as you refine everything and it will always be clear which file contains your most recent ouputs.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo do this, import the &#39;time&#39; library just below where you imported the &#39;csv&#39; library.\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E\\nimport time\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EAnd then add the following two lines of code right below where you were just working with the new CSV code:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E\\n#use the current date and time to create a unique output filename\\ntimestr = time.strftime(&quot;%Y-%m-%d-(%H-%M-%S)&quot;)\\nfilename = &#39;output-&#39; + str(timestr) + &#39;.csv&#39;\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThis will create a variable called &#39;filename&#39;, which we&#39;ll use when we make the new output file.\u003C\u002Fp\u003E\\n\u003Cp\u003EThe rest of the process involves creating that new output file, putting in the correct headers, pasting in the original data, and then pasting in our new outputs from our gazetteer matching. That involves quite a few tweaks to the original code, so to keep everything as clear as possible, I&#39;ve included the finished code below. I have appended &#39;NEW!&#39;, &#39;OLD!&#39; and &#39;CHANGED!&#39; in the comments for each section so that you can see at a glance which bits have changed:\u003C\u002Fp\u003E\\n\u003Cpre\u003E\u003Ccode class=\\\"language-python\\\"\u003E\\n#NEW!\\nimport csv\\nimport time\\n\\n#OLD! Import the keywords\\nf = open(&#39;gazetteer.txt&#39;, &#39;r&#39;)\\nallKeywords = f.read().lower().split(&quot;\\\\n&quot;)\\nf.close()\\n\\n\\n#CHANGED! Import the &#39;Details&#39; column from the CSV file\\nallTexts = []\\nfullRow = []\\nwith open(&#39;The_Dataset_-_Alumni_Oxonienses-Jas1.csv&#39;) as csvfile:\\n    reader = csv.DictReader(csvfile)\\n    for row in reader:\\n        #the full row for each entry, which will be used to recreate the improved CSV file in a moment\\n        fullRow.append((row[&#39;Name&#39;], row[&#39;Details&#39;], row[&#39;Matriculation Year&#39;]))\\n\\n        #the column we want to parse for our keywords\\n        row = row[&#39;Details&#39;].lower()\\n        allTexts.append(row)\\n\\n#NEW! a flag used to keep track of which row is being printed to the CSV file\\ncounter = 0\\n\\n#NEW! use the current date and time to create a unique output filename\\ntimestr = time.strftime(&quot;%Y-%m-%d-(%H-%M-%S)&quot;)\\nfilename = &#39;output-&#39; + str(timestr) + &#39;.csv&#39;\\n\\n#NEW! Open the new output CSV file to append (&#39;a&#39;) rows one at a time.\\nwith open(filename, &#39;a&#39;) as csvfile:\\n\\n    #NEW! define the column headers and write them to the new file\\n    fieldnames = [&#39;Name&#39;, &#39;Details&#39;, &#39;Matriculation Year&#39;, &#39;Placename&#39;]\\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\\n    writer.writeheader()\\n\\n    #NEW! define the output for each row and then print to the output csv file\\n    writer = csv.writer(csvfile)\\n\\n    #OLD! this is the same as before, for currentRow in fullRow:\\n    for entry in allTexts:\\n\\n        matches = 0\\n        storedMatches = []\\n\\n        #for each entry:\\n        allWords = entry.split(&#39; &#39;)\\n        for words in allWords:\\n\\n            #remove punctuation that will interfere with matching\\n            words = words.replace(&#39;,&#39;, &#39;&#39;)\\n            words = words.replace(&#39;.&#39;, &#39;&#39;)\\n            words = words.replace(&#39;;&#39;, &#39;&#39;)\\n\\n            #if a keyword match is found, store the result.\\n            if words in allKeywords:\\n                if words in storedMatches:\\n                    continue\\n                else:\\n                    storedMatches.append(words)\\n                matches += 1\\n\\n        #CHANGED! send any matches to a new row of the csv file.\\n        if matches == 0:\\n            newRow = fullRow[counter]\\n        else:\\n            matchTuple = tuple(storedMatches)\\n            newRow = fullRow[counter] + matchTuple\\n\\n        #NEW! write the result of each row to the csv file\\n        writer.writerows([newRow])\\n        counter += 1\\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\\n\u003Cp\u003EThe code is heavily commented so if you spend some time, you should be able to figure it out. Save this code and rerun it using Python and you should get a file called \u003Ccode\u003Eoutput.csv\u003C\u002Fcode\u003E appearing in your working directory, which if you open it should contain all of the same information as you had before, but without the need to do any cutting or pasting.\u003C\u002Fp\u003E\\n\u003Cp\u003ETo give a brief outline of what has been changed from the original version:\u003C\u002Fp\u003E\\n\u003Col\u003E\\n\u003Cli\u003EThe texts were extracted automatically from the original datafile instead of having to paste them into a \u003Ccode\u003Etexts.txt\u003C\u002Fcode\u003E file.\u003C\u002Fli\u003E\\n\u003Cli\u003EUsing the &#39;time&#39; library, we used the current date and time to create a unique and easily decypherable filename for our output file.\u003C\u002Fli\u003E\\n\u003Cli\u003EUsing the &#39;csv&#39; library we created a new .csv file using that filename, and put in the column headers we wanted to use.\u003C\u002Fli\u003E\\n\u003Cli\u003EWe then ran the same matching code as before, checking &#39;allTexts&#39; against &#39;allWords&#39; and storing the results.\u003C\u002Fli\u003E\\n\u003Cli\u003EInstead of printing the results to the screen, we stored each row&#39;s original data (Name, Details, Matriculation Year) + the matches to a \u003Ca href=\\\"https:\u002F\u002Fdocs.python.org\u002F3\u002Ftutorial\u002Fdatastructures.html#tuples-and-sequences\\\"\u003Etuple\u003C\u002Fa\u003E called &#39;newRow&#39;.\u003C\u002Fli\u003E\\n\u003Cli\u003EUsing the &#39;csv&#39; library we wrote the &#39;newRow&#39; data to the new CSV file, one row at a time.\u003C\u002Fli\u003E\\n\u003C\u002Fol\u003E\\n\u003Cp\u003EThis approach created longer and more complex code, but the result is a powerful program that reads from a CSV file, matches the texts against the contents of a gazetteer, and then automatically writes the output to a clean new CSV file with no intermediary steps for you the user. You didn&#39;t have to go that extra mile, but hopefully you can see the advantages if you made it all the way through.\u003C\u002Fp\u003E\\n\u003Ch2 id=\\\"suggested-further-reading\\\"\u003ESuggested Further Reading\u003C\u002Fh2\u003E\\n\u003Cp\u003EReaders who have completed this lesson might be interested in then geo-referencing the output using the Google API and mapping the results. You can learn more about this process from Fred Gibbs&#39;s tutorial, \u003Ca href=\\\"http:\u002F\u002Ffredgibbs.net\u002Ftutorials\u002Fextract-geocode-placenames-from-text-file.html\\\"\u003EExtract and Geocode Placenames from a Text File\u003C\u002Fa\u003E. This will let you visualise the practical outputs of this tutorial. Alternatively, readers may be interested in \u003Ca href=\\\"\u002Flessons\u002Fgeoreferencing-qgis\\\"\u003EJim Clifford et. al&#39;s tutorial on georeferencing in QGIS 2.0\u003C\u002Fa\u003E, an open source \u003Ca href=\\\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGeographic_information_system\\\"\u003EGIS\u003C\u002Fa\u003E program.\u003C\u002Fp\u003E\\n\"}"}</script></div>
	</body>
</html>
